<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-12-17 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-26
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-12-16/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-12-18/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-12-17">Arxiv Computer Vision Papers - 2025-12-17</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#mmgr-multi-modal-generative-reasoning" class="nav-link">MMGR: Multi-Modal Generative Reasoning</a>
                </li>
                <li class="nav-item">
                    <a href="#deep-learning-perspective-of-scene-understanding-in-autonomous-robots" class="nav-link">Deep Learning Perspective of Scene Understanding in Autonomous Robots</a>
                </li>
                <li class="nav-item">
                    <a href="#memflow-flowing-adaptive-memory-for-consistent-and-efficient-long-video-narratives" class="nav-link">MemFlow: Flowing Adaptive Memory for Consistent and Efficient Long Video Narratives</a>
                </li>
                <li class="nav-item">
                    <a href="#timelens-rethinking-video-temporal-grounding-with-multimodal-llms" class="nav-link">TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs</a>
                </li>
                <li class="nav-item">
                    <a href="#spherical-leech-quantization-for-visual-tokenization-and-generation" class="nav-link">Spherical Leech Quantization for Visual Tokenization and Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#crisp-contact-guided-real2sim-from-monocular-video-with-planar-scene-primitives" class="nav-link">CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives</a>
                </li>
                <li class="nav-item">
                    <a href="#native-and-compact-structured-latents-for-3d-generation" class="nav-link">Native and Compact Structured Latents for 3D Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#vasa-3d-lifelike-audio-driven-gaussian-head-avatars-from-a-single-image" class="nav-link">VASA-3D: Lifelike Audio-Driven Gaussian Head Avatars from a Single Image</a>
                </li>
                <li class="nav-item">
                    <a href="#art-articulated-reconstruction-transformer" class="nav-link">ART: Articulated Reconstruction Transformer</a>
                </li>
                <li class="nav-item">
                    <a href="#evolve-vla-test-time-training-from-environment-feedback-for-vision-language-action-models" class="nav-link">EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-12-17">Arxiv Computer Vision Papers - 2025-12-17</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，作为一名专注于计算机视觉和机器学习的研究助理，我将为您提供一份关于2025年12月16日 Arxiv 计算机视觉领域论文的简明执行摘要。</p>
<hr />
<p><strong>执行摘要：2025年12月16日 Arxiv 计算机视觉论文精选</strong></p>
<p><strong>日期：</strong> 2025年12月16日</p>
<p><strong>主要主题与趋势：</strong></p>
<p>本期 Arxiv 论文集聚焦于<strong>多模态融合、视频理解与生成、以及三维视觉的进步</strong>。特别引人注目的是，<strong>大型语言模型（LLMs）在视觉任务中的应用日益深入</strong>，从场景理解到视频叙事，再到三维生成，LLMs正成为连接不同模态信息和提升模型泛化能力的关键。此外，<strong>高效的视频处理和生成技术</strong>，以及<strong>从单目视频进行鲁棒的三维重建</strong>也是重要的研究方向。</p>
<p><strong>亮点与创新：</strong></p>
<ul>
<li><strong>MMGR: Multi-Modal Generative Reasoning</strong> 提出了一种新颖的多模态生成推理框架，预示着模型在理解和生成跨模态信息方面将有更强的能力。</li>
<li><strong>TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs</strong> 展示了如何利用多模态 LLMs 突破视频时间定位的局限，这对于理解长视频内容和进行更精细的视频分析至关重要。</li>
<li><strong>VASA-3D: Lifelike Audio-Driven Gaussian Head Avatars from a Single Image</strong> 在从单张图像生成逼真、由音频驱动的头部虚拟形象方面取得了显著进展，这在虚拟现实、元宇宙和内容创作领域具有巨大潜力。</li>
<li><strong>EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models</strong> 引入了一种在测试时利用环境反馈进行训练的技术，为构建更具适应性和鲁棒性的视觉-语言-动作模型提供了新思路。</li>
</ul>
<p><strong>新兴研究方向与技术：</strong></p>
<ul>
<li><strong>多模态 LLMs 在视觉任务中的深度整合：</strong> 不仅用于理解，更用于生成和推理，成为连接视觉和语言的强大桥梁。</li>
<li><strong>高效视频叙事与理解：</strong> MemFlow 和 TimeLens 等工作表明，如何高效处理长视频并提取有意义的叙事信息是当前研究的热点。</li>
<li><strong>从单目视频进行鲁棒的三维重建：</strong> CRISP 和 ART 等论文展示了在仅有单目视频输入的情况下，实现高质量三维场景和物体的重建，这对于机器人导航、AR/VR 应用等至关重要。</li>
<li><strong>紧凑且结构化的三维生成：</strong> Native and Compact Structured Latents for 3D Generation 探索了更高效的三维生成方法，有望降低计算成本并提高生成质量。</li>
<li><strong>量化技术在视觉生成中的应用：</strong> Spherical Leech Quantization for Visual Tokenization and Generation 提出了一种新的量化方法，可能为视觉令牌化和生成带来效率上的提升。</li>
</ul>
<p><strong>建议阅读全文的论文：</strong></p>
<p>考虑到其潜在的影响力和创新性，以下论文值得深入阅读：</p>
<ol>
<li><strong>MMGR: Multi-Modal Generative Reasoning</strong> (潜在的通用多模态能力提升)</li>
<li><strong>TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs</strong> (LLMs 在视频理解中的突破性应用)</li>
<li><strong>VASA-3D: Lifelike Audio-Driven Gaussian Head Avatars from a Single Image</strong> (在虚拟形象生成领域的显著进步)</li>
<li><strong>EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models</strong> (对未来智能体和机器人学习的重要启示)</li>
</ol>
<hr />
<p>这份摘要旨在帮助您快速把握本期 Arxiv 论文的重点，以便您能更有效地规划阅读和研究方向。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2512.14691v1">MMGR: Multi-Modal Generative Reasoning</a></li>
<li><a href="#2512.14020v1">Deep Learning Perspective of Scene Understanding in Autonomous Robots</a></li>
<li><a href="#2512.14699v1">MemFlow: Flowing Adaptive Memory for Consistent and Efficient Long Video Narratives</a></li>
<li><a href="#2512.14698v1">TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs</a></li>
<li><a href="#2512.14697v1">Spherical Leech Quantization for Visual Tokenization and Generation</a></li>
<li><a href="#2512.14696v1">CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives</a></li>
<li><a href="#2512.14692v1">Native and Compact Structured Latents for 3D Generation</a></li>
<li><a href="#2512.14677v1">VASA-3D: Lifelike Audio-Driven Gaussian Head Avatars from a Single Image</a></li>
<li><a href="#2512.14671v1">ART: Articulated Reconstruction Transformer</a></li>
<li><a href="#2512.14666v1">EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2512.14691v1'></a></p>
<h2 id="mmgr-multi-modal-generative-reasoning"><a href="https://arxiv.org/abs/2512.14691v1">MMGR: Multi-Modal Generative Reasoning</a></h2>
<p><strong>Authors:</strong> Zefan Cai, Haoyi Qiu, Tianyi Ma, Haozhe Zhao, Gengze Zhou, Kung-Hsiang Huang, Parisa Kordjamshidi, Minjia Zhang, Xiao Wen, Jiuxiang Gu, Nanyun Peng, Junjie Hu</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Categories:</strong> cs.CL, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：MMGR: Multi-Modal Generative Reasoning</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话）：</strong></p>
<p>这篇论文提出了MMGR，一个新颖的、基于多模态生成推理的评估框架。MMGR旨在弥补现有视频生成模型评估指标（如FVD）在衡量物理、逻辑和空间约束方面的不足，通过评估五种关键推理能力（物理、逻辑、3D空间、2D空间、时间）来更全面地衡量模型的“世界模拟”能力。研究结果揭示了当前领先的视频和图像生成模型在抽象推理和长时空规划方面存在显著的性能差距，为未来开发更具推理能力的生成模型指明了方向。</p>
<p><strong>2. 关键创新或方法论：</strong></p>
<ul>
<li><strong>多模态生成推理评估框架 (MMGR)：</strong> 这是论文的核心创新。MMGR不再局限于感知质量，而是引入了对模型“推理能力”的系统性评估。</li>
<li><strong>五种推理能力：</strong> 论文明确定义并量化了五种关键的推理能力：<ul>
<li><strong>物理推理 (Physical):</strong> 模型是否理解和遵循物理定律（如重力、碰撞）。</li>
<li><strong>逻辑推理 (Logical):</strong> 模型是否能进行因果推断和逻辑连贯性判断。</li>
<li><strong>3D空间推理 (3D Spatial):</strong> 模型是否理解物体在三维空间中的位置、关系和运动。</li>
<li><strong>2D空间推理 (2D Spatial):</strong> 模型是否理解物体在二维图像中的布局和关系。</li>
<li><strong>时间推理 (Temporal):</strong> 模型是否能生成在时间序列上连贯且符合因果关系的内容。</li>
</ul>
</li>
<li><strong>跨领域基准测试：</strong> MMGR在三个具有代表性的领域进行了评估：<ul>
<li><strong>抽象推理 (Abstract Reasoning):</strong> 使用ARC-AGI和Sudoku等任务，测试模型在非视觉、高度抽象的逻辑和模式识别能力。</li>
<li><strong>具身导航 (Embodied Navigation):</strong> 在模拟的3D环境中，测试模型进行导航、定位和规划的能力，这需要对物理和空间有深入理解。</li>
<li><strong>物理常识 (Physical Commonsense):</strong> 评估模型对体育运动和物体组合交互等场景的物理合理性判断。</li>
</ul>
</li>
<li><strong>细粒度指标：</strong> MMGR采用了需要“整体正确性”的细粒度指标，这意味着模型不仅要生成视觉上逼真的内容，还要在推理层面做到准确。</li>
</ul>
<p><strong>3. 对该领域的潜在影响：</strong></p>
<ul>
<li><strong>重新定义生成模型评估标准：</strong> MMGR有望推动生成模型评估从单纯的感知质量转向更注重“智能”和“理解”的推理能力。这将促使研究者和开发者更关注模型的逻辑一致性、物理合理性和空间理解能力。</li>
<li><strong>加速“世界模拟”模型的进步：</strong> 视频生成模型被视为潜在的“世界模拟器”。MMGR提供了一个明确的路径来诊断和改进这些模型在模拟真实世界复杂性方面的不足，从而加速通用人工智能（AGI）相关研究的进展。</li>
<li><strong>推动更鲁棒、更可靠的生成模型：</strong> 当前模型在某些推理任务上的低表现表明，它们可能只是在“模仿”数据中的表面模式，而非真正“理解”世界。MMGR的出现将激励研究者开发更具泛化能力、更不容易出现逻辑和物理错误的模型。</li>
<li><strong>为模型开发提供明确的方向：</strong> 通过揭示模型在不同推理能力上的弱点，MMGR为未来的模型架构设计、训练目标和数据收集提供了具体的改进方向。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用：</strong></p>
<ul>
<li><strong>视频生成与编辑：</strong> 提高视频生成内容的真实性和可信度，减少不合逻辑或物理上不可能的场景。</li>
<li><strong>具身智能与机器人：</strong> 训练机器人或虚拟代理进行更复杂的任务规划、导航和与环境的交互，需要强大的空间和物理推理能力。</li>
<li><strong>虚拟现实 (VR) 和增强现实 (AR)：</strong> 创建更逼真、更具交互性的虚拟环境，要求对物理和空间有准确的模拟。</li>
<li><strong>自动驾驶：</strong> 模拟复杂的交通场景，需要对物理定律、物体交互和时序变化有深刻理解。</li>
<li><strong>内容创作与游戏开发：</strong> 生成更具逻辑性和物理合理性的游戏场景、动画和特效。</li>
<li><strong>科学模拟与教育：</strong> 创建用于科学实验模拟或教育目的的交互式模型，需要精确的物理和逻辑表现。</li>
<li><strong>多模态理解与生成：</strong> 推动跨模态（如文本到视频、图像到视频）生成模型在理解和推理方面的进步。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性：</strong></p>
<ul>
<li><strong>评估的复杂性：</strong> MMGR引入了细粒度的、需要整体正确性的指标，这可能意味着评估过程本身比传统的感知指标更复杂、计算成本更高，并且可能需要更精细的标注数据或人工评估。</li>
<li><strong>基准测试的覆盖范围：</strong> 虽然MMGR涵盖了三个重要领域，但“世界模拟”的范畴非常广泛。摘要中提到的领域可能不足以完全捕捉所有潜在的推理失败模式。例如，对于更复杂的社会常识、情感推理等可能未被充分覆盖。</li>
<li><strong>模型性能的普遍性问题：</strong> 摘要指出“模型显示出中等成功...但表现不佳”，这表明即使是领先模型在MMGR框架下也存在显著的局限性。这可能意味着当前模型架构或训练范式在根本上存在不足，需要更颠覆性的创新。</li>
<li><strong>抽象推理的挑战：</strong> 模型在ARC-AGI等抽象推理任务上表现极差（低于10%准确率），这可能暗示了当前模型在处理非视觉、高度符号化和逻辑推理任务上的根本性困难，与它们主要基于视觉数据训练的模式有关。</li>
<li><strong>长时空规划的挑战：</strong> 在具身导航等任务中，模型在“长时空规划”上的挣扎，表明它们可能难以维持全局状态的一致性，并且在处理需要多步推理和长期依赖的任务时存在问题。</li>
</ul>
<p>总而言之，这篇论文通过MMGR框架，为评估和改进生成模型（尤其是视频生成模型）的“智能”层面——即推理能力——提供了重要的工具和视角。它揭示了当前模型在理解和模拟真实世界复杂性方面的深层挑战，并为未来的研究指明了方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.14691v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.14691v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.14020v1'></a></p>
<h2 id="deep-learning-perspective-of-scene-understanding-in-autonomous-robots"><a href="https://arxiv.org/abs/2512.14020v1">Deep Learning Perspective of Scene Understanding in Autonomous Robots</a></h2>
<p><strong>Authors:</strong> Afia Maham, Dur E Nayab Tashfa</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>This paper provides a review of deep learning applications in scene understanding in autonomous robots, including innovations in object detection, semantic and instance segmentation, depth estimation, 3D reconstruction, and visual SLAM. It emphasizes how these techniques address limitations of traditional geometric models, improve depth perception in real time despite occlusions and textureless surfaces, and enhance semantic reasoning to understand the environment better. When these perception modules are integrated into dynamic and unstructured environments, they become more effective in decisionmaking, navigation and interaction. Lastly, the review outlines the existing problems and research directions to advance learning-based scene understanding of autonomous robots.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Deep Learning Perspective of Scene Understanding in Autonomous Robots”的全面摘要，重点关注其核心问题、贡献、结果、局限性和未来方向：</p>
<p><strong>论文题目：</strong> Deep Learning Perspective of Scene Understanding in Autonomous Robots
<strong>作者：</strong> Afia Maham, Dur E Nayab Tashfa</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题/研究问题：</strong></p>
<p>该论文旨在解决传统几何模型在自主机器人场景理解方面存在的局限性，例如对纹理缺失表面和遮挡的感知能力不足，以及语义推理的欠缺。核心研究问题是如何利用深度学习技术，特别是卷积神经网络（CNN）、循环神经网络（RNN）和Transformer等模型，来提升自主机器人在动态和非结构化环境中的场景理解能力，从而实现更有效的决策、导航和交互。</p>
<p><strong>2. 关键创新或方法论贡献：</strong></p>
<p>论文全面回顾了深度学习在自主机器人场景理解中的应用，重点介绍了以下几个关键领域的技术进展：</p>
<ul>
<li><strong>对象检测与语义/实例分割：</strong> 强调了深度学习模型（如CNN、Transformer）在提高检测速度和准确性方面的作用，以及它们如何实现像素级别的场景理解。</li>
<li><strong>深度估计与3D重建：</strong> 探讨了单目深度估计、立体视觉和多视图立体（MVS）等技术，以及LiDAR和NeRF（Neural Radiance Fields）等方法如何实现更精确的深度感知和3D场景表示。</li>
<li><strong>视觉SLAM（Simultaneous Localization and Mapping）：</strong> 分析了深度学习如何增强传统视觉SLAM系统的鲁棒性，通过融合几何和语义信息，实现更高级别的场景理解和更可靠的定位与建图。</li>
<li><strong>模型架构：</strong> 详细介绍了CNN、RNN、GANs（Generative Adversarial Networks）和Transformer等核心深度学习架构在场景理解任务中的原理和应用。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<p>论文强调，深度学习技术显著克服了传统方法的不足，带来了以下重要成果：</p>
<ul>
<li><strong>提升感知能力：</strong> 深度学习模型能够从原始传感器数据中提取丰富的层次化特征，有效处理遮挡和纹理缺失的场景，实现实时、高精度的深度感知。</li>
<li><strong>增强语义理解：</strong> 通过语义分割和实例分割，机器人能够更深入地理解环境的组成部分及其相互关系，而不仅仅是识别对象。</li>
<li><strong>改进导航与交互：</strong> 更强的场景理解能力使得机器人能够做出更明智的决策，实现更安全、更高效的导航和更自然的与人类交互。</li>
<li><strong>推动自主性：</strong> 这些技术是实现完全自主机器人操作的关键，尤其是在复杂、动态和不可预测的环境中。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<p>论文也指出了当前深度学习在自主机器人场景理解方面存在的挑战：</p>
<ul>
<li><strong>计算效率与实时性：</strong> 在嵌入式设备上实现深度学习模型的实时运行仍然是一个主要挑战，需要高度优化的模型和高效的推理方法。</li>
<li><strong>数据依赖性：</strong> 深度学习模型通常需要大量的标注数据进行训练，这在机器人领域可能成本高昂且难以获取。</li>
<li><strong>鲁棒性：</strong> 机器人需要在各种光照、天气和环境变化下保持高水平的性能，而当前的算法在应对这些变化时仍有不足。</li>
<li><strong>可解释性与伦理问题：</strong> 深度学习模型的“黑箱”特性使得理解其决策过程变得困难，这影响了信任的建立以及在安全关键场景中的应用。</li>
<li><strong>动态环境处理：</strong> 动态物体的不可预测性给场景理解和导航带来了持续的挑战。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<p>基于上述挑战，论文提出了以下未来研究方向：</p>
<ul>
<li><strong>实时性能与计算效率的提升：</strong> 开发更轻量级、更高效的模型架构，以及模型量化、剪枝等技术，以适应资源受限的机器人平台。</li>
<li><strong>提高鲁棒性：</strong> 研究对抗性训练、领域自适应等方法，使模型能够更好地应对光照变化、遮挡和动态环境。</li>
<li><strong>模型可解释性与透明度：</strong> 发展能够提供决策依据和解释的深度学习模型，以增强信任并促进人机协作。</li>
<li><strong>数据效率与合成数据生成：</strong> 探索自监督学习、半监督学习以及高质量合成数据生成技术，以减少对昂贵标注数据的依赖。</li>
<li><strong>伦理与安全考量：</strong> 制定明确的法规和认证措施，确保机器人系统的公平性、安全性和隐私保护。</li>
<li><strong>多模态融合的深化：</strong> 进一步整合来自不同传感器（如相机、LiDAR、雷达）的数据，以提高感知能力和系统鲁棒性。</li>
<li><strong>AI驱动的传感器融合、量子辅助SLAM以及神经场景表示（如NeRFs）等新兴方向</strong>，有望进一步提升机器人感知的准确性、适应性和可靠性。</li>
</ul>
<p>总而言之，这篇论文为理解深度学习在自主机器人场景理解中的作用提供了一个全面的视角，突出了其巨大的潜力，同时也清晰地指出了当前的研究瓶颈和未来值得探索的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>When these perception modules are integrated into dynamic and unstructured environments, they become more effective in decisionmaking, navigation and interaction.</li>
<li>Lastly, the review outlines the existing problems and research directions to advance learning-based scene understanding of autonomous robots.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.14020v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.14020v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.14699v1'></a></p>
<h2 id="memflow-flowing-adaptive-memory-for-consistent-and-efficient-long-video-narratives"><a href="https://arxiv.org/abs/2512.14699v1">MemFlow: Flowing Adaptive Memory for Consistent and Efficient Long Video Narratives</a></h2>
<p><strong>Authors:</strong> Sihui Ji, Xi Chen, Shuai Yang, Xin Tao, Pengfei Wan, Hengshuang Zhao</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>The core challenge for streaming video generation is maintaining the content consistency in long context, which poses high requirement for the memory design. Most existing solutions maintain the memory by compressing historical frames with predefined strategies. However, different to-generate video chunks should refer to different historical cues, which is hard to satisfy with fixed strategies. In this work, we propose MemFlow to address this problem. Specifically, before generating the coming chunk, we dynamically update the memory bank by retrieving the most relevant historical frames with the text prompt of this chunk. This design enables narrative coherence even if new event happens or scenario switches in future frames. In addition, during generation, we only activate the most relevant tokens in the memory bank for each query in the attention layers, which effectively guarantees the generation efficiency. In this way, MemFlow achieves outstanding long-context consistency with negligible computation burden (7.9% speed reduction compared with the memory-free baseline) and keeps the compatibility with any streaming video generation model with KV cache.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“MemFlow: Flowing Adaptive Memory for Consistent and Efficient Long Video Narratives”的论文的中文摘要，其中包含了您要求的各个部分：</p>
<p><strong>论文题目：</strong> MemFlow: Flowing Adaptive Memory for Consistent and Efficient Long Video Narratives</p>
<p><strong>作者：</strong> Sihui Ji, Xi Chen, Shuai Yang, Xin Tao, Pengfei Wan, Hengshuang Zhao</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
该论文主要解决了流式视频生成中的核心挑战：<strong>如何在长视频生成过程中保持内容一致性</strong>。现有的方法通常采用预设策略来压缩历史帧以维护记忆，但这难以满足不同生成片段需要不同历史线索的需求，尤其是在出现新事件或场景切换时。</p>
<p><strong>2. 关键创新/方法贡献：</strong>
为了解决上述问题，作者提出了 <strong>MemFlow</strong> 框架，其核心创新在于两个方面：
*   <strong>叙事自适应记忆 (Narrative Adaptive Memory, NAM)：</strong> 在生成下一个视频片段之前，该机制会根据当前文本提示<strong>动态更新记忆库</strong>。它通过<strong>语义检索</strong>（基于文本查询和历史视觉键值的交叉注意力分数）来找出最相关的历史帧，并结合<strong>冗余移除</strong>（选择前一片段的第一帧作为代表性原型）来注入最新上下文，从而确保记忆库始终包含与当前生成内容语义对齐的历史信息。
*   <strong>稀疏记忆激活 (Sparse Memory Activation, SMA)：</strong> 为了平衡记忆带来的计算负担，SMA 采用<strong>相关性门控的记忆选择技术</strong>，仅激活记忆库中最相关的 token 用于注意力计算。这通过计算查询（当前片段）和键（记忆中的上下文）之间的相关性，并进行 top-k 选择来实现，从而在加速推理的同时保持视觉质量。</p>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>一致性与连贯性：</strong> MemFlow 在长视频生成中实现了出色的<strong>长上下文一致性</strong>和<strong>叙事连贯性</strong>，即使在出现新角色或场景切换时也能保持。定性结果（如图 1、3、4 所示）表明，相比于其他方法，MemFlow 能够更好地维持主体和背景的一致性，避免了不自然的场景过渡和重复角色的出现。
*   <strong>效率：</strong> 通过稀疏记忆激活，MemFlow 在保持高质量的同时，<strong>计算效率显著提升</strong>。与无记忆基线相比，仅有 7.9% 的速度下降。在 NVIDIA H100 GPU 上实现了 18.7 FPS 的推理速度，支持实时交互式视频生成。
*   <strong>泛化性：</strong> 该框架与任何支持 KV 缓存的流式视频生成模型兼容。
*   <strong>量化评估：</strong> 在多提示 60 秒视频生成任务中，MemFlow 在质量、一致性和美学得分上均表现优异（如表 1、3 所示）。在单提示 30 秒视频生成任务中，也取得了领先的性能（如表 4 所示）。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>记忆容量的影响：</strong> 论文在消融研究（图 5）中提到，过大的记忆容量（如 b=6 或 b=9）可能会导致性能不稳定，因为全局记忆的比例可能压倒局部上下文，干扰短时叙事流程。作者最终选择了 b=3 的容量，以在局部和全局上下文之间取得稳定平衡。
*   <strong>推理速度：</strong> 虽然效率显著提升，但与一些非常快的模型（如 LongLive）相比，MemFlow 在记忆更新和激活方面仍有少量速度上的权衡。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
虽然论文没有明确列出未来研究方向，但基于其工作，可以推测以下几个方向：
*   <strong>更精细的记忆检索和激活策略：</strong> 探索更复杂的注意力机制或更智能的 token 选择策略，以进一步优化记忆的利用效率和效果。
*   <strong>跨模态一致性：</strong> 将 MemFlow 的动态记忆机制扩展到更复杂的跨模态生成任务，例如视频与音频、文本描述的更深层一致性。
*   <strong>用户交互的精细化：</strong> 研究如何让用户通过更细粒度的指令来更精确地控制长视频的叙事和内容。
*   <strong>模型规模与效率的进一步平衡：</strong> 探索在更大模型规模下，如何更有效地应用 NAM 和 SMA 来维持长视频生成的一致性和效率。</p>
<p>总而言之，MemFlow 通过创新的动态记忆机制和高效的激活策略，有效地解决了长视频生成中的一致性难题，在保持高质量的同时实现了显著的效率提升，为流式视频生成领域带来了重要的贡献。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we propose MemFlow to address this problem.</li>
<li>This design enables narrative coherence even if new event happens or scenario switches in future frames.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.14699v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.14699v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.14698v1'></a></p>
<h2 id="timelens-rethinking-video-temporal-grounding-with-multimodal-llms"><a href="https://arxiv.org/abs/2512.14698v1">TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs</a></h2>
<p><strong>Authors:</strong> Jun Zhang, Teng Wang, Yuying Ge, Yixiao Ge, Xinhao Li, Ying Shan, Limin Wang</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.CL, cs.MM</p>
<p><strong>Abstract:</strong></p>
<p>This paper does not introduce a novel method but instead establishes a straightforward, incremental, yet essential baseline for video temporal grounding (VTG), a core capability in video understanding. While multimodal large language models (MLLMs) excel at various video understanding tasks, the recipes for optimizing them for VTG remain under-explored. In this paper, we present TimeLens, a systematic investigation into building MLLMs with strong VTG ability, along two primary dimensions: data quality and algorithmic design. We first expose critical quality issues in existing VTG benchmarks and introduce TimeLens-Bench, comprising meticulously re-annotated versions of three popular benchmarks with strict quality criteria. Our analysis reveals dramatic model re-rankings compared to legacy benchmarks, confirming the unreliability of prior evaluation standards. We also address noisy training data through an automated re-annotation pipeline, yielding TimeLens-100K, a large-scale, high-quality training dataset. Building on our data foundation, we conduct in-depth explorations of algorithmic design principles, yielding a series of meaningful insights and effective yet efficient practices. These include interleaved textual encoding for time representation, a thinking-free reinforcement learning with verifiable rewards (RLVR) approach as the training paradigm, and carefully designed recipes for RLVR training. These efforts culminate in TimeLens models, a family of MLLMs with state-of-the-art VTG performance among open-source models and even surpass proprietary models such as GPT-5 and Gemini-2.5-Flash. All codes, data, and models will be released to facilitate future research.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs”的全面中文摘要：</p>
<p><strong>论文题目：</strong> TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs</p>
<p><strong>作者：</strong> Jun Zhang, Teng Wang, Yuying Ge, Yixiao Ge, Xinhao Li, Ying Shan, Limin Wang</p>
<p><strong>摘要：</strong></p>
<p>这篇论文并不提出一种全新的方法，而是旨在为视频时序定位（Video Temporal Grounding, VTG）这一视频理解的核心能力，建立一个直接、渐进但至关重要的基线。尽管多模态大语言模型（MLLMs）在各种视频理解任务中表现出色，但针对 VTG 的优化方法仍有待深入探索。</p>
<p><strong>1. 研究问题与动机：</strong></p>
<p>论文的核心研究问题在于，如何有效地提升多模态大语言模型（MLLMs）在视频时序定位（VTG）任务上的能力。作者指出，现有的 VTG 模型在理解视频的“何时”方面存在局限性，这主要源于两个挑战：
*   <strong>从粗粒度语义聚合到细粒度时序感知：</strong> VTG 需要模型进行根本性的转变，从宏观的语义理解转向对时间维度的精细感知。
*   <strong>长时序视觉动态的建模：</strong> 区分查询事件需要对外观特征的长时序视觉动态进行建模，这在标注和学习上都极具挑战性。
此外，作者还发现现有 VTG 评估基准存在严重的质量问题，导致模型评估结果的误导性，并阻碍了研究的有效进展。</p>
<p><strong>2. 主要创新与方法贡献：</strong></p>
<p>TimeLens 项目通过系统性地研究数据质量和算法设计这两个核心维度，来解决上述问题：</p>
<ul>
<li>
<p><strong>数据质量的提升：</strong></p>
<ul>
<li><strong>TimeLens-Bench：</strong> 作者首先揭示了现有 VTG 基准的质量问题，并对三个流行基准（Charades-STA, ActivityNet Captions, QVHighlights）进行了细致的手动重新标注，创建了一个高质量、严格验证的评估套件 TimeLens-Bench。分析表明，使用 TimeLens-Bench 进行评估会显著改变模型的排名，揭示了传统基准的不可靠性。</li>
<li><strong>TimeLens-100K：</strong> 针对训练数据中的噪声问题，作者开发了一个自动化的重新标注流水线，生成了一个大规模、高质量的训练数据集 TimeLens-100K。</li>
</ul>
</li>
<li>
<p><strong>算法设计的探索：</strong></p>
<ul>
<li><strong>时间表示：</strong> 探索了多种时间编码方法，发现<strong>交错式文本前缀（interleaved textual prefix）</strong>结合原始时间戳（raw timestamps）是最有效且简洁的方法。</li>
<li><strong>训练范式：</strong> 深入研究了不同的训练范式，发现<strong>纯粹的无思考式强化学习（thinking-free RLVR）</strong>在性能和效率上均优于监督微调（SFT）和有思考式 RLVR。</li>
<li><strong>RLVR 训练技巧：</strong> 提出了两项关键的 RLVR 训练技巧：<strong>奖励指标平台期时进行早停（early stopping）</strong>以节省计算成本并防止性能下降；以及<strong>基于难度的样本采样（difficulty-aware sampling）</strong>，以确保模型能够有效地学习具有挑战性的样本。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>模型性能提升：</strong> 基于 TimeLens-Bench 和 TimeLens-100K，作者开发了 TimeLens 模型系列。这些模型在 VTG 任务上取得了<strong>最先进（state-of-the-art）的性能</strong>，不仅在开源模型中表现突出，甚至<strong>超越了 GPT-5 和 Gemini-2.5-Flash 等领先的闭源模型</strong>。</li>
<li><strong>基准的可靠性验证：</strong> TimeLens-Bench 的创建和使用，揭示了现有基准的局限性，并为未来 VTG 研究提供了更可靠的评估标准。</li>
<li><strong>数据质量的重要性：</strong> 研究强调了高质量数据对于训练高性能 VTG 模型的重要性，TimeLens-100K 的有效性得到了验证。</li>
<li><strong>算法设计的洞察：</strong> 论文提供了关于时间编码、训练范式和采样策略的宝贵洞察，为构建更强大的 VTG 模型提供了实践指导。</li>
<li><strong>开源贡献：</strong> 作者承诺开源所有代码、数据和模型，以促进该领域的研究。</li>
</ul>
<p><strong>4. 局限性：</strong></p>
<ul>
<li><strong>推理能力的需求：</strong> 作者指出，大多数现有的视频时序定位任务并不需要复杂的推理能力，主要依赖于模型的感知和定位能力。然而，某些特定的 VTG 任务可能确实需要推理能力，这部分内容并未在本研究中深入探讨。</li>
<li><strong>Qwen3-VL 的特殊处理：</strong> 对于 Qwen3-VL 模型，由于其已经过大规模多任务 RL 训练，作者需要采用一个小的 SFT 阶段来“重置”模型，以避免其在 RL 训练中生成缺乏多样性的 rollout。这是一种针对特定模型的 workaround，而非普遍适用的方法。</li>
</ul>
<p><strong>5. 未来研究方向：</strong></p>
<ul>
<li><strong>推理密集型 VTG 任务：</strong> 探索需要复杂推理能力的视频时序定位场景。</li>
<li><strong>更广泛的基准和模型评估：</strong> 利用 TimeLens-Bench 和 TimeLens-100K 进行更广泛的模型评估和比较。</li>
<li><strong>多模态融合的进一步优化：</strong> 探索更先进的多模态融合技术，以进一步提升 MLLMs 在 VTG 任务上的表现。</li>
</ul>
<p><strong>总结：</strong></p>
<p>TimeLens 论文通过对数据质量和算法设计的系统性研究，为视频时序定位（VTG）领域做出了重要贡献。它不仅提供了一个更可靠的评估基准（TimeLens-Bench）和一个高质量的训练数据集（TimeLens-100K），还提出了有效的算法设计原则，最终开发出了性能卓越的 TimeLens 模型。这项工作为未来构建更强大的视频理解模型奠定了坚实的基础，并强调了数据质量在推动人工智能模型发展中的关键作用。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This paper does not introduce a novel method but instead establishes a straightforward, incremental, yet essential baseline for video temporal grounding (VTG), a core capability in video understanding.</li>
<li>In this paper, we present TimeLens, a systematic investigation into building MLLMs with strong VTG ability, along two primary dimensions: data quality and algorithmic design.</li>
<li>These efforts culminate in TimeLens models, a family of MLLMs with state-of-the-art VTG performance among open-source models and even surpass proprietary models such as GPT-5 and Gemini-2.5-Flash.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.14698v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.14698v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.14697v1'></a></p>
<h2 id="spherical-leech-quantization-for-visual-tokenization-and-generation"><a href="https://arxiv.org/abs/2512.14697v1">Spherical Leech Quantization for Visual Tokenization and Generation</a></h2>
<p><strong>Authors:</strong> Yue Zhao, Hanwen Jiang, Zhenlin Xu, Chutong Yang, Ehsan Adeli, Philipp Krähenbühl</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG, eess.SP</p>
<p><strong>Abstract:</strong></p>
<p>Non-parametric quantization has received much attention due to its efficiency on parameters and scalability to a large codebook. In this paper, we present a unified formulation of different non-parametric quantization methods through the lens of lattice coding. The geometry of lattice codes explains the necessity of auxiliary loss terms when training auto-encoders with certain existing lookup-free quantization variants such as BSQ. As a step forward, we explore a few possible candidates, including random lattices, generalized Fibonacci lattices, and densest sphere packing lattices. Among all, we find the Leech lattice-based quantization method, which is dubbed as Spherical Leech Quantization (<script type="math/tex">Λ_{24}</script>-SQ), leads to both a simplified training recipe and an improved reconstruction-compression tradeoff thanks to its high symmetry and even distribution on the hypersphere. In image tokenization and compression tasks, this quantization approach achieves better reconstruction quality across all metrics than BSQ, the best prior art, while consuming slightly fewer bits. The improvement also extends to state-of-the-art auto-regressive image generation frameworks.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“Spherical Leech Quantization for Visual Tokenization and Generation”的论文的中文摘要，其中包含了您要求的各个部分：</p>
<p><strong>论文题目：</strong> Spherical Leech Quantization for Visual Tokenization and Generation</p>
<p><strong>作者：</strong> Yue Zhao, Hanwen Jiang, Zhenlin Xu, Chutong Yang, Ehsan Adeli, Philipp Krähenbühl</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题/研究问题：</strong>
该论文旨在解决现有非参数化量化方法在视觉标记化和生成任务中存在的局限性，特别是如何有效地扩展视觉词汇表以提升模型性能。现有方法如BSQ虽然在一定程度上提高了效率和可扩展性，但仍存在训练复杂、需要额外正则化项以及在某些情况下性能受限的问题。研究的核心问题是如何设计一种更优越的量化方法，能够简化训练过程，同时在图像重建和生成任务中取得更好的性能，并能有效利用大规模词汇表。</p>
<p><strong>2. 关键创新/方法贡献：</strong>
*   <strong>统一的格子编码视角：</strong> 论文提出了一种将现有非参数化量化方法（如LFQ、BSQ、FSQ）统一到格子编码（lattice coding）框架下的新视角。这使得研究者能够从几何学的角度理解这些方法的原理和局限性，并为设计新方法提供了理论基础。
*   <strong>球形李氏格子量化（<script type="math/tex">Λ_{24}</script>-SQ）：</strong> 论文的核心贡献是提出了名为“球形李氏格子量化”（<script type="math/tex">Λ_{24}</script>-SQ）的新型量化方法。该方法基于李氏格子（Leech lattice），这是一种在24维空间中具有极高对称性和最优密堆积特性的格子。
*   <strong>简化的训练流程：</strong> <script type="math/tex">Λ_{24}</script>-SQ 利用李氏格子的优良几何特性，使得自动编码器可以在更简单的损失函数组合下进行训练，无需复杂的正则化项（如熵惩罚），从而简化了训练过程。
*   <strong>改进的重建-压缩权衡：</strong> <script type="math/tex">Λ_{24}</script>-SQ 在图像标记化和压缩任务中，相比于最先进的BSQ方法，在各项指标上都取得了更好的重建质量，同时消耗的比特数略少，显著改善了率失真权衡。
*   <strong>大规模词汇表生成：</strong> 论文展示了如何利用<script type="math/tex">Λ_{24}</script>-SQ 训练具有大规模词汇表（高达约20万个条目）的视觉自回归生成模型，并且首次在没有复杂技巧的情况下，实现了接近“神谕”（oracle-like）水平的生成性能。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>图像重建和压缩：</strong> 在COCO2017和ImageNet-1k数据集上，基于<script type="math/tex">Λ_{24}</script>-SQ 的ViT模型在重建任务上显著优于BSQ-ViT，rFID降低了10-20%，同时在PSNR、SSIM等指标上也有提升。在Kodak数据集上的图像压缩实验也表明，<script type="math/tex">Λ_{24}</script>-SQ 在相同比特率下能获得更高的PSNR和MS-SSIM。
*   <strong>图像生成：</strong> 在ImageNet-1k数据集上，使用<script type="math/tex">Λ_{24}</script>-SQ 的自回归模型（如Infinity-CC）在生成任务上取得了最先进的性能，gFID显著降低，并且能够更好地捕捉图像的多样性。论文首次实现了大规模（~200K）词汇表的视觉自回归生成，并且性能接近理论最优。
*   <strong>理论和实践的结合：</strong> 论文成功地将抽象的格子理论应用于实际的视觉模型中，证明了数学上的最优格子结构能够带来实际的性能提升，并简化了模型训练。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>计算成本：</strong> 虽然<script type="math/tex">Λ_{24}</script>-SQ 简化了训练，但李氏格子本身在低维度的实现和处理可能仍然存在一定的计算挑战，尤其是在处理非常大的词汇表时，尽管论文通过一些技术（如tiling和JIT-compiling）来缓解。
*   <strong>大规模词汇表的训练挑战：</strong> 论文提到，即使使用<script type="math/tex">Λ_{24}</script>-SQ，训练具有非常大词汇表的模型仍然存在挑战，例如梯度范数爆炸和损失函数不稳定。为此，论文引入了Z-loss和分布式正交更新等技术来解决这些问题。
*   <strong>特定领域的适用性：</strong> 论文主要集中在图像标记化和生成任务上，其在其他视觉任务或模态上的适用性有待进一步探索。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>更大规模的实验：</strong> 验证<script type="math/tex">Λ_{24}</script>-SQ 在更大规模数据集和模型上的有效性，例如在更广泛的视觉任务中。
*   <strong>跨模态应用：</strong> 探索将<script type="math/tex">Λ_{24}</script>-SQ 应用于多模态学习，例如文本到图像生成，或结合文本信息进行更精细的视觉理解。
*   <strong>更高效的实现：</strong> 研究更高效的算法和硬件加速技术，以进一步降低处理大规模<script type="math/tex">Λ_{24}</script>-SQ 词汇表的计算成本。
*   <strong>理论的进一步深化：</strong> 探索李氏格子在其他非参数化量化方法中的应用，以及格子理论在其他机器学习领域（如自然语言处理）的潜在价值。
*   <strong>自适应词汇表：</strong> 研究如何根据数据特性动态调整词汇表的大小和结构，以进一步优化性能和效率。</p>
<p><strong>总结：</strong>
这篇论文通过将非参数化量化方法置于格子编码的理论框架下，并引入了基于李氏格子的球形李氏格子量化（<script type="math/tex">Λ_{24}</script>-SQ），在视觉标记化、图像压缩和生成领域取得了显著的突破。<script type="math/tex">Λ_{24}</script>-SQ 不仅简化了训练流程，而且在性能上超越了现有最优方法，尤其是在利用大规模词汇表进行高质量图像生成方面，为未来的视觉模型研究开辟了新的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we present a unified formulation of different non-parametric quantization methods through the lens of lattice coding.</li>
<li>The improvement also extends to state-of-the-art auto-regressive image generation frameworks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.14697v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.14697v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.14696v1'></a></p>
<h2 id="crisp-contact-guided-real2sim-from-monocular-video-with-planar-scene-primitives"><a href="https://arxiv.org/abs/2512.14696v1">CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives</a></h2>
<p><strong>Authors:</strong> Zihan Wang, Jiashun Wang, Jeff Tan, Yiwen Zhao, Jessica Hodgins, Shubham Tulsiani, Deva Ramanan</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Categories:</strong> cs.CV, cs.GR, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>We introduce CRISP, a method that recovers simulatable human motion and scene geometry from monocular video. Prior work on joint human-scene reconstruction relies on data-driven priors and joint optimization with no physics in the loop, or recovers noisy geometry with artifacts that cause motion tracking policies with scene interactions to fail. In contrast, our key insight is to recover convex, clean, and simulation-ready geometry by fitting planar primitives to a point cloud reconstruction of the scene, via a simple clustering pipeline over depth, normals, and flow. To reconstruct scene geometry that might be occluded during interactions, we make use of human-scene contact modeling (e.g., we use human posture to reconstruct the occluded seat of a chair). Finally, we ensure that human and scene reconstructions are physically-plausible by using them to drive a humanoid controller via reinforcement learning. Our approach reduces motion tracking failure rates from 55.2\% to 6.9\% on human-centric video benchmarks (EMDB, PROX), while delivering a 43\% faster RL simulation throughput. We further validate it on in-the-wild videos including casually-captured videos, Internet videos, and even Sora-generated videos. This demonstrates CRISP's ability to generate physically-valid human motion and interaction environments at scale, greatly advancing real-to-sim applications for robotics and AR/VR.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话的简洁总结）</strong></p>
<p>该论文提出了一种名为 CRISP 的新方法，能够从单目视频中恢复出可用于仿真的、物理上合理的（physically-plausible）人类运动和场景几何。其核心贡献在于通过一种新颖的、基于平面基元（planar primitives）的几何恢复方法，生成干净、无伪影且易于仿真的场景表示，并结合了人类与场景的接触信息来处理遮挡，最终显著提升了真实世界视频到仿真环境的迁移效果，降低了运动跟踪失败率。</p>
<p><strong>2. 关键创新点或方法论</strong></p>
<p>CRISP 的关键创新点在于其独特的场景几何恢复和人类运动重建策略：</p>
<ul>
<li><strong>基于平面基元的几何恢复 (Planar Primitive Fitting):</strong> 这是该方法的核心。不同于以往依赖数据驱动先验或直接优化点云的方法，CRISP 提出将场景点云重构为由<strong>凸形、干净、可仿真</strong>的平面基元组成。这通过对点云的深度、法线和光流信息进行聚类来实现。这种方法能够生成更结构化、更易于物理引擎理解和交互的几何表示，有效避免了传统方法中常见的几何伪影（artifacts）。</li>
<li><strong>利用人类-场景接触建模 (Human-Scene Contact Modeling):</strong> 为了解决场景中可能出现的遮挡问题（例如，椅子被人物遮挡的部分），CRISP 巧妙地利用了人类姿态信息来推断和重建被遮挡的场景几何。这是一种非常实用的方法，能够弥补单目视频在深度和完整性上的固有局限。</li>
<li><strong>物理一致性保证 (Physically-Plausible Reconstruction):</strong> 通过将恢复出的人类和场景几何驱动一个<strong>人形控制器（humanoid controller）</strong>，并利用<strong>强化学习（reinforcement learning）</strong>进行优化，CRISP 确保了最终的人类运动和场景交互是物理上合理的。这种端到端的训练方式，将几何恢复与运动控制紧密结合，是实现高质量 Real2Sim 的关键。</li>
<li><strong>简化的聚类流水线 (Simple Clustering Pipeline):</strong> 摘要中提到“simple clustering pipeline over depth, normals, and flow”，这暗示了其几何恢复过程可能比复杂的全局优化方法更高效且易于实现。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<p>CRISP 的研究对计算机视觉和机器人领域具有重要的潜在影响：</p>
<ul>
<li><strong>提升 Real2Sim 的准确性和鲁棒性:</strong> 通过生成更干净、更易于仿真的场景几何，CRISP 显著降低了运动跟踪失败率，这是 Real2Sim 应用中的一个长期痛点。这使得仿真环境能够更准确地反映真实世界的物理规律，从而训练出更可靠的机器人控制策略。</li>
<li><strong>推动虚拟现实/增强现实（AR/VR）和机器人应用:</strong> 能够从单目视频中可靠地恢复出可交互的虚拟环境和人类动作，为 AR/VR 中的沉浸式体验、虚拟人交互以及机器人学习和部署提供了强大的基础。</li>
<li><strong>降低数据采集和标注成本:</strong> 从单目视频中恢复信息，相比于需要多视角、深度传感器或复杂标注的数据集，大大降低了数据采集和处理的成本，使得大规模的 Real2Sim 应用成为可能。</li>
<li><strong>促进跨模态（视频到物理仿真）的融合:</strong> 该方法成功地将视觉信息（单目视频）与物理仿真相结合，展示了跨模态学习的巨大潜力。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>机器人学:</strong><ul>
<li><strong>机器人抓取和操作:</strong> 训练机器人执行复杂的操作任务，例如在真实环境中学习到的动作迁移到仿真中进行大规模测试和优化。</li>
<li><strong>人形机器人控制:</strong> 训练人形机器人进行行走、交互等复杂动作，特别是需要与环境进行精细物理交互的任务。</li>
<li><strong>自动驾驶:</strong> 模拟真实世界的交通场景和行人行为，用于训练和测试自动驾驶算法。</li>
</ul>
</li>
<li><strong>虚拟现实 (VR) 和增强现实 (AR):</strong><ul>
<li><strong>沉浸式内容创作:</strong> 从真实视频中快速生成可交互的虚拟场景和人物，用于游戏、电影制作和虚拟体验。</li>
<li><strong>虚拟人交互:</strong> 创建更逼真、更具交互性的虚拟角色，用于社交、教育和娱乐。</li>
</ul>
</li>
<li><strong>计算机动画和游戏开发:</strong><ul>
<li><strong>角色动画生成:</strong> 从视频中捕捉人物动作并将其应用到虚拟角色上，同时生成与之交互的物理环境。</li>
<li><strong>场景重建:</strong> 快速将真实世界的场景转化为可用于游戏引擎的3D模型。</li>
</ul>
</li>
<li><strong>人机交互 (HCI):</strong><ul>
<li><strong>手势识别和跟踪:</strong> 更准确地理解和模拟用户的手势和身体姿态。</li>
</ul>
</li>
</ul>
<p><strong>5. 可从摘要推断出的局限性</strong></p>
<p>尽管摘要展示了显著的进步，但仍可以推断出一些潜在的局限性：</p>
<ul>
<li><strong>对平面基元的依赖:</strong> 该方法的核心是拟合平面基元。对于高度非平面、曲率变化剧烈或包含大量自由曲面的场景，其几何恢复效果可能不如预期。虽然摘要提到“convex, clean, and simulation-ready geometry”，但对于复杂曲面物体的处理能力仍需进一步验证。</li>
<li><strong>单目视频的固有局限:</strong> 尽管利用了接触建模，但单目视频在深度估计和处理复杂遮挡方面仍存在固有的不确定性。对于非常严重的遮挡或场景中缺乏足够视觉线索的情况，恢复的几何可能仍然存在误差。</li>
<li><strong>对“干净”几何的定义:</strong> 摘要中强调“clean, and simulation-ready geometry”。“干净”的定义可能意味着对某些细节的简化或抽象，这可能导致在需要极高几何精度的应用中不够理想。</li>
<li><strong>计算复杂度:</strong> 虽然提到了“simple clustering pipeline”，但整个端到端的 Real2Sim 过程，特别是涉及强化学习训练的部分，可能仍然需要大量的计算资源和时间。</li>
<li><strong>“in-the-wild”视频的泛化性:</strong> 虽然论文声称在“in-the-wild”视频上进行了验证，包括“casually-captured videos, Internet videos, and even Sora-generated videos”，但这些视频的质量、多样性和复杂性差异很大。其在极端情况下的泛化能力仍需进一步考察。</li>
<li><strong>接触建模的准确性:</strong> 利用姿态信息推断被遮挡的几何是一种启发式方法，其准确性可能依赖于姿态估计的质量以及场景与人物的交互模式。</li>
</ul>
<p>总而言之，CRISP 是一项非常有前景的研究，它通过创新的几何恢复方法和对物理一致性的关注，显著推动了从真实视频到仿真环境的迁移能力。其对平面基元的利用以及接触建模的引入，为解决 Real2Sim 中的关键挑战提供了新的思路。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce CRISP, a method that recovers simulatable human motion and scene geometry from monocular video.</li>
<li>Our approach reduces motion tracking failure rates from 55.2\% to 6.9\% on human-centric video benchmarks (EMDB, PROX), while delivering a 43\% faster RL simulation throughput.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.14696v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.14696v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.14692v1'></a></p>
<h2 id="native-and-compact-structured-latents-for-3d-generation"><a href="https://arxiv.org/abs/2512.14692v1">Native and Compact Structured Latents for 3D Generation</a></h2>
<p><strong>Authors:</strong> Jianfeng Xiang, Xiaoxue Chen, Sicheng Xu, Ruicheng Wang, Zelong Lv, Yu Deng, Hongyuan Zhu, Yue Dong, Hao Zhao, Nicholas Jing Yuan, Jiaolong Yang</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Recent advancements in 3D generative modeling have significantly improved the generation realism, yet the field is still hampered by existing representations, which struggle to capture assets with complex topologies and detailed appearance. This paper present an approach for learning a structured latent representation from native 3D data to address this challenge. At its core is a new sparse voxel structure called O-Voxel, an omni-voxel representation that encodes both geometry and appearance. O-Voxel can robustly model arbitrary topology, including open, non-manifold, and fully-enclosed surfaces, while capturing comprehensive surface attributes beyond texture color, such as physically-based rendering parameters. Based on O-Voxel, we design a Sparse Compression VAE which provides a high spatial compression rate and a compact latent space. We train large-scale flow-matching models comprising 4B parameters for 3D generation using diverse public 3D asset datasets. Despite their scale, inference remains highly efficient. Meanwhile, the geometry and material quality of our generated assets far exceed those of existing models. We believe our approach offers a significant advancement in 3D generative modeling.</p>
<p><strong>Analysis:</strong></p>
<p>作为一名计算机视觉和机器学习领域的专家，我对这篇论文摘要进行了深入分析，并为您提供以下中文解读：</p>
<p><strong>1. 论文的主要贡献（2-3句话）</strong></p>
<p>该论文提出了一种新颖的、基于稀疏体素结构（O-Voxel）的3D生成方法，能够学习一种紧凑且结构化的潜在表示。这种表示能够有效捕捉具有复杂拓扑结构和精细外观的3D资产，显著提升了生成质量和效率。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<ul>
<li><strong>O-Voxel（Omni-Voxel）结构：</strong> 这是论文的核心创新。O-Voxel是一种新的稀疏体素结构，它能够同时编码3D资产的几何信息和外观信息。其关键优势在于能够<strong>鲁棒地建模任意拓扑结构</strong>，包括开放曲面、非流形曲面以及完全封闭的曲面。此外，它还能捕捉比传统纹理颜色更丰富的表面属性，例如<strong>物理渲染（PBR）参数</strong>。</li>
<li><strong>Sparse Compression VAE：</strong> 基于O-Voxel结构，论文设计了一个稀疏压缩变分自编码器（VAE）。这个VAE能够实现<strong>高空间压缩率</strong>，并生成一个<strong>紧凑的潜在空间</strong>。这意味着模型能够用更少的参数来表示复杂的3D数据，从而提高效率。</li>
<li><strong>大规模流匹配模型：</strong> 论文利用其提出的O-Voxel和VAE，训练了<strong>40亿参数</strong>的大规模流匹配（Flow Matching）模型。流匹配是一种新兴的生成模型技术，以其高效的采样速度和高质量的生成能力而闻名。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<ul>
<li><strong>突破3D生成瓶颈：</strong> 当前3D生成模型在处理复杂拓扑和精细外观方面存在挑战。O-Voxel的出现有望解决这一瓶颈，使得生成更逼真、更具细节的3D资产成为可能。</li>
<li><strong>提升生成效率：</strong> 紧凑的潜在空间和高效的推理能力将大大降低3D内容创作的门槛，使得更多研究者和开发者能够进行大规模3D生成实验和应用。</li>
<li><strong>推动3D内容生态发展：</strong> 更高质量、更易于生成的3D资产将极大地促进游戏、虚拟现实（VR）、增强现实（AR）、数字孪生等领域的3D内容创作和应用。</li>
<li><strong>为未来3D AI研究奠定基础：</strong> 该方法为学习更具表达力和效率的3D表示提供了新的思路，可能启发后续在3D理解、编辑和交互等方面的研究。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>游戏开发：</strong> 生成高质量、多样化的游戏资产，如角色、场景、道具等。</li>
<li><strong>虚拟现实/增强现实（VR/AR）：</strong> 创建沉浸式虚拟环境和逼真的AR体验所需的3D模型。</li>
<li><strong>数字孪生：</strong> 构建高保真度的物理世界数字副本，用于模拟、分析和预测。</li>
<li><strong>电影和动画制作：</strong> 加速3D角色、场景和特效的创建过程。</li>
<li><strong>3D内容创作平台：</strong> 为用户提供更强大、更易用的3D模型生成工具。</li>
<li><strong>机器人和自动驾驶：</strong> 生成逼真的3D环境用于训练和测试。</li>
<li><strong>医学可视化：</strong> 生成高精度的3D人体模型或器官模型。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<ul>
<li><strong>计算资源需求：</strong> 虽然摘要提到推理高效，但训练一个40亿参数的模型仍然需要巨大的计算资源，这可能限制了其在资源受限环境下的应用。</li>
<li><strong>数据依赖性：</strong> 模型的性能很大程度上依赖于训练数据的质量和多样性。如果训练数据存在偏差或不足，可能会影响生成的多样性和泛化能力。</li>
<li><strong>O-Voxel的实现细节：</strong> 摘要并未详细说明O-Voxel的具体实现方式，例如其稀疏化策略、编码方式等，这些细节可能会影响其在实际应用中的性能和可扩展性。</li>
<li><strong>“Comprehensive surface attributes”的定义：</strong> 摘要提到捕捉“comprehensive surface attributes beyond texture color”，但具体包含哪些属性以及其丰富程度，需要进一步的论文内容来阐述。</li>
<li><strong>潜在的“hallucination”问题：</strong> 尽管生成质量很高，但任何生成模型都可能存在生成不准确或不符合物理规律的细节（hallucination）的风险，尤其是在处理非常复杂的拓扑或极端外观时。</li>
</ul>
<p>总而言之，这篇论文通过引入O-Voxel这一创新的3D表示，并结合先进的生成模型技术，有望在3D生成领域取得重大突破，尤其是在处理复杂性和细节方面。其潜在影响广泛，但同时也需要关注其对计算资源的需求以及数据依赖性等问题。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>At its core is a new sparse voxel structure called O-Voxel, an omni-voxel representation that encodes both geometry and appearance.</li>
<li>We believe our approach offers a significant advancement in 3D generative modeling.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.14692v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.14692v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.14677v1'></a></p>
<h2 id="vasa-3d-lifelike-audio-driven-gaussian-head-avatars-from-a-single-image"><a href="https://arxiv.org/abs/2512.14677v1">VASA-3D: Lifelike Audio-Driven Gaussian Head Avatars from a Single Image</a></h2>
<p><strong>Authors:</strong> Sicheng Xu, Guojun Chen, Jiaolong Yang, Yizhong Zhang, Yu Deng, Steve Lin, Baining Guo</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>We propose VASA-3D, an audio-driven, single-shot 3D head avatar generator. This research tackles two major challenges: capturing the subtle expression details present in real human faces, and reconstructing an intricate 3D head avatar from a single portrait image. To accurately model expression details, VASA-3D leverages the motion latent of VASA-1, a method that yields exceptional realism and vividness in 2D talking heads. A critical element of our work is translating this motion latent to 3D, which is accomplished by devising a 3D head model that is conditioned on the motion latent. Customization of this model to a single image is achieved through an optimization framework that employs numerous video frames of the reference head synthesized from the input image. The optimization takes various training losses robust to artifacts and limited pose coverage in the generated training data. Our experiment shows that VASA-3D produces realistic 3D talking heads that cannot be achieved by prior art, and it supports the online generation of 512x512 free-viewpoint videos at up to 75 FPS, facilitating more immersive engagements with lifelike 3D avatars.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“VASA-3D: Lifelike Audio-Driven Gaussian Head Avatars from a Single Image”的全面中文摘要：</p>
<p><strong>论文题目：</strong> VASA-3D: Lifelike Audio-Driven Gaussian Head Avatars from a Single Image</p>
<p><strong>作者：</strong> Sicheng Xu, Guojun Chen, Jiaolong Yang, Yizhong Zhang, Yu Deng, Stephen Lin, Baining Guo (Microsoft Research Asia)</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
本文旨在解决生成逼真、可自由视角的3D头部虚拟形象（avatars）的两大核心挑战：
*   <strong>捕捉真实人脸的细微表情细节：</strong> 现有方法在表情的细致度和生动性上存在不足。
*   <strong>从单张肖像图像重建精细的3D头部模型：</strong> 大多数现有方法需要多视角或视频数据，限制了其应用。</p>
<p><strong>2. 关键创新/方法贡献：</strong>
VASA-3D 提出了一种创新的音频驱动、单镜头3D头部虚拟形象生成器，其核心贡献在于：
*   <strong>利用VASA-1的运动潜在空间（Motion Latent Space）：</strong> 借鉴了VASA-1在2D视频生成中展现出的卓越真实感和生动性，将其运动潜在信息应用于3D头部模型。
*   <strong>将2D运动潜在信息转化为3D：</strong> 设计了一个基于3D高斯泼溅（3D Gaussian Splatting）的3D头部模型，该模型能够被运动潜在信息驱动。
*   <strong>单图像定制化框架：</strong> 通过一个优化框架，利用输入图像合成的大量参考头部视频帧来定制3D模型。该框架采用了多种鲁棒的训练损失函数，以应对合成数据中可能存在的伪影和视角限制。
*   <strong>双重形变机制：</strong> 引入了“基础形变”（Base Deformation）和“VAS形变”（VAS Deformation）。基础形变由FLAME模型驱动，用于调整高斯体的几何属性；VAS形变则学习更精细的几何和颜色变化，以捕捉VASA-1运动潜在信息中的细微表情和动作，从而提升渲染质量。
*   <strong>鲁棒的训练策略：</strong> 针对合成数据中存在的时序不一致、视角覆盖不足以及过拟合等问题，设计了包括重构损失（Reconstruction Losses）、感知损失（Perceptual Losses）和SDS损失（SDS Loss）在内的多种损失函数，并引入了渲染一致性损失（Render Consistency Loss）和锐化损失（Sharpening Loss）来进一步优化结果。</p>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>逼真的3D头部虚拟形象：</strong> VASA-3D能够生成高度逼真的3D头部虚拟形象，在表情细节和生动性上超越了现有技术。
*   <strong>实时自由视角视频生成：</strong> 该方法支持实时生成512x512分辨率的自由视角（free-viewpoint）视频，帧率可达75 FPS，极大地提升了沉浸式虚拟交互体验。
*   <strong>单图像输入：</strong> 仅需一张肖像照片即可生成可驱动的3D头部模型，大大降低了使用门槛。
*   <strong>音频驱动：</strong> 可以通过任意语音音频片段驱动生成的3D头部模型，实现逼真的口型同步和面部表情。
*   <strong>用户研究结果：</strong> 在用户研究中，VASA-3D在视觉质量和整体真实感方面获得了显著优于其他方法的评价，用户偏好度高达93.91%。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>视角限制：</strong> 由于合成训练视频的视角限制，模型无法建模头部的后部。
*   <strong>动态元素：</strong> 与VASA-1类似，该方法不处理动态的配饰（如眼镜）。
*   <strong>仅限于头部：</strong> 模型目前仅限于生成头部，不包含身体部分。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>3D补全（3D Inpainting）：</strong> 解决头部后部建模的问题。
*   <strong>扩展到上身：</strong> 将模型扩展到包含身体部分，以生成更完整的虚拟形象。
*   <strong>处理动态配饰：</strong> 增强对眼镜等动态元素的建模能力。</p>
<p><strong>论文的创新性和重要性：</strong>
VASA-3D在3D头部虚拟形象生成领域取得了重要进展。它成功地将2D视频生成中的先进技术（如VASA-1的运动潜在空间）迁移到3D领域，并解决了从单张图像生成高质量3D模型的关键难题。通过引入创新的双重形变机制和鲁棒的训练策略，该方法显著提升了3D头部虚拟形象的真实感、表情细节和动画驱动能力，为实现更具沉浸感和交互性的虚拟体验铺平了道路。其单图像输入和实时生成能力使其在虚拟现实、远程会议、游戏等领域具有广泛的应用前景。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose VASA-3D, an audio-driven, single-shot 3D head avatar generator.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.14677v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.14677v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.14671v1'></a></p>
<h2 id="art-articulated-reconstruction-transformer"><a href="https://arxiv.org/abs/2512.14671v1">ART: Articulated Reconstruction Transformer</a></h2>
<p><strong>Authors:</strong> Zizhang Li, Cheng Zhang, Zhengqin Li, Henry Howard-Jenkins, Zhaoyang Lv, Chen Geng, Jiajun Wu, Richard Newcombe, Jakob Engel, Zhao Dong</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We introduce ART, Articulated Reconstruction Transformer -- a category-agnostic, feed-forward model that reconstructs complete 3D articulated objects from only sparse, multi-state RGB images. Previous methods for articulated object reconstruction either rely on slow optimization with fragile cross-state correspondences or use feed-forward models limited to specific object categories. In contrast, ART treats articulated objects as assemblies of rigid parts, formulating reconstruction as part-based prediction. Our newly designed transformer architecture maps sparse image inputs to a set of learnable part slots, from which ART jointly decodes unified representations for individual parts, including their 3D geometry, texture, and explicit articulation parameters. The resulting reconstructions are physically interpretable and readily exportable for simulation. Trained on a large-scale, diverse dataset with per-part supervision, and evaluated across diverse benchmarks, ART achieves significant improvements over existing baselines and establishes a new state of the art for articulated object reconstruction from image inputs.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“ART: Articulated Reconstruction Transformer”的全面中文摘要：</p>
<p><strong>论文题目：</strong> ART: Articulated Reconstruction Transformer</p>
<p><strong>作者：</strong> Zizhang Li, Cheng Zhang, Zhengqin Li, Henry Howard-Jenkins, Zhaoyang Lv, Chen Geng, Jiajun Wu, Richard Newcombe, Jakob Engel, Zhao Dong</p>
<p><strong>摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
该论文旨在解决从稀疏、多状态的RGB图像中重建完整3D关节式物体（articulated objects）的挑战。关节式物体在日常生活中无处不在，准确地重建它们的数字模型对于VR/AR、机器人和具身AI至关重要。现有方法要么依赖于缓慢且脆弱的优化过程，要么受限于特定物体类别，难以泛化到多样化的、未见过的物体。尤其是在输入数据稀疏的情况下，准确推断出复杂的3D形状、材质和运动结构是一个重大难题。</p>
<p><strong>2. 主要创新点/方法贡献：</strong>
*   <strong>ART模型：</strong> 提出了一种名为ART（Articulated Reconstruction Transformer）的类别无关、前馈模型。ART将关节式物体视为刚性部件的集合，并将重建任务转化为基于部件的预测。
*   <strong>Transformer架构：</strong> ART采用新设计的Transformer架构，将稀疏的图像输入映射到一组可学习的“部件槽”（part slots）。每个部件槽负责捕获物体的一个特定部件。
*   <strong>联合解码：</strong> ART能够从每个部件槽中联合解码出部件的统一表示，包括其3D几何、纹理以及显式的关节参数（如运动类型、轴、枢轴点和运动范围）。
*   <strong>部件级预测：</strong> 核心思想是将关节式物体的重建分解为对每个独立部件的几何、纹理和运动参数的预测，并利用部件级监督进行训练。
*   <strong>规范化“静止状态”（Rest State）：</strong> 采用一个固定的、预定义的“静止状态”作为所有关节参数的参考系，这有助于提高训练的稳定性和收敛速度，并确保跨序列的一致性。
*   <strong>大规模、多样化数据集：</strong> 通过整合来自PartNet-Mobility、程序化生成数据集和StorageFurniture数据集的3D模型，构建了一个大规模、多样化的数据集，用于训练ART模型。</p>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>性能提升：</strong> ART在多个基准测试中显著优于现有的前馈和优化方法，在关节式物体重建领域达到了新的SOTA（state-of-the-art）水平。
*   <strong>高保真度：</strong> ART能够从稀疏的输入中重建出物理上可解释且具有高保真度几何和纹理的3D关节式物体。
*   <strong>可导出性：</strong> 重建结果可以直接导出为URDF格式，方便在模拟器中使用，为下游应用（如机器人控制、VR/AR内容创建）提供了便利。
*   <strong>泛化能力：</strong> 类别无关的设计和大规模数据集的训练使得ART能够泛化到各种未见过的物体类别。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>已知部件数量：</strong> ART模型假设目标物体具有已知的部件数量，并且依赖于预先校准的相机位姿。
*   <strong>相机位姿：</strong> 模型需要预先知道相机的内参和外参。
*   <strong>数据依赖：</strong> 尽管ART在数据量和多样性方面有所突破，但仍依赖于高质量的3D数据集。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>学习无位姿（Pose-free）的变体：</strong> 开发能够处理自校准相机（self-calibrated cameras）的模型。
*   <strong>集成部件数量估计：</strong> 将部件数量的估计直接集成到模型中，使其能够处理未知部件数量的物体。
*   <strong>更大规模的数据集：</strong> 利用更大规模的数据集进一步提升模型的性能和泛化能力。</p>
<p><strong>总结：</strong>
论文“ART: Articulated Reconstruction Transformer”提出了一种创新的前馈模型，通过将关节式物体的重建任务分解为基于部件的预测，并利用Transformer架构联合解码部件的几何、纹理和运动参数，成功地解决了从稀疏多视角图像中重建高保真度3D关节式物体的难题。ART的贡献在于其新颖的模型架构、部件级预测范式、规范化静止状态的处理以及大规模多样化数据集的构建，这些都使其在性能上超越了现有方法，并为关节式物体重建领域开辟了新的可能性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce ART, Articulated Reconstruction Transformer -- a category-agnostic, feed-forward model that reconstructs complete 3D articulated objects from only sparse, multi-state RGB images.</li>
<li>Our newly designed transformer architecture maps sparse image inputs to a set of learnable part slots, from which ART jointly decodes unified representations for individual parts, including their 3D geometry, texture, and explicit articulation parameters.</li>
<li>Trained on a large-scale, diverse dataset with per-part supervision, and evaluated across diverse benchmarks, ART achieves significant improvements over existing baselines and establishes a new state of the art for articulated object reconstruction from image inputs.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.14671v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.14671v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.14666v1'></a></p>
<h2 id="evolve-vla-test-time-training-from-environment-feedback-for-vision-language-action-models"><a href="https://arxiv.org/abs/2512.14666v1">EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models</a></h2>
<p><strong>Authors:</strong> Zechen Bai, Chen Gao, Mike Zheng Shou</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Categories:</strong> cs.RO, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Achieving truly adaptive embodied intelligence requires agents that learn not just by imitating static demonstrations, but by continuously improving through environmental interaction, which is akin to how humans master skills through practice. Vision-Language-Action (VLA) models have advanced robotic manipulation by leveraging large language models, yet remain fundamentally limited by Supervised Finetuning (SFT): requiring hundreds of demonstrations per task, rigidly memorizing trajectories, and failing to adapt when deployment conditions deviate from training. We introduce EVOLVE-VLA, a test-time training framework enabling VLAs to continuously adapt through environment interaction with minimal or zero task-specific demonstrations. The key technical challenge is replacing oracle reward signals (unavailable at test time) with autonomous feedback. We address this through a learned progress estimator providing dense feedback, and critically, we design our framework to ``tame'' this inherently noisy signal via two mechanisms: (1) an accumulative progress estimation mechanism smoothing noisy point-wise estimates, and (2) a progressive horizon extension strategy enabling gradual policy evolution. EVOLVE-VLA achieves substantial gains: +8.6\% on long-horizon tasks, +22.0\% in 1-shot learning, and enables cross-task generalization -- achieving 20.8\% success on unseen tasks without task-specific demonstrations training (vs. 0\% for pure SFT). Qualitative analysis reveals emergent capabilities absent in demonstrations, including error recovery and novel strategies. This work represents a critical step toward VLAs that truly learn and adapt, moving beyond static imitation toward continuous self-improvements.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models”的全面中文摘要：</p>
<p><strong>论文题目：</strong> EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models</p>
<p><strong>作者：</strong> Zechen Bai, Chen Gao, Mike Zheng Shou</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
该论文旨在解决当前视觉-语言-动作（VLA）模型在机器人操作领域面临的核心问题：<strong>静态模仿学习的局限性</strong>。传统的监督微调（SFT）方法需要大量的任务演示数据，导致成本高昂，并且模型容易僵化，无法适应部署环境中与训练数据分布不同的情况。模型在遇到执行偏差时，往往会完全失败，缺乏纠错和泛化能力。因此，研究的核心问题是如何使VLA模型能够<strong>在部署时通过与环境的持续交互进行自适应学习</strong>，从而克服数据依赖和泛化能力不足的缺点。</p>
<p><strong>2. 主要创新点/方法贡献：</strong>
作者提出了<strong>EVOLVE-VLA</strong>，一个<strong>测试时训练（Test-Time Training, TTT）框架</strong>，使VLA模型能够在部署环境中持续学习和适应，而<strong>仅需极少甚至零任务特定的演示数据</strong>。其关键创新在于：</p>
<ul>
<li><strong>替代预言机奖励：</strong> 解决了测试时无法获得预言机奖励（如成功信号）的难题，通过引入一个<strong>学习到的任务进度估计器</strong>来提供密集反馈。</li>
<li><strong>“驯服”噪声信号：</strong> 针对进度估计器固有的噪声问题，提出了两个核心机制：<ul>
<li><strong>累积进度估计机制：</strong> 通过间隔采样和递归累积，平滑了点状的噪声估计，生成稳定可靠的进度信号。</li>
<li><strong>渐进式视野扩展策略：</strong> 将训练过程划分为多个阶段，逐步增加最大回滚（rollout）视野，使模型能够先掌握简单的子任务，再逐步应对复杂任务，从而提高对估计误差的鲁棒性。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
EVOLVE-VLA在LIBERO基准测试中取得了显著的性能提升：</p>
<ul>
<li><strong>整体性能提升：</strong> 在长时域任务上提升了+8.6%，在1次演示学习（1-shot learning）场景下提升了+22.0%。</li>
<li><strong>零样本跨任务泛化：</strong> 首次实现了在没有任务特定演示数据的情况下，模型能够通过TTT适应并完成未见过的任务，成功率达到20.8%（纯SFT模型为0%）。</li>
<li><strong>涌现能力：</strong> 定性分析表明，模型展现出了在演示数据中未出现过的能力，如<strong>错误恢复</strong>和<strong>发现新颖策略</strong>。</li>
</ul>
<p>这些结果表明，TTT是一种<strong>范式转变</strong>，能够使VLA模型真正实现<strong>学习和适应</strong>，从静态模仿转向持续的自我改进，为构建更通用的具身智能体奠定了基础。</p>
<p><strong>4. 论文中提到的局限性：</strong>
论文中提到一个潜在的挑战是<strong>环境的规则型成功标准与进度估计器评估的语义任务完成度之间的不匹配</strong>。这可能导致“奖励黑客”（reward hacking）现象，即模型为了最大化进度分数而优化，但并未真正满足环境的严格坐标或规则要求。反之亦然，环境可能基于坐标规则判定任务成功，但语义上并不完整。这表明需要改进进度估计器与环境真实成功标准的校准。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
作者提出了几个未来的研究方向：</p>
<ul>
<li><strong>更鲁棒的奖励模型：</strong> 开发与环境成功标准更语义对齐的奖励模型，以减少进度估计与真实成功之间的不匹配。</li>
<li><strong>提升零样本能力：</strong> 进一步消除对上下文示例的需求，实现真正的零样本跨任务泛化，甚至使奖励模型本身也具备更好的泛化能力。</li>
<li><strong>真实世界部署：</strong> 解决真实世界机器人部署中的挑战，如<strong>加速训练时间</strong>（通过sim-to-real、并行部署等）、<strong>确保探索安全性</strong>（通过动作约束、安全批评家等）以及<strong>更高效的在线学习算法</strong>。</li>
<li><strong>更复杂的探索策略和课程设计：</strong> 探索更先进的探索策略和课程设计，以提高样本效率并适应更复杂、长时域的任务。</li>
</ul>
<p>总而言之，EVOLVE-VLA论文提出了一种创新的测试时训练框架，通过引入自适应学习机制，显著克服了现有VLA模型的局限性，为实现更智能、更通用的机器人智能体开辟了新的道路。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce EVOLVE-VLA, a test-time training framework enabling VLAs to continuously adapt through environment interaction with minimal or zero task-specific demonstrations.</li>
<li>Qualitative analysis reveals emergent capabilities absent in demonstrations, including error recovery and novel strategies.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.14666v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.14666v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-12-17 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
