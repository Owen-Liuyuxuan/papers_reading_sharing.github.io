<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-09-30 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-15
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-09-29/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-10-01/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-09-30">Arxiv Computer Vision Papers - 2025-09-30</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#hunyuanimage-30-technical-report" class="nav-link">HunyuanImage 3.0 Technical Report</a>
                </li>
                <li class="nav-item">
                    <a href="#fast-feature-field-textf3-a-predictive-representation-of-events" class="nav-link">Fast Feature Field (\text{F}^3): A Predictive Representation of Events</a>
                </li>
                <li class="nav-item">
                    <a href="#score-distillation-of-flow-matching-models" class="nav-link">Score Distillation of Flow Matching Models</a>
                </li>
                <li class="nav-item">
                    <a href="#triangle-splatting-differentiable-rendering-with-opaque-triangles" class="nav-link">Triangle Splatting+: Differentiable Rendering with Opaque Triangles</a>
                </li>
                <li class="nav-item">
                    <a href="#unsupervised-representation-learning-for-3d-mesh-parameterization-with-semantic-and-visibility-objectives" class="nav-link">Unsupervised Representation Learning for 3D Mesh Parameterization with Semantic and Visibility Objectives</a>
                </li>
                <li class="nav-item">
                    <a href="#unilat3d-geometry-appearance-unified-latents-for-single-stage-3d-generation" class="nav-link">UniLat3D: Geometry-Appearance Unified Latents for Single-Stage 3D Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#bridge-building-reinforcement-learning-depth-to-image-data-generation-engine-for-monocular-depth-estimation" class="nav-link">BRIDGE -- Building Reinforcement-Learning Depth-to-Image Data Generation Engine for Monocular Depth Estimation</a>
                </li>
                <li class="nav-item">
                    <a href="#chargen-fast-and-fluent-portrait-modification" class="nav-link">CharGen: Fast and Fluent Portrait Modification</a>
                </li>
                <li class="nav-item">
                    <a href="#vt-fsl-bridging-vision-and-text-with-llms-for-few-shot-learning" class="nav-link">VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning</a>
                </li>
                <li class="nav-item">
                    <a href="#airoa-moma-dataset-a-large-scale-hierarchical-dataset-for-mobile-manipulation" class="nav-link">AIRoA MoMa Dataset: A Large-Scale Hierarchical Dataset for Mobile Manipulation</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-09-30">Arxiv Computer Vision Papers - 2025-09-30</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，这是一份针对2025年9月28日Arxiv计算机视觉论文的每日报告执行摘要，旨在帮助忙碌的研究人员快速了解最新进展：</p>
<hr />
<p><strong>Arxiv 计算机视觉每日报告执行摘要 (2025-09-28)</strong></p>
<p><strong>概述与主要趋势：</strong></p>
<p>今天的论文集呈现出计算机视觉领域几个关键趋势的持续深化和融合。<strong>生成式AI</strong> 仍然是核心焦点，尤其是在图像生成、3D内容创建和多模态交互方面。<strong>3D表示与生成</strong> 取得了显著进展，从新的渲染技术到统一的3D潜在空间。此外，<strong>效率和实用性</strong> 成为重要考量，体现在对快速模型、无监督学习和实际应用（如机器人操作和肖像修改）的关注。<strong>多模态学习</strong>，特别是结合大型语言模型（LLMs）进行少样本学习，也显示出其日益增长的重要性。</p>
<p><strong>特别重要或创新的论文：</strong></p>
<ul>
<li><strong>"HunyuanImage 3.0 Technical Report" (Siyu Cao et al.)</strong>: 作为大型图像生成模型的技术报告，这篇论文通常代表了该领域最前沿的进展，可能包含新的架构、训练策略或性能突破，对理解生成式AI的未来方向至关重要。</li>
<li><strong>"UniLat3D: Geometry-Appearance Unified Latents for Single-Stage 3D Generation" (Guanjun Wu et al.)</strong>: 这篇论文通过统一几何和外观的潜在空间实现单阶段3D生成，代表了3D内容创建效率和质量的重大飞跃，有望简化3D资产生成流程。</li>
<li><strong>"Score Distillation of Flow Matching Models" (Mingyuan Zhou et al.)</strong>: 结合了流匹配模型和分数蒸馏，这可能为生成模型提供新的训练范式，提升生成质量和效率，对扩散模型和生成式AI研究者具有重要意义。</li>
<li><strong>"VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning" (Wenhao Li et al.)</strong>: 利用LLMs连接视觉和文本进行少样本学习，这代表了多模态AI在解决数据稀缺问题上的一个强大方向，预示着LLMs在更广泛视觉任务中的应用潜力。</li>
</ul>
<p><strong>新兴研究方向或技术：</strong></p>
<ul>
<li><strong>统一的3D潜在空间 (Unified 3D Latents)</strong>：如UniLat3D所示，将几何和外观信息整合到单一潜在表示中，是实现高效、高质量3D生成的重要方向。</li>
<li><strong>事件相机数据的新型表示 (Predictive Representation of Events)</strong>：F3模型为事件相机数据提供了一种预测性表示，这对于处理高动态、低延迟视觉信息至关重要，可能在自动驾驶和机器人领域有广泛应用。</li>
<li><strong>基于强化学习的数据生成 (RL-based Data Generation)</strong>：BRIDGE项目利用RL生成深度估计数据，为解决特定任务的数据稀缺问题提供了新的思路。</li>
<li><strong>LLMs在视觉任务中的深度融合 (Deep Integration of LLMs in Vision)</strong>：VT-FSL展示了LLMs不仅作为文本理解工具，还能作为连接不同模态、赋能复杂视觉任务（如少样本学习）的强大桥梁。</li>
<li><strong>可微分渲染的实用化 (Practical Differentiable Rendering)</strong>：Triangle Splatting+通过不透明三角形实现可微分渲染，提高了渲染效率和实用性，对NeRFs和3D重建等领域有益。</li>
</ul>
<p><strong>建议阅读全文的论文：</strong></p>
<p>对于希望深入了解特定领域的忙碌研究人员，我建议优先阅读以下论文：</p>
<ul>
<li><strong>对于生成式AI和大型模型感兴趣的：</strong><ul>
<li>"HunyuanImage 3.0 Technical Report"</li>
<li>"Score Distillation of Flow Matching Models"</li>
</ul>
</li>
<li><strong>对于3D视觉和内容生成感兴趣的：</strong><ul>
<li>"UniLat3D: Geometry-Appearance Unified Latents for Single-Stage 3D Generation"</li>
<li>"Triangle Splatting+: Differentiable Rendering with Opaque Triangles"</li>
</ul>
</li>
<li><strong>对于多模态学习和LLMs应用感兴趣的：</strong><ul>
<li>"VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning"</li>
</ul>
</li>
<li><strong>对于机器人和实际应用感兴趣的：</strong><ul>
<li>"AIRoA MoMa Dataset: A Large-Scale Hierarchical Dataset for Mobile Manipulation"</li>
<li>"BRIDGE -- Building Reinforcement-Learning Depth-to-Image Data Generation Engine for Monocular Depth Estimation"</li>
</ul>
</li>
</ul>
<p>这份摘要旨在提供一个高层次的概览，帮助您快速识别与您研究兴趣最相关的最新进展。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2509.23951v1">HunyuanImage 3.0 Technical Report</a></li>
<li><a href="#2509.25146v1">Fast Feature Field (<script type="math/tex">\text{F}^3</script>): A Predictive Representation of Events</a></li>
<li><a href="#2509.25127v1">Score Distillation of Flow Matching Models</a></li>
<li><a href="#2509.25122v1">Triangle Splatting+: Differentiable Rendering with Opaque Triangles</a></li>
<li><a href="#2509.25094v1">Unsupervised Representation Learning for 3D Mesh Parameterization with Semantic and Visibility Objectives</a></li>
<li><a href="#2509.25079v1">UniLat3D: Geometry-Appearance Unified Latents for Single-Stage 3D Generation</a></li>
<li><a href="#2509.25077v1">BRIDGE -- Building Reinforcement-Learning Depth-to-Image Data Generation Engine for Monocular Depth Estimation</a></li>
<li><a href="#2509.25058v1">CharGen: Fast and Fluent Portrait Modification</a></li>
<li><a href="#2509.25033v1">VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning</a></li>
<li><a href="#2509.25032v1">AIRoA MoMa Dataset: A Large-Scale Hierarchical Dataset for Mobile Manipulation</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2509.23951v1'></a></p>
<h2 id="hunyuanimage-30-technical-report"><a href="https://arxiv.org/abs/2509.23951v1">HunyuanImage 3.0 Technical Report</a></h2>
<p><strong>Authors:</strong> Siyu Cao, Hangting Chen, Peng Chen, Yiji Cheng, Yutao Cui, Xinchi Deng, Ying Dong, Kipper Gong, Tianpeng Gu, Xiusen Gu, Tiankai Hang, Duojun Huang, Jie Jiang, Zhengkai Jiang, Weijie Kong, Changlin Li, Donghao Li, Junzhe Li, Xin Li, Yang Li, Zhenxi Li, Zhimin Li, Jiaxin Lin, Linus, Lucaz Liu, Shu Liu, Songtao Liu, Yu Liu, Yuhong Liu, Yanxin Long, Fanbin Lu, Qinglin Lu, Yuyang Peng, Yuanbo Peng, Xiangwei Shen, Yixuan Shi, Jiale Tao, Yangyu Tao, Qi Tian, Pengfei Wan, Chunyu Wang, Kai Wang, Lei Wang, Linqing Wang, Lucas Wang, Qixun Wang, Weiyan Wang, Hao Wen, Bing Wu, Jianbing Wu, Yue Wu, Senhao Xie, Fang Yang, Miles Yang, Xiaofeng Yang, Xuan Yang, Zhantao Yang, Jingmiao Yu, Zheng Yuan, Chao Zhang, Jian-Wei Zhang, Peizhen Zhang, Shi-Xue Zhang, Tao Zhang, Weigang Zhang, Yepeng Zhang, Yingfang Zhang, Zihao Zhang, Zijian Zhang, Penghao Zhao, Zhiyuan Zhao, Xuefei Zhe, Jianchen Zhu, Zhao Zhong</p>
<p><strong>Published:</strong> 2025-09-28</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We present HunyuanImage 3.0, a native multimodal model that unifies
multimodal understanding and generation within an autoregressive framework,
with its image generation module publicly available. The achievement of
HunyuanImage 3.0 relies on several key components, including meticulous data
curation, advanced architecture design, a native Chain-of-Thoughts schema,
progressive model pre-training, aggressive model post-training, and an
efficient infrastructure that enables large-scale training and inference. With
these advancements, we successfully trained a Mixture-of-Experts (MoE) model
comprising over 80 billion parameters in total, with 13 billion parameters
activated per token during inference, making it the largest and most powerful
open-source image generative model to date. We conducted extensive experiments
and the results of automatic and human evaluation of text-image alignment and
visual quality demonstrate that HunyuanImage 3.0 rivals previous
state-of-the-art models. By releasing the code and weights of HunyuanImage 3.0,
we aim to enable the community to explore new ideas with a state-of-the-art
foundation model, fostering a dynamic and vibrant multimodal ecosystem. All
open source assets are publicly available at
https://github.com/Tencent-Hunyuan/HunyuanImage-3.0</p>
<p><strong>Analysis:</strong></p>
<p>以下是“HunyuanImage 3.0 Technical Report”的摘要：</p>
<p><strong>1. 主要问题或研究问题</strong>
该论文旨在解决现有图像生成模型在透明度、可复现性以及在复杂文本指令遵循、推理和概念泛化方面的局限性。特别是，许多最先进的图像生成系统是闭源的，限制了研究社区的进一步探索。HunyuanImage 3.0的目标是开发一个开源的多模态模型，该模型在图像生成性能上可与领先的闭源模型媲美或超越，并能统一多模态理解和生成。</p>
<p><strong>2. 关键创新或方法论贡献</strong>
*   <strong>统一多模态框架：</strong> HunyuanImage 3.0是一个原生的多模态模型，在一个自回归框架内统一了多模态理解和生成，其图像生成模块已公开。
*   <strong>大规模MoE架构：</strong> 该模型基于Hunyuan-A13B，一个预训练的MoE大型语言模型（LLM），总参数超过800亿，推理时每token激活130亿参数，使其成为迄今为止最大、最强大的开源图像生成模型。
*   <strong>数据处理：</strong> 采用了细致的数据整理流程，包括三阶段过滤（从100亿原始图像中保留不到45%）、双语分层标注方案（支持短到超长的描述、风格属性和事实实体），以及通过专门代理和双向验证进行事实基础化。
*   <strong>推理数据集构建：</strong> 引入了自动化的思维链（CoT）推理过程，通过文本到文本（T2T）和文本到图像（T2TI）推理数据进行微调，以增强模型的逻辑推理和视觉表现能力。
*   <strong>广义因果注意力机制：</strong> 整合了文本和图像模态的注意力机制，确保文本token只关注前序token，而图像token可以关注同一图像段内的所有前序和后序图像token，以处理异构数据模态。
*   <strong>广义2D旋转位置嵌入（RoPE）：</strong> 扩展了RoPE以支持图像token的2D位置编码，同时保持与传统文本生成和预训练LLM的兼容性。
*   <strong>自动分辨率：</strong> 模型能够根据上下文自动确定图像的尺寸和宽高比，通过特殊的token来指导生成具有所需结构属性的图像。
*   <strong>多阶段训练策略：</strong> 采用渐进式预训练（包括T2I、LM、MMU、INTL和CoT任务），以及多阶段后训练优化（SFT、DPO、MixGRPO、SRPO和ReDA），以系统地提升生成能力、减少物理失真、增强文本-图像对齐和视觉质量。</p>
<p><strong>3. 主要结果及其意义</strong>
*   <strong>领先的图像生成性能：</strong> 自动和人工评估的文本-图像对齐和视觉质量结果表明，HunyuanImage 3.0与之前的最先进模型（如Seedream 4.0、Nano Banana、GPT-Image和HunyuanImage 2.1）相比，性能相当或超越。
*   <strong>开源影响力：</strong> 作为迄今为止最大、最强大的开源图像生成模型，其代码和权重（在GitHub上公开）的发布旨在促进社区探索新想法，推动多模态生态系统的发展。
*   <strong>SSAE评估：</strong> 在结构化语义对齐评估（SSAE）指标上，HunyuanImage 3.0在所有细粒度字段中都达到了与领先模型相当的性能。
*   <strong>GSB评估：</strong> 在GSB（Good/Same/Bad）评估中，HunyuanImage 3.0相对于HunyuanImage 2.1取得了14.10%的相对胜率，相对于Seedream 4.0、Nano Banana和GPT-Image也取得了显著胜率，表明其图像生成质量可与领先的闭源商业模型媲美。
*   <strong>专家激活分析：</strong> 专家模态偏好热图和KL散度分析表明，MoE中的专家在不同模态上变得越来越专业化，这表明MoE可以通过将不同模态的责任分散给专业化专家来增强多模态建模。</p>
<p><strong>4. 论文中提到的局限性</strong>
*   <strong>当前发布范围：</strong> 本次发布仅包含文本到图像（T2I）的能力，图像到图像（I2I）任务的训练仍在进行中，未来才会发布。
*   <strong>AIGC对数据分布的影响：</strong> AIGC图像的扩散通过扭曲自然数据分布和损害模型收敛性带来了重大挑战，尽管论文中提到了缓解策略。</p>
<p><strong>5. 潜在的未来研究方向</strong>
*   <strong>图像到图像任务：</strong> 论文明确指出，图像到图像任务的训练正在进行中，未来将发布此功能，这将是模型能力的重要扩展。
*   <strong>多模态生态系统探索：</strong> 通过发布开源模型，鼓励社区探索基于最先进基础模型的新想法，从而促进动态和充满活力的多模态生态系统。
*   <strong>更复杂的推理和泛化：</strong> 进一步探索和增强模型的思维链训练和推理能力，以处理更复杂、更细致的用户意图和场景。
*   <strong>数据策展和模型优化：</strong> 持续改进数据策展技术和模型优化策略，以进一步提升生成图像的真实感、清晰度和对人类偏好的对齐。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present HunyuanImage 3.0, a native multimodal model that unifies
multimodal understanding and generation within an autoregressive framework,
with its image generation module publicly available.</li>
<li>We conducted extensive experiments
and the results of automatic and human evaluation of text-image alignment and
visual quality demonstrate that HunyuanImage 3.0 rivals previous
state-of-the-art models.</li>
<li>By releasing the code and weights of HunyuanImage 3.0,
we aim to enable the community to explore new ideas with a state-of-the-art
foundation model, fostering a dynamic and vibrant multimodal ecosystem.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.23951v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.23951v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.25146v1'></a></p>
<h2 id="fast-feature-field-textf3-a-predictive-representation-of-events"><a href="https://arxiv.org/abs/2509.25146v1">Fast Feature Field (<script type="math/tex">\text{F}^3</script>): A Predictive Representation of Events</a></h2>
<p><strong>Authors:</strong> Richeek Das, Kostas Daniilidis, Pratik Chaudhari</p>
<p><strong>Published:</strong> 2025-09-29</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>This paper develops a mathematical argument and algorithms for building
representations of data from event-based cameras, that we call Fast Feature
Field (<script type="math/tex">\text{F}^3</script>). We learn this representation by predicting future events
from past events and show that it preserves scene structure and motion
information. <script type="math/tex">\text{F}^3</script> exploits the sparsity of event data and is robust to
noise and variations in event rates. It can be computed efficiently using ideas
from multi-resolution hash encoding and deep sets - achieving 120 Hz at HD and
440 Hz at VGA resolutions. <script type="math/tex">\text{F}^3</script> represents events within a contiguous
spatiotemporal volume as a multi-channel image, enabling a range of downstream
tasks. We obtain state-of-the-art performance on optical flow estimation,
semantic segmentation, and monocular metric depth estimation, on data from
three robotic platforms (a car, a quadruped robot and a flying platform),
across different lighting conditions (daytime, nighttime), environments
(indoors, outdoors, urban, as well as off-road) and dynamic vision sensors
(resolutions and event rates). Our implementations can predict these tasks at
25-75 Hz at HD resolution.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Richeek Das, Kostas Daniilidis, Pratik Chaudhari撰写的论文“Fast Feature Field (<script type="math/tex">\text{F}^3</script>): A Predictive Representation of Events”的全面摘要。</p>
<hr />
<p><strong>论文摘要：Fast Feature Field (<script type="math/tex">\text{F}^3</script>): 一种事件的预测性表示</strong></p>
<p>这篇论文介绍了Fast Feature Field (<script type="math/tex">\text{F}^3</script>)，这是一种从事件相机数据构建高效表示的新方法。事件相机因其高动态范围、高时间分辨率和低功耗而备受关注，但其原始数据固有的稀疏性、噪声和异步性给下游任务带来了挑战。</p>
<p><strong>1. 解决的主要问题或研究问题：</strong>
该研究旨在解决事件相机数据表示的根本性问题。传统的基于帧的计算机视觉算法难以直接处理事件数据，因为其稀疏、异步和噪声特性。论文的核心目标是开发一种能够有效捕捉场景结构和运动信息、对噪声和事件速率变化具有鲁棒性，并能高效计算的事件数据表示，从而为各种下游计算机视觉任务提供支持。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>预测性表示的学习：</strong> <script type="math/tex">\text{F}^3</script> 的核心创新在于它被学习为过去事件的统计量，足以预测未来事件。论文从数学上论证了这种预测性表示能够保留场景的结构和运动信息。
*   <strong>多分辨率哈希编码与深度集架构：</strong> 为了高效计算并利用事件数据的稀疏性，<script type="math/tex">\text{F}^3</script> 采用了多分辨率哈希编码（类似于神经渲染领域）和置换不变的深度集（Deep Set）架构。哈希编码将事件坐标映射到特征空间，并通过池化和卷积操作进行时间聚合和空间平滑。
*   <strong>鲁棒性训练目标：</strong> 针对事件数据的噪声和事件速率的极端不平衡，论文采用了一种加权Focal Loss变体作为训练目标，而非传统的均方误差。这使得 <script type="math/tex">\text{F}^3</script> 对噪声和事件速率变化具有更强的鲁棒性，并有助于防止特征坍塌。
*   <strong>多通道图像表示：</strong> <script type="math/tex">\text{F}^3</script> 将事件表示为一个连续时空体积内的多通道图像，使其能够无缝集成到任何标准计算机视觉算法和基于RGB数据的神经网络架构中。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>最先进的性能：</strong> <script type="math/tex">\text{F}^3</script> 在多个下游任务（光流估计、语义分割和单目深度估计）上取得了最先进的性能，显著优于现有方法。
*   <strong>高效计算：</strong> <script type="math/tex">\text{F}^3</script> 的计算效率极高，在HD分辨率下达到120 Hz，在VGA分辨率下达到440 Hz。基于<script type="math/tex">\text{F}^3</script> 的下游任务预测在HD分辨率下也能达到25-75 Hz，比现有最先进的事件基方法快2-5倍。
*   <strong>强大的泛化能力：</strong> <script type="math/tex">\text{F}^3</script> 在不同机器人平台（汽车、四足机器人、飞行平台）、不同光照条件（白天、夜晚）和不同环境（室内、室外、城市、越野）下表现出强大的泛化能力，无需额外的训练。
*   <strong>对机器人感知的影响：</strong> <script type="math/tex">\text{F}^3</script> 为实时、可泛化的事件感知提供了基础，有望使机器人能够在各种具有挑战性的条件下更有效地运行。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>当前实现中的稠密卷积：</strong> 尽管论文强调了利用事件数据稀疏性的重要性，但目前的<script type="math/tex">\text{F}^3</script> 实现仍使用稠密2D卷积层，而非稀疏卷积。作者指出，这是因为PyTorch中稠密卷积目前比稀疏卷积稍快，但预计未来会改变。
*   <strong>对伪标签和同步的依赖：</strong> 在语义分割和深度估计等任务中，论文依赖于从RGB图像生成的伪标签和LiDAR数据。这要求RGB相机和事件相机之间的时间戳精确同步和外部校准，DSEC数据集中存在一些对齐问题。
*   <strong>未完全利用事件极性：</strong> 论文为了简化和降低计算成本，忽略了事件的极性信息，而仅关注事件的存在与否。虽然作者提到其技术可以适应包含极性的事件，但这会增加内存和计算开销。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>更全局的特征表示：</strong> 论文指出，<script type="math/tex">\text{F}^3</script> 架构可以扩展以构建更全局的特征（例如，通过更多的卷积层和预测更大的未来补丁），从而产生更丰富的语义特征，类似于视觉皮层。
*   <strong>增量式更新：</strong> <script type="math/tex">\text{F}^3</script> 可以通过一些簿记操作在每个事件之后进行增量式更新，这对于光流和深度估计等任务非常有用。
*   <strong>稀疏卷积的优化：</strong> 随着稀疏卷积在硬件和软件层面得到更好的支持，基于稀疏卷积的<script type="math/tex">\text{F}^3</script> 实现有望在事件数量非常少（例如，极端低光照条件）的场景中表现更快。
*   <strong>跨模态数据池化：</strong> 作者暗示，将来自不同机器人平台和环境的数据进行池化，将进一步提高<script type="math/tex">\text{F}^3</script> 方法的鲁棒性。
*   <strong>神经形态计算和ASIC的应用：</strong> 将<script type="math/tex">\text{F}^3</script> 与神经形态计算或直接在像素上执行计算的ASIC结合，可以进一步提高实际性能和能源效率。</p>
<hr />
<p>总而言之，这篇论文通过引入Fast Feature Field (<script type="math/tex">\text{F}^3</script>)，为事件相机数据处理领域做出了重大贡献。它提供了一种数学上合理、计算高效且对噪声和事件速率鲁棒的事件表示，为事件相机在各种机器人感知任务中的广泛应用铺平了道路。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We obtain state-of-the-art performance on optical flow estimation,
semantic segmentation, and monocular metric depth estimation, on data from
three robotic platforms (a car, a quadruped robot and a flying platform),
across different lighting conditions (daytime, nighttime), environments
(indoors, outdoors, urban, as well as off-road) and dynamic vision sensors
(resolutions and event rates).</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.25146v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.25146v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.25127v1'></a></p>
<h2 id="score-distillation-of-flow-matching-models"><a href="https://arxiv.org/abs/2509.25127v1">Score Distillation of Flow Matching Models</a></h2>
<p><strong>Authors:</strong> Mingyuan Zhou, Yi Gu, Huangjie Zheng, Liangchen Song, Guande He, Yizhe Zhang, Wenze Hu, Yinfei Yang</p>
<p><strong>Published:</strong> 2025-09-29</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Diffusion models achieve high-quality image generation but are limited by
slow iterative sampling. Distillation methods alleviate this by enabling one-
or few-step generation. Flow matching, originally introduced as a distinct
framework, has since been shown to be theoretically equivalent to diffusion
under Gaussian assumptions, raising the question of whether distillation
techniques such as score distillation transfer directly. We provide a simple
derivation -- based on Bayes' rule and conditional expectations -- that unifies
Gaussian diffusion and flow matching without relying on ODE/SDE formulations.
Building on this view, we extend Score identity Distillation (SiD) to
pretrained text-to-image flow-matching models, including SANA, SD3-Medium,
SD3.5-Medium/Large, and FLUX.1-dev, all with DiT backbones. Experiments show
that, with only modest flow-matching- and DiT-specific adjustments, SiD works
out of the box across these models, in both data-free and data-aided settings,
without requiring teacher finetuning or architectural changes. This provides
the first systematic evidence that score distillation applies broadly to
text-to-image flow matching models, resolving prior concerns about stability
and soundness and unifying acceleration techniques across diffusion- and
flow-based generators. We will make the PyTorch implementation publicly
available.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Mingyuan Zhou等人撰写的论文“Score Distillation of Flow Matching Models”的全面摘要。</p>
<hr />
<h3 id="score-distillation-of-flow-matching-models_1">论文摘要：“Score Distillation of Flow Matching Models”</h3>
<p><strong>1. 解决的主要问题或研究问题：</strong>
扩散模型在图像生成方面表现出色，但其迭代采样过程缓慢，限制了实际应用。蒸馏方法旨在通过实现一步或几步生成来加速这一过程。流匹配（Flow Matching）作为一种独立的框架被引入，后来被证明在高斯假设下与扩散模型理论等价。这引发了一个关键问题：扩散模型中使用的蒸馏技术（特别是分数蒸馏）是否可以直接且有效地应用于流匹配模型，以及在应用过程中是否需要进行模型特定的调整或教师模型微调。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>统一的理论视角：</strong> 论文通过基于贝叶斯规则和条件期望的简单推导，统一了高斯扩散和流匹配，而无需依赖复杂的ODE/SDE公式。这表明在理论上，两种框架在最佳解决方案上是等价的，主要区别在于时间步长的加权分布。
*   <strong>SiD-DiT框架的扩展与应用：</strong> 论文将分数同一性蒸馏（Score identity Distillation, SiD）方法扩展到预训练的文本到图像流匹配模型，这些模型均采用DiT（Diffusion Transformer）骨干网络，包括SANA、SD3-Medium、SD3.5-Medium/Large和FLUX.1-dev。
*   <strong>开箱即用（Out-of-the-box）的适用性：</strong> 实验证明，SiD-DiT只需对流匹配和DiT模型进行适度调整，即可在数据无关（data-free）和数据辅助（data-aided）设置下，无需教师模型微调或架构更改，直接应用于这些模型。
*   <strong>对抗性学习集成：</strong> 在数据辅助设置中，SiD通过在判别器特征中引入空间池化，将对抗性学习（Diffusion GAN）整合到DiT骨干网络中，进一步提升了性能，且未引入额外参数。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>广泛适用性：</strong> 论文首次系统性地证明了分数蒸馏可以广泛应用于文本到图像流匹配模型，解决了先前关于稳定性和合理性的担忧。
*   <strong>性能提升：</strong> SiD-DiT在数据无关设置下，在SANA-Sprint模型上持续优于SANA-Sprint，并在SD3系列模型上匹配或超越教师模型性能。在数据辅助设置下，通过对抗性学习，SiD2-DiT在FID（Fréchet Inception Distance）方面实现了显著降低，同时保持了CLIP和GenEval分数。
*   <strong>效率与鲁棒性：</strong> SiD-DiT框架在不同架构、噪声调度和模型规模的DiT流匹配模型上展现出高效性和鲁棒性，使用单一代码库和超参数配置即可实现。
*   <strong>统一加速技术：</strong> 论文统一了扩散和流匹配生成器中的加速技术，为未来研究提供了坚实的理论和经验基础。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>FLUX.1-DEV的性能差距：</strong> SiD-DiT在FLUX.1-DEV上的性能提升相对温和，部分原因归因于指导机制的不匹配（FLUX.1-DEV采用学习到的指导嵌入，而非传统的CFG）。
*   <strong>度量指标的解释：</strong> 论文指出，FID、CLIP和GenEval等度量指标在比较不同模型家族和规模时应谨慎解释，视觉检查可能与这些指标的结论不完全一致。
*   <strong>数据质量对对抗性学习的影响：</strong> 虽然对抗性学习可以增加样本多样性并改善FID，但如果使用的数据集质量有限（如MidJourney-v6-llava），可能无法显著提升视觉质量，且生成的图像风格可能不符合用户偏好。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>针对FLUX.1-DEV的定制化：</strong> 通过将学习到的指导嵌入集成到蒸馏目标中，或开发结合CFG和模型特定指导的混合方法，进一步优化SiD-DiT以适应FLUX.1-DEV的独特设计。
*   <strong>探索时间步长加权分布的影响：</strong> 对不同p(t)和wt如何影响性能进行系统性研究，以更好地理解和优化蒸馏过程。
*   <strong>统一生成建模和快速采样策略：</strong> 论文为未来在统一生成建模和快速采样策略方面的研究提供了理论和经验基础，鼓励进一步探索。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>This provides
the first systematic evidence that score distillation applies broadly to
text-to-image flow matching models, resolving prior concerns about stability
and soundness and unifying acceleration techniques across diffusion- and
flow-based generators.</li>
<li>We will make the PyTorch implementation publicly
available.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.25127v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.25127v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.25122v1'></a></p>
<h2 id="triangle-splatting-differentiable-rendering-with-opaque-triangles"><a href="https://arxiv.org/abs/2509.25122v1">Triangle Splatting+: Differentiable Rendering with Opaque Triangles</a></h2>
<p><strong>Authors:</strong> Jan Held, Renaud Vandeghen, Sanghyun Son, Daniel Rebain, Matheus Gadelha, Yi Zhou, Ming C. Lin, Marc Van Droogenbroeck, Andrea Tagliasacchi</p>
<p><strong>Published:</strong> 2025-09-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Reconstructing 3D scenes and synthesizing novel views has seen rapid progress
in recent years. Neural Radiance Fields demonstrated that continuous volumetric
radiance fields can achieve high-quality image synthesis, but their long
training and rendering times limit practicality. 3D Gaussian Splatting (3DGS)
addressed these issues by representing scenes with millions of Gaussians,
enabling real-time rendering and fast optimization. However, Gaussian
primitives are not natively compatible with the mesh-based pipelines used in VR
headsets, and real-time graphics applications. Existing solutions attempt to
convert Gaussians into meshes through post-processing or two-stage pipelines,
which increases complexity and degrades visual quality. In this work, we
introduce Triangle Splatting+, which directly optimizes triangles, the
fundamental primitive of computer graphics, within a differentiable splatting
framework. We formulate triangle parametrization to enable connectivity through
shared vertices, and we design a training strategy that enforces opaque
triangles. The final output is immediately usable in standard graphics engines
without post-processing. Experiments on the Mip-NeRF360 and Tanks &amp; Temples
datasets show that Triangle Splatting+achieves state-of-the-art performance in
mesh-based novel view synthesis. Our method surpasses prior splatting
approaches in visual fidelity while remaining efficient and fast to training.
Moreover, the resulting semi-connected meshes support downstream applications
such as physics-based simulation or interactive walkthroughs. The project page
is https://trianglesplatting2.github.io/trianglesplatting2/.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供这篇论文“Triangle Splatting+: Differentiable Rendering with Opaque Triangles”的全面摘要。</p>
<hr />
<h3 id="triangle-splatting-differentiable-rendering-with-opaque-triangles_1">论文摘要：Triangle Splatting+: Differentiable Rendering with Opaque Triangles</h3>
<p><strong>1. 主要问题或研究问题：</strong>
近年来，3D场景重建和新视角合成取得了显著进展，特别是神经辐射场（NeRF）和3D高斯泼溅（3DGS）等方法。NeRF提供了高质量的图像合成，但训练和渲染时间长；3DGS实现了实时渲染和快速优化，但其高斯基元与VR头显和实时图形应用中使用的基于网格的渲染管线不兼容。现有解决方案通过后处理或两阶段管线将高斯转换为网格，这增加了复杂性并降低了视觉质量。</p>
<p>本研究旨在解决的核心问题是：<strong>如何在保持高视觉质量、实时性能和与现有图形管线兼容性的前提下，直接优化计算机图形学中最基本的基元——不透明三角形，以实现3D场景重建和新视角合成？</strong></p>
<p><strong>2. 关键创新或方法论贡献：</strong>
Triangle Splatting+引入了一个可微分的泼溅框架，直接优化三角形，并提出了以下关键创新：</p>
<ul>
<li><strong>顶点共享的三角形参数化：</strong> 论文重新定义了三角形的参数化方式，通过共享顶点集实现三角形之间的连接性，而非像之前方法那样保持孤立。这使得三角形能够通过共同顶点自然连接，从而形成半连接网格，提高了结构一致性。</li>
<li><strong>强制不透明三角形的训练策略：</strong> 设计了一种定制的训练策略，在训练过程中逐步强制三角形变为完全不透明。这解决了以往泼溅方法中三角形可能保持半透明的问题，确保最终输出的网格可以直接导入标准图形引擎而无需后处理。</li>
<li><strong>结合视觉保真度和几何精度的训练策略：</strong> 该策略在训练初期允许三角形平滑（软过渡）和半透明，以确保梯度流动，并在训练后期逐渐收敛到锐利、不透明的三角形。</li>
<li><strong>改进的剪枝和稠密化策略：</strong> 引入了基于最大体渲染权重（而非单纯不透明度）的剪枝策略，以有效移除冗余三角形，避免在三角形变得不透明后产生伪影。稠密化通过中点细分引入新的顶点和三角形，同时保持连接性。</li>
<li><strong>直接兼容游戏引擎：</strong> 最终输出是仅由不透明三角形组成的半连接网格，无需任何后处理即可立即用于标准图形引擎，支持物理交互、可步行场景和场景编辑等下游应用。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>最先进的性能：</strong> 在Mip-NeRF360和Tanks &amp; Temples数据集上的实验表明，Triangle Splatting+在基于网格的新视角合成方面取得了最先进的性能，在所有指标上均优于现有方法。与2DGS和Triangle Splatting等方法相比，在相似顶点数量下，PSNR提高了4-10 dB。
*   <strong>高视觉保真度与效率：</strong> 该方法在视觉保真度上超越了以往的泼溅方法，同时保持了高效和快速的训练速度（Mip-NeRF360数据集上39分钟，T&amp;T数据集上25分钟）。
*   <strong>支持下游应用：</strong> 生成的半连接网格能够支持物理模拟、交互式场景漫游、对象提取和移除等下游应用，而无需完全水密网格。这极大地扩展了辐射场表示的实用性。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>稀疏区域的重建：</strong> 在初始点云稀疏覆盖的背景区域，几何结构可能不完整，保真度较低。
*   <strong>训练视角外的性能下降：</strong> 当移动到训练视角范围之外时，视觉质量会下降，因为不透明三角形的使用会使伪影更加明显。
*   <strong>透明物体表示困难：</strong> 对于玻璃或瓶子等透明物体，仅使用不透明三角形进行表示仍然具有挑战性。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>更完整的点云初始化：</strong> 通过更完整的点云初始化来改善稀疏区域的重建质量。
*   <strong>结合替代表示：</strong> 引入其他表示形式，例如三角化的天空穹顶，以解决背景区域的局限性。
*   <strong>透明物体处理：</strong> 探索如何使用不透明三角形或结合其他机制来有效表示透明物体。</p>
<hr />
<p>总而言之，Triangle Splatting+通过直接优化具有共享顶点连接性的不透明三角形，成功地弥合了辐射场优化与传统计算机图形学之间的鸿沟。其创新性的参数化和训练策略不仅实现了卓越的视觉质量和高效的训练，还确保了输出与现有图形管线的无缝兼容性，为VR/AR应用、游戏引擎和模拟框架的实际集成铺平了道路。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Reconstructing 3D scenes and synthesizing novel views has seen rapid progress
in recent years.</li>
<li>Experiments on the Mip-NeRF360 and Tanks &amp; Temples
datasets show that Triangle Splatting+achieves state-of-the-art performance in
mesh-based novel view synthesis.</li>
<li>Our method surpasses prior splatting
approaches in visual fidelity while remaining efficient and fast to training.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.25122v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.25122v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.25094v1'></a></p>
<h2 id="unsupervised-representation-learning-for-3d-mesh-parameterization-with-semantic-and-visibility-objectives"><a href="https://arxiv.org/abs/2509.25094v1">Unsupervised Representation Learning for 3D Mesh Parameterization with Semantic and Visibility Objectives</a></h2>
<p><strong>Authors:</strong> AmirHossein Zamani, Bruno Roy, Arianna Rampini</p>
<p><strong>Published:</strong> 2025-09-29</p>
<p><strong>Categories:</strong> cs.GR, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent 3D generative models produce high-quality textures for 3D mesh
objects. However, they commonly rely on the heavy assumption that input 3D
meshes are accompanied by manual mesh parameterization (UV mapping), a manual
task that requires both technical precision and artistic judgment. Industry
surveys show that this process often accounts for a significant share of asset
creation, creating a major bottleneck for 3D content creators. Moreover,
existing automatic methods often ignore two perceptually important criteria:
(1) semantic awareness (UV charts should align semantically similar 3D parts
across shapes) and (2) visibility awareness (cutting seams should lie in
regions unlikely to be seen). To overcome these shortcomings and to automate
the mesh parameterization process, we present an unsupervised differentiable
framework that augments standard geometry-preserving UV learning with semantic-
and visibility-aware objectives. For semantic-awareness, our pipeline (i)
segments the mesh into semantic 3D parts, (ii) applies an unsupervised learned
per-part UV-parameterization backbone, and (iii) aggregates per-part charts
into a unified UV atlas. For visibility-awareness, we use ambient occlusion
(AO) as an exposure proxy and back-propagate a soft differentiable AO-weighted
seam objective to steer cutting seams toward occluded regions. By conducting
qualitative and quantitative evaluations against state-of-the-art methods, we
show that the proposed method produces UV atlases that better support texture
generation and reduce perceptible seam artifacts compared to recent baselines.
Our implementation code is publicly available at:
https://github.com/AHHHZ975/Semantic-Visibility-UV-Param.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供AmirHossein Zamani, Bruno Roy, Arianna Rampini撰写的论文“Unsupervised Representation Learning for 3D Mesh Parameterization with Semantic and Visibility Objectives”的全面摘要。</p>
<hr />
<p><strong>论文摘要：基于语义和可见性目标的3D网格参数化无监督表示学习</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
当前3D生成模型在生成高质量纹理方面表现出色，但普遍依赖于手动网格参数化（UV映射），这是一个耗时且需要技术和艺术判断的任务。现有自动化方法通常忽略了两个关键的感知标准：(1) 语义感知，即UV图表应与3D形状中语义相似的部分对齐；(2) 可见性感知，即切割缝应位于不易被观察到的区域。这导致了纹理生成和渲染中可见的接缝伪影，并限制了UV图表在语义上的一致性。本研究旨在解决这些局限性，自动化网格参数化过程，并使其生成的UV图表在语义和可见性方面更优。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
本文提出了一种无监督、可微分的框架，通过引入语义感知和可见性感知目标，增强了标准的几何保持UV学习。
*   <strong>语义感知（Semantic-Awareness）：</strong> 引入了一种“分区-参数化”策略。
    *   (i) 将网格分割成语义3D部分（使用形状直径函数ShDF进行分区）。
    *   (ii) 对每个语义部分独立应用一个无监督学习的、保持几何的UV参数化骨干网络。
    *   (iii) 将这些部分图表聚合并打包成一个统一的UV图集。
*   <strong>可见性感知（Visibility-Awareness）：</strong>
    *   使用环境光遮蔽（AO）作为曝光代理。
    *   反向传播一个软可微分的AO加权接缝目标，以引导切割缝向被遮挡区域移动，从而减少可见的接缝伪影。
*   <strong>两阶段训练流程：</strong>
    *   第一阶段：几何保持网格参数化学习，使用基于MLP的网络和可微分几何目标生成低失真UV映射。
    *   第二阶段：学习感知目标，引入语义感知和可见性感知模块，为纹D纹理绘制等任务提供指导。</p>
<p><strong>3. 主要结果及其意义：</strong>
通过与现有最先进方法的定性和定量评估，本文展示了：
*   <strong>语义一致性：</strong> 提出的方法能够生成UV图集，其图表与网格的3D语义部分更好地对齐，从而简化纹理编辑、传输和跨形状对应。
*   <strong>减少接缝伪影：</strong> 可见性感知目标成功地将切割缝引导到曝光度较低（更被遮挡）的区域，显著减少了纹理生成和渲染中可感知的接缝伪影。
*   <strong>几何质量保持：</strong> 尽管引入了语义和可见性目标，但模型仍能保持良好的几何特性，如共形性（角度保持）和等面积性（面积保持），仅有轻微的性能下降。
*   <strong>自动化和效率：</strong> 该框架实现了3D网格参数化的自动化，解决了手动UV映射的瓶颈问题。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>聚合器简单性：</strong> 当前的网格聚合器是简单且确定性的网格划分，虽然有效，但未来可以替换为更高级的打包求解器（启发式或基于优化的）。
*   <strong>定量指标的轻微下降：</strong> 尽管感知质量有所提高，但在某些数值指标（如共形性和等面积性）上，与纯几何优化的基线相比，存在轻微的下降。
*   <strong>AO作为可见性代理：</strong> 尽管AO是有效的可见性代理，但它可能无法完全捕捉所有与人类感知相关的可见性因素。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   联合学习UV参数化和纹理生成，以进一步提升3D生成模型和更广泛的3D内容创作。
*   探索更先进的打包求解器，以优化UV图集的空间利用率。
*   研究更复杂的可见性代理或直接的人类感知模型，以更精确地引导接缝放置。</p>
<hr />
<p>这篇论文通过将语义和可见性目标整合到无监督的3D网格参数化框架中，为计算机图形学领域做出了重要贡献，尤其是在自动化纹理映射和减少视觉伪影方面。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To overcome these shortcomings and to automate
the mesh parameterization process, we present an unsupervised differentiable
framework that augments standard geometry-preserving UV learning with semantic-
and visibility-aware objectives.</li>
<li>By conducting
qualitative and quantitative evaluations against state-of-the-art methods, we
show that the proposed method produces UV atlases that better support texture
generation and reduce perceptible seam artifacts compared to recent baselines.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.25094v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.25094v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.25079v1'></a></p>
<h2 id="unilat3d-geometry-appearance-unified-latents-for-single-stage-3d-generation"><a href="https://arxiv.org/abs/2509.25079v1">UniLat3D: Geometry-Appearance Unified Latents for Single-Stage 3D Generation</a></h2>
<p><strong>Authors:</strong> Guanjun Wu, Jiemin Fang, Chen Yang, Sikuang Li, Taoran Yi, Jia Lu, Zanwei Zhou, Jiazhong Cen, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Xinggang Wang, Qi Tian</p>
<p><strong>Published:</strong> 2025-09-29</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.GR</p>
<p><strong>Abstract:</strong></p>
<p>High-fidelity 3D asset generation is crucial for various industries. While
recent 3D pretrained models show strong capability in producing realistic
content, most are built upon diffusion models and follow a two-stage pipeline
that first generates geometry and then synthesizes appearance. Such a decoupled
design tends to produce geometry-texture misalignment and non-negligible cost.
In this paper, we propose UniLat3D, a unified framework that encodes geometry
and appearance in a single latent space, enabling direct single-stage
generation. Our key contribution is a geometry-appearance Unified VAE, which
compresses high-resolution sparse features into a compact latent representation
-- UniLat. UniLat integrates structural and visual information into a dense
low-resolution latent, which can be efficiently decoded into diverse 3D
formats, e.g., 3D Gaussians and meshes. Based on this unified representation,
we train a single flow-matching model to map Gaussian noise directly into
UniLat, eliminating redundant stages. Trained solely on public datasets,
UniLat3D produces high-quality 3D assets in seconds from a single image,
achieving superior appearance fidelity and geometric quality. More demos \&amp;
code are available at https://unilat3d.github.io/</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供对Guanjun Wu等人撰写的论文“UniLat3D: Geometry-Appearance Unified Latents for Single-Stage 3D Generation”的全面摘要。</p>
<hr />
<h3 id="unilat3d-3d">UniLat3D: 几何-外观统一潜在空间实现单阶段3D生成</h3>
<p><strong>1. 主要问题或研究问题：</strong>
当前高保真3D资产生成领域面临的主要挑战是，大多数现有方法（尤其是基于扩散模型的方法）采用两阶段流水线：首先生成几何结构，然后合成外观（纹理）。这种解耦设计常常导致几何与纹理不匹配（几何-纹理错位），并且引入了显著的计算成本和冗余步骤。论文旨在解决如何实现高效、高质量的单阶段3D生成，避免几何与外观分离带来的问题。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>统一的潜在表示（UniLat）：</strong> 论文提出了UniLat，这是一种新颖的统一潜在空间表示，能够将3D资产的几何信息和外观信息编码到单个紧凑的低分辨率潜在表示中。这与传统方法中几何和外观分离的潜在空间形成鲜明对比。
*   <strong>几何-外观统一VAE（UniVAE）：</strong> 引入了一个统一的变分自编码器（UniVAE），用于将高分辨率稀疏特征压缩成紧凑的UniLat表示。UniVAE能够将结构和视觉信息整合到密集的低分辨率潜在空间中。
*   <strong>单阶段生成框架：</strong> 基于UniLat统一表示，论文训练了一个单一的流匹配（flow-matching）模型，可以直接将高斯噪声映射到UniLat，从而实现直接的单阶段3D生成，消除了传统两阶段流水线中的冗余步骤。
*   <strong>多格式解码能力：</strong> UniLat可以高效地解码成多种3D格式，包括3D高斯（3D Gaussians）和网格（meshes），这增强了其通用性和实用性。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>卓越的性能：</strong> UniLat3D在公开数据集上进行训练，能够从单张图像在数秒内生成高质量的3D资产。
*   <strong>高外观保真度和几何质量：</strong> 实验结果表明，UniLat3D在外观保真度和几何质量方面表现出色，优于现有的两阶段方法，并能更好地与条件图像对齐。
*   <strong>效率提升：</strong> 通过单阶段生成，UniLat3D显著减少了生成时间，例如，3D高斯生成可在8秒内完成，使用FlashAttention-3甚至可缩短至3秒。网格生成虽然需要36秒，但考虑到更高的分辨率和后处理，仍具竞争力。
*   <strong>用户研究验证：</strong> 用户研究结果显示，UniLat3D在图像对齐和对象质量方面获得了超过35%的投票，优于其他模型。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>初步探索：</strong> 论文指出，UniLat3D模型仍处于初步探索阶段。
*   <strong>训练数据：</strong> 目前仅使用公开数据集进行训练。作者认为，注入更多高质量数据将无疑进一步提高模型性能并扩大规模。
*   <strong>高分辨率潜在空间的效率：</strong> 在更高分辨率（例如32³）下训练流匹配Transformer时，计算成本显著增加。目前流模型在处理高分辨率潜在空间时的效率仍有待提高。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>数据规模与质量：</strong> 探索如何利用更多高质量的训练数据来进一步提升模型性能和可扩展性。
*   <strong>流模型效率：</strong> 研究更高效的流模型设计，以适应更高分辨率的潜在空间，从而生成更详细的3D结果，例如通过块级计算和轻量级注意力机制。
*   <strong>多模态集成：</strong> 将UniLat集成到大型多模态模型中，以促进跨模态理解和生成。
*   <strong>扩展到4D表示：</strong> 将UniLat扩展到4D表示，以支持动态3D内容的生成。
*   <strong>统一对象与场景生成：</strong> 进一步统一对象和场景生成，利用紧凑的统一表示。</p>
<hr />
<p>总而言之，UniLat3D通过引入几何-外观统一的潜在空间和单阶段流匹配生成模型，为高保真3D资产生成提供了一个新颖且高效的范式，有效解决了传统两阶段方法中几何-纹理错位和计算成本高昂的问题。其在公开数据集上取得的卓越性能和用户研究结果，凸显了统一表示在3D生成领域的巨大潜力。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we propose UniLat3D, a unified framework that encodes geometry
and appearance in a single latent space, enabling direct single-stage
generation.</li>
<li>Our key contribution is a geometry-appearance Unified VAE, which
compresses high-resolution sparse features into a compact latent representation
-- UniLat.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.25079v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.25079v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.25077v1'></a></p>
<h2 id="bridge-building-reinforcement-learning-depth-to-image-data-generation-engine-for-monocular-depth-estimation"><a href="https://arxiv.org/abs/2509.25077v1">BRIDGE -- Building Reinforcement-Learning Depth-to-Image Data Generation Engine for Monocular Depth Estimation</a></h2>
<p><strong>Authors:</strong> Dingning Liu, Haoyu Guo, Jingyi Zhou, Tong He</p>
<p><strong>Published:</strong> 2025-09-29</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Monocular Depth Estimation (MDE) is a foundational task for computer vision.
Traditional methods are limited by data scarcity and quality, hindering their
robustness. To overcome this, we propose BRIDGE, an RL-optimized depth-to-image
(D2I) generation framework that synthesizes over 20M realistic and
geometrically accurate RGB images, each intrinsically paired with its ground
truth depth, from diverse source depth maps. Then we train our depth estimation
model on this dataset, employing a hybrid supervision strategy that integrates
teacher pseudo-labels with ground truth depth for comprehensive and robust
training. This innovative data generation and training paradigm enables BRIDGE
to achieve breakthroughs in scale and domain diversity, consistently
outperforming existing state-of-the-art approaches quantitatively and in
complex scene detail capture, thereby fostering general and robust depth
features. Code and models are available at
https://dingning-liu.github.io/bridge.github.io/.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Dingning Liu, Haoyu Guo, Jingyi Zhou, Tong He撰写的论文“BRIDGE -- Building Reinforcement-Learning Depth-to-Image Data Generation Engine for Monocular Depth Estimation”的全面摘要。</p>
<hr />
<p><strong>论文摘要：BRIDGE——构建用于单目深度估计的强化学习深度到图像数据生成引擎</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
单目深度估计（MDE）是计算机视觉中的一项基础任务，但传统方法受限于数据稀缺性和质量，这严重阻碍了其鲁棒性。现有数据集在高质量、精确的真值深度标注、细节和多样性方面存在不足，且未能充分利用现有深度数据，这成为MDE模型训练的关键瓶颈。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
该论文提出了BRIDGE框架，其核心创新在于：
*   <strong>RL优化的深度到图像（D2I）数据生成引擎：</strong> BRIDGE引入了一个强化学习（RL）优化的D2I生成模型。该模型能够从多样化的源深度图合成超过2000万张视觉真实且几何精确的RGB图像，每张图像都与其真值深度内在配对。这种方法有效缓解了数据稀缺性和质量问题，并扩展了训练数据的规模和领域多样性。RL优化确保了生成图像不仅视觉真实，而且几何精确和一致，避免了传统D2I模型中常见的几何伪影和结构失真。
*   <strong>混合监督训练策略：</strong> 为了充分利用生成数据，BRIDGE采用了一种混合监督策略。该策略结合了教师模型生成的伪标签和高精度真值深度。具体而言，首先使用教师模型生成初始伪标签，然后通过基于相似性的方法（如SSIM和梯度分析）筛选高精度真值深度区域，并将其与伪标签融合。这种两阶段训练过程（先用伪标签进行大规模预训练，再用真值深度进行精细调整）确保了模型在学习广泛几何一致性的同时，也能在关键区域实现高精度和细节捕捉。
*   <strong>高效的数据生成和利用范式：</strong> 通过RL驱动的D2I范式，BRIDGE能够高效生成大规模高质量RGB-D数据，有效解决了数据稀缺和质量问题。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>卓越的性能和训练效率：</strong> BRIDGE在多个挑战性基准测试（包括室内、室外和合成动画环境）上均取得了最先进（SOTA）的性能。它在定量和复杂场景细节捕捉方面持续优于现有方法，例如，仅使用约2000万数据（相比Depth Anything V2的6200万数据）就超越了Depth Anything V2等模型。
*   <strong>强大的泛化能力和鲁棒性：</strong> 该模型在零样本深度估计方面表现出色，尤其是在室内场景数据集（如NYUv2、ScanNet、ETH3D）上，能够完美地生成与目标对齐的精细结构预测。它还能准确估计反射表面（如镜子）的深度，并处理复杂细节和相似颜色的物体，展现了对“野外”数据的强大泛化能力。
*   <strong>促进通用和鲁棒的深度特征：</strong> 创新的数据生成和训练范式有助于培养更通用和鲁棒的深度特征，为更高效和可泛化的MDE解决方案铺平了道路。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>KITTI数据集上的性能：</strong> 尽管在大多数数据集上表现优异，但BRIDGE在KITTI数据集上未能达到最佳性能，这主要归因于该数据集固有的稀疏性。论文指出，模型旨在捕捉精细的全局和局部深度信息，而KITTI评估未能充分反映这一点。
*   <strong>伪标签的固有噪声和不准确性：</strong> 论文承认，教师模型生成的伪标签虽然覆盖范围广，但其固有的噪声和不准确性（尤其是在边界和精细细节处）仍然是进一步提升深度估计性能的瓶颈。混合监督策略正是为了缓解这一问题而设计的。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
论文中没有明确提出未来的研究方向，但从其贡献和局限性中可以推断出一些潜在方向：
*   <strong>进一步优化D2I引擎：</strong> 探索更先进的RL技术或生成模型架构，以进一步提高生成图像的视觉真实感、几何精确性和多样性，尤其是在处理更复杂的场景和物体时。
*   <strong>改进混合监督策略：</strong> 研究更智能的伪标签筛选和融合机制，例如，结合不确定性估计或自适应权重，以更好地平衡伪标签的广度和真值深度的精度。
*   <strong>解决特定数据集的局限性：</strong> 针对KITTI等稀疏或特定领域的数据集，开发更专门的训练策略或数据增强方法，以提升模型在这些场景下的性能。
*   <strong>探索更广泛的应用：</strong> 鉴于BRIDGE生成的RGB-D数据的高质量和多样性，可以探索其在其他3D计算机视觉任务中的应用，如3D重建、场景理解、机器人导航等。
*   <strong>模型可解释性：</strong> 深入研究RL-D2I引擎和MDE模型内部的工作机制，以提高模型的可解释性，从而更好地理解其成功之处并指导未来的改进。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To overcome this, we propose BRIDGE, an RL-optimized depth-to-image
(D2I) generation framework that synthesizes over 20M realistic and
geometrically accurate RGB images, each intrinsically paired with its ground
truth depth, from diverse source depth maps.</li>
<li>This innovative data generation and training paradigm enables BRIDGE
to achieve breakthroughs in scale and domain diversity, consistently
outperforming existing state-of-the-art approaches quantitatively and in
complex scene detail capture, thereby fostering general and robust depth
features.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.25077v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.25077v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.25058v1'></a></p>
<h2 id="chargen-fast-and-fluent-portrait-modification"><a href="https://arxiv.org/abs/2509.25058v1">CharGen: Fast and Fluent Portrait Modification</a></h2>
<p><strong>Authors:</strong> Jan-Niklas Dihlmann, Arnela Killguss, Hendrik P. A. Lensch</p>
<p><strong>Published:</strong> 2025-09-29</p>
<p><strong>Categories:</strong> cs.GR, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Interactive editing of character images with diffusion models remains
challenging due to the inherent trade-off between fine-grained control,
generation speed, and visual fidelity. We introduce CharGen, a
character-focused editor that combines attribute-specific Concept Sliders,
trained to isolate and manipulate attributes such as facial feature size,
expression, and decoration with the StreamDiffusion sampling pipeline for more
interactive performance. To counteract the loss of detail that often
accompanies accelerated sampling, we propose a lightweight Repair Step that
reinstates fine textures without compromising structural consistency.
Throughout extensive ablation studies and in comparison to open-source
InstructPix2Pix and closed-source Google Gemini, and a comprehensive user
study, CharGen achieves two-to-four-fold faster edit turnaround with precise
editing control and identity-consistent results. Project page:
https://chargen.jdihlmann.com/</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Jan-Niklas Dihlmann, Arnela Killguss, Hendrik P. A. Lensch撰写的论文“CharGen: Fast and Fluent Portrait Modification”的全面摘要。</p>
<hr />
<p><strong>论文摘要：CharGen: 快速流畅的人像修改</strong></p>
<p><strong>1. 主要问题或研究问题</strong>
当前，使用扩散模型进行交互式角色图像编辑面临着一个固有的挑战，即在精细控制、生成速度和视觉保真度之间存在权衡。现有的方法往往难以同时实现这三点，导致编辑过程要么缺乏精确控制，要么速度缓慢，要么在加速采样时丢失细节。</p>
<p><strong>2. 关键创新或方法论贡献</strong>
CharGen 提出了一种以角色为中心的编辑器，通过以下三个关键创新来解决上述问题：</p>
<ul>
<li><strong>属性特定概念滑块（Attribute-specific Concept Sliders）：</strong> CharGen 预训练了针对特定属性（如面部特征大小、表情、装饰等）的概念滑块。这些滑块通过 LoRA 适配器进行训练，旨在隔离和操纵特定属性，同时保持其他属性的独立性，从而实现精细、连续的控制。论文强调了 LoRA 合并（LoRA merging）策略，它通过对 LoRA 权重矩阵求和来预组合多个概念滑块，以确保在多属性编辑时的稳定性和一致性，避免了 LoRA 堆叠（LoRA stacking）可能导致的累积失真和细节丢失。</li>
<li><strong>StreamDiffusion 采样管线集成：</strong> 为了实现更具交互性的性能，CharGen 将预训练的概念滑块与 StreamDiffusion 采样管线集成。StreamDiffusion 以其实时生成能力而闻名，通过一系列优化（如 Stream Batch、RCFG、Input-Output Queues 等）显著加快了推理速度。这种集成使得 CharGen 能够实现两到四倍的编辑周转速度。</li>
<li><strong>轻量级修复步骤（Lightweight Repair Step）：</strong> 为了抵消加速采样过程中经常伴随的细节丢失，CharGen 引入了一个轻量级的修复步骤。该修复步骤旨在恢复精细纹理，同时不损害图像的结构一致性。论文探讨了两种修复方法：训练专门的修复滑块（Repair Slider）和基于 ControlNet 的修复，并最终选择了修复滑块，因为它在细节增强和结构保持之间取得了最佳平衡。</li>
</ul>
<p><strong>3. 主要结果及其意义</strong>
CharGen 在广泛的消融研究、与开源 InstructPix2Pix 和闭源 Google Gemini 的比较以及全面的用户研究中展示了其有效性：</p>
<ul>
<li><strong>编辑速度显著提升：</strong> CharGen 实现了两到四倍的编辑周转速度，显著优于 InstructPix2Pix 和 Google Gemini 等现有方法，使其更适合交互式工作流程。</li>
<li><strong>精确的编辑控制和身份一致性：</strong> 概念滑块提供了对特定面部属性的精细、连续控制，能够进行局部调整，同时保持角色身份的一致性。用户研究证实了 CharGen 在单属性和多属性编辑场景中的优势。</li>
<li><strong>视觉保真度高：</strong> 轻量级修复步骤成功地恢复了加速采样过程中丢失的细节，确保了高质量的视觉输出，同时保持了结构一致性。</li>
<li><strong>多属性编辑能力：</strong> LoRA 合并方法使得 CharGen 能够同时修改多个属性，并保持一致性，这在 InstructPix2Pix 和 Gemini 等方法中是一个挑战。</li>
</ul>
<p><strong>4. 论文中提及的局限性</strong>
*   <strong>强转换的局限性：</strong> CharGen 在处理极端年龄变化等强转换时表现出一定的局限性，其滑块训练更侧重于精细调整而非剧烈变化。
*   <strong>概念滑块间的干扰：</strong> 尽管 LoRA 合并实现了多滑块使用，但某些组合仍可能出现干扰效应，例如年龄修改会影响唇部大小，或化妆-年龄组合会降低图像清晰度。这表明独立训练的滑块可能缺乏跨属性感知。
*   <strong>潜在的偏见和伦理问题：</strong> 扩散模型固有的训练数据分布可能导致系统在不同人口群体之间表现出偏见。此外，系统操纵面部属性的能力可能被滥用于创建深度伪造或误导性内容。</p>
<p><strong>5. 潜在的未来研究方向</strong>
*   <strong>扩展到更广泛的图像编辑领域：</strong> 将 CharGen 的属性特定方法扩展到面部特征以外的更广泛图像编辑领域。
*   <strong>缓解概念滑块间的干扰：</strong> 开发策略以减轻概念滑块之间不必要的交互。
*   <strong>改进离散属性的训练方法：</strong> 改进针对具有离散而非连续变化的属性的训练方法。
*   <strong>探索更复杂的 LoRA 集成技术：</strong> 探索更复杂的 LoRA 集成技术，以增强细节生成。</p>
<hr />
<p>总而言之，CharGen 论文通过结合属性特定的概念滑块、StreamDiffusion 采样管线和轻量级修复步骤，成功地在交互式角色图像编辑中平衡了精细控制、生成速度和视觉保真度。它为计算机视觉领域的可控生成编辑提供了一个新颖且高效的解决方案，尽管仍需关注其在强转换、滑块交互和伦理方面的局限性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce CharGen, a
character-focused editor that combines attribute-specific Concept Sliders,
trained to isolate and manipulate attributes such as facial feature size,
expression, and decoration with the StreamDiffusion sampling pipeline for more
interactive performance.</li>
<li>To counteract the loss of detail that often
accompanies accelerated sampling, we propose a lightweight Repair Step that
reinstates fine textures without compromising structural consistency.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.25058v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.25058v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.25033v1'></a></p>
<h2 id="vt-fsl-bridging-vision-and-text-with-llms-for-few-shot-learning"><a href="https://arxiv.org/abs/2509.25033v1">VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning</a></h2>
<p><strong>Authors:</strong> Wenhao Li, Qiangchang Wang, Xianjing Meng, Zhibin Wu, Yilong Yin</p>
<p><strong>Published:</strong> 2025-09-29</p>
<p><strong>Categories:</strong> cs.CV, cs.LG, I.4.9</p>
<p><strong>Abstract:</strong></p>
<p>Few-shot learning (FSL) aims to recognize novel concepts from only a few
labeled support samples. Recent studies enhance support features by
incorporating additional semantic information or designing complex semantic
fusion modules. However, they still suffer from hallucinating semantics that
contradict the visual evidence due to the lack of grounding in actual
instances, resulting in noisy guidance and costly corrections. To address these
issues, we propose a novel framework, bridging Vision and Text with LLMs for
Few-Shot Learning (VT-FSL), which constructs precise cross-modal prompts
conditioned on Large Language Models (LLMs) and support images, seamlessly
integrating them through a geometry-aware alignment. It mainly consists of
Cross-modal Iterative Prompting (CIP) and Cross-modal Geometric Alignment
(CGA). Specifically, the CIP conditions an LLM on both class names and support
images to generate precise class descriptions iteratively in a single
structured reasoning pass. These descriptions not only enrich the semantic
understanding of novel classes but also enable the zero-shot synthesis of
semantically consistent images. The descriptions and synthetic images act
respectively as complementary textual and visual prompts, providing high-level
class semantics and low-level intra-class diversity to compensate for limited
support data. Furthermore, the CGA jointly aligns the fused textual, support,
and synthetic visual representations by minimizing the kernelized volume of the
3-dimensional parallelotope they span. It captures global and nonlinear
relationships among all representations, enabling structured and consistent
multimodal integration. The proposed VT-FSL method establishes new
state-of-the-art performance across ten diverse benchmarks, including standard,
cross-domain, and fine-grained few-shot learning scenarios. Code is available
at https://github.com/peacelwh/VT-FSL.</p>
<p><strong>Analysis:</strong></p>
<p>以下是Wenhao Li, Qiangchang Wang, Xianjing Meng, Zhibin Wu, Yilong Yin撰写的论文“VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning”的全面摘要：</p>
<p><strong>1. 主要问题或研究问题</strong>
该论文旨在解决少样本学习（FSL）中的一个核心挑战：如何有效地识别仅通过少量带标签支持样本学习到的新概念。现有方法通过整合额外的语义信息（如类别描述）或设计复杂的语义融合模块来增强支持特征，但这些方法往往会产生与视觉证据相矛盾的“幻觉”语义，导致指导信息不准确和修正成本高昂。</p>
<p><strong>2. 关键创新或方法论贡献</strong>
VT-FSL（Bridging Vision and Text with LLMs for Few-Shot Learning）提出了一种新颖的框架，通过以下两个关键模块解决了上述问题：</p>
<ul>
<li><strong>跨模态迭代提示（Cross-modal Iterative Prompting, CIP）</strong>：该模块利用大型语言模型（LLMs）和支持图像，以结构化推理的方式迭代生成精确的类别描述。这些描述不仅丰富了对新类别的语义理解，还能够零样本合成语义一致的图像。生成的描述和合成图像分别作为互补的文本和视觉提示，提供高级别的类别语义和低级别类别内多样性，以弥补有限支持数据的问题。</li>
<li><strong>跨模态几何对齐（Cross-modal Geometric Alignment, CGA）</strong>：该模块通过最小化融合的文本、支持和合成视觉表示所跨越的3维平行六面体的核化体积，共同对齐这些表示。CGA捕获了所有表示之间的全局和非线性关系，实现了结构化和一致的多模态整合。</li>
</ul>
<p><strong>3. 主要结果及其意义</strong>
VT-FSL方法在十个不同的基准测试中（包括标准、跨领域和细粒度少样本学习场景）建立了新的最先进性能。平均准确率提高了4.2%。具体来说：
*   在miniImageNet和tieredImageNet上，VT-FSL在1-shot和5-shot设置下均显著优于现有方法，例如在miniImageNet的1-shot任务中，比次优方法高出4.35%至15.31%。
*   在细粒度数据集（如CUB、Cars、Dogs）上，VT-FSL在挑战性的1-shot任务中比次优方法SUITED高出3.0%-10.3%，表明其能够捕获细微的类别间差异并保持类别内一致性。
*   消融研究证实了文本提示、视觉提示和核化体积对比损失（对齐损失）的互补性和有效性。
*   与仅依赖类别名称或简单提示的LLM方法相比，VT-FSL生成的文本语义更丰富、更精确。
*   核化体积对比学习（特别是使用RBF核）在捕获非线性关系和实现全局一致对齐方面优于InfoNCE和线性体积损失。
*   VT-FSL在训练和推理时间方面也表现出高效性，比现有LLM基线方法更快，同时实现了更高的准确率。</p>
<p><strong>4. 论文中提及的局限性</strong>
*   <strong>领域泛化能力</strong>：尽管在跨领域数据集（如Places和Plantae）上进行了评估，但这些设置与源领域仍存在一定相似性。VT-FSL在更具挑战性的分布偏移（如医学图像）下的鲁棒性尚未得到充分评估。
*   <strong>外部生成模型质量依赖</strong>：VT-FSL的性能依赖于外部生成模型（LLMs和文本到图像模型）的质量。较弱的LLMs可能产生通用或嘈杂的描述，低质量的图像合成可能引入误导性视觉信号。
*   <strong>核化体积对比损失的理论行为</strong>：在处理高维、嘈杂或语义纠缠的特征分布时，核化体积对比损失的理论行为仍有待深入探索。</p>
<p><strong>5. 潜在的未来研究方向</strong>
*   进一步研究核化体积对比损失的收敛特性和对核选择的敏感性。
*   探索如何提高VT-FSL在更具挑战性的分布偏移场景下的鲁棒性。
*   提升生成模型的质量，以提供更精确和视觉上更忠实的语义先验，从而进一步增强VT-FSL的性能。
*   深入理解VT-FSL的解释性和泛化能力，特别是在多模态少样本学习的背景下。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Few-shot learning (FSL) aims to recognize novel concepts from only a few
labeled support samples.</li>
<li>To address these
issues, we propose a novel framework, bridging Vision and Text with LLMs for
Few-Shot Learning (VT-FSL), which constructs precise cross-modal prompts
conditioned on Large Language Models (LLMs) and support images, seamlessly
integrating them through a geometry-aware alignment.</li>
<li>These descriptions not only enrich the semantic
understanding of novel classes but also enable the zero-shot synthesis of
semantically consistent images.</li>
<li>The proposed VT-FSL method establishes new
state-of-the-art performance across ten diverse benchmarks, including standard,
cross-domain, and fine-grained few-shot learning scenarios.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.25033v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.25033v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.25032v1'></a></p>
<h2 id="airoa-moma-dataset-a-large-scale-hierarchical-dataset-for-mobile-manipulation"><a href="https://arxiv.org/abs/2509.25032v1">AIRoA MoMa Dataset: A Large-Scale Hierarchical Dataset for Mobile Manipulation</a></h2>
<p><strong>Authors:</strong> Ryosuke Takanami, Petr Khrapchenkov, Shu Morikuni, Jumpei Arima, Yuta Takaba, Shunsuke Maeda, Takuya Okubo, Genki Sano, Satoshi Sekioka, Aoi Kadoya, Motonari Kambara, Naoya Nishiura, Haruto Suzuki, Takanori Yoshimoto, Koya Sakamoto, Shinnosuke Ono, Hu Yang, Daichi Yashima, Aoi Horo, Tomohiro Motoda, Kensuke Chiyoma, Hiroshi Ito, Koki Fukuda, Akihito Goto, Kazumi Morinaga, Yuya Ikeda, Riko Kawada, Masaki Yoshikawa, Norio Kosuge, Yuki Noguchi, Kei Ota, Tatsuya Matsushima, Yusuke Iwasawa, Yutaka Matsuo, Tetsuya Ogata</p>
<p><strong>Published:</strong> 2025-09-29</p>
<p><strong>Categories:</strong> cs.RO, cs.AI, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>As robots transition from controlled settings to unstructured human
environments, building generalist agents that can reliably follow natural
language instructions remains a central challenge. Progress in robust mobile
manipulation requires large-scale multimodal datasets that capture contact-rich
and long-horizon tasks, yet existing resources lack synchronized force-torque
sensing, hierarchical annotations, and explicit failure cases. We address this
gap with the AIRoA MoMa Dataset, a large-scale real-world multimodal dataset
for mobile manipulation. It includes synchronized RGB images, joint states,
six-axis wrist force-torque signals, and internal robot states, together with a
novel two-layer annotation schema of sub-goals and primitive actions for
hierarchical learning and error analysis. The initial dataset comprises 25,469
episodes (approx. 94 hours) collected with the Human Support Robot (HSR) and is
fully standardized in the LeRobot v2.1 format. By uniquely integrating mobile
manipulation, contact-rich interaction, and long-horizon structure, AIRoA MoMa
provides a critical benchmark for advancing the next generation of
Vision-Language-Action models. The first version of our dataset is now
available at https://huggingface.co/datasets/airoa-org/airoa-moma .</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Ryosuke Takanami等人撰写的论文“AIRoA MoMa Dataset: A Large-Scale Hierarchical Dataset for Mobile Manipulation”的全面摘要。</p>
<hr />
<p><strong>论文摘要：AIRoA MoMa Dataset: 用于移动操作的大规模分层数据集</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
随着机器人从受控环境向非结构化人类环境过渡，构建能够可靠遵循自然语言指令的通用智能体成为一个核心挑战。目前，鲁棒的移动操作需要大规模、多模态数据集，这些数据集应能捕捉接触密集型和长时程任务。然而，现有资源普遍缺乏同步的力-扭矩传感、分层标注以及明确的失败案例，这限制了视觉-语言-动作（VLA）模型在实际世界中的发展。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
为了解决上述空白，本论文引入了 <strong>AIRoA MoMa Dataset</strong>，这是一个大规模、真实世界、多模态的移动操作数据集，其关键创新和贡献包括：
*   <strong>多模态数据集成：</strong> 数据集同步包含了RGB图像、关节状态、六轴腕部力-扭矩信号以及机器人内部状态，特别强调了力-扭矩传感，以支持接触密集型任务的学习。
*   <strong>新颖的两层分层标注方案：</strong> 引入了子目标（sub-goals）和原始动作（primitive actions）的两层标注结构。这种分层设计有助于分层学习和细粒度的错误分析，使得模型能够学习高层任务规划和低层运动控制。
*   <strong>包含明确的失败案例：</strong> 数据集有意包含了失败案例的记录和标注（约占总数据集的6.6%），这对于研究错误检测、恢复和从负面示例中学习至关重要。
*   <strong>标准化数据格式和开放管道：</strong> 数据集完全标准化为LeRobot v2.1格式，确保了与现有VLA架构的直接兼容性、可复现性和广泛可访问性。同时，论文发布了一个开源数据处理和打包管道。
*   <strong>专注于移动操作、接触密集交互和长时程任务：</strong> 与现有主要关注桌面操作的数据集不同，AIRoA MoMa 明确地将移动操作、涉及物理接触的交互以及需要多步骤分解的长时程任务作为核心。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>数据集规模：</strong> 初始数据集包含25,469个情景（约94小时），由丰田人类支持机器人（HSR）收集。这些情景涵盖了七个主要的家庭任务和40多个子任务。
*   <strong>数据特性：</strong> 数据集展示了技能分布的长尾模式，基础操作（如“抓取”、“打开”、“放置”）占据主导地位。任务持续时间集中在短到中等范围（4到12秒），表明数据集主要由离散的、短时程活动组成，非常适合训练基础和反应性策略。
*   <strong>对VLA模型的推动：</strong> 通过独特地整合移动操作、接触密集交互和长时程结构，AIRoA MoMa 为下一代视觉-语言-动作模型的发展提供了一个关键基准，有望加速通用机器人智能体的开发。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>数据多样性仍有提升空间：</strong> 尽管数据集努力涵盖多种家庭环境和任务，但为了实现人类水平的通用能力，仍需要更多样化的机器人数据，涵盖更广泛的物体、环境、任务和情境。
*   <strong>隐私过滤的局限性：</strong> 尽管采用了基于YOLO的检测器进行隐私过滤，自动排除包含人类出现的情景，但仍需持续关注和改进隐私保护技术。
*   <strong>当前版本未包含恢复行为和人机交互信号：</strong> 论文提到未来计划将扩展数据集，以包含恢复行为和人机交互信号（如自然语言和语音），这表明当前版本尚未完全覆盖这些方面。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>扩展数据集覆盖范围：</strong> 计划通过增加情景数量、从多个站点和多样化环境中收集数据来扩展数据集的覆盖范围。
*   <strong>整合恢复行为和人机交互信号：</strong> 未来版本将纳入恢复行为以及自然语言和语音等人类-机器人交互信号，以进一步提升数据集的实用性。
*   <strong>推动通用机器人智能体发展：</strong> AIRoA MoMa 数据集将作为关键基准，促进能够处理接触密集、长时程移动操作任务的下一代通用机器人智能体的开发。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>It includes synchronized RGB images, joint states,
six-axis wrist force-torque signals, and internal robot states, together with a
novel two-layer annotation schema of sub-goals and primitive actions for
hierarchical learning and error analysis.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.25032v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.25032v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-09-30 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
