<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-09-30 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DEâ«¶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Birdâs-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">è®ºæéè¯»</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-Î±
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-09-29/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-10-01/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-09-30">Arxiv Computer Vision Papers - 2025-09-30</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#hunyuanimage-30-technical-report" class="nav-link">HunyuanImage 3.0 Technical Report</a>
                </li>
                <li class="nav-item">
                    <a href="#fast-feature-field-textf3-a-predictive-representation-of-events" class="nav-link">Fast Feature Field (\text{F}^3): A Predictive Representation of Events</a>
                </li>
                <li class="nav-item">
                    <a href="#score-distillation-of-flow-matching-models" class="nav-link">Score Distillation of Flow Matching Models</a>
                </li>
                <li class="nav-item">
                    <a href="#triangle-splatting-differentiable-rendering-with-opaque-triangles" class="nav-link">Triangle Splatting+: Differentiable Rendering with Opaque Triangles</a>
                </li>
                <li class="nav-item">
                    <a href="#unsupervised-representation-learning-for-3d-mesh-parameterization-with-semantic-and-visibility-objectives" class="nav-link">Unsupervised Representation Learning for 3D Mesh Parameterization with Semantic and Visibility Objectives</a>
                </li>
                <li class="nav-item">
                    <a href="#unilat3d-geometry-appearance-unified-latents-for-single-stage-3d-generation" class="nav-link">UniLat3D: Geometry-Appearance Unified Latents for Single-Stage 3D Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#bridge-building-reinforcement-learning-depth-to-image-data-generation-engine-for-monocular-depth-estimation" class="nav-link">BRIDGE -- Building Reinforcement-Learning Depth-to-Image Data Generation Engine for Monocular Depth Estimation</a>
                </li>
                <li class="nav-item">
                    <a href="#chargen-fast-and-fluent-portrait-modification" class="nav-link">CharGen: Fast and Fluent Portrait Modification</a>
                </li>
                <li class="nav-item">
                    <a href="#vt-fsl-bridging-vision-and-text-with-llms-for-few-shot-learning" class="nav-link">VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning</a>
                </li>
                <li class="nav-item">
                    <a href="#airoa-moma-dataset-a-large-scale-hierarchical-dataset-for-mobile-manipulation" class="nav-link">AIRoA MoMa Dataset: A Large-Scale Hierarchical Dataset for Mobile Manipulation</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-09-30">Arxiv Computer Vision Papers - 2025-09-30</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>å¥½çï¼è¿æ¯ä¸ä»½éå¯¹2025å¹´9æ28æ¥Arxivè®¡ç®æºè§è§è®ºæçæ¯æ¥æ¥åæ§è¡æè¦ï¼æ¨å¨å¸®å©å¿ç¢çç ç©¶äººåå¿«éäºè§£ææ°è¿å±ï¼</p>
<hr />
<p><strong>Arxiv è®¡ç®æºè§è§æ¯æ¥æ¥åæ§è¡æè¦ (2025-09-28)</strong></p>
<p><strong>æ¦è¿°ä¸ä¸»è¦è¶å¿ï¼</strong></p>
<p>ä»å¤©çè®ºæéåç°åºè®¡ç®æºè§è§é¢åå ä¸ªå³é®è¶å¿çæç»­æ·±ååèåã<strong>çæå¼AI</strong> ä»ç¶æ¯æ ¸å¿ç¦ç¹ï¼å°¤å¶æ¯å¨å¾åçæã3Dåå®¹åå»ºåå¤æ¨¡æäº¤äºæ¹é¢ã<strong>3Dè¡¨ç¤ºä¸çæ</strong> åå¾äºæ¾èè¿å±ï¼ä»æ°çæ¸²æææ¯å°ç»ä¸ç3Dæ½å¨ç©ºé´ãæ­¤å¤ï¼<strong>æçåå®ç¨æ§</strong> æä¸ºéè¦èéï¼ä½ç°å¨å¯¹å¿«éæ¨¡åãæ çç£å­¦ä¹ åå®éåºç¨ï¼å¦æºå¨äººæä½åèåä¿®æ¹ï¼çå³æ³¨ã<strong>å¤æ¨¡æå­¦ä¹ </strong>ï¼ç¹å«æ¯ç»åå¤§åè¯­è¨æ¨¡åï¼LLMsï¼è¿è¡å°æ ·æ¬å­¦ä¹ ï¼ä¹æ¾ç¤ºåºå¶æ¥çå¢é¿çéè¦æ§ã</p>
<p><strong>ç¹å«éè¦æåæ°çè®ºæï¼</strong></p>
<ul>
<li><strong>"HunyuanImage 3.0 Technical Report" (Siyu Cao et al.)</strong>: ä½ä¸ºå¤§åå¾åçææ¨¡åçææ¯æ¥åï¼è¿ç¯è®ºæéå¸¸ä»£è¡¨äºè¯¥é¢åæåæ²¿çè¿å±ï¼å¯è½åå«æ°çæ¶æãè®­ç»ç­ç¥ææ§è½çªç ´ï¼å¯¹çè§£çæå¼AIçæªæ¥æ¹åè³å³éè¦ã</li>
<li><strong>"UniLat3D: Geometry-Appearance Unified Latents for Single-Stage 3D Generation" (Guanjun Wu et al.)</strong>: è¿ç¯è®ºæéè¿ç»ä¸å ä½åå¤è§çæ½å¨ç©ºé´å®ç°åé¶æ®µ3Dçæï¼ä»£è¡¨äº3Dåå®¹åå»ºæçåè´¨éçéå¤§é£è·ï¼ææç®å3Dèµäº§çææµç¨ã</li>
<li><strong>"Score Distillation of Flow Matching Models" (Mingyuan Zhou et al.)</strong>: ç»åäºæµå¹éæ¨¡åååæ°è¸é¦ï¼è¿å¯è½ä¸ºçææ¨¡åæä¾æ°çè®­ç»èå¼ï¼æåçæè´¨éåæçï¼å¯¹æ©æ£æ¨¡ååçæå¼AIç ç©¶èå·æéè¦æä¹ã</li>
<li><strong>"VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning" (Wenhao Li et al.)</strong>: å©ç¨LLMsè¿æ¥è§è§åææ¬è¿è¡å°æ ·æ¬å­¦ä¹ ï¼è¿ä»£è¡¨äºå¤æ¨¡æAIå¨è§£å³æ°æ®ç¨ç¼ºé®é¢ä¸çä¸ä¸ªå¼ºå¤§æ¹åï¼é¢ç¤ºçLLMså¨æ´å¹¿æ³è§è§ä»»å¡ä¸­çåºç¨æ½åã</li>
</ul>
<p><strong>æ°å´ç ç©¶æ¹åæææ¯ï¼</strong></p>
<ul>
<li><strong>ç»ä¸ç3Dæ½å¨ç©ºé´ (Unified 3D Latents)</strong>ï¼å¦UniLat3Dæç¤ºï¼å°å ä½åå¤è§ä¿¡æ¯æ´åå°åä¸æ½å¨è¡¨ç¤ºä¸­ï¼æ¯å®ç°é«æãé«è´¨é3Dçæçéè¦æ¹åã</li>
<li><strong>äºä»¶ç¸æºæ°æ®çæ°åè¡¨ç¤º (Predictive Representation of Events)</strong>ï¼F3æ¨¡åä¸ºäºä»¶ç¸æºæ°æ®æä¾äºä¸ç§é¢æµæ§è¡¨ç¤ºï¼è¿å¯¹äºå¤çé«å¨æãä½å»¶è¿è§è§ä¿¡æ¯è³å³éè¦ï¼å¯è½å¨èªå¨é©¾é©¶åæºå¨äººé¢åæå¹¿æ³åºç¨ã</li>
<li><strong>åºäºå¼ºåå­¦ä¹ çæ°æ®çæ (RL-based Data Generation)</strong>ï¼BRIDGEé¡¹ç®å©ç¨RLçææ·±åº¦ä¼°è®¡æ°æ®ï¼ä¸ºè§£å³ç¹å®ä»»å¡çæ°æ®ç¨ç¼ºé®é¢æä¾äºæ°çæè·¯ã</li>
<li><strong>LLMså¨è§è§ä»»å¡ä¸­çæ·±åº¦èå (Deep Integration of LLMs in Vision)</strong>ï¼VT-FSLå±ç¤ºäºLLMsä¸ä»ä½ä¸ºææ¬çè§£å·¥å·ï¼è¿è½ä½ä¸ºè¿æ¥ä¸åæ¨¡æãèµè½å¤æè§è§ä»»å¡ï¼å¦å°æ ·æ¬å­¦ä¹ ï¼çå¼ºå¤§æ¡¥æ¢ã</li>
<li><strong>å¯å¾®åæ¸²æçå®ç¨å (Practical Differentiable Rendering)</strong>ï¼Triangle Splatting+éè¿ä¸éæä¸è§å½¢å®ç°å¯å¾®åæ¸²æï¼æé«äºæ¸²ææçåå®ç¨æ§ï¼å¯¹NeRFså3Déå»ºç­é¢åæçã</li>
</ul>
<p><strong>å»ºè®®éè¯»å¨æçè®ºæï¼</strong></p>
<p>å¯¹äºå¸ææ·±å¥äºè§£ç¹å®é¢åçå¿ç¢ç ç©¶äººåï¼æå»ºè®®ä¼åéè¯»ä»¥ä¸è®ºæï¼</p>
<ul>
<li><strong>å¯¹äºçæå¼AIåå¤§åæ¨¡åæå´è¶£çï¼</strong><ul>
<li>"HunyuanImage 3.0 Technical Report"</li>
<li>"Score Distillation of Flow Matching Models"</li>
</ul>
</li>
<li><strong>å¯¹äº3Dè§è§ååå®¹çææå´è¶£çï¼</strong><ul>
<li>"UniLat3D: Geometry-Appearance Unified Latents for Single-Stage 3D Generation"</li>
<li>"Triangle Splatting+: Differentiable Rendering with Opaque Triangles"</li>
</ul>
</li>
<li><strong>å¯¹äºå¤æ¨¡æå­¦ä¹ åLLMsåºç¨æå´è¶£çï¼</strong><ul>
<li>"VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning"</li>
</ul>
</li>
<li><strong>å¯¹äºæºå¨äººåå®éåºç¨æå´è¶£çï¼</strong><ul>
<li>"AIRoA MoMa Dataset: A Large-Scale Hierarchical Dataset for Mobile Manipulation"</li>
<li>"BRIDGE -- Building Reinforcement-Learning Depth-to-Image Data Generation Engine for Monocular Depth Estimation"</li>
</ul>
</li>
</ul>
<p>è¿ä»½æè¦æ¨å¨æä¾ä¸ä¸ªé«å±æ¬¡çæ¦è§ï¼å¸®å©æ¨å¿«éè¯å«ä¸æ¨ç ç©¶å´è¶£æç¸å³çææ°è¿å±ã</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2509.23951v1">HunyuanImage 3.0 Technical Report</a></li>
<li><a href="#2509.25146v1">Fast Feature Field (<script type="math/tex">\text{F}^3</script>): A Predictive Representation of Events</a></li>
<li><a href="#2509.25127v1">Score Distillation of Flow Matching Models</a></li>
<li><a href="#2509.25122v1">Triangle Splatting+: Differentiable Rendering with Opaque Triangles</a></li>
<li><a href="#2509.25094v1">Unsupervised Representation Learning for 3D Mesh Parameterization with Semantic and Visibility Objectives</a></li>
<li><a href="#2509.25079v1">UniLat3D: Geometry-Appearance Unified Latents for Single-Stage 3D Generation</a></li>
<li><a href="#2509.25077v1">BRIDGE -- Building Reinforcement-Learning Depth-to-Image Data Generation Engine for Monocular Depth Estimation</a></li>
<li><a href="#2509.25058v1">CharGen: Fast and Fluent Portrait Modification</a></li>
<li><a href="#2509.25033v1">VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning</a></li>
<li><a href="#2509.25032v1">AIRoA MoMa Dataset: A Large-Scale Hierarchical Dataset for Mobile Manipulation</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2509.23951v1'></a></p>
<h2 id="hunyuanimage-30-technical-report"><a href="https://arxiv.org/abs/2509.23951v1">HunyuanImage 3.0 Technical Report</a></h2>
<p><strong>Authors:</strong> Siyu Cao, Hangting Chen, Peng Chen, Yiji Cheng, Yutao Cui, Xinchi Deng, Ying Dong, Kipper Gong, Tianpeng Gu, Xiusen Gu, Tiankai Hang, Duojun Huang, Jie Jiang, Zhengkai Jiang, Weijie Kong, Changlin Li, Donghao Li, Junzhe Li, Xin Li, Yang Li, Zhenxi Li, Zhimin Li, Jiaxin Lin, Linus, Lucaz Liu, Shu Liu, Songtao Liu, Yu Liu, Yuhong Liu, Yanxin Long, Fanbin Lu, Qinglin Lu, Yuyang Peng, Yuanbo Peng, Xiangwei Shen, Yixuan Shi, Jiale Tao, Yangyu Tao, Qi Tian, Pengfei Wan, Chunyu Wang, Kai Wang, Lei Wang, Linqing Wang, Lucas Wang, Qixun Wang, Weiyan Wang, Hao Wen, Bing Wu, Jianbing Wu, Yue Wu, Senhao Xie, Fang Yang, Miles Yang, Xiaofeng Yang, Xuan Yang, Zhantao Yang, Jingmiao Yu, Zheng Yuan, Chao Zhang, Jian-Wei Zhang, Peizhen Zhang, Shi-Xue Zhang, Tao Zhang, Weigang Zhang, Yepeng Zhang, Yingfang Zhang, Zihao Zhang, Zijian Zhang, Penghao Zhao, Zhiyuan Zhao, Xuefei Zhe, Jianchen Zhu, Zhao Zhong</p>
<p><strong>Published:</strong> 2025-09-28</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We present HunyuanImage 3.0, a native multimodal model that unifies
multimodal understanding and generation within an autoregressive framework,
with its image generation module publicly available. The achievement of
HunyuanImage 3.0 relies on several key components, including meticulous data
curation, advanced architecture design, a native Chain-of-Thoughts schema,
progressive model pre-training, aggressive model post-training, and an
efficient infrastructure that enables large-scale training and inference. With
these advancements, we successfully trained a Mixture-of-Experts (MoE) model
comprising over 80 billion parameters in total, with 13 billion parameters
activated per token during inference, making it the largest and most powerful
open-source image generative model to date. We conducted extensive experiments
and the results of automatic and human evaluation of text-image alignment and
visual quality demonstrate that HunyuanImage 3.0 rivals previous
state-of-the-art models. By releasing the code and weights of HunyuanImage 3.0,
we aim to enable the community to explore new ideas with a state-of-the-art
foundation model, fostering a dynamic and vibrant multimodal ecosystem. All
open source assets are publicly available at
https://github.com/Tencent-Hunyuan/HunyuanImage-3.0</p>
<p><strong>Analysis:</strong></p>
<p>ä»¥ä¸æ¯âHunyuanImage 3.0 Technical Reportâçæè¦ï¼</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢</strong>
è¯¥è®ºææ¨å¨è§£å³ç°æå¾åçææ¨¡åå¨éæåº¦ãå¯å¤ç°æ§ä»¥åå¨å¤æææ¬æä»¤éµå¾ªãæ¨çåæ¦å¿µæ³åæ¹é¢çå±éæ§ãç¹å«æ¯ï¼è®¸å¤æåè¿çå¾åçæç³»ç»æ¯é­æºçï¼éå¶äºç ç©¶ç¤¾åºçè¿ä¸æ­¥æ¢ç´¢ãHunyuanImage 3.0çç®æ æ¯å¼åä¸ä¸ªå¼æºçå¤æ¨¡ææ¨¡åï¼è¯¥æ¨¡åå¨å¾åçææ§è½ä¸å¯ä¸é¢åçé­æºæ¨¡ååª²ç¾æè¶è¶ï¼å¹¶è½ç»ä¸å¤æ¨¡æçè§£åçæã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®</strong>
*   <strong>ç»ä¸å¤æ¨¡ææ¡æ¶ï¼</strong> HunyuanImage 3.0æ¯ä¸ä¸ªåççå¤æ¨¡ææ¨¡åï¼å¨ä¸ä¸ªèªåå½æ¡æ¶åç»ä¸äºå¤æ¨¡æçè§£åçæï¼å¶å¾åçææ¨¡åå·²å¬å¼ã
*   <strong>å¤§è§æ¨¡MoEæ¶æï¼</strong> è¯¥æ¨¡ååºäºHunyuan-A13Bï¼ä¸ä¸ªé¢è®­ç»çMoEå¤§åè¯­è¨æ¨¡åï¼LLMï¼ï¼æ»åæ°è¶è¿800äº¿ï¼æ¨çæ¶æ¯tokenæ¿æ´»130äº¿åæ°ï¼ä½¿å¶æä¸ºè¿ä»ä¸ºæ­¢æå¤§ãæå¼ºå¤§çå¼æºå¾åçææ¨¡åã
*   <strong>æ°æ®å¤çï¼</strong> éç¨äºç»è´çæ°æ®æ´çæµç¨ï¼åæ¬ä¸é¶æ®µè¿æ»¤ï¼ä»100äº¿åå§å¾åä¸­ä¿çä¸å°45%ï¼ãåè¯­åå±æ æ³¨æ¹æ¡ï¼æ¯æç­å°è¶é¿çæè¿°ãé£æ ¼å±æ§åäºå®å®ä½ï¼ï¼ä»¥åéè¿ä¸é¨ä»£çåååéªè¯è¿è¡äºå®åºç¡åã
*   <strong>æ¨çæ°æ®éæå»ºï¼</strong> å¼å¥äºèªå¨åçæç»´é¾ï¼CoTï¼æ¨çè¿ç¨ï¼éè¿ææ¬å°ææ¬ï¼T2Tï¼åææ¬å°å¾åï¼T2TIï¼æ¨çæ°æ®è¿è¡å¾®è°ï¼ä»¥å¢å¼ºæ¨¡åçé»è¾æ¨çåè§è§è¡¨ç°è½åã
*   <strong>å¹¿ä¹å ææ³¨æåæºå¶ï¼</strong> æ´åäºææ¬åå¾åæ¨¡æçæ³¨æåæºå¶ï¼ç¡®ä¿ææ¬tokenåªå³æ³¨ååºtokenï¼èå¾åtokenå¯ä»¥å³æ³¨åä¸å¾åæ®µåçææååºåååºå¾åtokenï¼ä»¥å¤çå¼ææ°æ®æ¨¡æã
*   <strong>å¹¿ä¹2Dæè½¬ä½ç½®åµå¥ï¼RoPEï¼ï¼</strong> æ©å±äºRoPEä»¥æ¯æå¾åtokenç2Dä½ç½®ç¼ç ï¼åæ¶ä¿æä¸ä¼ ç»ææ¬çæåé¢è®­ç»LLMçå¼å®¹æ§ã
*   <strong>èªå¨åè¾¨çï¼</strong> æ¨¡åè½å¤æ ¹æ®ä¸ä¸æèªå¨ç¡®å®å¾åçå°ºå¯¸åå®½é«æ¯ï¼éè¿ç¹æ®çtokenæ¥æå¯¼çæå·ææéç»æå±æ§çå¾åã
*   <strong>å¤é¶æ®µè®­ç»ç­ç¥ï¼</strong> éç¨æ¸è¿å¼é¢è®­ç»ï¼åæ¬T2IãLMãMMUãINTLåCoTä»»å¡ï¼ï¼ä»¥åå¤é¶æ®µåè®­ç»ä¼åï¼SFTãDPOãMixGRPOãSRPOåReDAï¼ï¼ä»¥ç³»ç»å°æåçæè½åãåå°ç©çå¤±çãå¢å¼ºææ¬-å¾åå¯¹é½åè§è§è´¨éã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹</strong>
*   <strong>é¢åçå¾åçææ§è½ï¼</strong> èªå¨åäººå·¥è¯ä¼°çææ¬-å¾åå¯¹é½åè§è§è´¨éç»æè¡¨æï¼HunyuanImage 3.0ä¸ä¹åçæåè¿æ¨¡åï¼å¦Seedream 4.0ãNano BananaãGPT-ImageåHunyuanImage 2.1ï¼ç¸æ¯ï¼æ§è½ç¸å½æè¶è¶ã
*   <strong>å¼æºå½±ååï¼</strong> ä½ä¸ºè¿ä»ä¸ºæ­¢æå¤§ãæå¼ºå¤§çå¼æºå¾åçææ¨¡åï¼å¶ä»£ç åæéï¼å¨GitHubä¸å¬å¼ï¼çåå¸æ¨å¨ä¿è¿ç¤¾åºæ¢ç´¢æ°æ³æ³ï¼æ¨å¨å¤æ¨¡æçæç³»ç»çåå±ã
*   <strong>SSAEè¯ä¼°ï¼</strong> å¨ç»æåè¯­ä¹å¯¹é½è¯ä¼°ï¼SSAEï¼ææ ä¸ï¼HunyuanImage 3.0å¨ææç»ç²åº¦å­æ®µä¸­é½è¾¾å°äºä¸é¢åæ¨¡åç¸å½çæ§è½ã
*   <strong>GSBè¯ä¼°ï¼</strong> å¨GSBï¼Good/Same/Badï¼è¯ä¼°ä¸­ï¼HunyuanImage 3.0ç¸å¯¹äºHunyuanImage 2.1åå¾äº14.10%çç¸å¯¹èçï¼ç¸å¯¹äºSeedream 4.0ãNano BananaåGPT-Imageä¹åå¾äºæ¾èèçï¼è¡¨æå¶å¾åçæè´¨éå¯ä¸é¢åçé­æºåä¸æ¨¡ååª²ç¾ã
*   <strong>ä¸å®¶æ¿æ´»åæï¼</strong> ä¸å®¶æ¨¡æåå¥½ç­å¾åKLæ£åº¦åæè¡¨æï¼MoEä¸­çä¸å®¶å¨ä¸åæ¨¡æä¸åå¾è¶æ¥è¶ä¸ä¸åï¼è¿è¡¨æMoEå¯ä»¥éè¿å°ä¸åæ¨¡æçè´£ä»»åæ£ç»ä¸ä¸åä¸å®¶æ¥å¢å¼ºå¤æ¨¡æå»ºæ¨¡ã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§</strong>
*   <strong>å½ååå¸èå´ï¼</strong> æ¬æ¬¡åå¸ä»åå«ææ¬å°å¾åï¼T2Iï¼çè½åï¼å¾åå°å¾åï¼I2Iï¼ä»»å¡çè®­ç»ä»å¨è¿è¡ä¸­ï¼æªæ¥æä¼åå¸ã
*   <strong>AIGCå¯¹æ°æ®åå¸çå½±åï¼</strong> AIGCå¾åçæ©æ£éè¿æ­æ²èªç¶æ°æ®åå¸åæå®³æ¨¡åæ¶ææ§å¸¦æ¥äºéå¤§ææï¼å°½ç®¡è®ºæä¸­æå°äºç¼è§£ç­ç¥ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å</strong>
*   <strong>å¾åå°å¾åä»»å¡ï¼</strong> è®ºææç¡®æåºï¼å¾åå°å¾åä»»å¡çè®­ç»æ­£å¨è¿è¡ä¸­ï¼æªæ¥å°åå¸æ­¤åè½ï¼è¿å°æ¯æ¨¡åè½åçéè¦æ©å±ã
*   <strong>å¤æ¨¡æçæç³»ç»æ¢ç´¢ï¼</strong> éè¿åå¸å¼æºæ¨¡åï¼é¼å±ç¤¾åºæ¢ç´¢åºäºæåè¿åºç¡æ¨¡åçæ°æ³æ³ï¼ä»èä¿è¿å¨æååæ»¡æ´»åçå¤æ¨¡æçæç³»ç»ã
*   <strong>æ´å¤æçæ¨çåæ³åï¼</strong> è¿ä¸æ­¥æ¢ç´¢åå¢å¼ºæ¨¡åçæç»´é¾è®­ç»åæ¨çè½åï¼ä»¥å¤çæ´å¤æãæ´ç»è´çç¨æ·æå¾ååºæ¯ã
*   <strong>æ°æ®ç­å±åæ¨¡åä¼åï¼</strong> æç»­æ¹è¿æ°æ®ç­å±ææ¯åæ¨¡åä¼åç­ç¥ï¼ä»¥è¿ä¸æ­¥æåçæå¾åççå®æãæ¸æ°åº¦åå¯¹äººç±»åå¥½çå¯¹é½ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present HunyuanImage 3.0, a native multimodal model that unifies
multimodal understanding and generation within an autoregressive framework,
with its image generation module publicly available.</li>
<li>We conducted extensive experiments
and the results of automatic and human evaluation of text-image alignment and
visual quality demonstrate that HunyuanImage 3.0 rivals previous
state-of-the-art models.</li>
<li>By releasing the code and weights of HunyuanImage 3.0,
we aim to enable the community to explore new ideas with a state-of-the-art
foundation model, fostering a dynamic and vibrant multimodal ecosystem.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.23951v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.23951v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.25146v1'></a></p>
<h2 id="fast-feature-field-textf3-a-predictive-representation-of-events"><a href="https://arxiv.org/abs/2509.25146v1">Fast Feature Field (<script type="math/tex">\text{F}^3</script>): A Predictive Representation of Events</a></h2>
<p><strong>Authors:</strong> Richeek Das, Kostas Daniilidis, Pratik Chaudhari</p>
<p><strong>Published:</strong> 2025-09-29</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>This paper develops a mathematical argument and algorithms for building
representations of data from event-based cameras, that we call Fast Feature
Field (<script type="math/tex">\text{F}^3</script>). We learn this representation by predicting future events
from past events and show that it preserves scene structure and motion
information. <script type="math/tex">\text{F}^3</script> exploits the sparsity of event data and is robust to
noise and variations in event rates. It can be computed efficiently using ideas
from multi-resolution hash encoding and deep sets - achieving 120 Hz at HD and
440 Hz at VGA resolutions. <script type="math/tex">\text{F}^3</script> represents events within a contiguous
spatiotemporal volume as a multi-channel image, enabling a range of downstream
tasks. We obtain state-of-the-art performance on optical flow estimation,
semantic segmentation, and monocular metric depth estimation, on data from
three robotic platforms (a car, a quadruped robot and a flying platform),
across different lighting conditions (daytime, nighttime), environments
(indoors, outdoors, urban, as well as off-road) and dynamic vision sensors
(resolutions and event rates). Our implementations can predict these tasks at
25-75 Hz at HD resolution.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Richeek Das, Kostas Daniilidis, Pratik Chaudhariæ°åçè®ºæâFast Feature Field (<script type="math/tex">\text{F}^3</script>): A Predictive Representation of Eventsâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼Fast Feature Field (<script type="math/tex">\text{F}^3</script>): ä¸ç§äºä»¶çé¢æµæ§è¡¨ç¤º</strong></p>
<p>è¿ç¯è®ºæä»ç»äºFast Feature Field (<script type="math/tex">\text{F}^3</script>)ï¼è¿æ¯ä¸ç§ä»äºä»¶ç¸æºæ°æ®æå»ºé«æè¡¨ç¤ºçæ°æ¹æ³ãäºä»¶ç¸æºå å¶é«å¨æèå´ãé«æ¶é´åè¾¨çåä½åèèå¤åå³æ³¨ï¼ä½å¶åå§æ°æ®åºæçç¨çæ§ãåªå£°åå¼æ­¥æ§ç»ä¸æ¸¸ä»»å¡å¸¦æ¥äºææã</p>
<p><strong>1. è§£å³çä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥ç ç©¶æ¨å¨è§£å³äºä»¶ç¸æºæ°æ®è¡¨ç¤ºçæ ¹æ¬æ§é®é¢ãä¼ ç»çåºäºå¸§çè®¡ç®æºè§è§ç®æ³é¾ä»¥ç´æ¥å¤çäºä»¶æ°æ®ï¼å ä¸ºå¶ç¨çãå¼æ­¥ååªå£°ç¹æ§ãè®ºæçæ ¸å¿ç®æ æ¯å¼åä¸ç§è½å¤ææææåºæ¯ç»æåè¿å¨ä¿¡æ¯ãå¯¹åªå£°åäºä»¶éçååå·æé²æ£æ§ï¼å¹¶è½é«æè®¡ç®çäºä»¶æ°æ®è¡¨ç¤ºï¼ä»èä¸ºåç§ä¸æ¸¸è®¡ç®æºè§è§ä»»å¡æä¾æ¯æã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>é¢æµæ§è¡¨ç¤ºçå­¦ä¹ ï¼</strong> <script type="math/tex">\text{F}^3</script> çæ ¸å¿åæ°å¨äºå®è¢«å­¦ä¹ ä¸ºè¿å»äºä»¶çç»è®¡éï¼è¶³ä»¥é¢æµæªæ¥äºä»¶ãè®ºæä»æ°å­¦ä¸è®ºè¯äºè¿ç§é¢æµæ§è¡¨ç¤ºè½å¤ä¿çåºæ¯çç»æåè¿å¨ä¿¡æ¯ã
*   <strong>å¤åè¾¨çåå¸ç¼ç ä¸æ·±åº¦éæ¶æï¼</strong> ä¸ºäºé«æè®¡ç®å¹¶å©ç¨äºä»¶æ°æ®çç¨çæ§ï¼<script type="math/tex">\text{F}^3</script> éç¨äºå¤åè¾¨çåå¸ç¼ç ï¼ç±»ä¼¼äºç¥ç»æ¸²æé¢åï¼åç½®æ¢ä¸åçæ·±åº¦éï¼Deep Setï¼æ¶æãåå¸ç¼ç å°äºä»¶åæ æ å°å°ç¹å¾ç©ºé´ï¼å¹¶éè¿æ± ååå·ç§¯æä½è¿è¡æ¶é´èååç©ºé´å¹³æ»ã
*   <strong>é²æ£æ§è®­ç»ç®æ ï¼</strong> éå¯¹äºä»¶æ°æ®çåªå£°åäºä»¶éççæç«¯ä¸å¹³è¡¡ï¼è®ºæéç¨äºä¸ç§å æFocal Lossåä½ä½ä¸ºè®­ç»ç®æ ï¼èéä¼ ç»çåæ¹è¯¯å·®ãè¿ä½¿å¾ <script type="math/tex">\text{F}^3</script> å¯¹åªå£°åäºä»¶éçååå·ææ´å¼ºçé²æ£æ§ï¼å¹¶æå©äºé²æ­¢ç¹å¾åå¡ã
*   <strong>å¤ééå¾åè¡¨ç¤ºï¼</strong> <script type="math/tex">\text{F}^3</script> å°äºä»¶è¡¨ç¤ºä¸ºä¸ä¸ªè¿ç»­æ¶ç©ºä½ç§¯åçå¤ééå¾åï¼ä½¿å¶è½å¤æ ç¼éæå°ä»»ä½æ åè®¡ç®æºè§è§ç®æ³ååºäºRGBæ°æ®çç¥ç»ç½ç»æ¶æä¸­ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æåè¿çæ§è½ï¼</strong> <script type="math/tex">\text{F}^3</script> å¨å¤ä¸ªä¸æ¸¸ä»»å¡ï¼åæµä¼°è®¡ãè¯­ä¹åå²ååç®æ·±åº¦ä¼°è®¡ï¼ä¸åå¾äºæåè¿çæ§è½ï¼æ¾èä¼äºç°ææ¹æ³ã
*   <strong>é«æè®¡ç®ï¼</strong> <script type="math/tex">\text{F}^3</script> çè®¡ç®æçæé«ï¼å¨HDåè¾¨çä¸è¾¾å°120 Hzï¼å¨VGAåè¾¨çä¸è¾¾å°440 Hzãåºäº<script type="math/tex">\text{F}^3</script> çä¸æ¸¸ä»»å¡é¢æµå¨HDåè¾¨çä¸ä¹è½è¾¾å°25-75 Hzï¼æ¯ç°ææåè¿çäºä»¶åºæ¹æ³å¿«2-5åã
*   <strong>å¼ºå¤§çæ³åè½åï¼</strong> <script type="math/tex">\text{F}^3</script> å¨ä¸åæºå¨äººå¹³å°ï¼æ±½è½¦ãåè¶³æºå¨äººãé£è¡å¹³å°ï¼ãä¸ååç§æ¡ä»¶ï¼ç½å¤©ãå¤æï¼åä¸åç¯å¢ï¼å®¤åãå®¤å¤ãåå¸ãè¶éï¼ä¸è¡¨ç°åºå¼ºå¤§çæ³åè½åï¼æ éé¢å¤çè®­ç»ã
*   <strong>å¯¹æºå¨äººæç¥çå½±åï¼</strong> <script type="math/tex">\text{F}^3</script> ä¸ºå®æ¶ãå¯æ³åçäºä»¶æç¥æä¾äºåºç¡ï¼ææä½¿æºå¨äººè½å¤å¨åç§å·ææææ§çæ¡ä»¶ä¸æ´ææå°è¿è¡ã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
*   <strong>å½åå®ç°ä¸­çç¨ å¯å·ç§¯ï¼</strong> å°½ç®¡è®ºæå¼ºè°äºå©ç¨äºä»¶æ°æ®ç¨çæ§çéè¦æ§ï¼ä½ç®åç<script type="math/tex">\text{F}^3</script> å®ç°ä»ä½¿ç¨ç¨ å¯2Då·ç§¯å±ï¼èéç¨çå·ç§¯ãä½èæåºï¼è¿æ¯å ä¸ºPyTorchä¸­ç¨ å¯å·ç§¯ç®åæ¯ç¨çå·ç§¯ç¨å¿«ï¼ä½é¢è®¡æªæ¥ä¼æ¹åã
*   <strong>å¯¹ä¼ªæ ç­¾ååæ­¥çä¾èµï¼</strong> å¨è¯­ä¹åå²åæ·±åº¦ä¼°è®¡ç­ä»»å¡ä¸­ï¼è®ºæä¾èµäºä»RGBå¾åçæçä¼ªæ ç­¾åLiDARæ°æ®ãè¿è¦æ±RGBç¸æºåäºä»¶ç¸æºä¹é´çæ¶é´æ³ç²¾ç¡®åæ­¥åå¤é¨æ ¡åï¼DSECæ°æ®éä¸­å­å¨ä¸äºå¯¹é½é®é¢ã
*   <strong>æªå®å¨å©ç¨äºä»¶ææ§ï¼</strong> è®ºæä¸ºäºç®ååéä½è®¡ç®ææ¬ï¼å¿½ç¥äºäºä»¶çææ§ä¿¡æ¯ï¼èä»å³æ³¨äºä»¶çå­å¨ä¸å¦ãè½ç¶ä½èæå°å¶ææ¯å¯ä»¥éåºåå«ææ§çäºä»¶ï¼ä½è¿ä¼å¢å åå­åè®¡ç®å¼éã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ´å¨å±çç¹å¾è¡¨ç¤ºï¼</strong> è®ºææåºï¼<script type="math/tex">\text{F}^3</script> æ¶æå¯ä»¥æ©å±ä»¥æå»ºæ´å¨å±çç¹å¾ï¼ä¾å¦ï¼éè¿æ´å¤çå·ç§¯å±åé¢æµæ´å¤§çæªæ¥è¡¥ä¸ï¼ï¼ä»èäº§çæ´ä¸°å¯çè¯­ä¹ç¹å¾ï¼ç±»ä¼¼äºè§è§ç®å±ã
*   <strong>å¢éå¼æ´æ°ï¼</strong> <script type="math/tex">\text{F}^3</script> å¯ä»¥éè¿ä¸äºç°¿è®°æä½å¨æ¯ä¸ªäºä»¶ä¹åè¿è¡å¢éå¼æ´æ°ï¼è¿å¯¹äºåæµåæ·±åº¦ä¼°è®¡ç­ä»»å¡éå¸¸æç¨ã
*   <strong>ç¨çå·ç§¯çä¼åï¼</strong> éçç¨çå·ç§¯å¨ç¡¬ä»¶åè½¯ä»¶å±é¢å¾å°æ´å¥½çæ¯æï¼åºäºç¨çå·ç§¯ç<script type="math/tex">\text{F}^3</script> å®ç°ææå¨äºä»¶æ°ééå¸¸å°ï¼ä¾å¦ï¼æç«¯ä½åç§æ¡ä»¶ï¼çåºæ¯ä¸­è¡¨ç°æ´å¿«ã
*   <strong>è·¨æ¨¡ææ°æ®æ± åï¼</strong> ä½èæç¤ºï¼å°æ¥èªä¸åæºå¨äººå¹³å°åç¯å¢çæ°æ®è¿è¡æ± åï¼å°è¿ä¸æ­¥æé«<script type="math/tex">\text{F}^3</script> æ¹æ³çé²æ£æ§ã
*   <strong>ç¥ç»å½¢æè®¡ç®åASICçåºç¨ï¼</strong> å°<script type="math/tex">\text{F}^3</script> ä¸ç¥ç»å½¢æè®¡ç®æç´æ¥å¨åç´ ä¸æ§è¡è®¡ç®çASICç»åï¼å¯ä»¥è¿ä¸æ­¥æé«å®éæ§è½åè½æºæçã</p>
<hr />
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæéè¿å¼å¥Fast Feature Field (<script type="math/tex">\text{F}^3</script>)ï¼ä¸ºäºä»¶ç¸æºæ°æ®å¤çé¢åååºäºéå¤§è´¡ç®ãå®æä¾äºä¸ç§æ°å­¦ä¸åçãè®¡ç®é«æä¸å¯¹åªå£°åäºä»¶éçé²æ£çäºä»¶è¡¨ç¤ºï¼ä¸ºäºä»¶ç¸æºå¨åç§æºå¨äººæç¥ä»»å¡ä¸­çå¹¿æ³åºç¨éºå¹³äºéè·¯ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We obtain state-of-the-art performance on optical flow estimation,
semantic segmentation, and monocular metric depth estimation, on data from
three robotic platforms (a car, a quadruped robot and a flying platform),
across different lighting conditions (daytime, nighttime), environments
(indoors, outdoors, urban, as well as off-road) and dynamic vision sensors
(resolutions and event rates).</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.25146v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.25146v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.25127v1'></a></p>
<h2 id="score-distillation-of-flow-matching-models"><a href="https://arxiv.org/abs/2509.25127v1">Score Distillation of Flow Matching Models</a></h2>
<p><strong>Authors:</strong> Mingyuan Zhou, Yi Gu, Huangjie Zheng, Liangchen Song, Guande He, Yizhe Zhang, Wenze Hu, Yinfei Yang</p>
<p><strong>Published:</strong> 2025-09-29</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Diffusion models achieve high-quality image generation but are limited by
slow iterative sampling. Distillation methods alleviate this by enabling one-
or few-step generation. Flow matching, originally introduced as a distinct
framework, has since been shown to be theoretically equivalent to diffusion
under Gaussian assumptions, raising the question of whether distillation
techniques such as score distillation transfer directly. We provide a simple
derivation -- based on Bayes' rule and conditional expectations -- that unifies
Gaussian diffusion and flow matching without relying on ODE/SDE formulations.
Building on this view, we extend Score identity Distillation (SiD) to
pretrained text-to-image flow-matching models, including SANA, SD3-Medium,
SD3.5-Medium/Large, and FLUX.1-dev, all with DiT backbones. Experiments show
that, with only modest flow-matching- and DiT-specific adjustments, SiD works
out of the box across these models, in both data-free and data-aided settings,
without requiring teacher finetuning or architectural changes. This provides
the first systematic evidence that score distillation applies broadly to
text-to-image flow matching models, resolving prior concerns about stability
and soundness and unifying acceleration techniques across diffusion- and
flow-based generators. We will make the PyTorch implementation publicly
available.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Mingyuan Zhouç­äººæ°åçè®ºæâScore Distillation of Flow Matching Modelsâçå¨é¢æè¦ã</p>
<hr />
<h3 id="score-distillation-of-flow-matching-models_1">è®ºææè¦ï¼âScore Distillation of Flow Matching Modelsâ</h3>
<p><strong>1. è§£å³çä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
æ©æ£æ¨¡åå¨å¾åçææ¹é¢è¡¨ç°åºè²ï¼ä½å¶è¿­ä»£éæ ·è¿ç¨ç¼æ¢ï¼éå¶äºå®éåºç¨ãè¸é¦æ¹æ³æ¨å¨éè¿å®ç°ä¸æ­¥æå æ­¥çææ¥å éè¿ä¸è¿ç¨ãæµå¹éï¼Flow Matchingï¼ä½ä¸ºä¸ç§ç¬ç«çæ¡æ¶è¢«å¼å¥ï¼åæ¥è¢«è¯æå¨é«æ¯åè®¾ä¸ä¸æ©æ£æ¨¡åçè®ºç­ä»·ãè¿å¼åäºä¸ä¸ªå³é®é®é¢ï¼æ©æ£æ¨¡åä¸­ä½¿ç¨çè¸é¦ææ¯ï¼ç¹å«æ¯åæ°è¸é¦ï¼æ¯å¦å¯ä»¥ç´æ¥ä¸ææå°åºç¨äºæµå¹éæ¨¡åï¼ä»¥åå¨åºç¨è¿ç¨ä¸­æ¯å¦éè¦è¿è¡æ¨¡åç¹å®çè°æ´ææå¸æ¨¡åå¾®è°ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>ç»ä¸ççè®ºè§è§ï¼</strong> è®ºæéè¿åºäºè´å¶æ¯è§ååæ¡ä»¶ææçç®åæ¨å¯¼ï¼ç»ä¸äºé«æ¯æ©æ£åæµå¹éï¼èæ éä¾èµå¤æçODE/SDEå¬å¼ãè¿è¡¨æå¨çè®ºä¸ï¼ä¸¤ç§æ¡æ¶å¨æä½³è§£å³æ¹æ¡ä¸æ¯ç­ä»·çï¼ä¸»è¦åºå«å¨äºæ¶é´æ­¥é¿çå æåå¸ã
*   <strong>SiD-DiTæ¡æ¶çæ©å±ä¸åºç¨ï¼</strong> è®ºæå°åæ°åä¸æ§è¸é¦ï¼Score identity Distillation, SiDï¼æ¹æ³æ©å±å°é¢è®­ç»çææ¬å°å¾åæµå¹éæ¨¡åï¼è¿äºæ¨¡ååéç¨DiTï¼Diffusion Transformerï¼éª¨å¹²ç½ç»ï¼åæ¬SANAãSD3-MediumãSD3.5-Medium/LargeåFLUX.1-devã
*   <strong>å¼ç®±å³ç¨ï¼Out-of-the-boxï¼çéç¨æ§ï¼</strong> å®éªè¯æï¼SiD-DiTåªéå¯¹æµå¹éåDiTæ¨¡åè¿è¡éåº¦è°æ´ï¼å³å¯å¨æ°æ®æ å³ï¼data-freeï¼åæ°æ®è¾å©ï¼data-aidedï¼è®¾ç½®ä¸ï¼æ éæå¸æ¨¡åå¾®è°ææ¶ææ´æ¹ï¼ç´æ¥åºç¨äºè¿äºæ¨¡åã
*   <strong>å¯¹ææ§å­¦ä¹ éæï¼</strong> å¨æ°æ®è¾å©è®¾ç½®ä¸­ï¼SiDéè¿å¨å¤å«å¨ç¹å¾ä¸­å¼å¥ç©ºé´æ± åï¼å°å¯¹ææ§å­¦ä¹ ï¼Diffusion GANï¼æ´åå°DiTéª¨å¹²ç½ç»ä¸­ï¼è¿ä¸æ­¥æåäºæ§è½ï¼ä¸æªå¼å¥é¢å¤åæ°ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>å¹¿æ³éç¨æ§ï¼</strong> è®ºæé¦æ¬¡ç³»ç»æ§å°è¯æäºåæ°è¸é¦å¯ä»¥å¹¿æ³åºç¨äºææ¬å°å¾åæµå¹éæ¨¡åï¼è§£å³äºååå³äºç¨³å®æ§ååçæ§çæå¿§ã
*   <strong>æ§è½æåï¼</strong> SiD-DiTå¨æ°æ®æ å³è®¾ç½®ä¸ï¼å¨SANA-Sprintæ¨¡åä¸æç»­ä¼äºSANA-Sprintï¼å¹¶å¨SD3ç³»åæ¨¡åä¸å¹éæè¶è¶æå¸æ¨¡åæ§è½ãå¨æ°æ®è¾å©è®¾ç½®ä¸ï¼éè¿å¯¹ææ§å­¦ä¹ ï¼SiD2-DiTå¨FIDï¼FrÃ©chet Inception Distanceï¼æ¹é¢å®ç°äºæ¾èéä½ï¼åæ¶ä¿æäºCLIPåGenEvalåæ°ã
*   <strong>æçä¸é²æ£æ§ï¼</strong> SiD-DiTæ¡æ¶å¨ä¸åæ¶æãåªå£°è°åº¦åæ¨¡åè§æ¨¡çDiTæµå¹éæ¨¡åä¸å±ç°åºé«ææ§åé²æ£æ§ï¼ä½¿ç¨åä¸ä»£ç åºåè¶åæ°éç½®å³å¯å®ç°ã
*   <strong>ç»ä¸å éææ¯ï¼</strong> è®ºæç»ä¸äºæ©æ£åæµå¹éçæå¨ä¸­çå éææ¯ï¼ä¸ºæªæ¥ç ç©¶æä¾äºåå®ççè®ºåç»éªåºç¡ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>FLUX.1-DEVçæ§è½å·®è·ï¼</strong> SiD-DiTå¨FLUX.1-DEVä¸çæ§è½æåç¸å¯¹æ¸©åï¼é¨ååå å½å äºæå¯¼æºå¶çä¸å¹éï¼FLUX.1-DEVéç¨å­¦ä¹ å°çæå¯¼åµå¥ï¼èéä¼ ç»çCFGï¼ã
*   <strong>åº¦éææ çè§£éï¼</strong> è®ºææåºï¼FIDãCLIPåGenEvalç­åº¦éææ å¨æ¯è¾ä¸åæ¨¡åå®¶æåè§æ¨¡æ¶åºè°¨æè§£éï¼è§è§æ£æ¥å¯è½ä¸è¿äºææ çç»è®ºä¸å®å¨ä¸è´ã
*   <strong>æ°æ®è´¨éå¯¹å¯¹ææ§å­¦ä¹ çå½±åï¼</strong> è½ç¶å¯¹ææ§å­¦ä¹ å¯ä»¥å¢å æ ·æ¬å¤æ ·æ§å¹¶æ¹åFIDï¼ä½å¦æä½¿ç¨çæ°æ®éè´¨éæéï¼å¦MidJourney-v6-llavaï¼ï¼å¯è½æ æ³æ¾èæåè§è§è´¨éï¼ä¸çæçå¾åé£æ ¼å¯è½ä¸ç¬¦åç¨æ·åå¥½ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>éå¯¹FLUX.1-DEVçå®å¶åï¼</strong> éè¿å°å­¦ä¹ å°çæå¯¼åµå¥éæå°è¸é¦ç®æ ä¸­ï¼æå¼åç»åCFGåæ¨¡åç¹å®æå¯¼çæ··åæ¹æ³ï¼è¿ä¸æ­¥ä¼åSiD-DiTä»¥éåºFLUX.1-DEVçç¬ç¹è®¾è®¡ã
*   <strong>æ¢ç´¢æ¶é´æ­¥é¿å æåå¸çå½±åï¼</strong> å¯¹ä¸åp(t)åwtå¦ä½å½±åæ§è½è¿è¡ç³»ç»æ§ç ç©¶ï¼ä»¥æ´å¥½å°çè§£åä¼åè¸é¦è¿ç¨ã
*   <strong>ç»ä¸çæå»ºæ¨¡åå¿«ééæ ·ç­ç¥ï¼</strong> è®ºæä¸ºæªæ¥å¨ç»ä¸çæå»ºæ¨¡åå¿«ééæ ·ç­ç¥æ¹é¢çç ç©¶æä¾äºçè®ºåç»éªåºç¡ï¼é¼å±è¿ä¸æ­¥æ¢ç´¢ã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>This provides
the first systematic evidence that score distillation applies broadly to
text-to-image flow matching models, resolving prior concerns about stability
and soundness and unifying acceleration techniques across diffusion- and
flow-based generators.</li>
<li>We will make the PyTorch implementation publicly
available.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.25127v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.25127v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.25122v1'></a></p>
<h2 id="triangle-splatting-differentiable-rendering-with-opaque-triangles"><a href="https://arxiv.org/abs/2509.25122v1">Triangle Splatting+: Differentiable Rendering with Opaque Triangles</a></h2>
<p><strong>Authors:</strong> Jan Held, Renaud Vandeghen, Sanghyun Son, Daniel Rebain, Matheus Gadelha, Yi Zhou, Ming C. Lin, Marc Van Droogenbroeck, Andrea Tagliasacchi</p>
<p><strong>Published:</strong> 2025-09-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Reconstructing 3D scenes and synthesizing novel views has seen rapid progress
in recent years. Neural Radiance Fields demonstrated that continuous volumetric
radiance fields can achieve high-quality image synthesis, but their long
training and rendering times limit practicality. 3D Gaussian Splatting (3DGS)
addressed these issues by representing scenes with millions of Gaussians,
enabling real-time rendering and fast optimization. However, Gaussian
primitives are not natively compatible with the mesh-based pipelines used in VR
headsets, and real-time graphics applications. Existing solutions attempt to
convert Gaussians into meshes through post-processing or two-stage pipelines,
which increases complexity and degrades visual quality. In this work, we
introduce Triangle Splatting+, which directly optimizes triangles, the
fundamental primitive of computer graphics, within a differentiable splatting
framework. We formulate triangle parametrization to enable connectivity through
shared vertices, and we design a training strategy that enforces opaque
triangles. The final output is immediately usable in standard graphics engines
without post-processing. Experiments on the Mip-NeRF360 and Tanks &amp; Temples
datasets show that Triangle Splatting+achieves state-of-the-art performance in
mesh-based novel view synthesis. Our method surpasses prior splatting
approaches in visual fidelity while remaining efficient and fast to training.
Moreover, the resulting semi-connected meshes support downstream applications
such as physics-based simulation or interactive walkthroughs. The project page
is https://trianglesplatting2.github.io/trianglesplatting2/.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾è¿ç¯è®ºæâTriangle Splatting+: Differentiable Rendering with Opaque Trianglesâçå¨é¢æè¦ã</p>
<hr />
<h3 id="triangle-splatting-differentiable-rendering-with-opaque-triangles_1">è®ºææè¦ï¼Triangle Splatting+: Differentiable Rendering with Opaque Triangles</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¿å¹´æ¥ï¼3Dåºæ¯éå»ºåæ°è§è§åæåå¾äºæ¾èè¿å±ï¼ç¹å«æ¯ç¥ç»è¾å°åºï¼NeRFï¼å3Dé«æ¯æ³¼æºï¼3DGSï¼ç­æ¹æ³ãNeRFæä¾äºé«è´¨éçå¾ååæï¼ä½è®­ç»åæ¸²ææ¶é´é¿ï¼3DGSå®ç°äºå®æ¶æ¸²æåå¿«éä¼åï¼ä½å¶é«æ¯åºåä¸VRå¤´æ¾åå®æ¶å¾å½¢åºç¨ä¸­ä½¿ç¨çåºäºç½æ ¼çæ¸²æç®¡çº¿ä¸å¼å®¹ãç°æè§£å³æ¹æ¡éè¿åå¤çæä¸¤é¶æ®µç®¡çº¿å°é«æ¯è½¬æ¢ä¸ºç½æ ¼ï¼è¿å¢å äºå¤ææ§å¹¶éä½äºè§è§è´¨éã</p>
<p>æ¬ç ç©¶æ¨å¨è§£å³çæ ¸å¿é®é¢æ¯ï¼<strong>å¦ä½å¨ä¿æé«è§è§è´¨éãå®æ¶æ§è½åä¸ç°æå¾å½¢ç®¡çº¿å¼å®¹æ§çåæä¸ï¼ç´æ¥ä¼åè®¡ç®æºå¾å½¢å­¦ä¸­æåºæ¬çåºåââä¸éæä¸è§å½¢ï¼ä»¥å®ç°3Dåºæ¯éå»ºåæ°è§è§åæï¼</strong></p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
Triangle Splatting+å¼å¥äºä¸ä¸ªå¯å¾®åçæ³¼æºæ¡æ¶ï¼ç´æ¥ä¼åä¸è§å½¢ï¼å¹¶æåºäºä»¥ä¸å³é®åæ°ï¼</p>
<ul>
<li><strong>é¡¶ç¹å±äº«çä¸è§å½¢åæ°åï¼</strong> è®ºæéæ°å®ä¹äºä¸è§å½¢çåæ°åæ¹å¼ï¼éè¿å±äº«é¡¶ç¹éå®ç°ä¸è§å½¢ä¹é´çè¿æ¥æ§ï¼èéåä¹åæ¹æ³é£æ ·ä¿æå­¤ç«ãè¿ä½¿å¾ä¸è§å½¢è½å¤éè¿å±åé¡¶ç¹èªç¶è¿æ¥ï¼ä»èå½¢æåè¿æ¥ç½æ ¼ï¼æé«äºç»æä¸è´æ§ã</li>
<li><strong>å¼ºå¶ä¸éæä¸è§å½¢çè®­ç»ç­ç¥ï¼</strong> è®¾è®¡äºä¸ç§å®å¶çè®­ç»ç­ç¥ï¼å¨è®­ç»è¿ç¨ä¸­éæ­¥å¼ºå¶ä¸è§å½¢åä¸ºå®å¨ä¸éæãè¿è§£å³äºä»¥å¾æ³¼æºæ¹æ³ä¸­ä¸è§å½¢å¯è½ä¿æåéæçé®é¢ï¼ç¡®ä¿æç»è¾åºçç½æ ¼å¯ä»¥ç´æ¥å¯¼å¥æ åå¾å½¢å¼æèæ éåå¤çã</li>
<li><strong>ç»åè§è§ä¿çåº¦åå ä½ç²¾åº¦çè®­ç»ç­ç¥ï¼</strong> è¯¥ç­ç¥å¨è®­ç»åæåè®¸ä¸è§å½¢å¹³æ»ï¼è½¯è¿æ¸¡ï¼ååéæï¼ä»¥ç¡®ä¿æ¢¯åº¦æµå¨ï¼å¹¶å¨è®­ç»åæéæ¸æ¶æå°éå©ãä¸éæçä¸è§å½¢ã</li>
<li><strong>æ¹è¿çåªæåç¨ å¯åç­ç¥ï¼</strong> å¼å¥äºåºäºæå¤§ä½æ¸²ææéï¼èéåçº¯ä¸éæåº¦ï¼çåªæç­ç¥ï¼ä»¥ææç§»é¤åä½ä¸è§å½¢ï¼é¿åå¨ä¸è§å½¢åå¾ä¸éæåäº§çä¼ªå½±ãç¨ å¯åéè¿ä¸­ç¹ç»åå¼å¥æ°çé¡¶ç¹åä¸è§å½¢ï¼åæ¶ä¿æè¿æ¥æ§ã</li>
<li><strong>ç´æ¥å¼å®¹æ¸¸æå¼æï¼</strong> æç»è¾åºæ¯ä»ç±ä¸éæä¸è§å½¢ç»æçåè¿æ¥ç½æ ¼ï¼æ éä»»ä½åå¤çå³å¯ç«å³ç¨äºæ åå¾å½¢å¼æï¼æ¯æç©çäº¤äºãå¯æ­¥è¡åºæ¯ååºæ¯ç¼è¾ç­ä¸æ¸¸åºç¨ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æåè¿çæ§è½ï¼</strong> å¨Mip-NeRF360åTanks &amp; Templesæ°æ®éä¸çå®éªè¡¨æï¼Triangle Splatting+å¨åºäºç½æ ¼çæ°è§è§åææ¹é¢åå¾äºæåè¿çæ§è½ï¼å¨ææææ ä¸åä¼äºç°ææ¹æ³ãä¸2DGSåTriangle Splattingç­æ¹æ³ç¸æ¯ï¼å¨ç¸ä¼¼é¡¶ç¹æ°éä¸ï¼PSNRæé«äº4-10 dBã
*   <strong>é«è§è§ä¿çåº¦ä¸æçï¼</strong> è¯¥æ¹æ³å¨è§è§ä¿çåº¦ä¸è¶è¶äºä»¥å¾çæ³¼æºæ¹æ³ï¼åæ¶ä¿æäºé«æåå¿«éçè®­ç»éåº¦ï¼Mip-NeRF360æ°æ®éä¸39åéï¼T&amp;Tæ°æ®éä¸25åéï¼ã
*   <strong>æ¯æä¸æ¸¸åºç¨ï¼</strong> çæçåè¿æ¥ç½æ ¼è½å¤æ¯æç©çæ¨¡æãäº¤äºå¼åºæ¯æ¼«æ¸¸ãå¯¹è±¡æååç§»é¤ç­ä¸æ¸¸åºç¨ï¼èæ éå®å¨æ°´å¯ç½æ ¼ãè¿æå¤§å°æ©å±äºè¾å°åºè¡¨ç¤ºçå®ç¨æ§ã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
*   <strong>ç¨çåºåçéå»ºï¼</strong> å¨åå§ç¹äºç¨çè¦ççèæ¯åºåï¼å ä½ç»æå¯è½ä¸å®æ´ï¼ä¿çåº¦è¾ä½ã
*   <strong>è®­ç»è§è§å¤çæ§è½ä¸éï¼</strong> å½ç§»å¨å°è®­ç»è§è§èå´ä¹å¤æ¶ï¼è§è§è´¨éä¼ä¸éï¼å ä¸ºä¸éæä¸è§å½¢çä½¿ç¨ä¼ä½¿ä¼ªå½±æ´å ææ¾ã
*   <strong>éæç©ä½è¡¨ç¤ºå°é¾ï¼</strong> å¯¹äºç»çæç¶å­ç­éæç©ä½ï¼ä»ä½¿ç¨ä¸éæä¸è§å½¢è¿è¡è¡¨ç¤ºä»ç¶å·ææææ§ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ´å®æ´çç¹äºåå§åï¼</strong> éè¿æ´å®æ´çç¹äºåå§åæ¥æ¹åç¨çåºåçéå»ºè´¨éã
*   <strong>ç»åæ¿ä»£è¡¨ç¤ºï¼</strong> å¼å¥å¶ä»è¡¨ç¤ºå½¢å¼ï¼ä¾å¦ä¸è§åçå¤©ç©ºç©¹é¡¶ï¼ä»¥è§£å³èæ¯åºåçå±éæ§ã
*   <strong>éæç©ä½å¤çï¼</strong> æ¢ç´¢å¦ä½ä½¿ç¨ä¸éæä¸è§å½¢æç»åå¶ä»æºå¶æ¥ææè¡¨ç¤ºéæç©ä½ã</p>
<hr />
<p>æ»èè¨ä¹ï¼Triangle Splatting+éè¿ç´æ¥ä¼åå·æå±äº«é¡¶ç¹è¿æ¥æ§çä¸éæä¸è§å½¢ï¼æåå°å¼¥åäºè¾å°åºä¼åä¸ä¼ ç»è®¡ç®æºå¾å½¢å­¦ä¹é´çé¸¿æ²ãå¶åæ°æ§çåæ°ååè®­ç»ç­ç¥ä¸ä»å®ç°äºåè¶çè§è§è´¨éåé«æçè®­ç»ï¼è¿ç¡®ä¿äºè¾åºä¸ç°æå¾å½¢ç®¡çº¿çæ ç¼å¼å®¹æ§ï¼ä¸ºVR/ARåºç¨ãæ¸¸æå¼æåæ¨¡ææ¡æ¶çå®ééæéºå¹³äºéè·¯ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Reconstructing 3D scenes and synthesizing novel views has seen rapid progress
in recent years.</li>
<li>Experiments on the Mip-NeRF360 and Tanks &amp; Temples
datasets show that Triangle Splatting+achieves state-of-the-art performance in
mesh-based novel view synthesis.</li>
<li>Our method surpasses prior splatting
approaches in visual fidelity while remaining efficient and fast to training.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.25122v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.25122v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.25094v1'></a></p>
<h2 id="unsupervised-representation-learning-for-3d-mesh-parameterization-with-semantic-and-visibility-objectives"><a href="https://arxiv.org/abs/2509.25094v1">Unsupervised Representation Learning for 3D Mesh Parameterization with Semantic and Visibility Objectives</a></h2>
<p><strong>Authors:</strong> AmirHossein Zamani, Bruno Roy, Arianna Rampini</p>
<p><strong>Published:</strong> 2025-09-29</p>
<p><strong>Categories:</strong> cs.GR, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent 3D generative models produce high-quality textures for 3D mesh
objects. However, they commonly rely on the heavy assumption that input 3D
meshes are accompanied by manual mesh parameterization (UV mapping), a manual
task that requires both technical precision and artistic judgment. Industry
surveys show that this process often accounts for a significant share of asset
creation, creating a major bottleneck for 3D content creators. Moreover,
existing automatic methods often ignore two perceptually important criteria:
(1) semantic awareness (UV charts should align semantically similar 3D parts
across shapes) and (2) visibility awareness (cutting seams should lie in
regions unlikely to be seen). To overcome these shortcomings and to automate
the mesh parameterization process, we present an unsupervised differentiable
framework that augments standard geometry-preserving UV learning with semantic-
and visibility-aware objectives. For semantic-awareness, our pipeline (i)
segments the mesh into semantic 3D parts, (ii) applies an unsupervised learned
per-part UV-parameterization backbone, and (iii) aggregates per-part charts
into a unified UV atlas. For visibility-awareness, we use ambient occlusion
(AO) as an exposure proxy and back-propagate a soft differentiable AO-weighted
seam objective to steer cutting seams toward occluded regions. By conducting
qualitative and quantitative evaluations against state-of-the-art methods, we
show that the proposed method produces UV atlases that better support texture
generation and reduce perceptible seam artifacts compared to recent baselines.
Our implementation code is publicly available at:
https://github.com/AHHHZ975/Semantic-Visibility-UV-Param.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾AmirHossein Zamani, Bruno Roy, Arianna Rampiniæ°åçè®ºæâUnsupervised Representation Learning for 3D Mesh Parameterization with Semantic and Visibility Objectivesâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼åºäºè¯­ä¹åå¯è§æ§ç®æ ç3Dç½æ ¼åæ°åæ çç£è¡¨ç¤ºå­¦ä¹ </strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
å½å3Dçææ¨¡åå¨çæé«è´¨éçº¹çæ¹é¢è¡¨ç°åºè²ï¼ä½æ®éä¾èµäºæå¨ç½æ ¼åæ°åï¼UVæ å°ï¼ï¼è¿æ¯ä¸ä¸ªèæ¶ä¸éè¦ææ¯åèºæ¯å¤æ­çä»»å¡ãç°æèªå¨åæ¹æ³éå¸¸å¿½ç¥äºä¸¤ä¸ªå³é®çæç¥æ åï¼(1) è¯­ä¹æç¥ï¼å³UVå¾è¡¨åºä¸3Då½¢ç¶ä¸­è¯­ä¹ç¸ä¼¼çé¨åå¯¹é½ï¼(2) å¯è§æ§æç¥ï¼å³åå²ç¼åºä½äºä¸æè¢«è§å¯å°çåºåãè¿å¯¼è´äºçº¹ççæåæ¸²æä¸­å¯è§çæ¥ç¼ä¼ªå½±ï¼å¹¶éå¶äºUVå¾è¡¨å¨è¯­ä¹ä¸çä¸è´æ§ãæ¬ç ç©¶æ¨å¨è§£å³è¿äºå±éæ§ï¼èªå¨åç½æ ¼åæ°åè¿ç¨ï¼å¹¶ä½¿å¶çæçUVå¾è¡¨å¨è¯­ä¹åå¯è§æ§æ¹é¢æ´ä¼ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
æ¬ææåºäºä¸ç§æ çç£ãå¯å¾®åçæ¡æ¶ï¼éè¿å¼å¥è¯­ä¹æç¥åå¯è§æ§æç¥ç®æ ï¼å¢å¼ºäºæ åçå ä½ä¿æUVå­¦ä¹ ã
*   <strong>è¯­ä¹æç¥ï¼Semantic-Awarenessï¼ï¼</strong> å¼å¥äºä¸ç§âååº-åæ°åâç­ç¥ã
    *   (i) å°ç½æ ¼åå²æè¯­ä¹3Dé¨åï¼ä½¿ç¨å½¢ç¶ç´å¾å½æ°ShDFè¿è¡ååºï¼ã
    *   (ii) å¯¹æ¯ä¸ªè¯­ä¹é¨åç¬ç«åºç¨ä¸ä¸ªæ çç£å­¦ä¹ çãä¿æå ä½çUVåæ°åéª¨å¹²ç½ç»ã
    *   (iii) å°è¿äºé¨åå¾è¡¨èåå¹¶æåæä¸ä¸ªç»ä¸çUVå¾éã
*   <strong>å¯è§æ§æç¥ï¼Visibility-Awarenessï¼ï¼</strong>
    *   ä½¿ç¨ç¯å¢åé®è½ï¼AOï¼ä½ä¸ºæåä»£çã
    *   ååä¼ æ­ä¸ä¸ªè½¯å¯å¾®åçAOå ææ¥ç¼ç®æ ï¼ä»¥å¼å¯¼åå²ç¼åè¢«é®æ¡åºåç§»å¨ï¼ä»èåå°å¯è§çæ¥ç¼ä¼ªå½±ã
*   <strong>ä¸¤é¶æ®µè®­ç»æµç¨ï¼</strong>
    *   ç¬¬ä¸é¶æ®µï¼å ä½ä¿æç½æ ¼åæ°åå­¦ä¹ ï¼ä½¿ç¨åºäºMLPçç½ç»åå¯å¾®åå ä½ç®æ çæä½å¤±çUVæ å°ã
    *   ç¬¬äºé¶æ®µï¼å­¦ä¹ æç¥ç®æ ï¼å¼å¥è¯­ä¹æç¥åå¯è§æ§æç¥æ¨¡åï¼ä¸ºçº¹Dçº¹çç»å¶ç­ä»»å¡æä¾æå¯¼ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
éè¿ä¸ç°ææåè¿æ¹æ³çå®æ§åå®éè¯ä¼°ï¼æ¬æå±ç¤ºäºï¼
*   <strong>è¯­ä¹ä¸è´æ§ï¼</strong> æåºçæ¹æ³è½å¤çæUVå¾éï¼å¶å¾è¡¨ä¸ç½æ ¼ç3Dè¯­ä¹é¨åæ´å¥½å°å¯¹é½ï¼ä»èç®åçº¹çç¼è¾ãä¼ è¾åè·¨å½¢ç¶å¯¹åºã
*   <strong>åå°æ¥ç¼ä¼ªå½±ï¼</strong> å¯è§æ§æç¥ç®æ æåå°å°åå²ç¼å¼å¯¼å°æååº¦è¾ä½ï¼æ´è¢«é®æ¡ï¼çåºåï¼æ¾èåå°äºçº¹ççæåæ¸²æä¸­å¯æç¥çæ¥ç¼ä¼ªå½±ã
*   <strong>å ä½è´¨éä¿æï¼</strong> å°½ç®¡å¼å¥äºè¯­ä¹åå¯è§æ§ç®æ ï¼ä½æ¨¡åä»è½ä¿æè¯å¥½çå ä½ç¹æ§ï¼å¦å±å½¢æ§ï¼è§åº¦ä¿æï¼åç­é¢ç§¯æ§ï¼é¢ç§¯ä¿æï¼ï¼ä»æè½»å¾®çæ§è½ä¸éã
*   <strong>èªå¨ååæçï¼</strong> è¯¥æ¡æ¶å®ç°äº3Dç½æ ¼åæ°åçèªå¨åï¼è§£å³äºæå¨UVæ å°çç¶é¢é®é¢ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>èåå¨ç®åæ§ï¼</strong> å½åçç½æ ¼èåå¨æ¯ç®åä¸ç¡®å®æ§çç½æ ¼ååï¼è½ç¶ææï¼ä½æªæ¥å¯ä»¥æ¿æ¢ä¸ºæ´é«çº§çæåæ±è§£å¨ï¼å¯åå¼æåºäºä¼åçï¼ã
*   <strong>å®éææ çè½»å¾®ä¸éï¼</strong> å°½ç®¡æç¥è´¨éæææé«ï¼ä½å¨æäºæ°å¼ææ ï¼å¦å±å½¢æ§åç­é¢ç§¯æ§ï¼ä¸ï¼ä¸çº¯å ä½ä¼åçåºçº¿ç¸æ¯ï¼å­å¨è½»å¾®çä¸éã
*   <strong>AOä½ä¸ºå¯è§æ§ä»£çï¼</strong> å°½ç®¡AOæ¯ææçå¯è§æ§ä»£çï¼ä½å®å¯è½æ æ³å®å¨ææææä¸äººç±»æç¥ç¸å³çå¯è§æ§å ç´ ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   èåå­¦ä¹ UVåæ°ååçº¹ççæï¼ä»¥è¿ä¸æ­¥æå3Dçææ¨¡ååæ´å¹¿æ³ç3Dåå®¹åä½ã
*   æ¢ç´¢æ´åè¿çæåæ±è§£å¨ï¼ä»¥ä¼åUVå¾éçç©ºé´å©ç¨çã
*   ç ç©¶æ´å¤æçå¯è§æ§ä»£çæç´æ¥çäººç±»æç¥æ¨¡åï¼ä»¥æ´ç²¾ç¡®å°å¼å¯¼æ¥ç¼æ¾ç½®ã</p>
<hr />
<p>è¿ç¯è®ºæéè¿å°è¯­ä¹åå¯è§æ§ç®æ æ´åå°æ çç£ç3Dç½æ ¼åæ°åæ¡æ¶ä¸­ï¼ä¸ºè®¡ç®æºå¾å½¢å­¦é¢åååºäºéè¦è´¡ç®ï¼å°¤å¶æ¯å¨èªå¨åçº¹çæ å°ååå°è§è§ä¼ªå½±æ¹é¢ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To overcome these shortcomings and to automate
the mesh parameterization process, we present an unsupervised differentiable
framework that augments standard geometry-preserving UV learning with semantic-
and visibility-aware objectives.</li>
<li>By conducting
qualitative and quantitative evaluations against state-of-the-art methods, we
show that the proposed method produces UV atlases that better support texture
generation and reduce perceptible seam artifacts compared to recent baselines.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.25094v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.25094v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.25079v1'></a></p>
<h2 id="unilat3d-geometry-appearance-unified-latents-for-single-stage-3d-generation"><a href="https://arxiv.org/abs/2509.25079v1">UniLat3D: Geometry-Appearance Unified Latents for Single-Stage 3D Generation</a></h2>
<p><strong>Authors:</strong> Guanjun Wu, Jiemin Fang, Chen Yang, Sikuang Li, Taoran Yi, Jia Lu, Zanwei Zhou, Jiazhong Cen, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Xinggang Wang, Qi Tian</p>
<p><strong>Published:</strong> 2025-09-29</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.GR</p>
<p><strong>Abstract:</strong></p>
<p>High-fidelity 3D asset generation is crucial for various industries. While
recent 3D pretrained models show strong capability in producing realistic
content, most are built upon diffusion models and follow a two-stage pipeline
that first generates geometry and then synthesizes appearance. Such a decoupled
design tends to produce geometry-texture misalignment and non-negligible cost.
In this paper, we propose UniLat3D, a unified framework that encodes geometry
and appearance in a single latent space, enabling direct single-stage
generation. Our key contribution is a geometry-appearance Unified VAE, which
compresses high-resolution sparse features into a compact latent representation
-- UniLat. UniLat integrates structural and visual information into a dense
low-resolution latent, which can be efficiently decoded into diverse 3D
formats, e.g., 3D Gaussians and meshes. Based on this unified representation,
we train a single flow-matching model to map Gaussian noise directly into
UniLat, eliminating redundant stages. Trained solely on public datasets,
UniLat3D produces high-quality 3D assets in seconds from a single image,
achieving superior appearance fidelity and geometric quality. More demos \&amp;
code are available at https://unilat3d.github.io/</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾å¯¹Guanjun Wuç­äººæ°åçè®ºæâUniLat3D: Geometry-Appearance Unified Latents for Single-Stage 3D Generationâçå¨é¢æè¦ã</p>
<hr />
<h3 id="unilat3d-3d">UniLat3D: å ä½-å¤è§ç»ä¸æ½å¨ç©ºé´å®ç°åé¶æ®µ3Dçæ</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
å½åé«ä¿ç3Dèµäº§çæé¢åé¢ä¸´çä¸»è¦æææ¯ï¼å¤§å¤æ°ç°ææ¹æ³ï¼å°¤å¶æ¯åºäºæ©æ£æ¨¡åçæ¹æ³ï¼éç¨ä¸¤é¶æ®µæµæ°´çº¿ï¼é¦åçæå ä½ç»æï¼ç¶ååæå¤è§ï¼çº¹çï¼ãè¿ç§è§£è¦è®¾è®¡å¸¸å¸¸å¯¼è´å ä½ä¸çº¹çä¸å¹éï¼å ä½-çº¹çéä½ï¼ï¼å¹¶ä¸å¼å¥äºæ¾èçè®¡ç®ææ¬ååä½æ­¥éª¤ãè®ºææ¨å¨è§£å³å¦ä½å®ç°é«æãé«è´¨éçåé¶æ®µ3Dçæï¼é¿åå ä½ä¸å¤è§åç¦»å¸¦æ¥çé®é¢ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>ç»ä¸çæ½å¨è¡¨ç¤ºï¼UniLatï¼ï¼</strong> è®ºææåºäºUniLatï¼è¿æ¯ä¸ç§æ°é¢çç»ä¸æ½å¨ç©ºé´è¡¨ç¤ºï¼è½å¤å°3Dèµäº§çå ä½ä¿¡æ¯åå¤è§ä¿¡æ¯ç¼ç å°åä¸ªç´§åçä½åè¾¨çæ½å¨è¡¨ç¤ºä¸­ãè¿ä¸ä¼ ç»æ¹æ³ä¸­å ä½åå¤è§åç¦»çæ½å¨ç©ºé´å½¢æé²æå¯¹æ¯ã
*   <strong>å ä½-å¤è§ç»ä¸VAEï¼UniVAEï¼ï¼</strong> å¼å¥äºä¸ä¸ªç»ä¸çååèªç¼ç å¨ï¼UniVAEï¼ï¼ç¨äºå°é«åè¾¨çç¨çç¹å¾åç¼©æç´§åçUniLatè¡¨ç¤ºãUniVAEè½å¤å°ç»æåè§è§ä¿¡æ¯æ´åå°å¯éçä½åè¾¨çæ½å¨ç©ºé´ä¸­ã
*   <strong>åé¶æ®µçææ¡æ¶ï¼</strong> åºäºUniLatç»ä¸è¡¨ç¤ºï¼è®ºæè®­ç»äºä¸ä¸ªåä¸çæµå¹éï¼flow-matchingï¼æ¨¡åï¼å¯ä»¥ç´æ¥å°é«æ¯åªå£°æ å°å°UniLatï¼ä»èå®ç°ç´æ¥çåé¶æ®µ3Dçæï¼æ¶é¤äºä¼ ç»ä¸¤é¶æ®µæµæ°´çº¿ä¸­çåä½æ­¥éª¤ã
*   <strong>å¤æ ¼å¼è§£ç è½åï¼</strong> UniLatå¯ä»¥é«æå°è§£ç æå¤ç§3Dæ ¼å¼ï¼åæ¬3Dé«æ¯ï¼3D Gaussiansï¼åç½æ ¼ï¼meshesï¼ï¼è¿å¢å¼ºäºå¶éç¨æ§åå®ç¨æ§ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>åè¶çæ§è½ï¼</strong> UniLat3Då¨å¬å¼æ°æ®éä¸è¿è¡è®­ç»ï¼è½å¤ä»åå¼ å¾åå¨æ°ç§åçæé«è´¨éç3Dèµäº§ã
*   <strong>é«å¤è§ä¿çåº¦åå ä½è´¨éï¼</strong> å®éªç»æè¡¨æï¼UniLat3Då¨å¤è§ä¿çåº¦åå ä½è´¨éæ¹é¢è¡¨ç°åºè²ï¼ä¼äºç°æçä¸¤é¶æ®µæ¹æ³ï¼å¹¶è½æ´å¥½å°ä¸æ¡ä»¶å¾åå¯¹é½ã
*   <strong>æçæåï¼</strong> éè¿åé¶æ®µçæï¼UniLat3Dæ¾èåå°äºçææ¶é´ï¼ä¾å¦ï¼3Dé«æ¯çæå¯å¨8ç§åå®æï¼ä½¿ç¨FlashAttention-3çè³å¯ç¼©ç­è³3ç§ãç½æ ¼çæè½ç¶éè¦36ç§ï¼ä½èèå°æ´é«çåè¾¨çååå¤çï¼ä»å·ç«äºåã
*   <strong>ç¨æ·ç ç©¶éªè¯ï¼</strong> ç¨æ·ç ç©¶ç»ææ¾ç¤ºï¼UniLat3Då¨å¾åå¯¹é½åå¯¹è±¡è´¨éæ¹é¢è·å¾äºè¶è¿35%çæç¥¨ï¼ä¼äºå¶ä»æ¨¡åã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>åæ­¥æ¢ç´¢ï¼</strong> è®ºææåºï¼UniLat3Dæ¨¡åä»å¤äºåæ­¥æ¢ç´¢é¶æ®µã
*   <strong>è®­ç»æ°æ®ï¼</strong> ç®åä»ä½¿ç¨å¬å¼æ°æ®éè¿è¡è®­ç»ãä½èè®¤ä¸ºï¼æ³¨å¥æ´å¤é«è´¨éæ°æ®å°æ çè¿ä¸æ­¥æé«æ¨¡åæ§è½å¹¶æ©å¤§è§æ¨¡ã
*   <strong>é«åè¾¨çæ½å¨ç©ºé´çæçï¼</strong> å¨æ´é«åè¾¨çï¼ä¾å¦32Â³ï¼ä¸è®­ç»æµå¹éTransformeræ¶ï¼è®¡ç®ææ¬æ¾èå¢å ãç®åæµæ¨¡åå¨å¤çé«åè¾¨çæ½å¨ç©ºé´æ¶çæçä»æå¾æé«ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ°æ®è§æ¨¡ä¸è´¨éï¼</strong> æ¢ç´¢å¦ä½å©ç¨æ´å¤é«è´¨éçè®­ç»æ°æ®æ¥è¿ä¸æ­¥æåæ¨¡åæ§è½åå¯æ©å±æ§ã
*   <strong>æµæ¨¡åæçï¼</strong> ç ç©¶æ´é«æçæµæ¨¡åè®¾è®¡ï¼ä»¥éåºæ´é«åè¾¨ççæ½å¨ç©ºé´ï¼ä»èçææ´è¯¦ç»ç3Dç»æï¼ä¾å¦éè¿åçº§è®¡ç®åè½»éçº§æ³¨æåæºå¶ã
*   <strong>å¤æ¨¡æéæï¼</strong> å°UniLatéæå°å¤§åå¤æ¨¡ææ¨¡åä¸­ï¼ä»¥ä¿è¿è·¨æ¨¡æçè§£åçæã
*   <strong>æ©å±å°4Dè¡¨ç¤ºï¼</strong> å°UniLatæ©å±å°4Dè¡¨ç¤ºï¼ä»¥æ¯æå¨æ3Dåå®¹ççæã
*   <strong>ç»ä¸å¯¹è±¡ä¸åºæ¯çæï¼</strong> è¿ä¸æ­¥ç»ä¸å¯¹è±¡ååºæ¯çæï¼å©ç¨ç´§åçç»ä¸è¡¨ç¤ºã</p>
<hr />
<p>æ»èè¨ä¹ï¼UniLat3Déè¿å¼å¥å ä½-å¤è§ç»ä¸çæ½å¨ç©ºé´ååé¶æ®µæµå¹éçææ¨¡åï¼ä¸ºé«ä¿ç3Dèµäº§çææä¾äºä¸ä¸ªæ°é¢ä¸é«æçèå¼ï¼ææè§£å³äºä¼ ç»ä¸¤é¶æ®µæ¹æ³ä¸­å ä½-çº¹çéä½åè®¡ç®ææ¬é«æçé®é¢ãå¶å¨å¬å¼æ°æ®éä¸åå¾çåè¶æ§è½åç¨æ·ç ç©¶ç»æï¼å¸æ¾äºç»ä¸è¡¨ç¤ºå¨3Dçæé¢åçå·¨å¤§æ½åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we propose UniLat3D, a unified framework that encodes geometry
and appearance in a single latent space, enabling direct single-stage
generation.</li>
<li>Our key contribution is a geometry-appearance Unified VAE, which
compresses high-resolution sparse features into a compact latent representation
-- UniLat.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.25079v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.25079v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.25077v1'></a></p>
<h2 id="bridge-building-reinforcement-learning-depth-to-image-data-generation-engine-for-monocular-depth-estimation"><a href="https://arxiv.org/abs/2509.25077v1">BRIDGE -- Building Reinforcement-Learning Depth-to-Image Data Generation Engine for Monocular Depth Estimation</a></h2>
<p><strong>Authors:</strong> Dingning Liu, Haoyu Guo, Jingyi Zhou, Tong He</p>
<p><strong>Published:</strong> 2025-09-29</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Monocular Depth Estimation (MDE) is a foundational task for computer vision.
Traditional methods are limited by data scarcity and quality, hindering their
robustness. To overcome this, we propose BRIDGE, an RL-optimized depth-to-image
(D2I) generation framework that synthesizes over 20M realistic and
geometrically accurate RGB images, each intrinsically paired with its ground
truth depth, from diverse source depth maps. Then we train our depth estimation
model on this dataset, employing a hybrid supervision strategy that integrates
teacher pseudo-labels with ground truth depth for comprehensive and robust
training. This innovative data generation and training paradigm enables BRIDGE
to achieve breakthroughs in scale and domain diversity, consistently
outperforming existing state-of-the-art approaches quantitatively and in
complex scene detail capture, thereby fostering general and robust depth
features. Code and models are available at
https://dingning-liu.github.io/bridge.github.io/.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Dingning Liu, Haoyu Guo, Jingyi Zhou, Tong Heæ°åçè®ºæâBRIDGE -- Building Reinforcement-Learning Depth-to-Image Data Generation Engine for Monocular Depth Estimationâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼BRIDGEââæå»ºç¨äºåç®æ·±åº¦ä¼°è®¡çå¼ºåå­¦ä¹ æ·±åº¦å°å¾åæ°æ®çæå¼æ</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
åç®æ·±åº¦ä¼°è®¡ï¼MDEï¼æ¯è®¡ç®æºè§è§ä¸­çä¸é¡¹åºç¡ä»»å¡ï¼ä½ä¼ ç»æ¹æ³åéäºæ°æ®ç¨ç¼ºæ§åè´¨éï¼è¿ä¸¥éé»ç¢äºå¶é²æ£æ§ãç°ææ°æ®éå¨é«è´¨éãç²¾ç¡®ççå¼æ·±åº¦æ æ³¨ãç»èåå¤æ ·æ§æ¹é¢å­å¨ä¸è¶³ï¼ä¸æªè½ååå©ç¨ç°ææ·±åº¦æ°æ®ï¼è¿æä¸ºMDEæ¨¡åè®­ç»çå³é®ç¶é¢ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥è®ºææåºäºBRIDGEæ¡æ¶ï¼å¶æ ¸å¿åæ°å¨äºï¼
*   <strong>RLä¼åçæ·±åº¦å°å¾åï¼D2Iï¼æ°æ®çæå¼æï¼</strong> BRIDGEå¼å¥äºä¸ä¸ªå¼ºåå­¦ä¹ ï¼RLï¼ä¼åçD2Içææ¨¡åãè¯¥æ¨¡åè½å¤ä»å¤æ ·åçæºæ·±åº¦å¾åæè¶è¿2000ä¸å¼ è§è§çå®ä¸å ä½ç²¾ç¡®çRGBå¾åï¼æ¯å¼ å¾åé½ä¸å¶çå¼æ·±åº¦åå¨éå¯¹ãè¿ç§æ¹æ³ææç¼è§£äºæ°æ®ç¨ç¼ºæ§åè´¨éé®é¢ï¼å¹¶æ©å±äºè®­ç»æ°æ®çè§æ¨¡åé¢åå¤æ ·æ§ãRLä¼åç¡®ä¿äºçæå¾åä¸ä»è§è§çå®ï¼èä¸å ä½ç²¾ç¡®åä¸è´ï¼é¿åäºä¼ ç»D2Iæ¨¡åä¸­å¸¸è§çå ä½ä¼ªå½±åç»æå¤±çã
*   <strong>æ··åçç£è®­ç»ç­ç¥ï¼</strong> ä¸ºäºååå©ç¨çææ°æ®ï¼BRIDGEéç¨äºä¸ç§æ··åçç£ç­ç¥ãè¯¥ç­ç¥ç»åäºæå¸æ¨¡åçæçä¼ªæ ç­¾åé«ç²¾åº¦çå¼æ·±åº¦ãå·ä½èè¨ï¼é¦åä½¿ç¨æå¸æ¨¡åçæåå§ä¼ªæ ç­¾ï¼ç¶åéè¿åºäºç¸ä¼¼æ§çæ¹æ³ï¼å¦SSIMåæ¢¯åº¦åæï¼ç­éé«ç²¾åº¦çå¼æ·±åº¦åºåï¼å¹¶å°å¶ä¸ä¼ªæ ç­¾èåãè¿ç§ä¸¤é¶æ®µè®­ç»è¿ç¨ï¼åç¨ä¼ªæ ç­¾è¿è¡å¤§è§æ¨¡é¢è®­ç»ï¼åç¨çå¼æ·±åº¦è¿è¡ç²¾ç»è°æ´ï¼ç¡®ä¿äºæ¨¡åå¨å­¦ä¹ å¹¿æ³å ä½ä¸è´æ§çåæ¶ï¼ä¹è½å¨å³é®åºåå®ç°é«ç²¾åº¦åç»èææã
*   <strong>é«æçæ°æ®çæåå©ç¨èå¼ï¼</strong> éè¿RLé©±å¨çD2Ièå¼ï¼BRIDGEè½å¤é«æçæå¤§è§æ¨¡é«è´¨éRGB-Dæ°æ®ï¼ææè§£å³äºæ°æ®ç¨ç¼ºåè´¨éé®é¢ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>åè¶çæ§è½åè®­ç»æçï¼</strong> BRIDGEå¨å¤ä¸ªæææ§åºåæµè¯ï¼åæ¬å®¤åãå®¤å¤ååæå¨ç»ç¯å¢ï¼ä¸ååå¾äºæåè¿ï¼SOTAï¼çæ§è½ãå®å¨å®éåå¤æåºæ¯ç»èæææ¹é¢æç»­ä¼äºç°ææ¹æ³ï¼ä¾å¦ï¼ä»ä½¿ç¨çº¦2000ä¸æ°æ®ï¼ç¸æ¯Depth Anything V2ç6200ä¸æ°æ®ï¼å°±è¶è¶äºDepth Anything V2ç­æ¨¡åã
*   <strong>å¼ºå¤§çæ³åè½ååé²æ£æ§ï¼</strong> è¯¥æ¨¡åå¨é¶æ ·æ¬æ·±åº¦ä¼°è®¡æ¹é¢è¡¨ç°åºè²ï¼å°¤å¶æ¯å¨å®¤ååºæ¯æ°æ®éï¼å¦NYUv2ãScanNetãETH3Dï¼ä¸ï¼è½å¤å®ç¾å°çæä¸ç®æ å¯¹é½çç²¾ç»ç»æé¢æµãå®è¿è½åç¡®ä¼°è®¡åå°è¡¨é¢ï¼å¦éå­ï¼çæ·±åº¦ï¼å¹¶å¤çå¤æç»èåç¸ä¼¼é¢è²çç©ä½ï¼å±ç°äºå¯¹âéå¤âæ°æ®çå¼ºå¤§æ³åè½åã
*   <strong>ä¿è¿éç¨åé²æ£çæ·±åº¦ç¹å¾ï¼</strong> åæ°çæ°æ®çæåè®­ç»èå¼æå©äºå¹å»æ´éç¨åé²æ£çæ·±åº¦ç¹å¾ï¼ä¸ºæ´é«æåå¯æ³åçMDEè§£å³æ¹æ¡éºå¹³äºéè·¯ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>KITTIæ°æ®éä¸çæ§è½ï¼</strong> å°½ç®¡å¨å¤§å¤æ°æ°æ®éä¸è¡¨ç°ä¼å¼ï¼ä½BRIDGEå¨KITTIæ°æ®éä¸æªè½è¾¾å°æä½³æ§è½ï¼è¿ä¸»è¦å½å äºè¯¥æ°æ®éåºæçç¨çæ§ãè®ºææåºï¼æ¨¡åæ¨å¨ææç²¾ç»çå¨å±åå±é¨æ·±åº¦ä¿¡æ¯ï¼èKITTIè¯ä¼°æªè½åååæ è¿ä¸ç¹ã
*   <strong>ä¼ªæ ç­¾çåºæåªå£°åä¸åç¡®æ§ï¼</strong> è®ºææ¿è®¤ï¼æå¸æ¨¡åçæçä¼ªæ ç­¾è½ç¶è¦çèå´å¹¿ï¼ä½å¶åºæçåªå£°åä¸åç¡®æ§ï¼å°¤å¶æ¯å¨è¾¹çåç²¾ç»ç»èå¤ï¼ä»ç¶æ¯è¿ä¸æ­¥æåæ·±åº¦ä¼°è®¡æ§è½çç¶é¢ãæ··åçç£ç­ç¥æ­£æ¯ä¸ºäºç¼è§£è¿ä¸é®é¢èè®¾è®¡çã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºæä¸­æ²¡ææç¡®æåºæªæ¥çç ç©¶æ¹åï¼ä½ä»å¶è´¡ç®åå±éæ§ä¸­å¯ä»¥æ¨æ­åºä¸äºæ½å¨æ¹åï¼
*   <strong>è¿ä¸æ­¥ä¼åD2Iå¼æï¼</strong> æ¢ç´¢æ´åè¿çRLææ¯æçææ¨¡åæ¶æï¼ä»¥è¿ä¸æ­¥æé«çæå¾åçè§è§çå®æãå ä½ç²¾ç¡®æ§åå¤æ ·æ§ï¼å°¤å¶æ¯å¨å¤çæ´å¤æçåºæ¯åç©ä½æ¶ã
*   <strong>æ¹è¿æ··åçç£ç­ç¥ï¼</strong> ç ç©¶æ´æºè½çä¼ªæ ç­¾ç­éåèåæºå¶ï¼ä¾å¦ï¼ç»åä¸ç¡®å®æ§ä¼°è®¡æèªéåºæéï¼ä»¥æ´å¥½å°å¹³è¡¡ä¼ªæ ç­¾çå¹¿åº¦åçå¼æ·±åº¦çç²¾åº¦ã
*   <strong>è§£å³ç¹å®æ°æ®éçå±éæ§ï¼</strong> éå¯¹KITTIç­ç¨çæç¹å®é¢åçæ°æ®éï¼å¼åæ´ä¸é¨çè®­ç»ç­ç¥ææ°æ®å¢å¼ºæ¹æ³ï¼ä»¥æåæ¨¡åå¨è¿äºåºæ¯ä¸çæ§è½ã
*   <strong>æ¢ç´¢æ´å¹¿æ³çåºç¨ï¼</strong> é´äºBRIDGEçæçRGB-Dæ°æ®çé«è´¨éåå¤æ ·æ§ï¼å¯ä»¥æ¢ç´¢å¶å¨å¶ä»3Dè®¡ç®æºè§è§ä»»å¡ä¸­çåºç¨ï¼å¦3Déå»ºãåºæ¯çè§£ãæºå¨äººå¯¼èªç­ã
*   <strong>æ¨¡åå¯è§£éæ§ï¼</strong> æ·±å¥ç ç©¶RL-D2Iå¼æåMDEæ¨¡ååé¨çå·¥ä½æºå¶ï¼ä»¥æé«æ¨¡åçå¯è§£éæ§ï¼ä»èæ´å¥½å°çè§£å¶æåä¹å¤å¹¶æå¯¼æªæ¥çæ¹è¿ã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To overcome this, we propose BRIDGE, an RL-optimized depth-to-image
(D2I) generation framework that synthesizes over 20M realistic and
geometrically accurate RGB images, each intrinsically paired with its ground
truth depth, from diverse source depth maps.</li>
<li>This innovative data generation and training paradigm enables BRIDGE
to achieve breakthroughs in scale and domain diversity, consistently
outperforming existing state-of-the-art approaches quantitatively and in
complex scene detail capture, thereby fostering general and robust depth
features.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.25077v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.25077v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.25058v1'></a></p>
<h2 id="chargen-fast-and-fluent-portrait-modification"><a href="https://arxiv.org/abs/2509.25058v1">CharGen: Fast and Fluent Portrait Modification</a></h2>
<p><strong>Authors:</strong> Jan-Niklas Dihlmann, Arnela Killguss, Hendrik P. A. Lensch</p>
<p><strong>Published:</strong> 2025-09-29</p>
<p><strong>Categories:</strong> cs.GR, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Interactive editing of character images with diffusion models remains
challenging due to the inherent trade-off between fine-grained control,
generation speed, and visual fidelity. We introduce CharGen, a
character-focused editor that combines attribute-specific Concept Sliders,
trained to isolate and manipulate attributes such as facial feature size,
expression, and decoration with the StreamDiffusion sampling pipeline for more
interactive performance. To counteract the loss of detail that often
accompanies accelerated sampling, we propose a lightweight Repair Step that
reinstates fine textures without compromising structural consistency.
Throughout extensive ablation studies and in comparison to open-source
InstructPix2Pix and closed-source Google Gemini, and a comprehensive user
study, CharGen achieves two-to-four-fold faster edit turnaround with precise
editing control and identity-consistent results. Project page:
https://chargen.jdihlmann.com/</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Jan-Niklas Dihlmann, Arnela Killguss, Hendrik P. A. Lenschæ°åçè®ºæâCharGen: Fast and Fluent Portrait Modificationâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼CharGen: å¿«éæµççäººåä¿®æ¹</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢</strong>
å½åï¼ä½¿ç¨æ©æ£æ¨¡åè¿è¡äº¤äºå¼è§è²å¾åç¼è¾é¢ä¸´çä¸ä¸ªåºæçææï¼å³å¨ç²¾ç»æ§å¶ãçæéåº¦åè§è§ä¿çåº¦ä¹é´å­å¨æè¡¡ãç°æçæ¹æ³å¾å¾é¾ä»¥åæ¶å®ç°è¿ä¸ç¹ï¼å¯¼è´ç¼è¾è¿ç¨è¦ä¹ç¼ºä¹ç²¾ç¡®æ§å¶ï¼è¦ä¹éåº¦ç¼æ¢ï¼è¦ä¹å¨å ééæ ·æ¶ä¸¢å¤±ç»èã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®</strong>
CharGen æåºäºä¸ç§ä»¥è§è²ä¸ºä¸­å¿çç¼è¾å¨ï¼éè¿ä»¥ä¸ä¸ä¸ªå³é®åæ°æ¥è§£å³ä¸è¿°é®é¢ï¼</p>
<ul>
<li><strong>å±æ§ç¹å®æ¦å¿µæ»åï¼Attribute-specific Concept Slidersï¼ï¼</strong> CharGen é¢è®­ç»äºéå¯¹ç¹å®å±æ§ï¼å¦é¢é¨ç¹å¾å¤§å°ãè¡¨æãè£é¥°ç­ï¼çæ¦å¿µæ»åãè¿äºæ»åéè¿ LoRA ééå¨è¿è¡è®­ç»ï¼æ¨å¨éç¦»åæçºµç¹å®å±æ§ï¼åæ¶ä¿æå¶ä»å±æ§çç¬ç«æ§ï¼ä»èå®ç°ç²¾ç»ãè¿ç»­çæ§å¶ãè®ºæå¼ºè°äº LoRA åå¹¶ï¼LoRA mergingï¼ç­ç¥ï¼å®éè¿å¯¹ LoRA æéç©éµæ±åæ¥é¢ç»åå¤ä¸ªæ¦å¿µæ»åï¼ä»¥ç¡®ä¿å¨å¤å±æ§ç¼è¾æ¶çç¨³å®æ§åä¸è´æ§ï¼é¿åäº LoRA å å ï¼LoRA stackingï¼å¯è½å¯¼è´çç´¯ç§¯å¤±çåç»èä¸¢å¤±ã</li>
<li><strong>StreamDiffusion éæ ·ç®¡çº¿éæï¼</strong> ä¸ºäºå®ç°æ´å·äº¤äºæ§çæ§è½ï¼CharGen å°é¢è®­ç»çæ¦å¿µæ»åä¸ StreamDiffusion éæ ·ç®¡çº¿éæãStreamDiffusion ä»¥å¶å®æ¶çæè½åèé»åï¼éè¿ä¸ç³»åä¼åï¼å¦ Stream BatchãRCFGãInput-Output Queues ç­ï¼æ¾èå å¿«äºæ¨çéåº¦ãè¿ç§éæä½¿å¾ CharGen è½å¤å®ç°ä¸¤å°ååçç¼è¾å¨è½¬éåº¦ã</li>
<li><strong>è½»éçº§ä¿®å¤æ­¥éª¤ï¼Lightweight Repair Stepï¼ï¼</strong> ä¸ºäºæµæ¶å ééæ ·è¿ç¨ä¸­ç»å¸¸ä¼´éçç»èä¸¢å¤±ï¼CharGen å¼å¥äºä¸ä¸ªè½»éçº§çä¿®å¤æ­¥éª¤ãè¯¥ä¿®å¤æ­¥éª¤æ¨å¨æ¢å¤ç²¾ç»çº¹çï¼åæ¶ä¸æå®³å¾åçç»æä¸è´æ§ãè®ºææ¢è®¨äºä¸¤ç§ä¿®å¤æ¹æ³ï¼è®­ç»ä¸é¨çä¿®å¤æ»åï¼Repair Sliderï¼ååºäº ControlNet çä¿®å¤ï¼å¹¶æç»éæ©äºä¿®å¤æ»åï¼å ä¸ºå®å¨ç»èå¢å¼ºåç»æä¿æä¹é´åå¾äºæä½³å¹³è¡¡ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹</strong>
CharGen å¨å¹¿æ³çæ¶èç ç©¶ãä¸å¼æº InstructPix2Pix åé­æº Google Gemini çæ¯è¾ä»¥åå¨é¢çç¨æ·ç ç©¶ä¸­å±ç¤ºäºå¶æææ§ï¼</p>
<ul>
<li><strong>ç¼è¾éåº¦æ¾èæåï¼</strong> CharGen å®ç°äºä¸¤å°ååçç¼è¾å¨è½¬éåº¦ï¼æ¾èä¼äº InstructPix2Pix å Google Gemini ç­ç°ææ¹æ³ï¼ä½¿å¶æ´éåäº¤äºå¼å·¥ä½æµç¨ã</li>
<li><strong>ç²¾ç¡®çç¼è¾æ§å¶åèº«ä»½ä¸è´æ§ï¼</strong> æ¦å¿µæ»åæä¾äºå¯¹ç¹å®é¢é¨å±æ§çç²¾ç»ãè¿ç»­æ§å¶ï¼è½å¤è¿è¡å±é¨è°æ´ï¼åæ¶ä¿æè§è²èº«ä»½çä¸è´æ§ãç¨æ·ç ç©¶è¯å®äº CharGen å¨åå±æ§åå¤å±æ§ç¼è¾åºæ¯ä¸­çä¼å¿ã</li>
<li><strong>è§è§ä¿çåº¦é«ï¼</strong> è½»éçº§ä¿®å¤æ­¥éª¤æåå°æ¢å¤äºå ééæ ·è¿ç¨ä¸­ä¸¢å¤±çç»èï¼ç¡®ä¿äºé«è´¨éçè§è§è¾åºï¼åæ¶ä¿æäºç»æä¸è´æ§ã</li>
<li><strong>å¤å±æ§ç¼è¾è½åï¼</strong> LoRA åå¹¶æ¹æ³ä½¿å¾ CharGen è½å¤åæ¶ä¿®æ¹å¤ä¸ªå±æ§ï¼å¹¶ä¿æä¸è´æ§ï¼è¿å¨ InstructPix2Pix å Gemini ç­æ¹æ³ä¸­æ¯ä¸ä¸ªææã</li>
</ul>
<p><strong>4. è®ºæä¸­æåçå±éæ§</strong>
*   <strong>å¼ºè½¬æ¢çå±éæ§ï¼</strong> CharGen å¨å¤çæç«¯å¹´é¾ååç­å¼ºè½¬æ¢æ¶è¡¨ç°åºä¸å®çå±éæ§ï¼å¶æ»åè®­ç»æ´ä¾§éäºç²¾ç»è°æ´èéå§çååã
*   <strong>æ¦å¿µæ»åé´çå¹²æ°ï¼</strong> å°½ç®¡ LoRA åå¹¶å®ç°äºå¤æ»åä½¿ç¨ï¼ä½æäºç»åä»å¯è½åºç°å¹²æ°æåºï¼ä¾å¦å¹´é¾ä¿®æ¹ä¼å½±ååé¨å¤§å°ï¼æåå¦-å¹´é¾ç»åä¼éä½å¾åæ¸æ°åº¦ãè¿è¡¨æç¬ç«è®­ç»çæ»åå¯è½ç¼ºä¹è·¨å±æ§æç¥ã
*   <strong>æ½å¨çåè§åä¼¦çé®é¢ï¼</strong> æ©æ£æ¨¡ååºæçè®­ç»æ°æ®åå¸å¯è½å¯¼è´ç³»ç»å¨ä¸åäººå£ç¾¤ä½ä¹é´è¡¨ç°åºåè§ãæ­¤å¤ï¼ç³»ç»æçºµé¢é¨å±æ§çè½åå¯è½è¢«æ»¥ç¨äºåå»ºæ·±åº¦ä¼ªé æè¯¯å¯¼æ§åå®¹ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å</strong>
*   <strong>æ©å±å°æ´å¹¿æ³çå¾åç¼è¾é¢åï¼</strong> å° CharGen çå±æ§ç¹å®æ¹æ³æ©å±å°é¢é¨ç¹å¾ä»¥å¤çæ´å¹¿æ³å¾åç¼è¾é¢åã
*   <strong>ç¼è§£æ¦å¿µæ»åé´çå¹²æ°ï¼</strong> å¼åç­ç¥ä»¥åè½»æ¦å¿µæ»åä¹é´ä¸å¿è¦çäº¤äºã
*   <strong>æ¹è¿ç¦»æ£å±æ§çè®­ç»æ¹æ³ï¼</strong> æ¹è¿éå¯¹å·æç¦»æ£èéè¿ç»­ååçå±æ§çè®­ç»æ¹æ³ã
*   <strong>æ¢ç´¢æ´å¤æç LoRA éæææ¯ï¼</strong> æ¢ç´¢æ´å¤æç LoRA éæææ¯ï¼ä»¥å¢å¼ºç»èçæã</p>
<hr />
<p>æ»èè¨ä¹ï¼CharGen è®ºæéè¿ç»åå±æ§ç¹å®çæ¦å¿µæ»åãStreamDiffusion éæ ·ç®¡çº¿åè½»éçº§ä¿®å¤æ­¥éª¤ï¼æåå°å¨äº¤äºå¼è§è²å¾åç¼è¾ä¸­å¹³è¡¡äºç²¾ç»æ§å¶ãçæéåº¦åè§è§ä¿çåº¦ãå®ä¸ºè®¡ç®æºè§è§é¢åçå¯æ§çæç¼è¾æä¾äºä¸ä¸ªæ°é¢ä¸é«æçè§£å³æ¹æ¡ï¼å°½ç®¡ä»éå³æ³¨å¶å¨å¼ºè½¬æ¢ãæ»åäº¤äºåä¼¦çæ¹é¢çå±éæ§ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce CharGen, a
character-focused editor that combines attribute-specific Concept Sliders,
trained to isolate and manipulate attributes such as facial feature size,
expression, and decoration with the StreamDiffusion sampling pipeline for more
interactive performance.</li>
<li>To counteract the loss of detail that often
accompanies accelerated sampling, we propose a lightweight Repair Step that
reinstates fine textures without compromising structural consistency.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.25058v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.25058v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.25033v1'></a></p>
<h2 id="vt-fsl-bridging-vision-and-text-with-llms-for-few-shot-learning"><a href="https://arxiv.org/abs/2509.25033v1">VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning</a></h2>
<p><strong>Authors:</strong> Wenhao Li, Qiangchang Wang, Xianjing Meng, Zhibin Wu, Yilong Yin</p>
<p><strong>Published:</strong> 2025-09-29</p>
<p><strong>Categories:</strong> cs.CV, cs.LG, I.4.9</p>
<p><strong>Abstract:</strong></p>
<p>Few-shot learning (FSL) aims to recognize novel concepts from only a few
labeled support samples. Recent studies enhance support features by
incorporating additional semantic information or designing complex semantic
fusion modules. However, they still suffer from hallucinating semantics that
contradict the visual evidence due to the lack of grounding in actual
instances, resulting in noisy guidance and costly corrections. To address these
issues, we propose a novel framework, bridging Vision and Text with LLMs for
Few-Shot Learning (VT-FSL), which constructs precise cross-modal prompts
conditioned on Large Language Models (LLMs) and support images, seamlessly
integrating them through a geometry-aware alignment. It mainly consists of
Cross-modal Iterative Prompting (CIP) and Cross-modal Geometric Alignment
(CGA). Specifically, the CIP conditions an LLM on both class names and support
images to generate precise class descriptions iteratively in a single
structured reasoning pass. These descriptions not only enrich the semantic
understanding of novel classes but also enable the zero-shot synthesis of
semantically consistent images. The descriptions and synthetic images act
respectively as complementary textual and visual prompts, providing high-level
class semantics and low-level intra-class diversity to compensate for limited
support data. Furthermore, the CGA jointly aligns the fused textual, support,
and synthetic visual representations by minimizing the kernelized volume of the
3-dimensional parallelotope they span. It captures global and nonlinear
relationships among all representations, enabling structured and consistent
multimodal integration. The proposed VT-FSL method establishes new
state-of-the-art performance across ten diverse benchmarks, including standard,
cross-domain, and fine-grained few-shot learning scenarios. Code is available
at https://github.com/peacelwh/VT-FSL.</p>
<p><strong>Analysis:</strong></p>
<p>ä»¥ä¸æ¯Wenhao Li, Qiangchang Wang, Xianjing Meng, Zhibin Wu, Yilong Yinæ°åçè®ºæâVT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learningâçå¨é¢æè¦ï¼</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢</strong>
è¯¥è®ºææ¨å¨è§£å³å°æ ·æ¬å­¦ä¹ ï¼FSLï¼ä¸­çä¸ä¸ªæ ¸å¿ææï¼å¦ä½ææå°è¯å«ä»éè¿å°éå¸¦æ ç­¾æ¯ææ ·æ¬å­¦ä¹ å°çæ°æ¦å¿µãç°ææ¹æ³éè¿æ´åé¢å¤çè¯­ä¹ä¿¡æ¯ï¼å¦ç±»å«æè¿°ï¼æè®¾è®¡å¤æçè¯­ä¹èåæ¨¡åæ¥å¢å¼ºæ¯æç¹å¾ï¼ä½è¿äºæ¹æ³å¾å¾ä¼äº§çä¸è§è§è¯æ®ç¸çç¾çâå¹»è§âè¯­ä¹ï¼å¯¼è´æå¯¼ä¿¡æ¯ä¸åç¡®åä¿®æ­£ææ¬é«æã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®</strong>
VT-FSLï¼Bridging Vision and Text with LLMs for Few-Shot Learningï¼æåºäºä¸ç§æ°é¢çæ¡æ¶ï¼éè¿ä»¥ä¸ä¸¤ä¸ªå³é®æ¨¡åè§£å³äºä¸è¿°é®é¢ï¼</p>
<ul>
<li><strong>è·¨æ¨¡æè¿­ä»£æç¤ºï¼Cross-modal Iterative Prompting, CIPï¼</strong>ï¼è¯¥æ¨¡åå©ç¨å¤§åè¯­è¨æ¨¡åï¼LLMsï¼åæ¯æå¾åï¼ä»¥ç»æåæ¨ççæ¹å¼è¿­ä»£çæç²¾ç¡®çç±»å«æè¿°ãè¿äºæè¿°ä¸ä»ä¸°å¯äºå¯¹æ°ç±»å«çè¯­ä¹çè§£ï¼è¿è½å¤é¶æ ·æ¬åæè¯­ä¹ä¸è´çå¾åãçæçæè¿°ååæå¾ååå«ä½ä¸ºäºè¡¥çææ¬åè§è§æç¤ºï¼æä¾é«çº§å«çç±»å«è¯­ä¹åä½çº§å«ç±»å«åå¤æ ·æ§ï¼ä»¥å¼¥è¡¥æéæ¯ææ°æ®çé®é¢ã</li>
<li><strong>è·¨æ¨¡æå ä½å¯¹é½ï¼Cross-modal Geometric Alignment, CGAï¼</strong>ï¼è¯¥æ¨¡åéè¿æå°åèåçææ¬ãæ¯æååæè§è§è¡¨ç¤ºæè·¨è¶ç3ç»´å¹³è¡å­é¢ä½çæ ¸åä½ç§¯ï¼å±åå¯¹é½è¿äºè¡¨ç¤ºãCGAæè·äºææè¡¨ç¤ºä¹é´çå¨å±åéçº¿æ§å³ç³»ï¼å®ç°äºç»æååä¸è´çå¤æ¨¡ææ´åã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹</strong>
VT-FSLæ¹æ³å¨åä¸ªä¸åçåºåæµè¯ä¸­ï¼åæ¬æ åãè·¨é¢ååç»ç²åº¦å°æ ·æ¬å­¦ä¹ åºæ¯ï¼å»ºç«äºæ°çæåè¿æ§è½ãå¹³ååç¡®çæé«äº4.2%ãå·ä½æ¥è¯´ï¼
*   å¨miniImageNetåtieredImageNetä¸ï¼VT-FSLå¨1-shotå5-shotè®¾ç½®ä¸åæ¾èä¼äºç°ææ¹æ³ï¼ä¾å¦å¨miniImageNetç1-shotä»»å¡ä¸­ï¼æ¯æ¬¡ä¼æ¹æ³é«åº4.35%è³15.31%ã
*   å¨ç»ç²åº¦æ°æ®éï¼å¦CUBãCarsãDogsï¼ä¸ï¼VT-FSLå¨æææ§ç1-shotä»»å¡ä¸­æ¯æ¬¡ä¼æ¹æ³SUITEDé«åº3.0%-10.3%ï¼è¡¨æå¶è½å¤æè·ç»å¾®çç±»å«é´å·®å¼å¹¶ä¿æç±»å«åä¸è´æ§ã
*   æ¶èç ç©¶è¯å®äºææ¬æç¤ºãè§è§æç¤ºåæ ¸åä½ç§¯å¯¹æ¯æå¤±ï¼å¯¹é½æå¤±ï¼çäºè¡¥æ§åæææ§ã
*   ä¸ä»ä¾èµç±»å«åç§°æç®åæç¤ºçLLMæ¹æ³ç¸æ¯ï¼VT-FSLçæçææ¬è¯­ä¹æ´ä¸°å¯ãæ´ç²¾ç¡®ã
*   æ ¸åä½ç§¯å¯¹æ¯å­¦ä¹ ï¼ç¹å«æ¯ä½¿ç¨RBFæ ¸ï¼å¨æè·éçº¿æ§å³ç³»åå®ç°å¨å±ä¸è´å¯¹é½æ¹é¢ä¼äºInfoNCEåçº¿æ§ä½ç§¯æå¤±ã
*   VT-FSLå¨è®­ç»åæ¨çæ¶é´æ¹é¢ä¹è¡¨ç°åºé«ææ§ï¼æ¯ç°æLLMåºçº¿æ¹æ³æ´å¿«ï¼åæ¶å®ç°äºæ´é«çåç¡®çã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§</strong>
*   <strong>é¢åæ³åè½å</strong>ï¼å°½ç®¡å¨è·¨é¢åæ°æ®éï¼å¦PlacesåPlantaeï¼ä¸è¿è¡äºè¯ä¼°ï¼ä½è¿äºè®¾ç½®ä¸æºé¢åä»å­å¨ä¸å®ç¸ä¼¼æ§ãVT-FSLå¨æ´å·æææ§çåå¸åç§»ï¼å¦å»å­¦å¾åï¼ä¸çé²æ£æ§å°æªå¾å°ååè¯ä¼°ã
*   <strong>å¤é¨çææ¨¡åè´¨éä¾èµ</strong>ï¼VT-FSLçæ§è½ä¾èµäºå¤é¨çææ¨¡åï¼LLMsåææ¬å°å¾åæ¨¡åï¼çè´¨éãè¾å¼±çLLMså¯è½äº§çéç¨æåæçæè¿°ï¼ä½è´¨éçå¾ååæå¯è½å¼å¥è¯¯å¯¼æ§è§è§ä¿¡å·ã
*   <strong>æ ¸åä½ç§¯å¯¹æ¯æå¤±ççè®ºè¡ä¸º</strong>ï¼å¨å¤çé«ç»´ãåææè¯­ä¹çº ç¼ çç¹å¾åå¸æ¶ï¼æ ¸åä½ç§¯å¯¹æ¯æå¤±ççè®ºè¡ä¸ºä»æå¾æ·±å¥æ¢ç´¢ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å</strong>
*   è¿ä¸æ­¥ç ç©¶æ ¸åä½ç§¯å¯¹æ¯æå¤±çæ¶æç¹æ§åå¯¹æ ¸éæ©çæææ§ã
*   æ¢ç´¢å¦ä½æé«VT-FSLå¨æ´å·æææ§çåå¸åç§»åºæ¯ä¸çé²æ£æ§ã
*   æåçææ¨¡åçè´¨éï¼ä»¥æä¾æ´ç²¾ç¡®åè§è§ä¸æ´å¿ å®çè¯­ä¹åéªï¼ä»èè¿ä¸æ­¥å¢å¼ºVT-FSLçæ§è½ã
*   æ·±å¥çè§£VT-FSLçè§£éæ§åæ³åè½åï¼ç¹å«æ¯å¨å¤æ¨¡æå°æ ·æ¬å­¦ä¹ çèæ¯ä¸ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Few-shot learning (FSL) aims to recognize novel concepts from only a few
labeled support samples.</li>
<li>To address these
issues, we propose a novel framework, bridging Vision and Text with LLMs for
Few-Shot Learning (VT-FSL), which constructs precise cross-modal prompts
conditioned on Large Language Models (LLMs) and support images, seamlessly
integrating them through a geometry-aware alignment.</li>
<li>These descriptions not only enrich the semantic
understanding of novel classes but also enable the zero-shot synthesis of
semantically consistent images.</li>
<li>The proposed VT-FSL method establishes new
state-of-the-art performance across ten diverse benchmarks, including standard,
cross-domain, and fine-grained few-shot learning scenarios.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.25033v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.25033v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.25032v1'></a></p>
<h2 id="airoa-moma-dataset-a-large-scale-hierarchical-dataset-for-mobile-manipulation"><a href="https://arxiv.org/abs/2509.25032v1">AIRoA MoMa Dataset: A Large-Scale Hierarchical Dataset for Mobile Manipulation</a></h2>
<p><strong>Authors:</strong> Ryosuke Takanami, Petr Khrapchenkov, Shu Morikuni, Jumpei Arima, Yuta Takaba, Shunsuke Maeda, Takuya Okubo, Genki Sano, Satoshi Sekioka, Aoi Kadoya, Motonari Kambara, Naoya Nishiura, Haruto Suzuki, Takanori Yoshimoto, Koya Sakamoto, Shinnosuke Ono, Hu Yang, Daichi Yashima, Aoi Horo, Tomohiro Motoda, Kensuke Chiyoma, Hiroshi Ito, Koki Fukuda, Akihito Goto, Kazumi Morinaga, Yuya Ikeda, Riko Kawada, Masaki Yoshikawa, Norio Kosuge, Yuki Noguchi, Kei Ota, Tatsuya Matsushima, Yusuke Iwasawa, Yutaka Matsuo, Tetsuya Ogata</p>
<p><strong>Published:</strong> 2025-09-29</p>
<p><strong>Categories:</strong> cs.RO, cs.AI, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>As robots transition from controlled settings to unstructured human
environments, building generalist agents that can reliably follow natural
language instructions remains a central challenge. Progress in robust mobile
manipulation requires large-scale multimodal datasets that capture contact-rich
and long-horizon tasks, yet existing resources lack synchronized force-torque
sensing, hierarchical annotations, and explicit failure cases. We address this
gap with the AIRoA MoMa Dataset, a large-scale real-world multimodal dataset
for mobile manipulation. It includes synchronized RGB images, joint states,
six-axis wrist force-torque signals, and internal robot states, together with a
novel two-layer annotation schema of sub-goals and primitive actions for
hierarchical learning and error analysis. The initial dataset comprises 25,469
episodes (approx. 94 hours) collected with the Human Support Robot (HSR) and is
fully standardized in the LeRobot v2.1 format. By uniquely integrating mobile
manipulation, contact-rich interaction, and long-horizon structure, AIRoA MoMa
provides a critical benchmark for advancing the next generation of
Vision-Language-Action models. The first version of our dataset is now
available at https://huggingface.co/datasets/airoa-org/airoa-moma .</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Ryosuke Takanamiç­äººæ°åçè®ºæâAIRoA MoMa Dataset: A Large-Scale Hierarchical Dataset for Mobile Manipulationâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼AIRoA MoMa Dataset: ç¨äºç§»å¨æä½çå¤§è§æ¨¡åå±æ°æ®é</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
éçæºå¨äººä»åæ§ç¯å¢åéç»æåäººç±»ç¯å¢è¿æ¸¡ï¼æå»ºè½å¤å¯é éµå¾ªèªç¶è¯­è¨æä»¤çéç¨æºè½ä½æä¸ºä¸ä¸ªæ ¸å¿ææãç®åï¼é²æ£çç§»å¨æä½éè¦å¤§è§æ¨¡ãå¤æ¨¡ææ°æ®éï¼è¿äºæ°æ®éåºè½æææ¥è§¦å¯éååé¿æ¶ç¨ä»»å¡ãç¶èï¼ç°æèµæºæ®éç¼ºä¹åæ­¥çå-æ­ç©ä¼ æãåå±æ æ³¨ä»¥åæç¡®çå¤±è´¥æ¡ä¾ï¼è¿éå¶äºè§è§-è¯­è¨-å¨ä½ï¼VLAï¼æ¨¡åå¨å®éä¸çä¸­çåå±ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ä¸ºäºè§£å³ä¸è¿°ç©ºç½ï¼æ¬è®ºæå¼å¥äº <strong>AIRoA MoMa Dataset</strong>ï¼è¿æ¯ä¸ä¸ªå¤§è§æ¨¡ãçå®ä¸çãå¤æ¨¡æçç§»å¨æä½æ°æ®éï¼å¶å³é®åæ°åè´¡ç®åæ¬ï¼
*   <strong>å¤æ¨¡ææ°æ®éæï¼</strong> æ°æ®éåæ­¥åå«äºRGBå¾åãå³èç¶æãå­è½´èé¨å-æ­ç©ä¿¡å·ä»¥åæºå¨äººåé¨ç¶æï¼ç¹å«å¼ºè°äºå-æ­ç©ä¼ æï¼ä»¥æ¯ææ¥è§¦å¯éåä»»å¡çå­¦ä¹ ã
*   <strong>æ°é¢çä¸¤å±åå±æ æ³¨æ¹æ¡ï¼</strong> å¼å¥äºå­ç®æ ï¼sub-goalsï¼ååå§å¨ä½ï¼primitive actionsï¼çä¸¤å±æ æ³¨ç»æãè¿ç§åå±è®¾è®¡æå©äºåå±å­¦ä¹ åç»ç²åº¦çéè¯¯åæï¼ä½¿å¾æ¨¡åè½å¤å­¦ä¹ é«å±ä»»å¡è§ååä½å±è¿å¨æ§å¶ã
*   <strong>åå«æç¡®çå¤±è´¥æ¡ä¾ï¼</strong> æ°æ®éææåå«äºå¤±è´¥æ¡ä¾çè®°å½åæ æ³¨ï¼çº¦å æ»æ°æ®éç6.6%ï¼ï¼è¿å¯¹äºç ç©¶éè¯¯æ£æµãæ¢å¤åä»è´é¢ç¤ºä¾ä¸­å­¦ä¹ è³å³éè¦ã
*   <strong>æ ååæ°æ®æ ¼å¼åå¼æ¾ç®¡éï¼</strong> æ°æ®éå®å¨æ ååä¸ºLeRobot v2.1æ ¼å¼ï¼ç¡®ä¿äºä¸ç°æVLAæ¶æçç´æ¥å¼å®¹æ§ãå¯å¤ç°æ§åå¹¿æ³å¯è®¿é®æ§ãåæ¶ï¼è®ºæåå¸äºä¸ä¸ªå¼æºæ°æ®å¤çåæåç®¡éã
*   <strong>ä¸æ³¨äºç§»å¨æä½ãæ¥è§¦å¯éäº¤äºåé¿æ¶ç¨ä»»å¡ï¼</strong> ä¸ç°æä¸»è¦å³æ³¨æ¡é¢æä½çæ°æ®éä¸åï¼AIRoA MoMa æç¡®å°å°ç§»å¨æä½ãæ¶åç©çæ¥è§¦çäº¤äºä»¥åéè¦å¤æ­¥éª¤åè§£çé¿æ¶ç¨ä»»å¡ä½ä¸ºæ ¸å¿ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æ°æ®éè§æ¨¡ï¼</strong> åå§æ°æ®éåå«25,469ä¸ªææ¯ï¼çº¦94å°æ¶ï¼ï¼ç±ä¸°ç°äººç±»æ¯ææºå¨äººï¼HSRï¼æ¶éãè¿äºææ¯æ¶µçäºä¸ä¸ªä¸»è¦çå®¶åº­ä»»å¡å40å¤ä¸ªå­ä»»å¡ã
*   <strong>æ°æ®ç¹æ§ï¼</strong> æ°æ®éå±ç¤ºäºæè½åå¸çé¿å°¾æ¨¡å¼ï¼åºç¡æä½ï¼å¦âæåâãâæå¼âãâæ¾ç½®âï¼å æ®ä¸»å¯¼å°ä½ãä»»å¡æç»­æ¶é´éä¸­å¨ç­å°ä¸­ç­èå´ï¼4å°12ç§ï¼ï¼è¡¨ææ°æ®éä¸»è¦ç±ç¦»æ£çãç­æ¶ç¨æ´»å¨ç»æï¼éå¸¸éåè®­ç»åºç¡åååºæ§ç­ç¥ã
*   <strong>å¯¹VLAæ¨¡åçæ¨å¨ï¼</strong> éè¿ç¬ç¹å°æ´åç§»å¨æä½ãæ¥è§¦å¯éäº¤äºåé¿æ¶ç¨ç»æï¼AIRoA MoMa ä¸ºä¸ä¸ä»£è§è§-è¯­è¨-å¨ä½æ¨¡åçåå±æä¾äºä¸ä¸ªå³é®åºåï¼ææå ééç¨æºå¨äººæºè½ä½çå¼åã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>æ°æ®å¤æ ·æ§ä»ææåç©ºé´ï¼</strong> å°½ç®¡æ°æ®éåªåæ¶µçå¤ç§å®¶åº­ç¯å¢åä»»å¡ï¼ä½ä¸ºäºå®ç°äººç±»æ°´å¹³çéç¨è½åï¼ä»éè¦æ´å¤æ ·åçæºå¨äººæ°æ®ï¼æ¶µçæ´å¹¿æ³çç©ä½ãç¯å¢ãä»»å¡åæå¢ã
*   <strong>éç§è¿æ»¤çå±éæ§ï¼</strong> å°½ç®¡éç¨äºåºäºYOLOçæ£æµå¨è¿è¡éç§è¿æ»¤ï¼èªå¨æé¤åå«äººç±»åºç°çææ¯ï¼ä½ä»éæç»­å³æ³¨åæ¹è¿éç§ä¿æ¤ææ¯ã
*   <strong>å½åçæ¬æªåå«æ¢å¤è¡ä¸ºåäººæºäº¤äºä¿¡å·ï¼</strong> è®ºææå°æªæ¥è®¡åå°æ©å±æ°æ®éï¼ä»¥åå«æ¢å¤è¡ä¸ºåäººæºäº¤äºä¿¡å·ï¼å¦èªç¶è¯­è¨åè¯­é³ï¼ï¼è¿è¡¨æå½åçæ¬å°æªå®å¨è¦çè¿äºæ¹é¢ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ©å±æ°æ®éè¦çèå´ï¼</strong> è®¡åéè¿å¢å ææ¯æ°éãä»å¤ä¸ªç«ç¹åå¤æ ·åç¯å¢ä¸­æ¶éæ°æ®æ¥æ©å±æ°æ®éçè¦çèå´ã
*   <strong>æ´åæ¢å¤è¡ä¸ºåäººæºäº¤äºä¿¡å·ï¼</strong> æªæ¥çæ¬å°çº³å¥æ¢å¤è¡ä¸ºä»¥åèªç¶è¯­è¨åè¯­é³ç­äººç±»-æºå¨äººäº¤äºä¿¡å·ï¼ä»¥è¿ä¸æ­¥æåæ°æ®éçå®ç¨æ§ã
*   <strong>æ¨å¨éç¨æºå¨äººæºè½ä½åå±ï¼</strong> AIRoA MoMa æ°æ®éå°ä½ä¸ºå³é®åºåï¼ä¿è¿è½å¤å¤çæ¥è§¦å¯éãé¿æ¶ç¨ç§»å¨æä½ä»»å¡çä¸ä¸ä»£éç¨æºå¨äººæºè½ä½çå¼åã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>It includes synchronized RGB images, joint states,
six-axis wrist force-torque signals, and internal robot states, together with a
novel two-layer annotation schema of sub-goals and primitive actions for
hierarchical learning and error analysis.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.25032v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.25032v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-09-30 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
