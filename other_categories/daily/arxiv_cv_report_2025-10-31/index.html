<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-10-31 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DEâ«¶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Birdâs-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">è®ºæéè¯»</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-Î±
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-10-30/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../../depth_completion/DNet/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-10-31">Arxiv Computer Vision Papers - 2025-10-31</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#arxiv-2025-10-30" class="nav-link">Arxiv è®¡ç®æºè§è§æ¯æ¥æ¥åæ§è¡æè¦ (2025-10-30)</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#the-impact-and-outlook-of-3d-gaussian-splatting" class="nav-link">The Impact and Outlook of 3D Gaussian Splatting</a>
                </li>
                <li class="nav-item">
                    <a href="#all-you-need-for-object-detection-from-pixels-points-and-prompts-to-next-gen-fusion-and-multimodal-llmsvlms-in-autonomous-vehicles" class="nav-link">All You Need for Object Detection: From Pixels, Points, and Prompts to Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles</a>
                </li>
                <li class="nav-item">
                    <a href="#emu35-native-multimodal-models-are-world-learners" class="nav-link">Emu3.5: Native Multimodal Models are World Learners</a>
                </li>
                <li class="nav-item">
                    <a href="#multimodal-spatial-reasoning-in-the-large-model-era-a-survey-and-benchmarks" class="nav-link">Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks</a>
                </li>
                <li class="nav-item">
                    <a href="#are-video-models-ready-as-zero-shot-reasoners-an-empirical-study-with-the-mme-cof-benchmark" class="nav-link">Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark</a>
                </li>
                <li class="nav-item">
                    <a href="#the-quest-for-generalizable-motion-generation-data-model-and-evaluation" class="nav-link">The Quest for Generalizable Motion Generation: Data, Model, and Evaluation</a>
                </li>
                <li class="nav-item">
                    <a href="#heir-learning-graph-based-motion-hierarchies" class="nav-link">HEIR: Learning Graph-Based Motion Hierarchies</a>
                </li>
                <li class="nav-item">
                    <a href="#which-way-does-time-flow-a-psychophysics-grounded-evaluation-for-vision-language-models" class="nav-link">Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for Vision-Language Models</a>
                </li>
                <li class="nav-item">
                    <a href="#crag-mm-multi-modal-multi-turn-comprehensive-rag-benchmark" class="nav-link">CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark</a>
                </li>
                <li class="nav-item">
                    <a href="#basicavsr-arbitrary-scale-video-super-resolution-via-image-priors-and-enhanced-motion-compensation" class="nav-link">BasicAVSR: Arbitrary-Scale Video Super-Resolution via Image Priors and Enhanced Motion Compensation</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-10-31">Arxiv Computer Vision Papers - 2025-10-31</h1>
<h2 id="executive-summary">Executive Summary</h2>
<h2 id="arxiv-2025-10-30">Arxiv è®¡ç®æºè§è§æ¯æ¥æ¥åæ§è¡æè¦ (2025-10-30)</h2>
<p><strong>æ¦è¿°ï¼</strong></p>
<p>ä»æ¥ Arxiv è®¡ç®æºè§è§è®ºæä¸»è¦å´ç»<strong>å¤æ¨¡æå­¦ä¹ ã3D è§è§ãè§é¢çè§£ä¸çæ</strong>ä¸å¤§æ ¸å¿ä¸»é¢å±å¼ãæ¾èè¶å¿åæ¬å¤§åæ¨¡åï¼LLMs/VLMsï¼å¨è§è§ä»»å¡ä¸­çæ·±åº¦èåãå¯¹æ´éç¨åé²æ£æ¨¡åçéæ±ï¼ä»¥åå¯¹æ°æ®ãè¯ä¼°ååºåçæç»­å³æ³¨ã</p>
<p><strong>ä¸»è¦ä¸»é¢åè¶å¿ï¼</strong></p>
<ol>
<li><strong>å¤æ¨¡æä¸å¤§åæ¨¡åèå (LLMs/VLMs)ï¼</strong> å¤ç¯è®ºææ¢è®¨äºå¤§åè¯­è¨æ¨¡ååè§è§è¯­è¨æ¨¡åå¨åç§è§è§ä»»å¡ä¸­çåºç¨ï¼ä»ç®æ æ£æµãç©ºé´æ¨çå°è§é¢çè§£ãè¿è¡¨æé¢åæ­£ç§¯ææ¢ç´¢å¦ä½å©ç¨è¿äºæ¨¡åçå¼ºå¤§æ³åè½ååä¸çç¥è¯ã</li>
<li><strong>3D è§è§ä¸æ°è¡¨ç¤ºï¼</strong> 3D Gaussian Splatting ä½ä¸ºä¸ç§æ°å´ç3Dè¡¨ç¤ºæ¹æ³ï¼å¶å½±åååæ¯åå°å³æ³¨ãè¿åæ äºå¯¹æ´é«æãé«è´¨é3Déå»ºåæ¸²æææ¯çæç»­è¿½æ±ã</li>
<li><strong>è§é¢çè§£ä¸çæï¼</strong> è§é¢ä½ä¸ºä¸ç§å¤æçæ°æ®å½¢å¼ï¼å¶çè§£ï¼é¶æ ·æ¬æ¨çãæ¶é´æµåï¼åçæï¼éç¨è¿å¨çæãè¶åè¾¨çï¼æ¯éè¦çç ç©¶æ¹åãå¯¹è§é¢ä¸­è¿å¨åæ¶é´ä¿¡æ¯çå»ºæ¨¡æ¯å³é®ææã</li>
<li><strong>åºåä¸è¯ä¼°ï¼</strong> å¤ç¯è®ºææåºäºæ°çåºååè¯ä¼°æ¹æ³ï¼æ¨å¨æ´å¨é¢ãæ´ç»è´å°è¡¡éæ¨¡åçæ§è½ï¼ç¹å«æ¯å¨å¤æ¨¡ææ¨çãé¶æ ·æ¬è½ååRAGï¼æ£ç´¢å¢å¼ºçæï¼æ¹é¢ãè¿å¼ºè°äºé¢åå¯¹ä¸¥è°¨è¯ä¼°çéè§ã</li>
<li><strong>éç¨æ§ä¸æ³åè½åï¼</strong> âéç¨è¿å¨çæâãâé¶æ ·æ¬æ¨çâç­è¯æ±é¢ç¹åºç°ï¼è¡¨æç ç©¶äººåæ­£åªåæå»ºè½å¤éåºæ´å¹¿æ³åºæ¯åä»»å¡çæ¨¡åï¼èéä»éäºç¹å®æ°æ®éã</li>
</ol>
<p><strong>ç¹å«éè¦æåæ°è®ºæï¼</strong></p>
<ul>
<li><strong>"Emu3.5: Native Multimodal Models are World Learners" (Yufeng Cui et al.)ï¼</strong> è¿ç¯è®ºæå¯è½ä»£è¡¨äºå¤æ¨¡ææ¨¡ååå±çä¸ä¸ªéè¦éç¨ç¢ï¼å¼ºè°äºåçå¤æ¨¡ææ¨¡åä½ä¸ºâä¸çå­¦ä¹ èâçæ½åãå¦æå¶æåºçæ¹æ³è½æææåæ¨¡åçæ³åè½ååå¯¹ä¸çç¥è¯ççè§£ï¼å°å·ææ·±è¿å½±åã</li>
<li><strong>"All You Need for Object Detection: From Pixels, Points, and Prompts to Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles" (Sayed Pedram Haeri Boroujeni et al.)ï¼</strong> è¿ç¯ç»¼è¿°æ§è®ºæå¨é¢æ¢è®¨äºç®æ æ£æµçæªæ¥è¶å¿ï¼ç¹å«æ¯å¤æ¨¡æèååLLMs/VLMsçåºç¨ï¼å¯¹èªå¨é©¾é©¶é¢åå·ææå¯¼æä¹ã</li>
<li><strong>"The Impact and Outlook of 3D Gaussian Splatting" (Bernhard Kerbl)ï¼</strong> ä½ä¸ºå¯¹æ°å´3Dè¡¨ç¤ºæ¹æ³çæ·±å¥åæï¼è¿ç¯è®ºæå¯¹äºçè§£ååºç¨3D Gaussian Splattingè³å³éè¦ï¼å¯è½é¢ç¤ºç3Déå»ºåæ¸²æé¢åçæ°èå¼ã</li>
</ul>
<p><strong>æ°å´ç ç©¶æ¹åæææ¯ï¼</strong></p>
<ul>
<li><strong>å¤æ¨¡æRAG (Retrieval-Augmented Generation)ï¼</strong> CRAG-MM åºåçæåºè¡¨æï¼å°æ£ç´¢æºå¶ä¸å¤æ¨¡æå¤§åæ¨¡åç»åï¼ä»¥æé«å¶ç¥è¯è·ååæ¨çè½åï¼æ¯ä¸ä¸ªéè¦çç ç©¶æ¹åã</li>
<li><strong>åºäºå¾çè¿å¨å±æ¬¡å­¦ä¹ ï¼</strong> HEIR è®ºææ¢ç´¢äºå©ç¨å¾ç»ææ¥å»ºæ¨¡è¿å¨çå±æ¬¡å³ç³»ï¼è¿å¯è½ä¸ºæ´å¤æãæ´èªç¶çè¿å¨çæåçè§£æä¾æ°çæè·¯ã</li>
<li><strong>å¿çç©çå­¦é©±å¨çè¯ä¼°ï¼</strong> "Which Way Does Time Flow?" è®ºææåºäºä¸ç§æ°é¢çãåºäºäººç±»æç¥ï¼å¿çç©çå­¦ï¼çè¯ä¼°æ¹æ³ï¼è¿å¯è½ä¿ä½¿æä»¬éæ°æèåè®¾è®¡æ´ç¬¦åäººç±»è®¤ç¥çè§è§-è¯­è¨æ¨¡åè¯ä¼°æ åã</li>
</ul>
<p><strong>å»ºè®®éè¯»å¨æçè®ºæï¼</strong></p>
<p>å¯¹äºå¿ç¢çç ç©¶äººåï¼ä»¥ä¸è®ºæå¯è½æå¼å¾ä¼åéè¯»å¨æï¼</p>
<ol>
<li><strong>"Emu3.5: Native Multimodal Models are World Learners" (Yufeng Cui et al.)ï¼</strong> å¦ææ¨å³æ³¨å¤æ¨¡æå¤§æ¨¡åçææ°è¿å±åæªæ¥æ¹åï¼è¿ç¯è®ºææ¯å¿è¯»ã</li>
<li><strong>"All You Need for Object Detection: From Pixels, Points, and Prompts to Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles" (Sayed Pedram Haeri Boroujeni et al.)ï¼</strong> å¯¹äºä»äºç®æ æ£æµï¼ç¹å«æ¯èªå¨é©¾é©¶é¢åçç ç©¶äººåï¼è¿ç¯ç»¼è¿°æä¾äºå¨é¢çè§è§ååç»æ§åæã</li>
<li><strong>"The Impact and Outlook of 3D Gaussian Splatting" (Bernhard Kerbl)ï¼</strong> å¦ææ¨å¯¹3Dè§è§ãæ°é¢ç3Dè¡¨ç¤ºæ¹æ³æå®æ¶æ¸²ææå´è¶£ï¼è¿ç¯è®ºæå°æä¾å³é®è§è§£ã</li>
<li><strong>"Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks" (Xu Zheng et al.)ï¼</strong> å¯¹äºå³æ³¨å¤æ¨¡ææ¨çåç©ºé´çè§£çç ç©¶äººåï¼è¿ç¯ç»¼è¿°ååºåè®ºææä¾äºå¨é¢çèæ¯åè¯ä¼°å·¥å·ã</li>
</ol>
<p>æ¬æ¬¡æ¥åæ­ç¤ºäºè®¡ç®æºè§è§é¢åå¨å¤æ¨¡æã3Dåè§é¢çè§£æ¹é¢æç»­çå¿«éåå±ï¼å¤§åæ¨¡åæ­£æä¸ºæ¨å¨è¿äºè¿æ­¥çæ ¸å¿é©±å¨åã</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2510.26694v1">The Impact and Outlook of 3D Gaussian Splatting</a></li>
<li><a href="#2510.26641v1">All You Need for Object Detection: From Pixels, Points, and Prompts to Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles</a></li>
<li><a href="#2510.26583v1">Emu3.5: Native Multimodal Models are World Learners</a></li>
<li><a href="#2510.25760v1">Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks</a></li>
<li><a href="#2510.26802v1">Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark</a></li>
<li><a href="#2510.26794v1">The Quest for Generalizable Motion Generation: Data, Model, and Evaluation</a></li>
<li><a href="#2510.26786v1">HEIR: Learning Graph-Based Motion Hierarchies</a></li>
<li><a href="#2510.26241v1">Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for Vision-Language Models</a></li>
<li><a href="#2510.26160v1">CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark</a></li>
<li><a href="#2510.26149v1">BasicAVSR: Arbitrary-Scale Video Super-Resolution via Image Priors and Enhanced Motion Compensation</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2510.26694v1'></a></p>
<h2 id="the-impact-and-outlook-of-3d-gaussian-splatting"><a href="https://arxiv.org/abs/2510.26694v1">The Impact and Outlook of 3D Gaussian Splatting</a></h2>
<p><strong>Authors:</strong> Bernhard Kerbl</p>
<p><strong>Published:</strong> 2025-10-30</p>
<p><strong>Categories:</strong> cs.CV, cs.GR</p>
<p><strong>Abstract:</strong></p>
<p>Since its introduction, 3D Gaussian Splatting (3DGS) has rapidly transformed
the landscape of 3D scene representations, inspiring an extensive body of
associated research. Follow-up work includes analyses and contributions that
enhance the efficiency, scalability, and real-world applicability of 3DGS. In
this summary, we present an overview of several key directions that have
emerged in the wake of 3DGS. We highlight advances enabling resource-efficient
training and rendering, the evolution toward dynamic (or four-dimensional,
4DGS) representations, and deeper exploration of the mathematical foundations
underlying its appearance modeling and rendering process. Furthermore, we
examine efforts to bring 3DGS to mobile and virtual reality platforms, its
extension to massive-scale environments, and recent progress toward
near-instant radiance field reconstruction via feed-forward or distributed
computation. Collectively, these developments illustrate how 3DGS has evolved
from a breakthrough representation into a versatile and foundational tool for
3D vision and graphics.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Bernhard Kerblæ°åçè®ºæâThe Impact and Outlook of 3D Gaussian Splattingâçå¨é¢æè¦ã</p>
<hr />
<h3 id="3d-gaussian-splatting">è®ºææè¦ï¼3D Gaussian Splatting çå½±åä¸å±æ</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨æ»ç»åå±æ3D Gaussian Splatting (3DGS)ææ¯èªå¼å¥ä»¥æ¥å¨3Dåºæ¯è¡¨ç¤ºé¢åæå¸¦æ¥çåé©æ§å½±åãå·ä½èè¨ï¼å®æ¢è®¨äº3DGSå¦ä½ä»ä¸ä¸ªçªç ´æ§çè¡¨ç¤ºæ¹æ³æ¼åä¸º3Dè§è§åå¾å½¢é¢åä¸­å¤åè½ä¸åºç¡æ§çå·¥å·ï¼å¹¶è¯å«äºå¶å¨æçãå¯æ©å±æ§åå®éåºç¨æ¹é¢çå³é®åå±æ¹åã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è®ºææ²¡ææåºæ°çåä¸æ¹æ³è®ºï¼èæ¯å¯¹3DGSçæç³»ç»ä¸­çä¸ç³»åå³é®åæ°è¿è¡äºç»¼è¿°ååç±»ï¼</p>
<ul>
<li><strong>èµæºåéä¸ç3DGS (3DGS with Limited Resources)ï¼</strong> ä»ç»äºéè¿åç¼©ï¼å¦åªæãSHé¶æ°èªéåºãéåï¼ãé¢ç®æç¥è®­ç»ï¼å¦éå¶æ¨¡åå¢é¿ãä¼åé«æ¯åå¸ï¼åç®æ³ä¼åæ¥éä½3DGSçåå­åè®¡ç®éæ±çæ¹æ³ï¼ä½¿å¶è½å¨ååç¡¬ä»¶ãç§»å¨åWebå¹³å°é¨ç½²ã</li>
<li><strong>å¨æ3D Gaussian Splatting (Dynamic 3D Gaussian Splatting, 4DGS)ï¼</strong> è§£å³äºéæ3DGSæ æ³å¤çéåæ§è¿å¨åºæ¯çé®é¢ãå³é®è´¡ç®åæ¬ï¼<ul>
<li><strong>æ¶é´è¿è´¯æ§ï¼</strong> å¼å¥äºé«æ¯åºåçæä¹æ§åè¿å¨è·è¸ªï¼èéæ¯å¸§éæ°æåã</li>
<li><strong>å®æ¶æ¸²æï¼</strong> å°é«æ¯æåå°4Dè¡¨ç¤ºï¼å¹¶è®¾è®¡äºè½ä»¥äº¤äºéçæ¸²ææ¶ç©ºé«æ¯çæ¸²æå¨ã</li>
<li><strong>å¯æ©å±æ§ï¼</strong> æåºäºå¤çº§æ¶é´é«æ¯å±æ¬¡ç»æï¼ä»¥å¤çé¿æ¶é´çå¨æè§é¢ï¼éè¿æ¶é´å¤ç¨åæçå·¥ä½éæ¥æ§å¶æ¨¡åå¤§å°ã</li>
</ul>
</li>
<li><strong>3DGSçæ°å­¦å¤ææ§ (Mathematical Intricacies of 3DGS)ï¼</strong> æ·±å¥æ¢è®¨äº3DGSçæ°å­¦åºç¡ï¼åæ¬ï¼<ul>
<li><strong>æé¯é½¿ï¼</strong> æåºäºMip-Splattingãå¤å°ºåº¦é«æ¯è¡¨ç¤ºåèªéåº3Då¹³æ»æ»¤æ³¢å¨ç­æ¹æ³ï¼ä»¥è§£å³ç¼©æ¾ååè¾¨çååå¯¼è´çé¯é½¿åéæ ·ä¸å¹éé®é¢ã</li>
<li><strong>å¤è§æ¨¡åï¼</strong> æ¢è®¨äºåä¼ è¾çä½æ¸²æä¸åæ åæè¡¡ï¼æ¾æ¸äº3DGSå¤è§æ¨¡åçç®ååè®¾ä½æ¶æç«æå¤±æã</li>
<li><strong>ç¸åè¯¯å·®ï¼</strong> åæäºå¹¿è§æå¤å´è§å¾ä¸­é«æ¯æå½±çå ä½ç¸åï¼å¹¶æåºäºæ¹è¿çæå½±æ¹æ¡åéå¼æ ¡æ­£å±æ¥åè½»è¿äºè¯¯å·®ã</li>
</ul>
</li>
<li><strong>3DGSç¨äºèæç°å® (3DGS for Virtual Reality)ï¼</strong> éå¯¹VRå¹³å°å¯¹æ¸²ææçãåå­éå¶åè§åºçè¦æ±ï¼æåºäºä¸é¨çè§£å³æ¹æ¡ï¼å¦ï¼<ul>
<li><strong>æ³¨è§ç¹æ¸²æ (Foveated Radiance Field Rendering)ï¼</strong> ç»åé«åè¾¨çç¥ç»ç¹æ¸²æï¼æ³¨è§ç¹åºåï¼åä½å¼é3DGSï¼å¤å´åºåï¼ï¼ä»¥ä¼åæ§è½èä¸çºç²æç¥è´¨éã</li>
<li><strong>ç³»ç»çº§ä¼åï¼</strong> éè¿ç¨³å®æ·±åº¦åå¯è§æ§è¿æ¸¡ãæ ¡æ­£æå½±ç¸ååå®ç°æ³¨è§ç¹åæ åå¨æ¥æé«æ¸²æç¨³å®æ§ï¼æ¶é¤VRä½éªä¸­çä¼ªå½±ã</li>
</ul>
</li>
<li><strong>å³æ¶3DGSéå»º (Toward Instant 3DGS Reconstruction)ï¼</strong> æ¢ç´¢äºå¨æ°ç§çè³æ´ç­æ¶é´åå®ç°åºæ¯éå»ºçæ¹æ³ï¼åæ¬ï¼<ul>
<li><strong>åé¦æ³åï¼</strong> PixelSplatç­æ¹æ³éè¿ç¥ç»ç½ç»ç´æ¥é¢æµé«æ¯åæ°ï¼å®ç°ä»å°éå¾åå°å®æ¶éå»ºã</li>
<li><strong>ç¨çè¾å¥ä¸å¿«éæ¨çï¼</strong> GS-LRMç­æ¨¡åå©ç¨Transformerä»2-4å¼ å¾åå¿«ééå»ºå¯éé«æ¯åºåã</li>
<li><strong>æ å§¿æå¾åæµä¸å¤§è§æ¨¡åºæ¯ï¼</strong> æåºäºå¨æè·è¿ç¨ä¸­è¿è¡å¿«éå§¿æåå§åãå¢éå¼é«æ¯çæåèç±»ï¼å®ç°æ å§¿æå¾åæµçè¿å®æ¶éå»ºã</li>
<li><strong>å®æ¶ç´æ­ï¼</strong> éå¯¹ä½è²èµäºç­å¨æãå¤§è§æ¨¡åºæ¯ï¼éè¿å¤æåå¤´è¾å¥ãåå¸å¼å¤çåç²å°ç»çæ¼å/ç¯å¢åç¦»ï¼å®ç°äº¤äºå¼èªç±è§ç¹æ¢ç´¢ã</li>
</ul>
</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
è®ºææ»ç»äº3DGSå¨ä»¥ä¸å ä¸ªæ¹é¢åå¾äºæ¾èè¿å±ï¼</p>
<ul>
<li><strong>æçåå¯æ©å±æ§ï¼</strong> 3DGSå·²ä»æåçèµæºå¯éåæ¹æ³åå±ä¸ºè½å¤å¨åç§ç¡¬ä»¶ä¸é«æè¿è¡ï¼å¹¶è½å¤çå¤§è§æ¨¡åé¿æ¶é´çå¨æåºæ¯ã</li>
<li><strong>è§è§ä¿çåº¦ï¼</strong> éè¿å¯¹æ°å­¦åºç¡çæ·±å¥çè§£åæ¹è¿ï¼3DGSå¨æé¯é½¿ãå¤è§å»ºæ¨¡åç¸åæ ¡æ­£æ¹é¢åå¾äºæ¾èæåï¼æä¾äºæ´é«è´¨éçæ¸²æç»æã</li>
<li><strong>å®æ¶äº¤äºæ§ï¼</strong> å¨æ3DGSåå³æ¶éå»ºææ¯ä½¿å¾3DGSè½å¤æè·åæ¸²æå¨ææ§è½ï¼å¹¶å®ç°è¿ä¹å®æ¶çåºæ¯éå»ºï¼æå¤§å°æå®½äºå¶åºç¨èå´ï¼å°¤å¶æ¯å¨VRåç´æ­é¢åã</li>
<li><strong>åºç¨æ°ä¸»åï¼</strong> èµæºåéä¸ç3DGSä½¿å¶è½å¤é¨ç½²å¨ç§»å¨åWebå¹³å°ï¼éä½äºå®æ¶è¾å°åºæ¸²æçé¨æ§ã</li>
</ul>
<p>è¿äºè¿å±å±åè¡¨æï¼3DGSå·²æä¸ºè¿æ¥å­¦ä¹ å3Dè¡¨ç¤ºåå®æ¶å¾å½¢çå³é®ææ¯ï¼ä¸ºé«æãå¯æ©å±åå¯è®¿é®ç3Dåºæ¯æè·åæ¸²æçæªæ¥å¥ å®äºåºç¡ã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
è®ºæä¸»è¦ä»¥ç»¼è¿°å½¢å¼åç°ï¼å æ­¤å¹¶æªç´æ¥æåºèªèº«æ¹æ³çå±éæ§ãç¶èï¼å®éè¿è®¨è®ºç°æç ç©¶æ¥é´æ¥æåº3DGSå¨åå±è¿ç¨ä¸­é¢ä¸´çææåå±éï¼ä¾å¦ï¼</p>
<ul>
<li><strong>åå§3DGSçèµæºéæ±ï¼</strong> åå§çæ¬ç3DGSéè¦å¤§éåå­åè®¡ç®èµæºï¼éå¶äºå¶å¨ååç¡¬ä»¶ä¸çé¨ç½²ã</li>
<li><strong>éæåºæ¯åè®¾ï¼</strong> ç»å¸3DGSåè®¾åºæ¯æ¯éæçï¼æ æ³ç´æ¥å¤çéåæ§è¿å¨åå¨æååã</li>
<li><strong>æ°å­¦ç®åå¯¼è´çä¼ªå½±ï¼</strong> åå§3DGSå¨ä½æ¸²æçè®ºä¸çç®åå¯¼è´äºé¯é½¿ãå¤è§å»ºæ¨¡ä¸åç¡®åå¤å´è§å¾ç¸åç­ä¼ªå½±ã</li>
<li><strong>VRå¹³å°çä¸¥èè¦æ±ï¼</strong> VRå¯¹é«å¸§çãç«ä½æ¸²æãå¤§è§åºåä½å»¶è¿ææé«è¦æ±ï¼åå§3DGSé¾ä»¥ç´æ¥æ»¡è¶³ã</li>
<li><strong>éå»ºæ¶é´ï¼</strong> æ©æ3DGSéå»ºéè¦æ°åéçè³æ°å°æ¶ï¼éå¶äºå¶å¨å®æ¶åºç¨ä¸­çæ½åã</li>
<li><strong>ç ç©¶èå´çå¹¿åº¦ï¼</strong> è®ºææå°ï¼3DGSçç ç©¶é¢åå·²åå¾å¦æ­¤å¹¿éï¼ä»¥è³äºåä¸çå¨é¢ç»¼è¿°åå¾ä¸å¯è¡ï¼è¿æ¬èº«ä¹æç¤ºäºè¯¥é¢åä»æè®¸å¤æªè§£å³çé®é¢åææã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºæéè¿æ»ç»ç°æè¿å±ï¼é´æ¥æåºäºæªæ¥çç ç©¶æ¹åï¼ä¸»è¦åæ¬ï¼</p>
<ul>
<li><strong>è¿ä¸æ­¥çèµæºä¼åï¼</strong> æç»­æ¢ç´¢æ´é«æçåç¼©ãè®­ç»åæ¸²æç­ç¥ï¼ä»¥æ¯ææ´å¹¿æ³çè®¾å¤ååºç¨åºæ¯ã</li>
<li><strong>æ´é²æ£åéç¨çå¨æåºæ¯è¡¨ç¤ºï¼</strong> åå±è½å¤å¤çæ´å¤æãæ´é¿æ¶é´ãæ´ä¸å¯é¢æµçå¨æåºæ¯ç4DGSæ¨¡åï¼å¹¶æé«å¶å¨ä¸åè¿å¨ç±»åä¸çæ³åè½åã</li>
<li><strong>æ´ç²¾ç¡®çæ°å­¦æ¨¡åï¼</strong> æ·±å¥ç ç©¶3DGSçæ°å­¦åºç¡ï¼å¼åæ´ç²¾ç¡®çæå½±ãå¤è§ååä¼ è¾æ¨¡åï¼ä»¥æ¶é¤å©ä½çä¼ªå½±å¹¶æé«æ¸²æè´¨éã</li>
<li><strong>VR/ARçæ·±åº¦éæï¼</strong> éå¯¹å¤´æ´å¼æ¾ç¤ºå¨çç¹å®éæ±ï¼è¿ä¸æ­¥ä¼å3DGSå¨æ¸²æç¨³å®æ§ãå»¶è¿åæç¥è´¨éæ¹é¢çè¡¨ç°ï¼ä½¿å¶æä¸ºVR/ARåå®¹åå»ºçæ ¸å¿ææ¯ã</li>
<li><strong>å®æ¶åå³æ¶éå»ºççªç ´ï¼</strong> æ¨å¨åé¦ç½ç»ååå¸å¼è®¡ç®å¨3DGSéå»ºä¸­çåºç¨ï¼å®ç°ä»ä»»æè¾å¥ï¼åæ¬æ å§¿æå¾åæµï¼å°é«è´¨é3DGSåºæ¯çå³æ¶çæã</li>
<li><strong>å¯ç¼è¾æ§åè¯­ä¹çè§£ï¼</strong> è®ºææå°3DGSä½ä¸ºâå¯ç¼è¾âçéª¨å¹²ï¼æç¤ºäºæªæ¥ç ç©¶å¯è½å³æ³¨å¦ä½å°è¯­ä¹ä¿¡æ¯èå¥3DGSï¼ä½¿å¶è½å¤è¿è¡æ´é«çº§å«çåºæ¯çè§£åç¼è¾æä½ã</li>
<li><strong>å¤§è§æ¨¡ç¯å¢çæ©å±ï¼</strong> è¿ä¸æ­¥æ¢ç´¢3DGSå¨å¤çè¶å¤§è§æ¨¡åºæ¯ï¼å¦åå¸å°ºåº¦ï¼æ¶çå¯æ©å±æ§é®é¢ã</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>Follow-up work includes analyses and contributions that
enhance the efficiency, scalability, and real-world applicability of 3DGS.</li>
<li>In
this summary, we present an overview of several key directions that have
emerged in the wake of 3DGS.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.26694v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.26694v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.26641v1'></a></p>
<h2 id="all-you-need-for-object-detection-from-pixels-points-and-prompts-to-next-gen-fusion-and-multimodal-llmsvlms-in-autonomous-vehicles"><a href="https://arxiv.org/abs/2510.26641v1">All You Need for Object Detection: From Pixels, Points, and Prompts to Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles</a></h2>
<p><strong>Authors:</strong> Sayed Pedram Haeri Boroujeni, Niloufar Mehrabi, Hazim Alzorgan, Ahmad Sarlak, Mahlagha Fazeli, Abolfazl Razi</p>
<p><strong>Published:</strong> 2025-10-30</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Autonomous Vehicles (AVs) are transforming the future of transportation
through advances in intelligent perception, decision-making, and control
systems. However, their success is tied to one core capability, reliable object
detection in complex and multimodal environments. While recent breakthroughs in
Computer Vision (CV) and Artificial Intelligence (AI) have driven remarkable
progress, the field still faces a critical challenge as knowledge remains
fragmented across multimodal perception, contextual reasoning, and cooperative
intelligence. This survey bridges that gap by delivering a forward-looking
analysis of object detection in AVs, emphasizing emerging paradigms such as
Vision-Language Models (VLMs), Large Language Models (LLMs), and Generative AI
rather than re-examining outdated techniques. We begin by systematically
reviewing the fundamental spectrum of AV sensors (camera, ultrasonic, LiDAR,
and Radar) and their fusion strategies, highlighting not only their
capabilities and limitations in dynamic driving environments but also their
potential to integrate with recent advances in LLM/VLM-driven perception
frameworks. Next, we introduce a structured categorization of AV datasets that
moves beyond simple collections, positioning ego-vehicle, infrastructure-based,
and cooperative datasets (e.g., V2V, V2I, V2X, I2I), followed by a
cross-analysis of data structures and characteristics. Ultimately, we analyze
cutting-edge detection methodologies, ranging from 2D and 3D pipelines to
hybrid sensor fusion, with particular attention to emerging transformer-driven
approaches powered by Vision Transformers (ViTs), Large and Small Language
Models (SLMs), and VLMs. By synthesizing these perspectives, our survey
delivers a clear roadmap of current capabilities, open challenges, and future
opportunities.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºSayed Pedram Haeri Boroujeniç­äººæ°åçè®ºæâAll You Need for Object Detection: From Pixels, Points, and Prompts to Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehiclesâçå¨é¢æè¦ï¼</p>
<p><strong>è®ºææè¦ï¼èªå¨é©¾é©¶ä¸­å¤æ¨¡æç®æ æ£æµçä¸ä¸ä»£èåä¸LLMs/VLMs</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³èªå¨é©¾é©¶ï¼AVsï¼é¢åä¸­ç®æ æ£æµé¢ä¸´çå³é®ææãå°½ç®¡è®¡ç®æºè§è§ï¼CVï¼åäººå·¥æºè½ï¼AIï¼åå¾äºæ¾èè¿å±ï¼ä½è¯¥é¢åä»é¢ä¸´ç¥è¯ç¢çåçé®é¢ï¼å°¤å¶æ¯å¨å¤æ¨¡ææç¥ãä¸ä¸ææ¨çåååæºè½æ¹é¢ãå æ­¤ï¼æ ¸å¿ç ç©¶é®é¢æ¯å¦ä½éè¿æ´åæ°å´èå¼ï¼å¦è§è§-è¯­è¨æ¨¡åï¼VLMsï¼ãå¤§åè¯­è¨æ¨¡åï¼LLMsï¼åçæå¼AIï¼ä»¥ååè¿çä¼ æå¨èåç­ç¥ï¼å®ç°å¤æå¤æ¨¡æç¯å¢ä¸çå¯é ç®æ æ£æµï¼ä»èæ¨å¨èªå¨é©¾é©¶ç³»ç»æ´å®å¨ãæ´æºè½çåå±ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥ç»¼è¿°è®ºæéè¿ä»¥ä¸å ä¸ªæ¹é¢ååºäºå³é®åæ°åæ¹æ³è®ºè´¡ç®ï¼
*   <strong>ç³»ç»æ§ä¼ æå¨ç»¼è¿°ä¸èåç­ç¥ï¼</strong> è®ºæå¨é¢åé¡¾äºèªå¨é©¾é©¶è½¦è¾ä¸­ä½¿ç¨çåç§ä¼ æå¨ï¼æåå¤´ãè¶å£°æ³¢ãæ¿åé·è¾¾åé·è¾¾ï¼åå¶èåç­ç¥ï¼ä¸ä»å¼ºè°äºå®ä»¬å¨å¨æé©¾é©¶ç¯å¢ä¸­çè½ååå±éæ§ï¼è¿æ¢è®¨äºå®ä»¬ä¸LLM/VLMé©±å¨æç¥æ¡æ¶éæçæ½åã
*   <strong>ç»æåèªå¨é©¾é©¶æ°æ®éåç±»ï¼</strong> è®ºææåºäºä¸ç§æ°é¢çèªå¨é©¾é©¶æ°æ®éåç±»æ¹æ³ï¼è¶è¶äºç®åçéåï¼å°æ°æ®éåä¸ºèªæè½¦è¾ãåºç¡è®¾æ½åååæ°æ®éï¼ä¾å¦V2VãV2IãV2XãI2Iï¼ï¼å¹¶å¯¹æ°æ®ç»æåç¹æ§è¿è¡äºäº¤ååæã
*   <strong>åæ²¿æ£æµæ¹æ³åæï¼</strong> è®ºææ·±å¥åæäºä»2Då3Dç®¡éå°æ··åä¼ æå¨èåçå°ç«¯æ£æµæ¹æ³ï¼ç¹å«å³æ³¨ç±è§è§Transformerï¼ViTsï¼ãå¤§ååå°åè¯­è¨æ¨¡åï¼SLMsï¼ä»¥åVLMsé©±å¨çæ°å´Transformeræ¹æ³ã
*   <strong>å¤æ¨¡æAIæ´åï¼</strong> è®ºæå¼ºè°äºå¤æ¨¡æAIï¼åæ¬LLMsåVLMsï¼å¨å¢å¼ºä¸ä¸æçè§£åæé«æ£æµç²¾åº¦æ¹é¢çä½ç¨ï¼éè¿èåæ¥èªä¸åä¼ æå¨è¾å¥ï¼è§è§åç©ºé´çº¿ç´¢ï¼çæ°æ®æ¥å®ç°ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
è¯¥ç»¼è¿°éè¿ç»¼åè¿äºè§è§ï¼ä¸ºèªå¨é©¾é©¶ç®æ æ£æµçå½åè½åãå¼æ¾ææåæªæ¥æºéæä¾äºæ¸æ°çè·¯çº¿å¾ãä¸»è¦ç»æåæä¹åæ¬ï¼
*   <strong>å¤æ¨¡æèåçå¿è¦æ§ï¼</strong> è®ºæå¼ºè°ï¼å¯é çç®æ æ£æµæ¬è´¨ä¸æ¯å¤æ¨¡æçï¼éè¦å¹³è¡¡è®¡ç®æçãç¯å¢éåºæ§åè¯­ä¹ä¸°å¯æ§ã
*   <strong>æ°å´èå¼çæ½åï¼</strong> LLMsåVLMsç­æ°å´èå¼å¨è¯­ä¹çè§£ãä¸ä¸ææ¨çåé¶æ ·æ¬æ£æµæ¹é¢å±ç°åºå·¨å¤§æ½åï¼è½å¤å¤çä¼ ç»æ¨¡åé¾ä»¥è§£å³çå¤æææ¨¡ç³æåµã
*   <strong>æ§è½æåï¼</strong> èåæ¹æ³ï¼ç¹å«æ¯2D-3Dèåï¼å¨KITTIåNuScenesç­æ°æ®éä¸è¡¨ç°åºä¼äºåä¸æ¨¡ææ¹æ³çæ§è½ï¼å°¤å¶æ¯å¨è½¦è¾æ£æµæ¹é¢ï¼è¿å¾çäºç»åäºæ¿åé·è¾¾çç²¾ç¡®å ä½ä¿¡æ¯åæåå¤´çä¸°å¯è¯­ä¹ç»èã
*   <strong>å¯¹ç ç©¶åå®è·µçæå¯¼ï¼</strong> è¯¥ç»¼è¿°ä¸ºç ç©¶äººåãä»ä¸èåå¼åäººåæä¾äºæå¨åèï¼æ¨å¨å éèªå¨é©¾é©¶ç³»ç»å¨å®å¨æ§ãå¯é æ§åæºè½æ§æ¹é¢çåæ°ã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
è®ºæä¹å¦è¯å°æåºäºå½åæ¹æ³çå±éæ§ï¼
*   <strong>ç¥è¯ç¢çåï¼</strong> å°½ç®¡æè¿å±ï¼ä½å¤æ¨¡ææç¥ãä¸ä¸ææ¨çåååæºè½é¢åçç¥è¯ä»ç¶ç¢çåã
*   <strong>å°ç©ä½åé¨åé®æ¡æ£æµçææï¼</strong> èåæ¹æ³å¨è¡äººæ£æµç­å°ç©ä½æé¨åé®æ¡ç©ä½æ£æµæ¹é¢çæ¹è¿æéï¼è¿å¯è½æ¯ç±äºæ¿åé·è¾¾ç¹äºå¯åº¦ä½ãå¾åçº¹ççº¿ç´¢å¤å«ä»·å¼æéä»¥åæ ¡åååæ­¥è¯¯å·®ç­å ç´ é æçã
*   <strong>æ°æ®ä¾èµæ§ï¼</strong> èªå¨é©¾é©¶ç³»ç»é«åº¦ä¾èµå¤§è§æ¨¡ãé«è´¨éãå¤æ ·ååæ æ³¨è¯å¥½çæ°æ®éï¼ä½ç°å®ä¸çæ°æ®éå¾å¾å­å¨é¿å°¾åå¸åç¨æäºä»¶è¦çä¸è¶³çé®é¢ã
*   <strong>è®¡ç®åèµæºéæ±ï¼</strong> å¤æ¨¡ææ°æ®å¤çåèåå¸¦æ¥äºé«è®¡ç®ææ¬åå­å¨éæ±ï¼å°¤å¶æ¯å¨å®æ¶å³ç­åºæ¯ä¸­ã
*   <strong>æ³åè½åï¼</strong> ç°ææ¨¡åå¨æªè§æ¡ä»¶ä¸çæ³åè½åæéï¼å®¹æåºç°åå·®æè¿åº¦ä¸ä¸åã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºææåºäºä»¥ä¸å ä¸ªæåæ¯çæªæ¥ç ç©¶æ¹åï¼
*   <strong>å¨æãä¸ä¸ææç¥ä¼ æå¨èåï¼</strong> å¼åè½å¤æ ¹æ®å½åé©¾é©¶åºæ¯ãç¯å¢æ¡ä»¶åç³»ç»ä¸ç¡®å®æ§èªéåºè°æ´å¤æ¨¡æè¾å¥ï¼æåå¤´ãæ¿åé·è¾¾ãé·è¾¾ï¼ä¼åçº§çèåç®¡éã
*   <strong>åºç¡æ¨¡åæ´åï¼</strong> å©ç¨å¨æµ·éãè·¨é¢åå¤æ¨¡ææ°æ®éä¸è®­ç»çåºç¡æ¨¡åï¼ä½¿èªå¨é©¾é©¶è½¦è¾è½å¤æ¨çç¨æåæªè§åºæ¯ï¼è¶è¶å½ååºåæµè¯çè¦çèå´ã
*   <strong>è·¨è½¦è¾ååæç¥ï¼</strong> å¢å¼ºLLM/VLMæ¨ççè·¨è½¦è¾ååæç¥ï¼éè¿äº¤æ¢åç¼©è¯­ä¹è¡¨ç¤ºèéåå§ä¼ æå¨æ°æ®æ¥åå°å¸¦å®½éæ±ï¼åæ¶ä¿æåºæ¯çè§£ã
*   <strong>æ¨¡æå°ç°å®åéåºï¼</strong> å©ç¨çææ¨¡ååç¥ç»æ¸²æçæ¨¡æå°ç°å®åéåºææ¯ï¼å¼¥ååææ°æ®ä¸ç°å®ä¸çæ°æ®ä¹é´çæ§è½å·®è·ã
*   <strong>ä¸ç¡®å®æ§æç¥æç¥ç³»ç»ï¼</strong> å¼åè½å¤èªæè¯ä¼°æ£æµå¯é æ§å¹¶å¨æè°æ´è§ååæ§å¶ç­ç¥çæç¥ç³»ç»ï¼ä»¥å®ç°æ´å®å¨ãæ´å¯è§£éåæ´å¼å¾ä¿¡èµçèªå¨é©¾é©¶å³ç­ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Next, we introduce a structured categorization of AV datasets that
moves beyond simple collections, positioning ego-vehicle, infrastructure-based,
and cooperative datasets (e.g., V2V, V2I, V2X, I2I), followed by a
cross-analysis of data structures and characteristics.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.26641v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.26641v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.26583v1'></a></p>
<h2 id="emu35-native-multimodal-models-are-world-learners"><a href="https://arxiv.org/abs/2510.26583v1">Emu3.5: Native Multimodal Models are World Learners</a></h2>
<p><strong>Authors:</strong> Yufeng Cui, Honghao Chen, Haoge Deng, Xu Huang, Xinghang Li, Jirong Liu, Yang Liu, Zhuoyan Luo, Jinsheng Wang, Wenxuan Wang, Yueze Wang, Chengyuan Wang, Fan Zhang, Yingli Zhao, Ting Pan, Xianduo Li, Zecheng Hao, Wenxuan Ma, Zhuo Chen, Yulong Ao, Tiejun Huang, Zhongyuan Wang, Xinlong Wang</p>
<p><strong>Published:</strong> 2025-10-30</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We introduce Emu3.5, a large-scale multimodal world model that natively
predicts the next state across vision and language. Emu3.5 is pre-trained
end-to-end with a unified next-token prediction objective on a corpus of
vision-language interleaved data containing over 10 trillion tokens, primarily
derived from sequential frames and transcripts of internet videos. The model
naturally accepts interleaved vision-language inputs and generates interleaved
vision-language outputs. Emu3.5 is further post-trained with large-scale
reinforcement learning to enhance multimodal reasoning and generation. To
improve inference efficiency, we propose Discrete Diffusion Adaptation (DiDA),
which converts token-by-token decoding into bidirectional parallel prediction,
accelerating per-image inference by about 20x without sacrificing performance.
Emu3.5 exhibits strong native multimodal capabilities, including long-horizon
vision-language generation, any-to-image (X2I) generation, and complex
text-rich image generation. It also exhibits generalizable world-modeling
abilities, enabling spatiotemporally consistent world exploration and
open-world embodied manipulation across diverse scenarios and tasks. For
comparison, Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image
(Nano Banana) on image generation and editing tasks and demonstrates superior
results on a suite of interleaved generation tasks. We open-source Emu3.5 at
https://github.com/baaivision/Emu3.5 to support community research.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°å¯¹è¿ç¯è®ºææè¦è¿è¡åæã</p>
<hr />
<p><strong>è®ºææè¦åæï¼Emu3.5: Native Multimodal Models are World Learners</strong></p>
<p><strong>1. è®ºæä¸»è¦è´¡ç®çç®æ´æ»ç» (2-3 å¥è¯)</strong></p>
<p>Emu3.5 å¼å¥äºä¸ä¸ªå¤§è§æ¨¡å¤æ¨¡æä¸çæ¨¡åï¼è½å¤åçé¢æµè§è§åè¯­è¨çä¸ä¸ä¸ªç¶æãå®éè¿ç»ä¸çä¸ä¸ token é¢æµç®æ å¨è¶è¿ 10 ä¸äº¿ token çè§è§-è¯­è¨äº¤éæ°æ®ä¸è¿è¡ç«¯å°ç«¯é¢è®­ç»ï¼å¹¶ç»åå¤§è§æ¨¡å¼ºåå­¦ä¹ è¿è¡åè®­ç»ï¼ä»¥å¢å¼ºå¤æ¨¡ææ¨çåçæè½åãæ­¤å¤ï¼è¯¥å·¥ä½è¿æåºäº Discrete Diffusion Adaptation (DiDA) ææ¯ï¼æ¾èæåäºæ¨çæçã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºæ¹æ³</strong></p>
<ul>
<li><strong>åçå¤æ¨¡æä¸çæ¨¡å (Native Multimodal World Model):</strong> Emu3.5 çæ ¸å¿åæ°å¨äºå¶ä½ä¸ºâä¸çæ¨¡åâçå®ä½ï¼è½å¤åçé¢æµè§è§åè¯­è¨çä¸ä¸ä¸ªç¶æãè¿æå³çå®ä¸ä»ä»æ¯å¤çå¤æ¨¡æè¾å¥ï¼èæ¯è¯å¾çè§£åæ¨¡æä¸ççå¨ææ¼åï¼è¿å¨ç°æçå¤§åå¤æ¨¡ææ¨¡åä¸­æ¯ä¸ä¸ªæ´å®å¤§çç®æ ã</li>
<li><strong>ç»ä¸çä¸ä¸ token é¢æµç®æ  (Unified Next-Token Prediction Objective):</strong> éç¨è¿ç§ç»ä¸çè®­ç»ç®æ ï¼ä½¿å¾æ¨¡åè½å¤æ ç¼å¤çè§è§åè¯­è¨çäº¤éæ°æ®ï¼å¹¶çæäº¤éçè§è§-è¯­è¨è¾åºï¼è¿ä½ç°äºå¶å¯¹å¤æ¨¡ææ°æ®çæ·±åº¦èååçè§£ã</li>
<li><strong>å¤§è§æ¨¡è§è§-è¯­è¨äº¤éæ°æ®é¢è®­ç» (Large-scale Vision-Language Interleaved Data Pre-training):</strong> è¶è¿ 10 ä¸äº¿ token çæ°æ®éï¼ä¸»è¦æ¥æºäºäºèç½è§é¢çåºåå¸§åè½¬å½ææ¬ï¼è¿ä¸ºæ¨¡åå­¦ä¹ å¤æçæ¶ç©ºåè¯­ä¹å³ç³»æä¾äºæå¶ä¸°å¯çåºç¡ã</li>
<li><strong>å¤§è§æ¨¡å¼ºåå­¦ä¹ åè®­ç» (Large-scale Reinforcement Learning Post-training):</strong> ç»å RL è¿ä¸æ­¥æåå¤æ¨¡ææ¨çåçæè½åï¼è¿è¡¨ææ¨¡åä¸ä»è½âé¢æµâï¼è¿è½éè¿ä¸ç¯å¢çäº¤äºæ¥ä¼åå¶è¡ä¸ºåè¾åºï¼è¿å¯¹äºå®ç°æ´é«çº§å«çæºè½è³å³éè¦ã</li>
<li><strong>Discrete Diffusion Adaptation (DiDA) æåæ¨çæç:</strong> è¿æ¯ä¸é¡¹éè¦çå·¥ç¨åæ°ï¼å°é token è§£ç è½¬æ¢ä¸ºååå¹¶è¡é¢æµï¼å°æ¯å¾åæ¨çéåº¦æåçº¦ 20 åï¼åæ¶ä¸çºç²æ§è½ãè¿è§£å³äºå¤§åçææ¨¡åå¨å®éåºç¨ä¸­çä¸ä¸ªå³é®ç¶é¢ã</li>
</ul>
<p><strong>3. å¯¹é¢åæ½å¨å½±å</strong></p>
<ul>
<li><strong>æ¨å¨å¤æ¨¡æä¸çæ¨¡åçåå±:</strong> Emu3.5 çå·¥ä½ä¸ºæå»ºè½å¤çè§£åé¢æµå¤æä¸çå¨æçå¤æ¨¡ææ¨¡åæ ç«äºæ°çæ æï¼å¯è½æ¿åæ´å¤ç ç©¶èæ¢ç´¢âä¸çæ¨¡åâå¨å¤æ¨¡æé¢åçåºç¨ã</li>
<li><strong>æåå¤æ¨¡æçæåæ¨çè½å:</strong> å¶å¨é¿æ¶åºè§è§-è¯­è¨çæãä»»æå°å¾å (X2I) çæåå¤æææ¬ä¸°å¯å¾åçææ¹é¢çå¼ºå¤§è½åï¼å°æå¤§å°æå±å¤æ¨¡ææ¨¡åçåºç¨è¾¹çã</li>
<li><strong>å éå·èº«æºè½åæºå¨äººé¢åè¿æ­¥:</strong> å·å¤æ¶ç©ºä¸è´çä¸çæ¢ç´¢åå¼æ¾ä¸çå·èº«æä½è½åï¼æå³ç Emu3.5 å¯ä»¥ä½ä¸ºå·èº«æºè½ä½ææºå¨äººçæ ¸å¿æç¥åå³ç­æ¨¡åï¼æ¨å¨è¿äºé¢åçåå±ã</li>
<li><strong>ä¸ºé«æå¤æ¨¡ææ¨çæä¾æ°æè·¯:</strong> DiDA ææ¯ä¸ºå¤§åå¤æ¨¡ææ¨¡åçé¨ç½²åå®éåºç¨æä¾äºéè¦çæçä¼åæ¹æ¡ï¼å¯è½è¢«å¶ä»ç ç©¶èåé´åæ¹è¿ã</li>
<li><strong>ä¿è¿å¼æ¾ç ç©¶åç¤¾åºåä½:</strong> å¼æº Emu3.5 å°æå¤§å°éä½ç ç©¶é¨æ§ï¼å éç¤¾åºå¨å¤æ¨¡æ AI é¢åçåæ°ååå±ã</li>
</ul>
<p><strong>4. ç¸å³é¢åæåºç¨åç</strong></p>
<ul>
<li><strong>å·èº«æºè½ (Embodied AI) åæºå¨äººå­¦ (Robotics):</strong> æ¨¡åçâä¸çå»ºæ¨¡âè½ååå¼æ¾ä¸çå·èº«æä½è½åä½¿å¶æä¸ºæºå¨äººè§åãæç¥åå³ç­ççæ³åºç¡æ¨¡åã</li>
<li><strong>è§é¢çè§£ä¸çæ (Video Understanding and Generation):</strong> åºäºäºèç½è§é¢æ°æ®è®­ç»ï¼æ¨¡åå¨é¿æ¶åºè§è§-è¯­è¨çææ¹é¢è¡¨ç°åºè²ï¼å¯ç¨äºè§é¢åå®¹åä½ãç¼è¾ãæè¦åé¢æµã</li>
<li><strong>å¤æ¨¡æåå®¹åä½ (Multimodal Content Creation):</strong> ä»»æå°å¾å (X2I) çæåå¤æææ¬ä¸°å¯å¾åçæè½åï¼å°èµè½è®¾è®¡å¸ãèºæ¯å®¶ååå®¹åä½èï¼å®ç°æ´çµæ´»ãæ´ä¸°å¯çåä½ã</li>
<li><strong>èæç°å® (VR) / å¢å¼ºç°å® (AR) ååå®å® (Metaverse):</strong> è½å¤è¿è¡æ¶ç©ºä¸è´çä¸çæ¢ç´¢ï¼æå©äºæå»ºæ´çå®ãæ´å·äº¤äºæ§çèæç¯å¢ã</li>
<li><strong>äººæºäº¤äº (Human-Computer Interaction):</strong> æ´èªç¶ãæ´æºè½çå¤æ¨¡æäº¤äºçé¢ï¼è½å¤çè§£ç¨æ·çè§è§åè¯­è¨æå¾ï¼å¹¶ä»¥å¤æ¨¡ææ¹å¼ååºã</li>
<li><strong>æè²åæ¨¡æè®­ç» (Education and Simulation Training):</strong> åå»ºé¼ççæ¨¡æç¯å¢ï¼ç¨äºæå­¦åè®­ç»ã</li>
</ul>
<p><strong>5. ä»æè¦ä¸­å¯æ¨æ­çå±éæ§</strong></p>
<ul>
<li><strong>æ°æ®åå·®åæ³åæ§ææ:</strong> å°½ç®¡ä½¿ç¨äºè¶è¿ 10 ä¸äº¿ token çå¤§è§æ¨¡æ°æ®ï¼ä½æ°æ®ä¸»è¦æ¥æºäºâäºèç½è§é¢âï¼è¿å¯è½å¼å¥ç¹å®çæ°æ®åå·®ï¼ä¾å¦ï¼ç¹å®æåãåå®¹ç±»åãè´¨éç­ï¼ï¼å½±åæ¨¡åå¨æäºç¹å®é¢åæä½èµæºåºæ¯ä¸çæ³åè½åã</li>
<li><strong>âä¸çæ¨¡åâçå®ä¹åè¯ä¼°æ å:</strong> æè¦ä¸­æå°âä¸çæ¨¡åâï¼ä½å¶å·ä½å®ä¹åå¦ä½å¨é¢è¯ä¼°å¶âä¸çå»ºæ¨¡âè½åä»éå¨è®ºææ­£æä¸­è¯¦ç»éè¿°ãä¾å¦ï¼å®æ¯å¦è½çè§£ç©çå®å¾ãå æå³ç³»ç­æ·±å±æ¬¡çä¸çç¥è¯ï¼</li>
<li><strong>è®¡ç®èµæºéæ±:</strong> è®­ç»ä¸ä¸ªå¨ 10 ä¸äº¿ token ä¸é¢è®­ç»å¹¶ç»åå¤§è§æ¨¡å¼ºåå­¦ä¹ çæ¨¡åï¼ä»¥åå¶æ¬èº«ä½ä¸ºâå¤§è§æ¨¡å¤æ¨¡ææ¨¡åâï¼æ çéè¦å·¨å¤§çè®¡ç®èµæºï¼è¿å¯¹äºå°åç ç©¶å¢éæä¸ªäººèè¨æ¯é¾ä»¥å¤ç°çãDiDA è§£å³äºæ¨çæçï¼ä½è®­ç»ææ¬ä»ç¶å¾é«ã</li>
<li><strong>å¼ºåå­¦ä¹ çææ:</strong> å¤§è§æ¨¡å¼ºåå­¦ä¹ çè®­ç»éå¸¸éå¸¸å¤æï¼å­å¨å¥å±è®¾è®¡ãæ¢ç´¢-å©ç¨å°å¢ãæ¶ææ§ç­ææï¼å¶ç¨³å®æ§ãæçåæç»ææçé²æ£æ§éè¦è¿ä¸æ­¥éªè¯ã</li>
<li><strong>ä¸ç°ææ¨¡åçè¯¦ç»å¯¹æ¯:</strong> æè¦ä¸­æå°ä¸ Gemini 2.5 Flash Image (Nano Banana) çæ¯è¾ï¼ä½æ´å¨é¢çåºåæµè¯åä¸ SOTA æ¨¡åçè¯¦ç»æ§è½å¯¹æ¯ï¼å°¤å¶æ¯å¨âä¸çå»ºæ¨¡âåå·èº«æºè½ä»»å¡ä¸ï¼å°æ¯è¯ä¼°å¶çæ­£ä¼å¿çå³é®ã</li>
<li><strong>âåçå¤æ¨¡æâçå®ç°ç»è:</strong> æè¦ä¸­å¼ºè°âåçå¤æ¨¡æâï¼ä½å¶å·ä½æ¶æï¼ä¾å¦ï¼æ¯å¦æ¯ç»ä¸ç Transformer æ¶æï¼å¦ä½ç¼ç è§è§åè¯­è¨ä¿¡æ¯ï¼çç»èå¹¶æªæåï¼è¿ä¼å½±åå¯¹å¶ææ¯æ·±åº¦ççè§£ã</li>
</ul>
<hr />
<p>æ»èè¨ä¹ï¼Emu3.5 æ¯ä¸é¡¹éå¿ååä¸å·æåç»æ§çå·¥ä½ï¼å®å°å¤æ¨¡æå­¦ä¹ æ¨åäºâä¸çæ¨¡åâçæ°é«åº¦ï¼å¹¶å¨æçä¼åæ¹é¢åå¾äºæ¾èè¿å±ãå¶å¼æºå°ä¸ºæ´ä¸ªè®¡ç®æºè§è§åæºå¨å­¦ä¹ ç¤¾åºå¸¦æ¥å·¨å¤§çä»·å¼ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce Emu3.5, a large-scale multimodal world model that natively
predicts the next state across vision and language.</li>
<li>To
improve inference efficiency, we propose Discrete Diffusion Adaptation (DiDA),
which converts token-by-token decoding into bidirectional parallel prediction,
accelerating per-image inference by about 20x without sacrificing performance.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.26583v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.26583v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.25760v1'></a></p>
<h2 id="multimodal-spatial-reasoning-in-the-large-model-era-a-survey-and-benchmarks"><a href="https://arxiv.org/abs/2510.25760v1">Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks</a></h2>
<p><strong>Authors:</strong> Xu Zheng, Zihao Dongfang, Lutao Jiang, Boyuan Zheng, Yulong Guo, Zhenquan Zhang, Giuliano Albanese, Runyi Yang, Mengjiao Ma, Zixin Zhang, Chenfei Liao, Dingcheng Zhen, Yuanhuiyi Lyu, Yuqian Fu, Bin Ren, Linfeng Zhang, Danda Pani Paudel, Nicu Sebe, Luc Van Gool, Xuming Hu</p>
<p><strong>Published:</strong> 2025-10-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Humans possess spatial reasoning abilities that enable them to understand
spaces through multimodal observations, such as vision and sound. Large
multimodal reasoning models extend these abilities by learning to perceive and
reason, showing promising performance across diverse spatial tasks. However,
systematic reviews and publicly available benchmarks for these models remain
limited. In this survey, we provide a comprehensive review of multimodal
spatial reasoning tasks with large models, categorizing recent progress in
multimodal large language models (MLLMs) and introducing open benchmarks for
evaluation. We begin by outlining general spatial reasoning, focusing on
post-training techniques, explainability, and architecture. Beyond classical 2D
tasks, we examine spatial relationship reasoning, scene and layout
understanding, as well as visual question answering and grounding in 3D space.
We also review advances in embodied AI, including vision-language navigation
and action models. Additionally, we consider emerging modalities such as audio
and egocentric video, which contribute to novel spatial understanding through
new sensors. We believe this survey establishes a solid foundation and offers
insights into the growing field of multimodal spatial reasoning. Updated
information about this survey, codes and implementation of the open benchmarks
can be found at https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°å¯¹è¿ç¯è®ºææè¦è¿è¡åæã</p>
<hr />
<p><strong>è®ºæåæï¼Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks</strong></p>
<p><strong>1. è®ºæä¸»è¦è´¡ç®çç®æ´æ»ç» (2-3 å¥è¯)</strong></p>
<p>è¿ç¯è®ºæå¯¹å¤§åå¤æ¨¡ææ¨çæ¨¡åå¨ç©ºé´æ¨çé¢åçææ°è¿å±è¿è¡äºå¨é¢ç»¼è¿°ï¼å¹¶é¦æ¬¡ç³»ç»å°åç±»äºå¤æ¨¡æå¤§è¯­è¨æ¨¡åï¼MLLMsï¼å¨è¯¥é¢åçåºç¨ãå®ä¸ä»æ¶µçäºä»2Då°3Dç©ºé´æ¨ççåç§ä»»å¡ï¼è¿å¼å¥äºå¼æ¾åºåæ¥è¯ä¼°è¿äºæ¨¡åçæ§è½ï¼æ¨å¨ä¸ºå¤æ¨¡æç©ºé´æ¨çé¢åå¥ å®åå®åºç¡å¹¶æä¾æ·±å¥è§è§£ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®º</strong></p>
<p>è¯¥è®ºæçå³é®åæ°å¨äºå¶<strong>ç³»ç»æ§çç»¼è¿°åå¼æ¾åºåçå¼å¥</strong>ãå®ä¸ä»ä»æ¯ç®åå°ç½åç°æå·¥ä½ï¼èæ¯ï¼
*   <strong>å¨é¢åç±»åç»ç»</strong>äºå¤§åå¤æ¨¡ææ¨¡åå¨ç©ºé´æ¨çä»»å¡ä¸­çåºç¨ï¼æ¶µçäºåè®­ç»ææ¯ãå¯è§£éæ§åæ¶æç­éç¨æ¹é¢ã
*   <strong>æ©å±äºç©ºé´æ¨ççèç´</strong>ï¼ä»ç»å¸ç2Dä»»å¡æ·±å¥å°3Dç©ºé´ä¸­çç©ºé´å³ç³»æ¨çãåºæ¯åå¸å±çè§£ãè§è§é®ç­åå®ä½ã
*   <strong>æ´åäºæ°å´æ¨¡æååºç¨</strong>ï¼å¦å·èº«AIï¼è§è§-è¯­è¨å¯¼èªãå¨ä½æ¨¡åï¼ãé³é¢åç¬¬ä¸äººç§°è§è§è§é¢ï¼å¼ºè°äºæ°ä¼ æå¨å¯¹ç©ºé´çè§£çè´¡ç®ã
*   <strong>æä¾äºå¼æ¾åºå</strong>ï¼è¿å¯¹äºæ¨å¨è¯¥é¢åçç ç©¶åå¬å¹³è¯ä¼°ä¸åæ¨¡åçæ§è½è³å³éè¦ã</p>
<p><strong>3. å¯¹é¢åæ½å¨å½±å</strong></p>
<p>è¿ç¯è®ºæå¯¹è®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åå·ææ¾èçæ½å¨å½±åï¼
*   <strong>æ åååç»ä¸åï¼</strong> éè¿æä¾å¨é¢çç»¼è¿°åå¼æ¾åºåï¼å®æå©äºæ ååå¤æ¨¡æç©ºé´æ¨çä»»å¡çå®ä¹åè¯ä¼°æ¹æ³ï¼åå°ç ç©¶ç¢çåã
*   <strong>å éç ç©¶è¿å±ï¼</strong> ç ç©¶äººåå¯ä»¥å©ç¨å¶åç±»æ¡æ¶å¿«éäºè§£ç°æææ¯ï¼å¹¶å©ç¨å¼æ¾åºåè¿è¡æ¨¡åå¼ååæ¯è¾ï¼ä»èå éæ°ç®æ³åæ¨¡åçè¿­ä»£ã
*   <strong>å¯åæ°æ¹åï¼</strong> å¯¹æ°å´æ¨¡æï¼å¦é³é¢ãç¬¬ä¸äººç§°è§é¢ï¼åå·èº«AIçå³æ³¨ï¼å°é¼å±ç ç©¶äººåæ¢ç´¢æ´å¹¿æ³ãæ´å¤æçç©ºé´æ¨çåºæ¯ååºç¨ã
*   <strong>æè²ååèä»·å¼ï¼</strong> å¯¹äºåå­¦èåç»éªä¸°å¯çç ç©¶äººåæ¥è¯´ï¼å®å°æä¸ºä¸ä¸ªå®è´µçåèèµæï¼å¸®å©ä»ä»¬çè§£å¤æ¨¡æç©ºé´æ¨ççç°ç¶åæªæ¥è¶å¿ã</p>
<p><strong>4. ç¸å³é¢åæåºç¨åç</strong></p>
<p>ä»¥ä¸é¢åååºç¨å°ä»è¿é¡¹ç ç©¶ä¸­åçï¼
*   <strong>å·èº«AIåæºå¨äººå­¦ï¼</strong> è§è§-è¯­è¨å¯¼èªãæºå¨äººæä½ãäººæºäº¤äºç­éè¦æºå¨äººçè§£åæ¨çç©çç©ºé´çåºç¨å°ç´æ¥åçã
*   <strong>èªå¨é©¾é©¶ï¼</strong> è½¦è¾éè¦å¯¹å¨å´ç¯å¢è¿è¡å¤æ¨¡æï¼è§è§ãé·è¾¾ãæ¿åé·è¾¾ãå£°é³ï¼çç©ºé´æ¨çï¼ä»¥è¿è¡è·¯å¾è§åãéç¢ç©æ£æµåè¡ä¸ºé¢æµã
*   <strong>èæç°å®/å¢å¼ºç°å® (VR/AR)ï¼</strong> å¨èææå¢å¼ºç¯å¢ä¸­å®ç°é¼ççäº¤äºåæ²æµ¸å¼ä½éªï¼éè¦å¯¹ç¨æ·åç¯å¢è¿è¡ç²¾ç¡®çç©ºé´çè§£ã
*   <strong>æºè½å®¶å±åæºæ§åå¸ï¼</strong> éè¿å¤æ¨¡æä¼ æå¨ï¼æåå¤´ãéº¦åé£ï¼çè§£å®¤åå¸å±ãäººåæ´»å¨åç¯å¢ååï¼å®ç°æ´æºè½çæå¡ã
*   <strong>å¤åªä½åå®¹çè§£ï¼</strong> å¯¹è§é¢ãå¾ååé³é¢ç­å¤æ¨¡ææ°æ®è¿è¡æ´æ·±å±æ¬¡çè¯­ä¹åç©ºé´çè§£ï¼ä¾å¦äºä»¶æ£æµãåºæ¯æè¿°ååå®¹æ£ç´¢ã
*   <strong>å»çå½±ååæï¼</strong> ç»åä¸åæ¨¡æï¼å¦CTãMRIãè¶å£°ï¼è¿è¡çç¶å®ä½åä¸ç»´ç»æçè§£ã</p>
<p><strong>5. ä»æè¦ä¸­å¯æ¨æ­çå±éæ§</strong></p>
<p>å°½ç®¡æè¦å¼ºè°äºå¨é¢æ§åè´¡ç®ï¼ä½ä»å¯æ¨æ­åºä¸äºæ½å¨çå±éæ§ï¼
*   <strong>æ·±åº¦ä¸å¹¿åº¦çæè¡¡ï¼</strong> ä½ä¸ºä¸ä¸ªç»¼è¿°ï¼å®å¯è½æ æ³å¯¹æ¯ä¸ªå·ä½ä»»å¡ææ¨¡åæ¶æè¿è¡æå¶æ·±å¥çææ¯ç»èæ¢è®¨ï¼å¯è½æ´ä¾§éäºå¹¿åº¦ã
*   <strong>åºåçè¦çèå´ï¼</strong> æè¦æå°âå¼æ¾åºåâï¼ä½å¹¶æªå·ä½è¯´æè¿äºåºåçè§æ¨¡ãå¤æ ·æ§ä»¥åæ¯å¦æ¶µçäºæææ°å´æ¨¡æåå¤æä»»å¡ãåºåçè´¨éåå¨é¢æ§å°ç´æ¥å½±åå¶å½±ååã
*   <strong>æ¶ææ§ï¼</strong> è®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢ååå±è¿éï¼å°½ç®¡åå¸æ¥ææ¯2025å¹´10æï¼ä½ç»¼è¿°åå®¹å¨æ°åæ¶å¯è½å·²ç»é¢ä¸´ä¸äºææ°è¿å±çææãæè¦ä¸­æå°âUpdated information... can be found at GitHubâï¼è¿è¡¨æä½èä¹æè¯å°äºæ¶ææ§é®é¢ï¼å¹¶è¯å¾éè¿å¨çº¿æ´æ°æ¥ç¼è§£ã
*   <strong>å¯¹âå¤§åæ¨¡åâçå®ä¹ï¼</strong> æè¦ä¸­å¤æ¬¡æå°âå¤§åå¤æ¨¡ææ¨çæ¨¡åâåâMLLMsâï¼ä½å¹¶æªæç¡®çå®å¶è§æ¨¡æå·ä½ç±»åï¼ä¾å¦ï¼æ¯å¦ä»éäºTransformeræ¶æï¼æèæ¯å¦åå«å¶ä»ç±»åçæ·±åº¦å­¦ä¹ æ¨¡åï¼ã
*   <strong>å¯è§£éæ§ï¼Explainabilityï¼çæ·±åº¦ï¼</strong> æè¦ä¸­æå°äºå¯è§£éæ§ï¼ä½ç»¼è¿°å¨å¤å¤§ç¨åº¦ä¸è½æ·±å¥æ¢è®¨å¤§åæ¨¡åç©ºé´æ¨ççå¯è§£éæ§ææåè§£å³æ¹æ¡ï¼ä»æå¾è§å¯ãè¿æ¬èº«å°±æ¯ä¸ä¸ªå¤æçç ç©¶é¢åã</p>
<hr />
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæéè¿å¶å¨é¢çç»¼è¿°åå¼æ¾åºåçå¼å¥ï¼æææä¸ºå¤æ¨¡æç©ºé´æ¨çé¢åçéè¦éç¨ç¢ï¼ä¸ºæªæ¥çç ç©¶ååºç¨æä¾åå®çåºç¡åæ¸æ°çè·¯çº¿å¾ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Additionally, we consider emerging modalities such as audio
and egocentric video, which contribute to novel spatial understanding through
new sensors.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.25760v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.25760v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.26802v1'></a></p>
<h2 id="are-video-models-ready-as-zero-shot-reasoners-an-empirical-study-with-the-mme-cof-benchmark"><a href="https://arxiv.org/abs/2510.26802v1">Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark</a></h2>
<p><strong>Authors:</strong> Ziyu Guo, Xinyan Chen, Renrui Zhang, Ruichuan An, Yu Qi, Dongzhi Jiang, Xiangtai Li, Manyuan Zhang, Hongsheng Li, Pheng-Ann Heng</p>
<p><strong>Published:</strong> 2025-10-30</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.CL</p>
<p><strong>Abstract:</strong></p>
<p>Recent video generation models can produce high-fidelity, temporally coherent
videos, indicating that they may encode substantial world knowledge. Beyond
realistic synthesis, they also exhibit emerging behaviors indicative of visual
perception, modeling, and manipulation. Yet, an important question still
remains: Are video models ready to serve as zero-shot reasoners in challenging
visual reasoning scenarios? In this work, we conduct an empirical study to
comprehensively investigate this question, focusing on the leading and popular
Veo-3. We evaluate its reasoning behavior across 12 dimensions, including
spatial, geometric, physical, temporal, and embodied logic, systematically
characterizing both its strengths and failure modes. To standardize this study,
we curate the evaluation data into MME-CoF, a compact benchmark that enables
in-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our
findings reveal that while current video models demonstrate promising reasoning
patterns on short-horizon spatial coherence, fine-grained grounding, and
locally consistent dynamics, they remain limited in long-horizon causal
reasoning, strict geometric constraints, and abstract logic. Overall, they are
not yet reliable as standalone zero-shot reasoners, but exhibit encouraging
signs as complementary visual engines alongside dedicated reasoning models.
Project page: https://video-cof.github.io</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Ziyu Guoç­äººæ°åçè®ºæâAre Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmarkâçå¨é¢æè¦ã</p>
<hr />
<h3 id="mme-cof">è®ºææè¦ï¼è§é¢æ¨¡åæ¯å¦å·²åå¤å¥½æä¸ºé¶æ ·æ¬æ¨çå¨ï¼ä¸é¡¹åºäºMME-CoFåºåçå®è¯ç ç©¶</h3>
<p><strong>1. æ ¸å¿é®é¢/ç ç©¶é®é¢ï¼</strong>
è¯¥ç ç©¶æ¨å¨è§£å³ä¸ä¸ªå³é®é®é¢ï¼å½åçè§é¢çææ¨¡åæ¯å¦å·²åå¤å¥½å¨å·ææææ§çè§è§æ¨çåºæ¯ä¸­åå½é¶æ ·æ¬æ¨çå¨ï¼å°½ç®¡æè¿çè§é¢æ¨¡åè½å¤çæé«ä¿çãæ¶é´è¿è´¯çè§é¢ï¼å¹¶å±ç°åºè§è§æç¥ãå»ºæ¨¡åæä½çåæ­¥è½åï¼ä½å®ä»¬æ¯å¦çæ­£å·å¤å¼ºå¤§çé¶æ ·æ¬æ¨çè½åä»ä¸æç¡®ã</p>
<p><strong>2. ä¸»è¦åæ°ææ¹æ³å­¦è´¡ç®ï¼</strong>
*   <strong>é¦æ¬¡å®è¯ç ç©¶ï¼</strong> æ¬æé¦æ¬¡å¯¹é¢åçè§é¢æ¨¡åï¼ç¹å«æ¯Veo-3ï¼è¿è¡äºå¨é¢çå®è¯ç ç©¶ï¼ä»¥æ·±å¥æ¢ç©¶å¶ä½ä¸ºé¶æ ·æ¬æ¨çå¨çæ½åã
*   <strong>MME-CoFåºåçåå»ºï¼</strong> ä¸ºäºæ ååè¯ä¼°ï¼ä½èå¢éç²¾å¿ç­åå¹¶å¼å¥äºMME-CoFï¼Chain-of-Frameï¼åºåãè¿æ¯ä¸ä¸ªç´§åçåºåï¼åå«12ä¸ªæ¨çç»´åº¦ï¼åæ¬ç©ºé´ãå ä½ãç©çãæ¶é´ãå·èº«é»è¾ç­ï¼ï¼æ¨å¨å¯¹è§é¢æ¨¡åçCoFï¼Chain-of-Frameï¼æ¨çè½åè¿è¡æ·±å¥åå¨é¢çè¯ä¼°ãCoFæ¨çåé´äºå¤§åè¯­è¨æ¨¡åä¸­çæç»´é¾ï¼CoTï¼æ¦å¿µï¼å¼ºè°è§é¢æ¨¡åéè¿çæä¸ç³»åå¸§æ¥éæ­¥è§£å³é®é¢çè½åã
*   <strong>ç³»ç»æ§è¯ä¼°æ¡æ¶ï¼</strong> ç ç©¶ç³»ç»å°è¯ä¼°äºæ¨¡åçæ¨çè¡ä¸ºï¼ä¸ä»è¯å«äºå¶ä¼å¿ï¼ä¹æ­ç¤ºäºå¶å¤±è´¥æ¨¡å¼ãè¯ä¼°éç¨å®æ§ï¼å¥½ãä¸­ãå·®ï¼åå®éï¼æåçï¼ç¸ç»åçæ¹å¼ï¼å¹¶å¯¹æç¤ºè®¾è®¡è¿è¡äºæ ååï¼ä»¥ç¡®ä¿å¬å¹³æ§åä¸è´æ§ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>å±é¨æ¨çè½åçªåºï¼</strong> ç ç©¶åç°ï¼å½åè§é¢æ¨¡åå¨ç­æ¶ç©ºè¿è´¯æ§ãç»ç²åº¦å®ä½åå±é¨ä¸è´æ§å¨ææ¹é¢å±ç°åºæåæ¯çæ¨çæ¨¡å¼ãè¿è¡¨ææ¨¡åè½å¤ææååç°è§è§ä¸çä¸­çä¸äºåºæ¬ç»æåå¨æã
*   <strong>é¿ç¨æ¨çè½ååéï¼</strong> ç¶èï¼æ¨¡åå¨é¿ç¨å ææ¨çãä¸¥æ ¼å ä½çº¦æåæ½è±¡é»è¾æ¹é¢ä»ç¶å­å¨å±éæ§ãå®ä»¬å¨å¤çå¤ææ¨çæ¡ä»¶æ¶è¡¨ç°ä¸ä½³ï¼ä¾å¦å¨æ©æ¦ãåé©±å¨æåçº¦æçäº¤äºä¸æ æ³ä¿æå®éçç©ççº¦æåå æä¿çåº¦ã
*   <strong>éç¬ç«é¶æ ·æ¬æ¨çå¨ï¼</strong> æ»ä½èè¨ï¼ç ç©¶å¾åºç»è®ºï¼å½åè§é¢æ¨¡åå°æªå¯é å°æä¸ºç¬ç«çé¶æ ·æ¬æ¨çå¨ãå®ä»¬çè¡ä¸ºæ´å¤å°ç±å­¦ä¹ å°çè¡¨é¢çº§æ¨¡å¼èéåå¨åçéç¨ååé©±å¨ã
*   <strong>ä½ä¸ºè¾å©è§è§å¼æçæ½åï¼</strong> å°½ç®¡å­å¨å±éæ§ï¼ä½æ¨¡åå±ç°åºçæ°å´æ¨çè¿¹è±¡ä»¤äººé¼èï¼è¡¨æå®ä»¬ä½ä¸ºä¸ç¨æ¨çæ¨¡åä¹å¤çè¡¥åè§è§å¼æå·æå·¨å¤§æ½åã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>é¿ç¨å ææ¨çä¸è¶³ï¼</strong> æ¨¡åå¨éè¦é¿ç¨å æé¾æå¤æ­¥éª¤è§åçä»»å¡ä¸­è¡¨ç°ä¸ä½³ã
*   <strong>å ä½åæ½è±¡é»è¾çå¼±ç¹ï¼</strong> å¨ä¸¥æ ¼å ä½çº¦æåæ½è±¡é»è¾ä»»å¡ä¸­ï¼æ¨¡åé¾ä»¥ä¿æä¸è´æ§ï¼ç»å¸¸äº§çä¸åç¡®æä¸åé»è¾çç»æã
*   <strong>å¯¹ä¸ä¸ç¥è¯çç¼ºä¹ï¼</strong> å¨æ¶åä¸ä¸é¢åï¼å¦å»å­¦æ¨çï¼çä»»å¡ä¸­ï¼æ¨¡åå ç¼ºä¹é¢åçè§£èè¡¨ç°åºæ¾èçå±éæ§ï¼å¯¼è´å¾åå¤±çææ æ³åç¡®è¯å«ç®æ ã
*   <strong>æ¨¡å¼é©±å¨èéååé©±å¨ï¼</strong> æ¨¡åçæ¨çè¡ä¸ºä¼¼ä¹æ´å¤å°æ¯æ¨¡å¼é©±å¨èéååé©±å¨ï¼å¾åäºè§è§ä¸çåçæ§èéç²¾ç¡®çç©ºé´æ¨çæä¸¥æ ¼éµå®å ä½æä»¤ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>å¢å¼ºé¿ç¨å ææ¨çï¼</strong> æªæ¥çç ç©¶å¯ä»¥æ¢ç´¢å¦ä½æé«è§é¢æ¨¡åå¨é¿ç¨å ææ¨çåå¤æ­¥éª¤è§åä»»å¡ä¸­çè½åã
*   <strong>æåå ä½åæ½è±¡é»è¾çè§£ï¼</strong> éå¯¹å ä½çº¦æåæ½è±¡é»è¾çå¼±ç¹è¿è¡æ¹è¿ï¼å¯è½æ¶åæ´æ·±å±æ¬¡çç»æåç¥è¯å­¦ä¹ ã
*   <strong>æ´åé¢åä¸ä¸ç¥è¯ï¼</strong> æ¢ç´¢å¦ä½å°ç¹å®é¢åçä¸ä¸ç¥è¯ï¼å¦å»å­¦æ¯è¯­åæ¦å¿µï¼æ´åå°è§é¢æ¨¡åä¸­ï¼ä»¥æé«å¶å¨ä¸ä¸ä»»å¡ä¸­çè¡¨ç°ã
*   <strong>ç»åä¸ç¨æ¨çæ¨¡åï¼</strong> é´äºè§é¢æ¨¡åä½ä¸ºè¡¥åè§è§å¼æçæ½åï¼æªæ¥çå·¥ä½å¯ä»¥ç ç©¶å¦ä½ææå°å°è§é¢æ¨¡åä¸ä¸ç¨æ¨çæ¨¡åç»åï¼ä»¥å®ç°æ´å¼ºå¤§ãæ´å¯é çè§è§æ¨çç³»ç»ã</p>
<hr />
<p>è¿ç¯è®ºæéè¿MME-CoFåºåå¯¹è§é¢æ¨¡åçé¶æ ·æ¬æ¨çè½åè¿è¡äºæ·±å¥èç³»ç»çè¯ä¼°ï¼æ­ç¤ºäºå¶å¨å±é¨è§è§çè§£æ¹é¢çä¼å¿åå¨å¤æãé¿ç¨æ¨çæ¹é¢çå±éæ§ãå¶ç»è®ºä¸ºæªæ¥è§é¢çææ¨¡åçåå±ææäºæ¹åï¼å³å¨è¿½æ±é«ä¿ççæçåæ¶ï¼ä¹åºçéæåå¶æ·±å±æ¬¡çæ¨çè½åï¼å¹¶æ¢ç´¢å¶ä½ä¸ºè¾å©è§è§å¼æçæ½åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Overall, they are
not yet reliable as standalone zero-shot reasoners, but exhibit encouraging
signs as complementary visual engines alongside dedicated reasoning models.</li>
<li>Project page: https://video-cof.github.io</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.26802v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.26802v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.26794v1'></a></p>
<h2 id="the-quest-for-generalizable-motion-generation-data-model-and-evaluation"><a href="https://arxiv.org/abs/2510.26794v1">The Quest for Generalizable Motion Generation: Data, Model, and Evaluation</a></h2>
<p><strong>Authors:</strong> Jing Lin, Ruisi Wang, Junzhe Lu, Ziqi Huang, Guorui Song, Ailing Zeng, Xian Liu, Chen Wei, Wanqi Yin, Qingping Sun, Zhongang Cai, Lei Yang, Ziwei Liu</p>
<p><strong>Published:</strong> 2025-10-30</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Despite recent advances in 3D human motion generation (MoGen) on standard
benchmarks, existing models still face a fundamental bottleneck in their
generalization capability. In contrast, adjacent generative fields, most
notably video generation (ViGen), have demonstrated remarkable generalization
in modeling human behaviors, highlighting transferable insights that MoGen can
leverage. Motivated by this observation, we present a comprehensive framework
that systematically transfers knowledge from ViGen to MoGen across three key
pillars: data, modeling, and evaluation. First, we introduce ViMoGen-228K, a
large-scale dataset comprising 228,000 high-quality motion samples that
integrates high-fidelity optical MoCap data with semantically annotated motions
from web videos and synthesized samples generated by state-of-the-art ViGen
models. The dataset includes both text-motion pairs and text-video-motion
triplets, substantially expanding semantic diversity. Second, we propose
ViMoGen, a flow-matching-based diffusion transformer that unifies priors from
MoCap data and ViGen models through gated multimodal conditioning. To enhance
efficiency, we further develop ViMoGen-light, a distilled variant that
eliminates video generation dependencies while preserving strong
generalization. Finally, we present MBench, a hierarchical benchmark designed
for fine-grained evaluation across motion quality, prompt fidelity, and
generalization ability. Extensive experiments show that our framework
significantly outperforms existing approaches in both automatic and human
evaluations. The code, data, and benchmark will be made publicly available.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºâThe Quest for Generalizable Motion Generation: Data, Model, and Evaluationâè®ºæçå¨é¢æè¦ï¼ç±Jing Linç­äººæ°åï¼</p>
<p><strong>è®ºææè¦ï¼éç¨è¿å¨çæï¼æ°æ®ãæ¨¡åä¸è¯ä¼°</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
å°½ç®¡3Däººä½è¿å¨çæï¼MoGenï¼å¨æ ååºåæµè¯ä¸åå¾äºè¿å±ï¼ä½ç°ææ¨¡åå¨æ³åè½åæ¹é¢é¢ä¸´æ ¹æ¬æ§ç¶é¢ãä¸æ­¤å½¢æå¯¹æ¯çæ¯ï¼è§é¢çæï¼ViGenï¼ç­ç¸é»çæé¢åå¨å»ºæ¨¡äººç±»è¡ä¸ºæ¹é¢å±ç°åºåè¶çæ³åè½åãæ¬ç ç©¶æ¨å¨è§£å³MoGençæ³åè½åä¸è¶³é®é¢ï¼å¹¶ç³»ç»å°å°ViGençç¥è¯è½¬ç§»å°MoGené¢åã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥è®ºææåºäºä¸ä¸ªå¨é¢çæ¡æ¶ï¼éè¿æ°æ®ãæ¨¡ååè¯ä¼°ä¸ä¸ªå³é®æ¯æ±ï¼ç³»ç»å°å°ViGençç¥è¯è½¬ç§»å°MoGenï¼å¹¶å¼å¥äºä»¥ä¸åæ°ï¼</p>
<ul>
<li><strong>ViMoGen-228Kæ°æ®éï¼</strong> è¿æ¯ä¸ä¸ªå¤§è§æ¨¡æ°æ®éï¼åå«228,000ä¸ªé«è´¨éè¿å¨æ ·æ¬ãå®æ´åäºé«ä¿çåå­¦MoCapæ°æ®ãæ¥èªç½ç»è§é¢çè¯­ä¹æ æ³¨è¿å¨ä»¥åæåè¿ViGenæ¨¡åçæçåææ ·æ¬ãè¯¥æ°æ®éåå«ææ¬-è¿å¨å¯¹åææ¬-è§é¢-è¿å¨ä¸åç»ï¼æ¾èæ©å±äºè¯­ä¹å¤æ ·æ§ã</li>
<li><strong>ViMoGenæ¨¡åï¼</strong> è¿æ¯ä¸ä¸ªåºäºæµå¹éçæ©æ£Transformeræ¨¡åï¼éè¿é¨æ§å¤æ¨¡ææ¡ä»¶ä½ç¨ï¼ç»ä¸äºMoCapæ°æ®åViGenæ¨¡åçåéªç¥è¯ã</li>
<li><strong>ViMoGen-lightæ¨¡åï¼</strong> ä¸ºäºæé«æçï¼è¯¥è®ºæè¿ä¸æ­¥å¼åäºViMoGençç²¾ç®çæ¬ï¼å®æ¶é¤äºè§é¢çæä¾èµæ§ï¼åæ¶ä¿æäºå¼ºå¤§çæ³åè½åã</li>
<li><strong>MBenchåºåï¼</strong> è¿æ¯ä¸ä¸ªåå±åºåï¼æ¨å¨å¯¹è¿å¨è´¨éãæç¤ºå¿ å®åº¦åæ³åè½åè¿è¡ç»ç²åº¦è¯ä¼°ãå®è§£å³äºç°æè¯ä¼°å¥ä»¶ç¼ºä¹ç»ä¸æ§ãæ æ³ç³»ç»è¯ä¼°æ¨¡åãè¡¡éçæè¿å¨çåä¸ªæ¹é¢ä»¥åä¸ºç ç©¶äººåæä¾å¯æä½åé¦çé®é¢ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
å¹¿æ³çå®éªè¡¨æï¼è¯¥æ¡æ¶å¨èªå¨åäººå·¥è¯ä¼°ä¸­åæ¾èä¼äºç°ææ¹æ³ã</p>
<ul>
<li><strong>ViMoGençä¼è¶æ³åè½åï¼</strong> å¨æ¨¡åViMoGenå¨è¿å¨æ¡ä»¶ä¸è´æ§åæ³åè½åç­å³é®è¯­ä¹ææ ä¸æ¾èä¼äºææåºçº¿ï¼å±ç¤ºäºå©ç¨T2Væ¨¡åä¸°å¯è¯­ä¹åéªçå¼ºå¤§ä¼å¿ã</li>
<li><strong>ViMoGen-lightçæçä¸æ³åå¹³è¡¡ï¼</strong> è¸é¦åçViMoGen-lightåä½å¨æ³ååæ°ä¸ä¸æå¼ºçåºçº¿æå¹³ï¼è¯æäºè¿äºç¥è¯å¯ä»¥ææå°è½¬ç§»å°ä¸éè¦è§é¢çææ¨ççæçæ¨¡åä¸­ã</li>
<li><strong>MBenchçå¯é æ§ï¼</strong> MBenchçåé¡¹ææ ä¸äººç±»åå¥½é«åº¦ä¸è´ï¼è¡¨æææåºçèªå¨ææ çå¯é æ§ã</li>
<li><strong>æ°æ®å¤æ ·æ§çéè¦æ§ï¼</strong> éæ­¥æ·»å æ¥èªä¸åæ¥æºçæ°æ®ï¼ç¹å«æ¯åæè§é¢æ°æ®ï¼æ¾èæé«äºæ¨¡åçå¨ä½åç¡®æ§åæ³åè½åï¼å³ä½¿æ¯å°è§æ¨¡æ°æ®éçè¯­ä¹å¤æ ·æ§ä¹å¯¹æ³åäº§çäºéå¤§å½±åã</li>
<li><strong>ææ¬ç¼ç å¨åæç¤ºé£æ ¼çå½±åï¼</strong> T5-XXLåå¤æ¨¡æå¤§è¯­è¨æ¨¡åï¼MLLMï¼å¨æ³åè½åæ¹é¢æ¾èä¼äºCLIPãä½¿ç¨æè¿°æ§è§é¢é£æ ¼ææ¬è¿è¡è®­ç»ï¼å¹¶å¨ç®æ´è¿å¨é£æ ¼ææ¬ä¸è¿è¡æµè¯ï¼åå¾äºæä½³çæ´ä½æ§è½ï¼è¡¨æä¸°å¯çæè¿°ä½ä¸ºææçæ°æ®å¢å¼ºï¼æé«äºæ¨¡åçé²æ£æ§åä¸é¢è®­ç»ææ¬ç¼ç å¨ææçå¯¹é½ã</li>
</ul>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
è®ºæä¸­æ²¡ææç¡®ååºæ¬ç ç©¶çå±éæ§ãç¶èï¼ä»å¶å¯¹ç°æMoGenæ¨¡åæ³åè½åä¸è¶³çè®¨è®ºä¸­ï¼å¯ä»¥æ¨æ­åºä»¥ä¸å ç¹ï¼</p>
<ul>
<li><strong>ç°æMoGenæ¨¡åçæ³åç¶é¢ï¼</strong> ç°ææ¨¡åå¨å¤çå¤æ ·ååé¿å°¾æä»¤æ¶è¡¨ç°ä¸ä½³ï¼è¿æ­£æ¯æ¬ç ç©¶è¯å¾è§£å³çæ ¸å¿é®é¢ã</li>
<li><strong>MoCapæ°æ®çå±éæ§ï¼</strong> åå­¦MoCapæ°æ®éè½ç¶æä¾é«ç²¾åº¦è¿å¨ä¿¡å·ï¼ä½è§æ¨¡æéï¼è¯­ä¹å¤æ ·æ§ä¸è¶³ï¼ä¸æ¶éææ¬é«æã</li>
<li><strong>è§é¢çææ¨¡åå¨è¿å¨è´¨éä¸çä¸è¶³ï¼</strong> æ©æå°è¯å©ç¨è§é¢çææ¨¡åç¥è¯çæ¹æ³ï¼éå¸¸ç¼ºä¹é²æ£çè¿å¨è´¨éï¼å¹¶ä¸è®¡ç®ææ¬é«ãæ¨çéåº¦æ¢ã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºææç¡®æåºï¼æ¬ç ç©¶ä¸ºæªæ¥æ¢ç´¢éç¨è¿å¨åºç¡æ¨¡åå¥ å®äºåºç¡ãè¿æç¤ºäºä»¥ä¸æ½å¨çæªæ¥ç ç©¶æ¹åï¼</p>
<ul>
<li><strong>éç¨è¿å¨åºç¡æ¨¡åï¼</strong> ç»§ç»­æ¨è¿éç¨è¿å¨åºç¡æ¨¡åçå¼åï¼è¯¥æ¨¡åè½å¤å¤çåç§è¿å¨ä»»å¡ååºæ¯ã</li>
<li><strong>å¤æ¨¡æèåçè¿ä¸æ­¥ä¼åï¼</strong> æ¢ç´¢æ´é«æãæ´éæçæ¹å¼æ¥èåæ¥èªä¸åæ¨¡æï¼å¦MoCapãè§é¢ãææ¬ï¼çç¥è¯ï¼ä»¥è¿ä¸æ­¥æé«è¿å¨çææ¨¡åçæ§è½åæ³åè½åã</li>
<li><strong>æ°æ®å¢å¼ºååæï¼</strong> è¿ä¸æ­¥ç ç©¶å¦ä½å©ç¨åææ°æ®ååè¿çææ¬ç¼ç å¨æ¥æ©å±è¯­ä¹è¦çèå´ï¼ç¹å«æ¯å¨ç°å®ä¸çæ°æ®ä¸­é¾ä»¥æææä»£è¡¨æ§ä¸è¶³çé¢åã</li>
<li><strong>è¯ä¼°åºåçæç»­åå±ï¼</strong> éçæ¨¡åè½åçæåï¼MBenchç­è¯ä¼°åºåéè¦ä¸æ­æ¼è¿ï¼ä»¥éåºæ°çææåè¯ä¼°éæ±ï¼ç¡®ä¿å¯¹æ¨¡åæ§è½çåç¡®åç»è´è¯ä¼°ã</li>
<li><strong>æçä¸æ§è½çå¹³è¡¡ï¼</strong> æç»­ä¼åæ¨¡åæ¶æï¼å¦ViMoGen-lightæç¤ºï¼å¨ä¿æå¼ºå¤§æ³åè½åçåæ¶ï¼è¿ä¸æ­¥éä½è®¡ç®å¼éã</li>
</ul>
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæéè¿å¼å¥å¤§è§æ¨¡æ°æ®éãåæ°çæ¨¡åæ¶æåå¨é¢çè¯ä¼°åºåï¼ä¸ºè§£å³3Däººä½è¿å¨çæé¢åçæ³åè½åç¶é¢ååºäºéè¦è´¡ç®ãå®æåå°å°è§é¢çæé¢åçç¥è¯è½¬ç§»å°è¿å¨çæä¸­ï¼ä¸ºæªæ¥éç¨è¿å¨åºç¡æ¨¡åçåå±éºå¹³äºéè·¯ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Motivated by this observation, we present a comprehensive framework
that systematically transfers knowledge from ViGen to MoGen across three key
pillars: data, modeling, and evaluation.</li>
<li>First, we introduce ViMoGen-228K, a
large-scale dataset comprising 228,000 high-quality motion samples that
integrates high-fidelity optical MoCap data with semantically annotated motions
from web videos and synthesized samples generated by state-of-the-art ViGen
models.</li>
<li>Second, we propose
ViMoGen, a flow-matching-based diffusion transformer that unifies priors from
MoCap data and ViGen models through gated multimodal conditioning.</li>
<li>Finally, we present MBench, a hierarchical benchmark designed
for fine-grained evaluation across motion quality, prompt fidelity, and
generalization ability.</li>
<li>Extensive experiments show that our framework
significantly outperforms existing approaches in both automatic and human
evaluations.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.26794v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.26794v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.26786v1'></a></p>
<h2 id="heir-learning-graph-based-motion-hierarchies"><a href="https://arxiv.org/abs/2510.26786v1">HEIR: Learning Graph-Based Motion Hierarchies</a></h2>
<p><strong>Authors:</strong> Cheng Zheng, William Koch, Baiang Li, Felix Heide</p>
<p><strong>Published:</strong> 2025-10-30</p>
<p><strong>Categories:</strong> cs.CV, cs.GR, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Hierarchical structures of motion exist across research fields, including
computer vision, graphics, and robotics, where complex dynamics typically arise
from coordinated interactions among simpler motion components. Existing methods
to model such dynamics typically rely on manually-defined or heuristic
hierarchies with fixed motion primitives, limiting their generalizability
across different tasks. In this work, we propose a general hierarchical motion
modeling method that learns structured, interpretable motion relationships
directly from data. Our method represents observed motions using graph-based
hierarchies, explicitly decomposing global absolute motions into
parent-inherited patterns and local motion residuals. We formulate hierarchy
inference as a differentiable graph learning problem, where vertices represent
elemental motions and directed edges capture learned parent-child dependencies
through graph neural networks. We evaluate our hierarchical reconstruction
approach on three examples: 1D translational motion, 2D rotational motion, and
dynamic 3D scene deformation via Gaussian splatting. Experimental results show
that our method reconstructs the intrinsic motion hierarchy in 1D and 2D cases,
and produces more realistic and interpretable deformations compared to the
baseline on dynamic 3D Gaussian splatting scenes. By providing an adaptable,
data-driven hierarchical modeling paradigm, our method offers a formulation
applicable to a broad range of motion-centric tasks. Project Page:
https://light.princeton.edu/HEIR/</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Cheng Zheng, William Koch, Baiang Li, Felix Heideæ°åçè®ºæâHEIR: Learning Graph-Based Motion Hierarchiesâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼HEIR: å­¦ä¹ åºäºå¾çè¿å¨å±æ¬¡ç»æ</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³ç°æè¿å¨å»ºæ¨¡æ¹æ³å¨å¤çå¤æå¨æç³»ç»ä¸­çå±éæ§ãè¿äºç³»ç»éå¸¸ç±ç®åè¿å¨ç»ä»¶çåè°äº¤äºäº§çï¼å½¢æå±æ¬¡ç»æãä¼ ç»æ¹æ³å¾å¾ä¾èµäºæå¨å®ä¹æå¯åå¼çåºå®è¿å¨åè¯­å±æ¬¡ç»æï¼è¿éå¶äºå®ä»¬å¨ä¸åä»»å¡ä¸­çæ³åè½åãå æ­¤ï¼æ ¸å¿é®é¢æ¯å¦ä½ä»æ°æ®ä¸­ç´æ¥å­¦ä¹ ç»æåãå¯è§£éçè¿å¨å³ç³»ï¼ä»¥å®ç°æ´éç¨åéåºæ§å¼ºçè¿å¨å»ºæ¨¡ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è´¡ç®ï¼</strong>
HEIRï¼Hierarchical Motion Learningï¼æ¹æ³æåºäºä»¥ä¸å³é®åæ°ï¼
*   <strong>æ°æ®é©±å¨çå±æ¬¡ç»æå­¦ä¹ ï¼</strong> è®ºææåºäºä¸ç§éç¨çåå±è¿å¨å»ºæ¨¡æ¹æ³ï¼è½å¤ç´æ¥ä»æ°æ®ä¸­å­¦ä¹ ç»æåãå¯è§£éçè¿å¨å³ç³»ï¼èéä¾èµé¢å®ä¹æå¯åå¼è§åã
*   <strong>åºäºå¾çå±æ¬¡è¡¨ç¤ºï¼</strong> è¯¥æ¹æ³ä½¿ç¨åºäºå¾çå±æ¬¡ç»ææ¥è¡¨ç¤ºè§æµå°çè¿å¨ãè¿å¨åç´ è¢«å»ºæ¨¡ä¸ºå¾çé¡¶ç¹ï¼èå­¦ä¹ å°çç¶å­ä¾èµå³ç³»åéè¿æåè¾¹æè·ï¼è¿äºè¾¹ç±å¾ç¥ç»ç½ç»ï¼GNNï¼æ¨æ­ã
*   <strong>è¿å¨åè§£ï¼</strong> å¨å±ç»å¯¹è¿å¨è¢«æç¡®å°åè§£ä¸ºç¶çº§ç»§æ¿æ¨¡å¼åå±é¨è¿å¨æ®å·®ãè¿ç§åè§£ä½¿å¾æ¨¡åè½å¤åºåä¸åå±æ¬¡çè¿å¨è´¡ç®ã
*   <strong>å¯å¾®åå¾å­¦ä¹ ï¼</strong> å±æ¬¡ç»ææ¨æ­è¢«å¬å¼åä¸ºä¸ä¸ªå¯å¾®åçå¾å­¦ä¹ é®é¢ï¼åè®¸ç«¯å°ç«¯å°ä¼åå¾ç»æåè¿å¨åæ°ãéè¿Gumbel-Softmaxæå·§ï¼å®ç°äºç¦»æ£å±æ¬¡ç»æéæ ·çå¯å¾®åæ§ã
*   <strong>æè½¬ç»§æ¿è½åï¼</strong> è¯¥æ¹æ³éè¿å°ç¼ç å¨ä¿®æ¹ä¸ºå¨æåæ ç³»èéç¬å¡å°åæ ç³»ä¸­é¢æµç¸å¯¹éåº¦ï¼ä»èè½å¤å¤çæè½¬è¿å¨çå±æ¬¡ç»æã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
è¯¥æ¹æ³å¨ä¸ä¸ªå·ææææ§çä»»å¡ä¸è¿è¡äºè¯ä¼°ï¼
*   <strong>1D å¹³ç§»è¿å¨å2D æè½¬è¿å¨ï¼</strong> å¨è¿äºåæåºåæµè¯ä¸­ï¼HEIRæåå°éå»ºäºåå¨çè¿å¨å±æ¬¡ç»æï¼éªè¯äºå¶å¨è¯å«ååè§£åºæ¬è¿å¨æ¨¡å¼æ¹é¢çè½åã
*   <strong>å¨æ3Dé«æ¯æ³¼æºåºæ¯åå½¢ï¼</strong> å¨å¨æ3Dåºæ¯åå½¢ä»»å¡ä¸­ï¼ä¸ç°æåºçº¿æ¹æ³ï¼å¦SC-GSï¼ç¸æ¯ï¼HEIRçæäºæ´çå®ãæ´å¯è§£éçåå½¢ãå®è½æ´å¥½å°ä¿æåºæ¯çç»æå®æ´æ§åç©çä¸è´æ§ï¼é¿åäºä¸èªç¶çæ­æ²åéä½ã
*   <strong>æ³åæ§åéåºæ§ï¼</strong> å®éªç»æè¡¨æï¼HEIRæä¾äºä¸ä¸ªéåºæ§å¼ºãæ°æ®é©±å¨çå±æ¬¡å»ºæ¨¡èå¼ï¼éç¨äºå¹¿æ³çä»¥è¿å¨ä¸ºä¸­å¿çåºç¨ã</p>
<p>è¿äºç»æçæä¹å¨äºï¼HEIRåæäºä¼ ç»æ¹æ³å¨æ³åæ§åå¯è§£éæ§æ¹é¢çéå¶ï¼ä¸ºå¤æå¨æç³»ç»çè¿å¨å»ºæ¨¡æä¾äºä¸ä¸ªæ´å¼ºå¤§ãæ´çµæ´»çæ¡æ¶ã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
è®ºæä¹å¦è¯å°æåºäºå½åæ¹æ³çå±éæ§ï¼
*   <strong>æ°æ®ä¾èµæ§ï¼</strong> è¯¥æ¹æ³æè·çå±æ¬¡è¿å¨ç»æåéäºè¾å¥æ°æ®çå­å¨æ§åå¯è§æµæ§ãå¦æè¿å¨è½¨è¿¹ä¸­æªåæ åºæ½å¨æä»»å¡é©±å¨çè¯­ä¹ï¼ä¾å¦ï¼å¦æä¸ä¸ªç©ä½é¨ä»¶å¨è®­ç»æ°æ®ä¸­ä¿æéæ­¢ï¼ï¼åæ æ³æ¨æ­åºå¶è¿å¨ã
*   <strong>åä¸ç¶çº§åè®¾ï¼</strong> å½åå¬å¼åè®¾æ¯ä¸ªè¿å¨åç´ åªæä¸ä¸ªç¶çº§ï¼è¿å¯è½éå¶äºå¨å·æéå æå¤æºè¿å¨å½±åçç³»ç»ä¸­è¡¨è¾¾è½åã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
åºäºä¸è¿°å±éæ§ï¼è®ºææåºäºä»¥ä¸æªæ¥ç ç©¶æ¹åï¼
*   <strong>æ¹è¿é¿ç¨ä¾èµæ£æµï¼</strong> å¯ä»¥ç¨ç¨çéæ ·çå¨å±æ³¨æåå±ãè¨èåå¾é»å±æå°ééæºåå§åçé¿ç¨è¾¹æ¥æ¿ä»£k-NNï¼ä»¥å¢å¼ºé¿ç¨ä¾èµæ£æµè½åã
*   <strong>éæå±é¨åæ§ï¼</strong> å­¦ä¹ å°çæ¾å¼å±æ¬¡ç»æåè®¸å¨åå½¢è¿ç¨ä¸­éæ©æ§å°æ·»å å±é¨åæ§ï¼ä»¥è¿ä¸æ­¥é¿åä¸å¿è¦çåå½¢ä¼ªå½±ã
*   <strong>è·¨é¢åæ¢ç´¢ï¼</strong> é¼å±è¿ä¸æ­¥æ¢ç´¢å¯å­¦ä¹ çå±æ¬¡è¡¨ç¤ºå¨ä¸åé¢åçåºç¨ã</p>
<hr />
<p>æ»èè¨ä¹ï¼HEIRè®ºæéè¿å¼å¥ä¸ç§æ°é¢çãæ°æ®é©±å¨çãåºäºå¾çå±æ¬¡è¿å¨å»ºæ¨¡æ¹æ³ï¼æ¾èæ¨å¨äºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åå¨çè§£åæ§å¶å¤æå¨æç³»ç»æ¹é¢çè¿å±ãå¶æ ¸å¿è´¡ç®å¨äºè½å¤ä»æ°æ®ä¸­èªå¨å­¦ä¹ å¯è§£éçè¿å¨å±æ¬¡ç»æï¼å¹¶å°å¶åºç¨äºåç§è¿å¨ç¸å³ä»»å¡ï¼å°¤å¶æ¯å¨å¨æ3Dåºæ¯åå½¢ä¸­å±ç°åºåè¶çæ§è½ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we propose a general hierarchical motion
modeling method that learns structured, interpretable motion relationships
directly from data.</li>
<li>Our method represents observed motions using graph-based
hierarchies, explicitly decomposing global absolute motions into
parent-inherited patterns and local motion residuals.</li>
<li>Experimental results show
that our method reconstructs the intrinsic motion hierarchy in 1D and 2D cases,
and produces more realistic and interpretable deformations compared to the
baseline on dynamic 3D Gaussian splatting scenes.</li>
<li>By providing an adaptable,
data-driven hierarchical modeling paradigm, our method offers a formulation
applicable to a broad range of motion-centric tasks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.26786v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.26786v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.26241v1'></a></p>
<h2 id="which-way-does-time-flow-a-psychophysics-grounded-evaluation-for-vision-language-models"><a href="https://arxiv.org/abs/2510.26241v1">Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for Vision-Language Models</a></h2>
<p><strong>Authors:</strong> Shiho Matta, Lis Kanashiro Pereira, Peitao Han, Fei Cheng, Shigeru Kitazawa</p>
<p><strong>Published:</strong> 2025-10-30</p>
<p><strong>Categories:</strong> cs.CV, cs.CL</p>
<p><strong>Abstract:</strong></p>
<p>Modern vision-language models (VLMs) excel at many multimodal tasks, yet
their grasp of temporal information in video remains weak and, crucially,
under-evaluated. We probe this gap with a deceptively simple but revealing
challenge: judging the arrow of time (AoT)-whether a short clip is played
forward or backward. We introduce AoT-PsyPhyBENCH, a psychophysically validated
benchmark that tests whether VLMs can infer temporal direction in natural
videos using the same stimuli and behavioral baselines established for humans.
Our comprehensive evaluation of open-weight and proprietary, reasoning and
non-reasoning VLMs reveals that most models perform near chance, and even the
best lag far behind human accuracy on physically irreversible processes (e.g.,
free fall, diffusion/explosion) and causal manual actions (division/addition)
that humans recognize almost instantly. These results highlight a fundamental
gap in current multimodal systems: while they capture rich visual-semantic
correlations, they lack the inductive biases required for temporal continuity
and causal understanding. We release the code and data for AoT-PsyPhyBENCH to
encourage further progress in the physical and temporal reasoning capabilities
of VLMs.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°å¯¹è¿ç¯è®ºææè¦è¿è¡è¯¦ç»åæã</p>
<hr />
<p><strong>è®ºææè¦åæï¼Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for Vision-Language Models</strong></p>
<p><strong>1. è®ºæä¸»è¦è´¡ç®çç®æ´æ»ç» (2-3 å¥è¯)</strong></p>
<p>è¿ç¯è®ºæçæ ¸å¿è´¡ç®å¨äºæ­ç¤ºäºå½åè§è§-è¯­è¨æ¨¡åï¼VLMsï¼å¨çè§£è§é¢æ¶é´ä¿¡æ¯æ¹é¢çæ ¹æ¬æ§å¼±ç¹ãä½èéè¿å¼å¥ä¸ä¸ªåä¸º AoT-PsyPhyBENCH çå¿çç©çå­¦éªè¯åºåï¼ç³»ç»å°è¯ä¼°äºVLMså¤æ­è§é¢æ¶é´æ¹åï¼å³âæ¶é´ä¹ç®­âï¼çè½åï¼å¹¶åç°å¤§å¤æ°æ¨¡åè¡¨ç°æ¥è¿éæºï¼è¿ä½äºäººç±»å¨ç©çä¸å¯éè¿ç¨åå ææå¨æä½ä¸çåç¡®æ§ãè¿å¸æ¾äºç°æVLMå¨æ¶é´è¿ç»­æ§åå æçè§£æ¹é¢ç¼ºä¹å¿è¦çå½çº³åç½®ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³å­¦æ¹æ³</strong></p>
<p>å³é®åæ°å¨äºå¶<strong>å¿çç©çå­¦éªè¯çåºåï¼AoT-PsyPhyBENCHï¼</strong>ã
*   <strong>ä»»å¡è®¾è®¡ï¼</strong> éç¨âå¤æ­æ¶é´ä¹ç®­âï¼AoTï¼è¿ä¸çä¼¼ç®åä½æå·å¯åæ§çä»»å¡ï¼å³å¤æ­ç­è§é¢çæ®µæ¯æ­£åæ­æ¾è¿æ¯ååæ­æ¾ãè¿ä¸ªä»»å¡ç´æ¥ä¸ææå°æµè¯äºæ¨¡åå¯¹æ¶é´æ¹åççè§£ã
*   <strong>äººç±»è¡ä¸ºåºçº¿ï¼</strong> å³é®ä¹å¤å¨äºï¼è¯¥åºåä½¿ç¨äºä¸äººç±»ç¸åçåºæ¿åè¡ä¸ºåºçº¿è¿è¡éªè¯ãè¿æå³çå®ä¸ä»ä»æ¯ä¸ä¸ªæ°æ®éï¼èæ¯ä¸ä¸ªç»è¿ç²¾å¿è®¾è®¡åæ ¡åçè¯ä¼°å·¥å·ï¼è½å¤ç´æ¥ä¸äººç±»è®¤ç¥è½åè¿è¡æ¯è¾ã
*   <strong>åºæ¿ç±»åï¼</strong> ä¸æ³¨äºç©çä¸å¯éè¿ç¨ï¼å¦èªç±è½ä½ãæ©æ£/çç¸ï¼åå ææå¨æä½ï¼å¦åå²/æ·»å ï¼ï¼è¿äºæ¯äººç±»è½å¤å ä¹å³æ¶è¯å«æ¶é´æ¹åçåºæ¯ï¼ä»èè½å¤æ¸æ°å°æ­ç¤ºæ¨¡åä¸äººç±»çå·®è·ã</p>
<p><strong>3. å¯¹é¢åæ½å¨å½±å</strong></p>
<ul>
<li><strong>æ¨å¨æ¶é´æ¨çç ç©¶ï¼</strong> è¯¥ç ç©¶å°æå¤§å°æ¨å¨è®¡ç®æºè§è§åå¤æ¨¡æå­¦ä¹ é¢åå¯¹æ¶é´æ¨çåå æçè§£çç ç©¶ãå®æç¡®æåºäºå½åVLMçä¸ä¸ªæ ¸å¿ç­æ¿ï¼ä¸ºæªæ¥çæ¨¡åè®¾è®¡åç®æ³å¼åæä¾äºæ¸æ°çæ¹åã</li>
<li><strong>æ°çè¯ä¼°èå¼ï¼</strong> AoT-PsyPhyBENCHæä¾äºä¸ä¸ªæ°é¢ä¸ä¸¥æ ¼çè¯ä¼°èå¼ï¼å¯ä»¥ä½ä¸ºæªæ¥VLMå¨æ¶é´çè§£è½åæ¹é¢çéè¦åºåãå¶å¿çç©çå­¦åºç¡ä½¿å¶æ¯çº¯ç²¹çæ°æ®éæ´å·ç§å­¦æ§åå¯ä¿¡åº¦ã</li>
<li><strong>å¯åæ¨¡åæ¶ææ¹è¿ï¼</strong> è®ºæç»ææç¤ºï¼ä»ä»ææè§è§-è¯­ä¹ç¸å³æ§ä¸è¶³ä»¥å®ç°çæ­£çæ¶é´åå æçè§£ãè¿å¯è½ä¼ä¿ä½¿ç ç©¶äººåæ¢ç´¢æ°çæ¨¡åæ¶æãè®­ç»ç®æ æé¢è®­ç»ä»»å¡ï¼ä»¥å¼å¥æ´å¼ºçæ¶åºå½çº³åç½®ã</li>
<li><strong>æ´é²æ£çVLMï¼</strong> æç»ç®æ æ¯å¼ååºå¯¹æ¶é´ä¿¡æ¯ææ´æ·±å»çè§£çVLMï¼è¿å°ä½¿å¶å¨è§é¢çè§£ãäºä»¶é¢æµãæºå¨äººæä½ç­éè¦æ¶é´æ¨ççåºç¨ä¸­è¡¨ç°æ´åºè²ã</li>
</ul>
<p><strong>4. ç¸å³é¢åæåºç¨åç</strong></p>
<ul>
<li><strong>è§é¢çè§£ä¸åæï¼</strong> æ¾èæåVLMå¨è§é¢åå®¹çè§£ãäºä»¶æ£æµãè¡ä¸ºè¯å«ç­æ¹é¢çè½åï¼å°¤å¶æ¯å¨éè¦çè§£äºä»¶é¡ºåºåå æå³ç³»æ¶ã</li>
<li><strong>æºå¨äººå­¦ä¸å·èº«æºè½ï¼</strong> æºå¨äººéè¦çè§£ç©çä¸ççå æå³ç³»åæ¶é´æµéï¼ä»¥ä¾¿è¿è¡è§åãé¢æµåäº¤äºãæ´å¼ºçæ¶åºæ¨çè½åå°ä½¿æºå¨äººè½å¤æ´å¥½å°çè§£å¶è¡å¨çåæã</li>
<li><strong>èªå¨é©¾é©¶ï¼</strong> é¢æµéè·¯ä¸å¶ä»è½¦è¾åè¡äººçè¡ä¸ºï¼çè§£äº¤éäºä»¶çæ¼åï¼é½éè¦å¼ºå¤§çæ¶é´æ¨çè½åã</li>
<li><strong>åå®¹çæä¸ç¼è¾ï¼</strong> å¨çæè¿è´¯çè§é¢åå®¹æè¿è¡è§é¢ç¼è¾æ¶ï¼çè§£æ¶é´ä¹ç®­åå æå³ç³»è³å³éè¦ã</li>
<li><strong>ç§å­¦æ¨¡æä¸é¢æµï¼</strong> å¨ç©çãåå­¦ç­é¢åï¼çè§£è¿ç¨çä¸å¯éæ§åæ¶é´æ¹åå¯¹äºæ¨¡æåé¢æµè³å³éè¦ã</li>
</ul>
<p><strong>5. ä»æè¦ä¸­å¯æ¨æ­çå±éæ§</strong></p>
<ul>
<li><strong>ä»»å¡èå´ï¼</strong> å°½ç®¡âæ¶é´ä¹ç®­âä»»å¡éå¸¸å·æå¯åæ§ï¼ä½å®ä¸»è¦å³æ³¨çæ¯<strong>æ¶é´æ¹åçå¤æ­</strong>ï¼èä¸æ¯æ´å¤æç<strong>æ¶é´åºåé¢æµãäºä»¶æç»­æ¶é´ä¼°è®¡æå¤äºä»¶æ¶é´å³ç³»æ¨ç</strong>ãVLMå¨è¿äºæ´å¤æçä»»å¡ä¸çè¡¨ç°å¯è½éè¦è¿ä¸æ­¥çè¯ä¼°ã</li>
<li><strong>è§é¢é¿åº¦ï¼</strong> æè¦ä¸­æå°âç­çæ®µâï¼è¿æå³çè¯¥åºåå¯è½ä¸»è¦å³æ³¨ç¬æ¶æç­æ¶ç¨çæ¶é´æ¨çãå¯¹äºé¿è§é¢ä¸­è·¨è¶è¾é¿æ¶é´çäºä»¶é¾æå¤æåäºï¼å¶è¯ä¼°è½åå¯è½æéã</li>
<li><strong>æ°æ®å¤æ ·æ§ï¼</strong> å°½ç®¡æå°äºâèªç¶è§é¢âï¼ä½å·ä½çæ°æ®éè§æ¨¡ãåºæ¯å¤æ ·æ§ä»¥åæ¯å¦æ¶µçäºææç±»åçæ¶é´æ¨çææï¼ä¾å¦ï¼ç¤¾ä¼äºå¨ä¸­çæ¶é´æ¨çï¼å¨æè¦ä¸­å¹¶æªè¯¦ç»è¯´æã</li>
<li><strong>æ¨¡åç±»åï¼</strong> æè¦æå°è¯ä¼°äºâå¼æ¾æéåä¸æãæ¨çåéæ¨çVLMâï¼ä½å¹¶æªå·ä½ååºæ¨¡ååç§°æå¶æ¶æç»èãè¿ä½¿å¾æä»¬æ æ³ç´æ¥å¤æ­åªäºç¹å®ç±»åçVLMè¡¨ç°æ´å·®ï¼ä»¥åæ¯å¦å­å¨æäºæ¶æå¯¹æ¶é´æ¨çæ´å·æ½åã</li>
<li><strong>âå¿çç©çå­¦éªè¯âçæ·±åº¦ï¼</strong> æè¦å¼ºè°äºå¿çç©çå­¦éªè¯ï¼ä½å·ä½éªè¯è¿ç¨çç»èï¼ä¾å¦ï¼äººç±»åä¸èçæ°éãå®éªè®¾è®¡ãæ°æ®åææ¹æ³ï¼å¨æè¦ä¸­æ æ³å¾ç¥ãè¿ä¼å½±åæä»¬å¯¹å¶âéªè¯âç¨åº¦çå¤æ­ã</li>
</ul>
<hr />
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæéè¿ä¸ä¸ªå·§å¦ä¸å·æå¿çç©çå­¦åºç¡çè¯ä¼°åºåï¼ç²¾åå°æåºäºå½åVLMå¨æ¶é´çè§£æ¹é¢çæ ¸å¿ç¼ºé·ãå®ä¸ä»æä¾äºä¸ä¸ªæåçè¯æ®ï¼ä¹ä¸ºæªæ¥çç ç©¶ææäºæ¹åï¼æææ¨å¨è®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åå¨å®ç°çæ­£æºè½çè§é¢çè§£æ¹é¢è¿åºéè¦ä¸æ­¥ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce AoT-PsyPhyBENCH, a psychophysically validated
benchmark that tests whether VLMs can infer temporal direction in natural
videos using the same stimuli and behavioral baselines established for humans.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.26241v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.26241v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.26160v1'></a></p>
<h2 id="crag-mm-multi-modal-multi-turn-comprehensive-rag-benchmark"><a href="https://arxiv.org/abs/2510.26160v1">CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark</a></h2>
<p><strong>Authors:</strong> Jiaqi Wang, Xiao Yang, Kai Sun, Parth Suresh, Sanat Sharma, Adam Czyzewski, Derek Andersen, Surya Appini, Arkav Banerjee, Sajal Choudhary, Shervin Ghasemlou, Ziqiang Guan, Akil Iyer, Haidar Khan, Lingkun Kong, Roy Luo, Tiffany Ma, Zhen Qiao, David Tran, Wenfang Xu, Skyler Yeatman, Chen Zhou, Gunveer Gujral, Yinglong Xia, Shane Moon, Nicolas Scheffer, Nirav Shah, Eun Chang, Yue Liu, Florian Metze, Tammy Stark, Zhaleh Feizollahi, Andrea Jessee, Mangesh Pujari, Ahmed Aly, Babak Damavandi, Rakesh Wanga, Anuj Kumar, Rohit Patel, Wen-tau Yih, Xin Luna Dong</p>
<p><strong>Published:</strong> 2025-10-30</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Wearable devices such as smart glasses are transforming the way people
interact with their surroundings, enabling users to seek information regarding
entities in their view. Multi-Modal Retrieval-Augmented Generation (MM-RAG)
plays a key role in supporting such questions, yet there is still no
comprehensive benchmark for this task, especially regarding wearables
scenarios. To fill this gap, we present CRAG-MM -- a Comprehensive RAG
benchmark for Multi-modal Multi-turn conversations. CRAG-MM contains a diverse
set of 6.5K (image, question, answer) triplets and 2K visual-based multi-turn
conversations across 13 domains, including 6.2K egocentric images designed to
mimic captures from wearable devices. We carefully constructed the questions to
reflect real-world scenarios and challenges, including five types of
image-quality issues, six question types, varying entity popularity, differing
information dynamism, and different conversation turns. We design three tasks:
single-source augmentation, multi-source augmentation, and multi-turn
conversations -- each paired with an associated retrieval corpus and APIs for
both image-KG retrieval and webpage retrieval. Our evaluation shows that
straightforward RAG approaches achieve only 32% and 43% truthfulness on CRAG-MM
single- and multi-turn QA, respectively, whereas state-of-the-art industry
solutions have similar quality (32%/45%), underscoring ample room for
improvement. The benchmark has hosted KDD Cup 2025, attracting about 1K
participants and 5K submissions, with winning solutions improving baseline
performance by 28%, highlighting its early impact on advancing the field.</p>
<p><strong>Analysis:</strong></p>
<p>ä»¥ä¸æ¯Jiaqi Wangç­äººæ°åçè®ºæâCRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmarkâçå¨é¢æè¦ï¼</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢</strong>
è¯¥è®ºææ¨å¨è§£å³å½åå¤æ¨¡ææ£ç´¢å¢å¼ºçæï¼MM-RAGï¼é¢åç¼ºä¹éå¯¹å¯ç©¿æ´è®¾å¤åºæ¯çå¨é¢åºåæµè¯çé®é¢ãéçæºè½ç¼éç­å¯ç©¿æ´è®¾å¤çå´èµ·ï¼ç¨æ·éè¦è½å¤éè¿è§è§ä¿¡æ¯æ¥è¯¢å¨å´å®ä½ï¼ä½ç°æçVQAåºåæµè¯ä¸è¶³ä»¥å¨é¢è¯ä¼°æ­¤ç±»åºæ¯ä¸çäºå®æ§é®é¢ï¼ç¹å«æ¯æ¶åå¤è½®å¯¹è¯åçå®ä¸çå¾åè´¨éææçæåµã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®</strong>
*   <strong>CRAG-MMåºåæµè¯çæåºï¼</strong> å¼å¥äºä¸ä¸ªåä¸ºCRAG-MMçç»¼åæ§MM-RAGåºåæµè¯ï¼ä¸é¨éå¯¹å¤æ¨¡æå¤è½®å¯¹è¯ã
*   <strong>æ°æ®éçç¬ç¹æ§ï¼</strong>
    *   åå«6.5Kï¼å¾åãé®é¢ãç­æ¡ï¼ä¸åç»å2Kåºäºè§è§çå¤è½®å¯¹è¯ï¼æ¶µç13ä¸ªé¢åã
    *   åå«6.2Kæ¨¡ä»¿å¯ç©¿æ´è®¾å¤æè·çä»¥èªæä¸ºä¸­å¿çå¾åï¼åæ äºçå®ä¸ççå¾åè´¨éææï¼å¦ä½åãæ¨¡ç³ãæªæ­ãé®æ¡ãæè½¬ï¼ã
    *   é®é¢è®¾è®¡åæ äºçå®ä¸çåºæ¯åææï¼åæ¬äºç§å¾åè´¨éé®é¢ãå­ç§é®é¢ç±»åãä¸åå®ä½æµè¡åº¦ãä¿¡æ¯å¨ææ§ä»¥åä¸åçå¯¹è¯è½®æ¬¡ã
*   <strong>ä¸é¡¹ä»»å¡è®¾è®¡ï¼</strong>
    *   <strong>åæºå¢å¼ºï¼</strong> æµè¯åºäºå¾åç¥è¯å¾è°±ï¼KGï¼çæ£ç´¢è½åã
    *   <strong>å¤æºå¢å¼ºï¼</strong> å¨å¾åKGæ£ç´¢çåºç¡ä¸ï¼å¼å¥ç½é¡µæ£ç´¢ã
    *   <strong>å¤è½®å¯¹è¯ï¼</strong> è¯ä¼°ç³»ç»è¿è¡å¤è½®å¯¹è¯çè½åï¼åæ¬ä¸ä¸æçè§£åè¯é¢è½¬ç§»ã
*   <strong>æ£ç´¢è¯­æåºåAPIï¼</strong> ä¸ºå¾åKGæ£ç´¢åç½é¡µæ£ç´¢æä¾äºç¸å³çæ£ç´¢è¯­æåºåAPIï¼ä»¥ç¡®ä¿å¬å¹³è¯ä¼°ï¼å¹¶æ¨¡æçå®ä¸ççæ£ç´¢æ¡ä»¶ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹</strong>
*   <strong>åºçº¿æ§è½ï¼</strong> è®ºæè¯ä¼°æ¾ç¤ºï¼ç´æ¥çRAGæ¹æ³å¨CRAG-MMåè½®QAä¸ä»è¾¾å°32%ççå®æ§ï¼å¨å¤è½®QAä¸è¾¾å°43%ççå®æ§ãæåè¿çè¡ä¸è§£å³æ¹æ¡ä¹ä»è¾¾å°32%/45%çç±»ä¼¼è´¨éï¼è¿è¡¨æè¯¥é¢åä»æå·¨å¤§çæ¹è¿ç©ºé´ã
*   <strong>æææ§æ­ç¤ºï¼</strong> CRAG-MMæ­ç¤ºäºå½åMM-RAGç³»ç»é¢ä¸´çè¯¸å¤ææï¼åæ¬ï¼
    *   ä½è´¨éå¾åï¼å¦ä½åãé®æ¡ï¼å¯¼è´çå®æ§æ¾èä¸éï¼é«è¾¾46%ï¼ï¼å¸æ¾äºå¾åçè§£é²æ£æ§çéæ±ã
    *   å®ä½è¯å«å¨ä»ä¾èµè§è§ä¿¡æ¯æ¶æ´å°é¾ï¼ä¸é37%ï¼ã
    *   å¤çä¸æµè¡å®ä½ãéè¦å¤é¨ç¥è¯ãæéè¦ç»¼åå¤æºä¿¡æ¯çå¤æé®é¢æ¶ï¼ç³»ç»è¡¨ç°ä¸ä½³ã
    *   å¤è½®å¯¹è¯ä»æ¯å·¨å¤§ææï¼è®¸å¤å¯¹è¯å è¿ç»­éè¯¯æç¼ºå¤±ç­æ¡èæåç»æ­¢ï¼è¶è¿44%ï¼ã
*   <strong>KDD Cup 2025çå½±åï¼</strong> è¯¥åºåæµè¯ä½ä¸ºKDD Cup 2025ææèµçåºç¡ï¼å¸å¼äºçº¦1Kåä¸èå5Kæäº¤ï¼è·èè§£å³æ¹æ¡å°åºçº¿æ§è½æé«äº28%ï¼çªæ¾äºå¶å¨æ¨å¨è¯¥é¢ååå±æ¹é¢çæ©æå½±åã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§</strong>
*   <strong>SOTAè§£å³æ¹æ¡çå±éæ§ï¼</strong> å°½ç®¡è¡ä¸SOTAè§£å³æ¹æ¡å¨åç¡®æ§ä¸æææé«ï¼ä½å¹»è§çä»ç¶å¾é«ï¼åè½®31%-49%ï¼å¤è½®26%-35%ï¼ï¼è¡¨æå¨æå»ºå¯ä¿¡èµçè§è§QAç³»ç»æ¹é¢ä»å­å¨æ¾èå·®è·ã
*   <strong>è¯ä¼°çå±éæ§ï¼</strong> è®ºææåºï¼ä¸åè§£å³æ¹æ¡ï¼å¦ç´æ¥RAGãæè¡æ¦è·èå¢éãSOTAï¼ä¹é´ççå®æ§åæ°ä¸å®å¨å¯æ¯ï¼å ä¸ºè¡ä¸è§£å³æ¹æ¡ä½¿ç¨äºæ´å¤§çæ¨¡ååå¯è½æ´ä¸°å¯çç¥è¯åºã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å</strong>
*   <strong>å¾åçè§£çé²æ£æ§ï¼</strong> éè¦å¼åæ´é²æ£çå¾åçè§£ææ¯ï¼ä»¥åºå¯¹ä½è´¨éï¼å¦ä½åãæ¨¡ç³ãæªæ­ãé®æ¡ãæè½¬ï¼çä»¥èªæä¸ºä¸­å¿çå¾åã
*   <strong>æ´æºè½çå¾åæç´¢ï¼</strong> æ¹è¿å¾åæç´¢æºå¶ï¼ä»¥æ´å¥½å°å¤çå®ä½è¯å«åå¾åçè§£çææã
*   <strong>å¤æºä¿¡æ¯èåï¼</strong> æåMM-RAGç³»ç»ç»¼åæ¥èªä¸åæ¥æºï¼å¾åKGåç½é¡µï¼ä¿¡æ¯çè½åï¼ç¹å«æ¯å¯¹äºå¤æé®é¢åä¸æµè¡å®ä½ã
*   <strong>å¤è½®å¯¹è¯ç®¡çï¼</strong> è§£å³å¤è½®å¯¹è¯ä¸­çä¸ä¸æçè§£ãè¯é¢è½¬ç§»åé¿åæ©æç»æ­¢çé®é¢ï¼ä»¥å®ç°æ´æµçãæ´èªç¶çå¯¹è¯ä½éªã
*   <strong>åå°å¹»è§ï¼</strong> å¼åæ´ææçç­ç¥æ¥åå°MM-LLMçå¹»è§ï¼æé«ç­æ¡çå¯ä¿¡èµæ§ã
*   <strong>æ¨¡åä¸ç¡®å®æ§å¤çï¼</strong> é¼å±æ¨¡åå¨ä¸ç¡®å®æ¶æ´å¥½å°æç»åç­ï¼ä»¥éä½å¹»è§çå¹¶æé«æ´ä½è´¨éã</p>
<p>æ»èè¨ä¹ï¼CRAG-MMåºåæµè¯ä¸ºå¯ç©¿æ´AIåºç¨ä¸­çMM-RAGç³»ç»æä¾äºä¸ä¸ªåææªæçå¨é¢è¯ä¼°æ¡æ¶ï¼éè¿å¶ç¬ç¹çæ°æ®éãä»»å¡è®¾è®¡åå¯è®¿é®çAPIï¼æ­ç¤ºäºå½åææ¯çå³é®ææåæªæ¥åå±æ¹åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To fill this gap, we present CRAG-MM -- a Comprehensive RAG
benchmark for Multi-modal Multi-turn conversations.</li>
<li>Our evaluation shows that
straightforward RAG approaches achieve only 32% and 43% truthfulness on CRAG-MM
single- and multi-turn QA, respectively, whereas state-of-the-art industry
solutions have similar quality (32%/45%), underscoring ample room for
improvement.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.26160v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.26160v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.26149v1'></a></p>
<h2 id="basicavsr-arbitrary-scale-video-super-resolution-via-image-priors-and-enhanced-motion-compensation"><a href="https://arxiv.org/abs/2510.26149v1">BasicAVSR: Arbitrary-Scale Video Super-Resolution via Image Priors and Enhanced Motion Compensation</a></h2>
<p><strong>Authors:</strong> Wei Shang, Wanying Zhang, Shuhang Gu, Pengfei Zhu, Qinghua Hu, Dongwei Ren</p>
<p><strong>Published:</strong> 2025-10-30</p>
<p><strong>Categories:</strong> cs.CV, I.4.3</p>
<p><strong>Abstract:</strong></p>
<p>Arbitrary-scale video super-resolution (AVSR) aims to enhance the resolution
of video frames, potentially at various scaling factors, which presents several
challenges regarding spatial detail reproduction, temporal consistency, and
computational complexity. In this paper, we propose a strong baseline BasicAVSR
for AVSR by integrating four key components: 1) adaptive multi-scale frequency
priors generated from image Laplacian pyramids, 2) a flow-guided propagation
unit to aggregate spatiotemporal information from adjacent frames, 3) a
second-order motion compensation unit for more accurate spatial alignment of
adjacent frames, and 4) a hyper-upsampling unit to generate scale-aware and
content-independent upsampling kernels. To meet diverse application demands, we
instantiate three propagation variants: (i) a unidirectional RNN unit for
strictly online inference, (ii) a unidirectional RNN unit empowered with a
limited lookahead that tolerates a small output delay, and (iii) a
bidirectional RNN unit designed for offline tasks where computational resources
are less constrained. Experimental results demonstrate the effectiveness and
adaptability of our model across these different scenarios. Through extensive
experiments, we show that BasicAVSR significantly outperforms existing methods
in terms of super-resolution quality, generalization ability, and inference
speed. Our work not only advances the state-of-the-art in AVSR but also extends
its core components to multiple frameworks for diverse scenarios. The code is
available at https://github.com/shangwei5/BasicAVSR.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°å¯¹è¿ç¯è®ºææè¦è¿è¡æ·±å¥åæã</p>
<hr />
<p><strong>è®ºææè¦åæï¼BasicAVSR: Arbitrary-Scale Video Super-Resolution via Image Priors and Enhanced Motion Compensation</strong></p>
<p><strong>1. è®ºæä¸»è¦è´¡ç®çç®æ´æ»ç» (2-3 å¥è¯)</strong></p>
<p>è¿ç¯è®ºææåºäºä¸ç§åä¸º BasicAVSR çå¼ºå¤§åºçº¿æ¨¡åï¼ç¨äºä»»æå°ºåº¦è§é¢è¶åè¾¨ç (AVSR) ä»»å¡ãè¯¥æ¨¡åéè¿æ´åå¾ååéªãå¢å¼ºçè¿å¨è¡¥å¿åçµæ´»çæ¶ç©ºä¿¡æ¯èåç­ç¥ï¼ææè§£å³äº AVSR å¨ç©ºé´ç»èãæ¶é´ä¸è´æ§åè®¡ç®æçæ¹é¢çææãBasicAVSR å¨è¶åè¾¨çè´¨éãæ³åè½ååæ¨çéåº¦æ¹é¢æ¾èè¶è¶äºç°ææ¹æ³ï¼å¹¶æä¾äºéåºä¸ååºç¨åºæ¯çå¤ç§ä¼ æ­åä½ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³å­¦æ¹æ³</strong></p>
<p>è¯¥è®ºæçå³é®åæ°å¨äºå¶éæäºä¸ç³»åç²¾å¿è®¾è®¡çç»ä»¶ï¼ä»¥ååè§£å³ AVSR çæ ¸å¿ææï¼</p>
<ul>
<li><strong>èªéåºå¤å°ºåº¦é¢çåéª (Adaptive Multi-scale Frequency Priors):</strong> å©ç¨å¾åææ®ææ¯éå­å¡çæå¤å°ºåº¦é¢çåéªï¼è¿æå©äºå¨ä¸åå°ºåº¦ä¸æ´å¥½å°æ¢å¤ç©ºé´ç»èï¼å¹¶å¯è½ä¸ºæ¨¡åæä¾æ´ä¸°å¯ççº¹çä¿¡æ¯ãè¿æ¯å°å¾åè¶åé¢åçææåéªç¥è¯å¼å¥è§é¢è¶åçä¸ç§æ¹å¼ã</li>
<li><strong>æµå¼å¯¼ä¼ æ­åå (Flow-guided Propagation Unit):</strong> è¿æ¯ä¸ä¸ªæ ¸å¿çæ¶ç©ºä¿¡æ¯èåæºå¶ï¼å©ç¨åæµä¿¡æ¯å¼å¯¼ç¸é»å¸§çä¿¡æ¯ä¼ æ­ï¼ä»¥ä¿ææ¶é´ä¸è´æ§ã</li>
<li><strong>äºé¶è¿å¨è¡¥å¿åå (Second-order Motion Compensation Unit):</strong> ç¸æ¯äºä¼ ç»çä¸é¶è¿å¨è¡¥å¿ï¼äºé¶è¡¥å¿è½å¤å®ç°æ´ç²¾ç¡®çç¸é»å¸§ç©ºé´å¯¹é½ï¼è¿å¯¹äºåå°è¿å¨ä¼ªå½±åæé«ç»èæ¢å¤è³å³éè¦ã</li>
<li><strong>è¶ä¸éæ ·åå (Hyper-upsampling Unit):</strong> è½å¤çæå°ºåº¦æç¥ä¸åå®¹æ å³çä¸éæ ·æ ¸ï¼è¿ä½¿å¾æ¨¡åè½å¤çµæ´»å°å¤çä»»æå°ºåº¦çè¶åè¾¨çä»»å¡ï¼èæ éä¸ºæ¯ä¸ªç¹å®å°ºåº¦è®­ç»åç¬çæ¨¡åã</li>
<li><strong>çµæ´»çä¼ æ­åä½ (Flexible Propagation Variants):</strong> éå¯¹ä¸åçåºç¨éæ±ï¼å¨çº¿æ¨çãæéå»¶è¿å¨çº¿æ¨çãç¦»çº¿ä»»å¡ï¼ï¼æä¾äºååRNNãå¸¦æéåç»çååRNNåååRNNä¸ç§ä¼ æ­åååä½ï¼æå¤§å°å¢å¼ºäºæ¨¡åçå®ç¨æ§åéåºæ§ã</li>
</ul>
<p><strong>3. å¯¹é¢åæ½å¨å½±å</strong></p>
<ul>
<li><strong>æ ç«æ°çAVSRåºçº¿ (New SOTA Baseline for AVSR):</strong> BasicAVSR å¨è¶åè¾¨çè´¨éãæ³åè½ååæ¨çéåº¦æ¹é¢çæ¾èæåï¼ä½¿å¶æææä¸ºAVSRé¢åæ°çSOTAåºçº¿ï¼ä¸ºæªæ¥çç ç©¶æä¾ä¸ä¸ªå¼ºå¤§çèµ·ç¹åæ¯è¾æ åã</li>
<li><strong>æ¨å¨AVSRçå®ç¨å (Promoting Practical AVSR):</strong> æä¾äºéåºä¸åè®¡ç®çº¦æåå»¶è¿è¦æ±çä¼ æ­åä½ï¼ä½¿å¾AVSRææ¯è½å¤æ´å¥½å°åºç¨äºå®æ¶å¨çº¿ç³»ç»ãæµåªä½æå¡ä»¥åç¦»çº¿è§é¢å¤çç­å¤ç§å®éåºæ¯ã</li>
<li><strong>ç»ä»¶åè®¾è®¡ææ³çå¯å (Inspiration for Modular Design):</strong> è®ºæéè¿éæå¤ä¸ªç²¾å¿è®¾è®¡çç»ä»¶æ¥è§£å³å¤æé®é¢ï¼è¿ç§æ¨¡ååçè®¾è®¡ææ³å¯ä»¥å¯åå¶ä»è®¡ç®æºè§è§ä»»å¡çç ç©¶ã</li>
<li><strong>å¾ååéªå¨è§é¢ä»»å¡ä¸­çåºç¨ (Application of Image Priors in Video Tasks):</strong> å°å¾åææ®ææ¯éå­å¡çé¢çåéªå¼å¥è§é¢è¶åï¼å±ç¤ºäºè·¨é¢åç¥è¯è¿ç§»çæææ§ï¼å¯è½é¼å±æ´å¤å¾åå¤çææ¯å¨è§é¢ä»»å¡ä¸­çæ¢ç´¢ã</li>
</ul>
<p><strong>4. ç¸å³é¢åæåºç¨åç</strong></p>
<ul>
<li><strong>è§é¢æµåªä½æå¡ (Video Streaming Services):</strong> å¯ä»¥å¨å¸¦å®½æéçæåµä¸æä¾æ´é«è´¨éçè§é¢åå®¹ï¼æ¹åç¨æ·è§çä½éªã</li>
<li><strong>è§é¢ä¼è®®/å®æ¶éä¿¡ (Video Conferencing/Real-time Communication):</strong> æåä½åè¾¨çæåå¤´è¾å¥çè§é¢è´¨éï¼ä½¿è¿ç¨äº¤æµæ´æ¸æ°ã</li>
<li><strong>è§é¢çæ§ (Video Surveillance):</strong> å¢å¼ºçæ§è§é¢çç»èï¼æå©äºè¯å«ç®æ ååæäºä»¶ã</li>
<li><strong>å»å­¦å½±å (Medical Imaging):</strong> æé«ä½åè¾¨çå»å­¦è§é¢ï¼å¦è¶å£°ãåçª¥éï¼çæ¸æ°åº¦ï¼è¾å©å»çè¯æ­ã</li>
<li><strong>åå®¹åä½ä¸åæå¶ä½ (Content Creation and Post-production):</strong> æåæ§è§é¢ç´ ææä½åè¾¨çææåå®¹çè´¨éï¼éä½å¶ä½ææ¬ã</li>
<li><strong>èæç°å®/å¢å¼ºç°å® (VR/AR):</strong> ä¸ºé«åè¾¨çæ¾ç¤ºå¨æä¾é«è´¨éçè§é¢åå®¹ï¼æåæ²æµ¸æã</li>
</ul>
<p><strong>5. ä»æè¦ä¸­å¯æ¨æ­çå±éæ§</strong></p>
<ul>
<li><strong>è®¡ç®å¤æåº¦ (Computational Complexity):</strong> å°½ç®¡è®ºæå£°ç§°å¨æ¨çéåº¦ä¸æææåï¼ä½äºé¶è¿å¨è¡¥å¿åå¤å°ºåº¦é¢çåéªçå¼å¥ï¼ä»¥åRNNååæ¬èº«ï¼å¯è½ä»ç¶å¸¦æ¥ä¸å®çè®¡ç®å¼éãå°¤å¶æ¯å¨å¤çæé«åè¾¨çææé¿è§é¢åºåæ¶ï¼è®¡ç®èµæºå¯è½ä»æ¯ææï¼å°½ç®¡æä¾äºä¸ååä½æ¥ç¼è§£ã</li>
<li><strong>åæµä¼°è®¡çä¾èµæ§ (Dependency on Optical Flow Estimation):</strong> æµå¼å¯¼ä¼ æ­åè¿å¨è¡¥å¿é½é«åº¦ä¾èµäºåç¡®çåæµä¼°è®¡ãå¨å¿«éè¿å¨ãé®æ¡ãåç§å§çååç­å¤æåºæ¯ä¸ï¼åæµä¼°è®¡çè¯¯å·®å¯è½ä¼å½±åè¶åè¾¨ççæ§è½ãæè¦ä¸­æªæåå¦ä½å¤çåæµä¼°è®¡ä¸åç¡®çæåµã</li>
<li><strong>æ³åè½åè¾¹ç (Generalization Ability Boundaries):</strong> å°½ç®¡å£°ç§°å·æè¯å¥½çæ³åè½åï¼ä½å¶å¨æç«¯æªè§è¿çè§é¢åå®¹ãè¿å¨æ¨¡å¼æåªå£°ç±»åä¸çè¡¨ç°ä»éè¿ä¸æ­¥éªè¯ãä¾å¦ï¼å¨å¡éå¨ç»ãCGè§é¢ç­ä¸çå®ä¸çè§é¢å·®å¼è¾å¤§çæ°æ®ä¸ï¼å¶æ§è½å¯è½ææä¸åã</li>
<li><strong>âåºçº¿âçå«ä¹ (Implication of "Baseline"):</strong> è®ºæå°å¶æ¨¡åç§°ä¸ºâå¼ºå¤§åºçº¿ (strong baseline)âï¼è¿å¯è½æç¤ºæªæ¥ä»æè¿ä¸æ­¥ä¼åçç©ºé´ï¼æèå¶è®¾è®¡çå¿µæ´ä¾§éäºæä¾ä¸ä¸ªåå®çåºç¡ï¼èéæè´çæ§è½çªç ´ï¼å°½ç®¡å®éªç»ææ¾ç¤ºå¶è¶è¶äºç°ææ¹æ³ï¼ã</li>
<li><strong>è¶åæ°è°ä¼ (Hyperparameter Tuning):</strong> å¤ä¸ªç»ä»¶çéæå¯è½æå³çæ¨¡åå·æè¾å¤çè¶åæ°ï¼å¶è°ä¼è¿ç¨å¯è½è¾ä¸ºå¤æåèæ¶ã</li>
</ul>
<hr />
<p>æ»èè¨ä¹ï¼BasicAVSR æ¯ä¸é¡¹ä»¤äººå´å¥çç ç©¶ï¼å®éè¿ç»åå¾ååéªãåè¿çè¿å¨è¡¥å¿åçµæ´»çæ¶ç©ºèåç­ç¥ï¼ä¸ºä»»æå°ºåº¦è§é¢è¶åè¾¨çé¢åå¸¦æ¥äºæ¾èçè¿æ­¥ãå¶å¯¹ä¸ååºç¨åºæ¯çéåºæ§èèï¼ä½¿å¶å¨çè®ºåå®è·µä¸é½å·æéè¦çæä¹ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we propose a strong baseline BasicAVSR
for AVSR by integrating four key components: 1) adaptive multi-scale frequency
priors generated from image Laplacian pyramids, 2) a flow-guided propagation
unit to aggregate spatiotemporal information from adjacent frames, 3) a
second-order motion compensation unit for more accurate spatial alignment of
adjacent frames, and 4) a hyper-upsampling unit to generate scale-aware and
content-independent upsampling kernels.</li>
<li>Through extensive
experiments, we show that BasicAVSR significantly outperforms existing methods
in terms of super-resolution quality, generalization ability, and inference
speed.</li>
<li>Our work not only advances the state-of-the-art in AVSR but also extends
its core components to multiple frameworks for diverse scenarios.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.26149v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.26149v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-10-31 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
