<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2026-01-09 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-08
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2026-01-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-13
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2026-01-08/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2026-01-12/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2026-01-09">Arxiv Computer Vision Papers - 2026-01-09</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#arxiv-2026-01-08" class="nav-link">Arxiv 计算机视觉领域论文日报 (2026-01-08) 执行摘要</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#a-vision-for-multisensory-intelligence-sensing-synergy-and-science" class="nav-link">A Vision for Multisensory Intelligence: Sensing, Synergy, and Science</a>
                </li>
                <li class="nav-item">
                    <a href="#mesh4d-4d-mesh-reconstruction-and-tracking-from-monocular-video" class="nav-link">Mesh4D: 4D Mesh Reconstruction and Tracking from Monocular Video</a>
                </li>
                <li class="nav-item">
                    <a href="#last_0-latent-spatio-temporal-chain-of-thought-for-robotic-vision-language-action-model" class="nav-link">LaST_{0}: Latent Spatio-Temporal Chain-of-Thought for Robotic Vision-Language-Action Model</a>
                </li>
                <li class="nav-item">
                    <a href="#pixel-perfect-visual-geometry-estimation" class="nav-link">Pixel-Perfect Visual Geometry Estimation</a>
                </li>
                <li class="nav-item">
                    <a href="#grex-generalized-referring-expression-segmentation-comprehension-and-generation" class="nav-link">GREx: Generalized Referring Expression Segmentation, Comprehension, and Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#generate-transfer-adapt-learning-functional-dexterous-grasping-from-a-single-human-demonstration" class="nav-link">Generate, Transfer, Adapt: Learning Functional Dexterous Grasping from a Single Human Demonstration</a>
                </li>
                <li class="nav-item">
                    <a href="#robovip-multi-view-video-generation-with-visual-identity-prompting-augments-robot-manipulation" class="nav-link">RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation</a>
                </li>
                <li class="nav-item">
                    <a href="#plenoptic-video-generation" class="nav-link">Plenoptic Video Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#objectforesight-predicting-future-3d-object-trajectories-from-human-videos" class="nav-link">ObjectForesight: Predicting Future 3D Object Trajectories from Human Videos</a>
                </li>
                <li class="nav-item">
                    <a href="#learning-latent-action-world-models-in-the-wild" class="nav-link">Learning Latent Action World Models In The Wild</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2026-01-09">Arxiv Computer Vision Papers - 2026-01-09</h1>
<h2 id="executive-summary">Executive Summary</h2>
<h2 id="arxiv-2026-01-08">Arxiv 计算机视觉领域论文日报 (2026-01-08) 执行摘要</h2>
<p><strong>研究助理：[您的名字]</strong></p>
<p><strong>日期：2026-01-08</strong></p>
<p><strong>1. 主要主题与趋势：</strong></p>
<p>本期 Arxiv 计算机视觉论文集中体现了以下几个关键主题：</p>
<ul>
<li><strong>多模态融合与理解：</strong> 多篇论文强调了将视觉信息与其他模态（如语言、动作、触觉）相结合，以实现更全面、更智能的理解和交互。</li>
<li><strong>三维重建与理解：</strong> 对三维场景、物体和运动的精确重建与跟踪是另一大热点，尤其是在单目视频和动态场景下。</li>
<li><strong>机器人感知与控制：</strong> 机器人领域的进步显著，论文聚焦于提升机器人的视觉理解能力、动作规划以及与环境的交互。</li>
<li><strong>生成模型与数据增强：</strong> 利用生成模型来创建逼真数据、增强模型鲁棒性以及实现新颖的视觉内容生成。</li>
<li><strong>因果推理与预测：</strong> 探索对未来事件的预测，特别是物体轨迹和人类行为的预测，为更高级的智能系统奠定基础。</li>
</ul>
<p><strong>2. 重点与创新论文：</strong></p>
<ul>
<li><strong>"A Vision for Multisensory Intelligence: Sensing, Synergy, and Science" (Paul Pu Liang):</strong> 这篇综述性论文为多感官智能的未来发展描绘了宏伟蓝图，强调了不同感官信息协同作用的重要性，是理解该领域整体趋势的基石。</li>
<li><strong>"Mesh4D: 4D Mesh Reconstruction and Tracking from Monocular Video" (Zeren Jiang et al.):</strong> 在单目视频中实现高质量的四维网格重建和跟踪，对于动态场景的三维理解具有重要意义，是三维重建领域的一项重要进展。</li>
<li><strong>"LaST<script type="math/tex">_{0}</script>: Latent Spatio-Temporal Chain-of-Thought for Robotic Vision-Language-Action Model" (Zhuoyang Liu et al.):</strong> 提出了一种新颖的“思维链”方法，将视觉、语言和动作信息进行时空上的推理，有望显著提升机器人理解和执行复杂任务的能力。</li>
<li><strong>"Generate, Transfer, Adapt: Learning Functional Dexterous Grasping from a Single Human Demonstration" (Xingyi He et al.):</strong> 仅凭一次人类演示即可学习到功能性的灵巧抓取，展示了强大的泛化和迁移学习能力，对机器人抓取任务具有实际应用价值。</li>
</ul>
<p><strong>3. 新兴研究方向与技术：</strong></p>
<ul>
<li><strong>多模态“思维链”推理：</strong> 将类似人类的链式思考过程应用于多模态信息融合，以实现更深层次的理解和推理。</li>
<li><strong>基于生成模型的四维场景理解：</strong> 利用生成模型来辅助或驱动三维重建和动态场景的理解。</li>
<li><strong>零样本/少样本的机器人任务学习：</strong> 通过少量甚至零次演示来学习新的机器人任务，提高机器人的适应性和灵活性。</li>
<li><strong>人类视频中的未来预测：</strong> 结合人类行为和物体运动信息，预测未来三维物体轨迹，为自主系统提供预见性能力。</li>
</ul>
<p><strong>4. 建议阅读全文的论文：</strong></p>
<p>为了快速掌握本期 Arxiv 论文的精髓，建议优先阅读以下论文：</p>
<ul>
<li><strong>"A Vision for Multisensory Intelligence: Sensing, Synergy, and Science" (Paul Pu Liang):</strong> 提供对多感官智能的全面视角和未来展望。</li>
<li><strong>"Mesh4D: 4D Mesh Reconstruction and Tracking from Monocular Video" (Zeren Jiang et al.):</strong> 深入了解单目视频下的先进三维重建技术。</li>
<li><strong>"LaST<script type="math/tex">_{0}</script>: Latent Spatio-Temporal Chain-of-Thought for Robotic Vision-Language-Action Model" (Zhuoyang Liu et al.):</strong> 探索机器人领域前沿的推理和控制方法。</li>
<li><strong>"Generate, Transfer, Adapt: Learning Functional Dexterous Grasping from a Single Human Demonstration" (Xingyi He et al.):</strong> 学习如何从单次演示中实现高效的机器人抓取。</li>
</ul>
<p>这份摘要旨在为忙碌的研究人员提供一个快速了解 Arxiv 计算机视觉领域最新进展的窗口。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2601.04563v1">A Vision for Multisensory Intelligence: Sensing, Synergy, and Science</a></li>
<li><a href="#2601.05251v1">Mesh4D: 4D Mesh Reconstruction and Tracking from Monocular Video</a></li>
<li><a href="#2601.05248v1">LaST<script type="math/tex">_{0}</script>: Latent Spatio-Temporal Chain-of-Thought for Robotic Vision-Language-Action Model</a></li>
<li><a href="#2601.05246v1">Pixel-Perfect Visual Geometry Estimation</a></li>
<li><a href="#2601.05244v1">GREx: Generalized Referring Expression Segmentation, Comprehension, and Generation</a></li>
<li><a href="#2601.05243v1">Generate, Transfer, Adapt: Learning Functional Dexterous Grasping from a Single Human Demonstration</a></li>
<li><a href="#2601.05241v1">RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation</a></li>
<li><a href="#2601.05239v1">Plenoptic Video Generation</a></li>
<li><a href="#2601.05237v1">ObjectForesight: Predicting Future 3D Object Trajectories from Human Videos</a></li>
<li><a href="#2601.05230v1">Learning Latent Action World Models In The Wild</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2601.04563v1'></a></p>
<h2 id="a-vision-for-multisensory-intelligence-sensing-synergy-and-science"><a href="https://arxiv.org/abs/2601.04563v1">A Vision for Multisensory Intelligence: Sensing, Synergy, and Science</a></h2>
<p><strong>Authors:</strong> Paul Pu Liang</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Categories:</strong> cs.LG, cs.AI, cs.CL, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Our experience of the world is multisensory, spanning a synthesis of language, sight, sound, touch, taste, and smell. Yet, artificial intelligence has primarily advanced in digital modalities like text, vision, and audio. This paper outlines a research vision for multisensory artificial intelligence over the next decade. This new set of technologies can change how humans and AI experience and interact with one another, by connecting AI to the human senses and a rich spectrum of signals from physiological and tactile cues on the body, to physical and social signals in homes, cities, and the environment. We outline how this field must advance through three interrelated themes of sensing, science, and synergy. Firstly, research in sensing should extend how AI captures the world in richer ways beyond the digital medium. Secondly, developing a principled science for quantifying multimodal heterogeneity and interactions, developing unified modeling architectures and representations, and understanding cross-modal transfer. Finally, we present new technical challenges to learn synergy between modalities and between humans and AI, covering multisensory integration, alignment, reasoning, generation, generalization, and experience. Accompanying this vision paper are a series of projects, resources, and demos of latest advances from the Multisensory Intelligence group at the MIT Media Lab, see https://mit-mi.github.io/.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对Paul Pu Liang撰写的论文“A Vision for Multisensory Intelligence: Sensing, Synergy, and Science”的全面中文摘要：</p>
<p><strong>论文题目：</strong> A Vision for Multisensory Intelligence: Sensing, Synergy, and Science (多感官智能的愿景：感知、协同与科学)</p>
<p><strong>作者：</strong> Paul Pu Liang</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题/研究问题：</strong>
该论文旨在为未来十年多感官人工智能（Multisensory Intelligence）的研究描绘一个愿景。当前人工智能主要集中在文本、视觉和音频等数字模态，而人类对世界的体验是多感官的，融合了语言、视觉、听觉、触觉、味觉和嗅觉。论文的核心问题是如何将人工智能的能力扩展到更广泛的感官领域，使其能够理解和与人类的感官以及物理世界进行更深层次的交互，从而增强人机交互、提高生产力、创造力和福祉。</p>
<p><strong>2. 关键创新/方法论贡献：</strong>
论文提出了一个多感官智能的研究框架，围绕三个相互关联的主题展开：</p>
<ul>
<li><strong>感知 (Sensing)：</strong> 强调AI需要超越传统的数字模态，以更丰富的方式感知世界，包括扩展人类的感官能力，并从生理、触觉、物理环境和社会信号中捕捉信息。这需要开发新的传感器和将这些信号转化为结构化表示的方法。</li>
<li><strong>科学 (Science)：</strong> 提出需要发展一门“多感官科学”，以系统地理解和量化不同感官模态的异质性（heterogeneity）和它们之间的相互作用。这包括开发统一的建模架构和表示方法，以及理解跨模态的知识迁移。</li>
<li><strong>协同 (Synergy)：</strong> 关注如何学习模态之间的协同作用，以及人类与多感官AI之间的协同。这涵盖了多感官整合、对齐、推理、生成、泛化和体验等方面的技术挑战，旨在实现“整体大于部分之和”的智能能力。</li>
</ul>
<p>论文还详细阐述了六个核心技术挑战：整合（Integration）、对齐（Alignment）、推理（Reasoning）、生成（Generation）、泛化（Generalization）和体验（Experience），并为每个挑战提出了开放性的研究方向。</p>
<p><strong>3. 主要成果及其意义：</strong>
该论文本身并非一项实验性研究，而是一个前瞻性的愿景陈述。其主要贡献在于：</p>
<ul>
<li><strong>定义了“多感官智能”的新研究范式：</strong> 将AI的研究范围从数字模态扩展到更广泛的物理和生物感官，为该领域的研究提供了清晰的方向和目标。</li>
<li><strong>提出了一个全面的研究框架：</strong> 通过“感知、科学、协同”三个主题，系统地梳理了多感官智能发展的关键要素。</li>
<li><strong>识别了关键的技术挑战：</strong> 详细列举了整合、对齐、推理、生成、泛化和体验等六个核心挑战，为研究人员提供了具体的研究切入点。</li>
<li><strong>强调了人机协同的重要性：</strong> 论文不仅关注AI自身能力的提升，更强调AI如何与人类协同工作，共同创造新的体验和价值。</li>
</ul>
<p>其意义在于，它为人工智能的未来发展指明了一个重要方向，有望推动AI在更广泛的现实世界应用中取得突破，从而深刻影响人类的生活方式、工作效率和创造力。</p>
<p><strong>4. 论文中提到的局限性：</strong>
论文本身是一篇愿景论文，主要侧重于提出研究方向和挑战，并未进行具体的实验验证。因此，其局限性主要体现在：</p>
<ul>
<li><strong>缺乏实证数据和具体模型：</strong> 论文主要基于理论和现有研究的趋势进行推演，并未提供具体的模型实现或实验结果来证明其愿景的可行性。</li>
<li><strong>挑战的复杂性：</strong> 论文提出的许多挑战（如跨模态的深度理解、协同作用的学习、以及与人类的无缝交互）在技术上仍然非常艰巨，实现起来需要长期的研究投入。</li>
<li><strong>对现有研究的总结和展望：</strong> 论文在一定程度上是对现有研究的总结，并在此基础上提出未来方向，其新颖性更多体现在框架的构建和前瞻性上。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
论文为未来的研究提供了丰富的方向，主要包括：</p>
<ul>
<li><strong>新型感知技术：</strong> 开发能够捕捉更多样化、更精细的物理和生物信号的传感器和数据采集方法。</li>
<li><strong>异质性建模：</strong> 研究如何有效地处理和融合不同模态之间在结构、分布和信息量上的巨大差异。</li>
<li><strong>统一的表示学习：</strong> 探索能够跨越多种模态的通用表示方法，以促进知识迁移和泛化。</li>
<li><strong>跨模态交互机制：</strong> 深入理解不同模态之间的相互作用，并开发能够有效利用这些交互来提升智能的算法。</li>
<li><strong>多感官推理和生成：</strong> 构建能够进行复杂多步推理和生成高质量、多样化多感官内容的模型。</li>
<li><strong>人机协同智能：</strong> 设计能够与人类进行自然、自适应、富有同理心交互的智能体，并实现人机共创。</li>
<li><strong>泛化能力提升：</strong> 研究如何将高资源模态的知识有效地迁移到低资源模态，以及如何实现跨多个模态的泛化。</li>
<li><strong>伦理与安全：</strong> 在追求多感官智能的同时，需要关注其在公平性、可解释性、安全性和隐私保护等方面的伦理问题。</li>
</ul>
<p><strong>对计算机视觉领域的意义：</strong>
这篇论文对计算机视觉领域具有重要的启示意义。它强调了视觉信息并非孤立存在，而是与其他感官模态相互作用，共同构成了我们对世界的理解。对于计算机视觉研究者而言，这意味着：</p>
<ul>
<li><strong>超越纯视觉的范式：</strong> 需要将视觉信息与其他模态（如语言、音频、触觉等）结合，以构建更全面、更鲁棒的视觉理解系统。</li>
<li><strong>新的数据和任务：</strong> 催生对包含多感官信息的数据集的需求，以及需要解决的新的多模态任务，例如视觉与触觉的融合理解、视觉与语言的深度协同等。</li>
<li><strong>更强的泛化和推理能力：</strong> 通过融合多感官信息，可以提升视觉模型的泛化能力，使其能够更好地理解上下文、进行推理，并适应更复杂的现实世界场景。</li>
<li><strong>人机交互的革新：</strong> 视觉在人机交互中扮演着核心角色，多感官智能的愿景将推动视觉系统在理解用户意图、情感和环境方面取得更大进展，从而实现更自然、更智能的人机交互。</li>
</ul>
<p>总而言之，这篇论文为人工智能的未来发展描绘了一个激动人心的多感官智能愿景，并为相关领域的研究者提供了清晰的路线图和挑战。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This new set of technologies can change how humans and AI experience and interact with one another, by connecting AI to the human senses and a rich spectrum of signals from physiological and tactile cues on the body, to physical and social signals in homes, cities, and the environment.</li>
<li>Finally, we present new technical challenges to learn synergy between modalities and between humans and AI, covering multisensory integration, alignment, reasoning, generation, generalization, and experience.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.04563v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.04563v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.05251v1'></a></p>
<h2 id="mesh4d-4d-mesh-reconstruction-and-tracking-from-monocular-video"><a href="https://arxiv.org/abs/2601.05251v1">Mesh4D: 4D Mesh Reconstruction and Tracking from Monocular Video</a></h2>
<p><strong>Authors:</strong> Zeren Jiang, Chuanxia Zheng, Iro Laina, Diane Larlus, Andrea Vedaldi</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We propose Mesh4D, a feed-forward model for monocular 4D mesh reconstruction. Given a monocular video of a dynamic object, our model reconstructs the object's complete 3D shape and motion, represented as a deformation field. Our key contribution is a compact latent space that encodes the entire animation sequence in a single pass. This latent space is learned by an autoencoder that, during training, is guided by the skeletal structure of the training objects, providing strong priors on plausible deformations. Crucially, skeletal information is not required at inference time. The encoder employs spatio-temporal attention, yielding a more stable representation of the object's overall deformation. Building on this representation, we train a latent diffusion model that, conditioned on the input video and the mesh reconstructed from the first frame, predicts the full animation in one shot. We evaluate Mesh4D on reconstruction and novel view synthesis benchmarks, outperforming prior methods in recovering accurate 3D shape and deformation.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Mesh4D: 4D Mesh Reconstruction and Tracking from Monocular Video”的全面中文摘要，重点关注其研究问题、方法创新、主要结果、局限性以及未来研究方向：</p>
<p><strong>论文题目：</strong> Mesh4D: 4D Mesh Reconstruction and Tracking from Monocular Video
<strong>作者：</strong> Zeren Jiang, Chuanxia Zheng, Iro Laina, Diane Larlus, Andrea Vedaldi</p>
<p><strong>摘要</strong></p>
<p><strong>1. 研究问题：</strong>
该论文旨在解决一个核心的计算机视觉挑战：<strong>从单目视频中重建动态物体的完整三维形状和运动（即4D网格重建与跟踪）</strong>。传统的单目4D重建方法往往难以捕捉物体的完整三维结构，尤其是在物体被遮挡或运动复杂的情况下，并且通常需要逐帧独立处理，难以实现全局的、连贯的运动跟踪。</p>
<p><strong>2. 主要创新与方法贡献：</strong>
Mesh4D 提出了一种新颖的<strong>前馈模型</strong>，其核心创新在于：</p>
<ul>
<li><strong>紧凑的潜在空间编码：</strong> 引入了一个创新的<strong>变分自编码器（VAE）</strong>，能够将整个动画序列的变形信息编码到一个紧凑的潜在空间中，实现“一次性”编码。</li>
<li><strong>利用骨骼结构进行训练（推理时无需）：</strong> 在VAE训练阶段，利用训练对象的<strong>骨骼结构</strong>作为先验信息，指导模型学习更合理的变形模式。这一点至关重要，因为它在推理时不需要骨骼信息，大大扩展了模型的适用性。</li>
<li><strong>时空注意力机制：</strong> VAE的编码器采用了<strong>时空注意力（spatio-temporal attention）</strong>机制，能够捕捉物体上不同点之间的长期时空关联，从而获得更稳定、更准确的整体变形表示。</li>
<li><strong>基于潜在扩散模型的生成：</strong> 借鉴了扩散模型的强大生成能力，Mesh4D训练了一个<strong>潜在扩散模型</strong>，以输入视频和第一帧重建的网格作为条件，一次性预测出完整的动画变形场。</li>
<li><strong>端到端的前馈框架：</strong> 整个模型是一个端到端的前馈网络，无需逐帧优化或复杂的后处理，大大提高了效率和灵活性。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
Mesh4D 在重建和新视角合成（NVS）的基准测试中取得了显著成果，<strong>超越了现有方法</strong>，尤其在恢复准确的3D形状和变形方面表现出色。</p>
<ul>
<li><strong>几何重建与跟踪：</strong> 在提出的基准测试（基于Objaverse数据集）上，Mesh4D 实现了<strong>最先进的几何重建和跟踪性能</strong>。</li>
<li><strong>新视角合成：</strong> 在新视角合成任务上，Mesh4D 在帧间质量和视频一致性方面均取得了最佳结果，能够生成更平滑、更连贯的动态视频。</li>
<li><strong>鲁棒性：</strong> 该方法能够处理各种物体和动画，并且通过时空注意力机制，能够更好地处理遮挡和复杂运动。</li>
<li><strong>效率：</strong> 作为前馈模型，Mesh4D 相比于优化类方法，在推理速度上具有明显优势。</li>
</ul>
<p><strong>4. 提及的局限性：</strong>
论文中也指出了 Mesh4D 的一些局限性：</p>
<ul>
<li><strong>对高质量初始网格的依赖：</strong> 模型在训练阶段依赖于高质量的<strong>初始网格和骨骼信息</strong>。如果第一帧的初始网格重建不准确（例如，无法正确预测分离的腿部），可能会影响后续的4D重建结果。</li>
<li><strong>拓扑变化限制：</strong> 对于动画过程中<strong>拓扑变化非常剧烈</strong>的物体，模型可能难以准确捕捉。</li>
<li><strong>非刚性变形的挑战：</strong> 重建<strong>极端非刚性</strong>的物体仍然是一个挑战。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
基于论文的局限性和研究内容，可以推测出以下潜在的未来研究方向：</p>
<ul>
<li><strong>改进初始网格重建：</strong> 探索更鲁棒的初始网格重建方法，或者开发能够自动选择最佳参考帧的机制，以应对初始网格不准确的情况。</li>
<li><strong>处理剧烈的拓扑变化：</strong> 研究能够动态适应和重建拓扑变化的4D重建模型。</li>
<li><strong>更广泛的物体类别和运动：</strong> 扩展模型以处理更广泛的物体类别，特别是那些具有复杂拓扑结构或极端非刚性变形的物体。</li>
<li><strong>无监督或弱监督学习：</strong> 探索减少对骨骼等显式监督信号的依赖，进一步走向无监督或弱监督的4D重建。</li>
<li><strong>实时性提升：</strong> 进一步优化模型结构和推理过程，以实现更高质量的实时4D重建。</li>
</ul>
<p>总而言之，Mesh4D 是一项重要的工作，它通过创新的潜在空间编码、时空注意力机制和基于扩散模型的生成方法，显著提升了单目视频4D网格重建和跟踪的性能，为动态场景理解和三维内容生成开辟了新的可能性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose Mesh4D, a feed-forward model for monocular 4D mesh reconstruction.</li>
<li>Our key contribution is a compact latent space that encodes the entire animation sequence in a single pass.</li>
<li>We evaluate Mesh4D on reconstruction and novel view synthesis benchmarks, outperforming prior methods in recovering accurate 3D shape and deformation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.05251v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.05251v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.05248v1'></a></p>
<h2 id="last_0-latent-spatio-temporal-chain-of-thought-for-robotic-vision-language-action-model"><a href="https://arxiv.org/abs/2601.05248v1">LaST<script type="math/tex">_{0}</script>: Latent Spatio-Temporal Chain-of-Thought for Robotic Vision-Language-Action Model</a></h2>
<p><strong>Authors:</strong> Zhuoyang Liu, Jiaming Liu, Hao Chen, Ziyu Guo, Chengkai Hou, Chenyang Gu, Jiale Yu, Xiangju Mi, Renrui Zhang, Zhengping Che, Jian Tang, Pheng-Ann Heng, Shanghang Zhang</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Categories:</strong> cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Vision-Language-Action (VLA) models have recently demonstrated strong generalization capabilities in robotic manipulation. Some existing VLA approaches attempt to improve action accuracy by explicitly generating linguistic reasoning traces or future visual observations before action execution. However, explicit reasoning typically incurs non-negligible inference latency, which constrains the temporal resolution required for robotic manipulation. Moreover, such reasoning is confined to the linguistic space, imposing a representational bottleneck that struggles to faithfully capture ineffable physical attributes. To mitigate these limitations, we propose LaST<script type="math/tex">_0</script>, a framework that enables efficient reasoning before acting through a Latent Spatio-Temporal Chain-of-Thought (CoT), capturing fine-grained physical and robotic dynamics that are often difficult to verbalize. Specifically, we introduce a token-efficient latent CoT space that models future visual dynamics, 3D structural information, and robot proprioceptive states, and further extends these representations across time to enable temporally consistent implicit reasoning trajectories. Furthermore, LaST<script type="math/tex">_0</script> adopts a dual-system architecture implemented via a Mixture-of-Transformers design, where a reasoning expert conducts low-frequency latent inference and an acting expert generates high-frequency actions conditioned on robotics-oriented latent representations. To facilitate coordination, LaST<script type="math/tex">_0</script> is trained with heterogeneous operation frequencies, enabling adaptive switching between reasoning and action inference rates during deployment. Across ten simulated and six real-world manipulation tasks, LaST<script type="math/tex">_0</script> improves mean success rates by 8% and 13% over prior VLA methods, respectively, while achieving substantially faster inference. Project website: https://sites.google.com/view/last0</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“LaST<script type="math/tex">_{0}</script>: Latent Spatio-Temporal Chain-of-Thought for Robotic Vision-Language-Action Model”的全面中文摘要：</p>
<p><strong>论文题目：</strong> LaST<script type="math/tex">_{0}</script>: Latent Spatio-Temporal Chain-of-Thought for Robotic Vision-Language-Action Model</p>
<p><strong>作者：</strong> Zhuoyang Liu, Jiaming Liu, Hao Chen, Ziyu Guo, Chengkai Hou, Chenyang Gu, Jiale Yu, Xiangju Mi, Renrui Zhang, Zhengping Che, Jian Tang, Pheng-Ann Heng, Shanghang Zhang</p>
<hr />
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
现有的视觉-语言-动作（VLA）模型在机器人操作任务中展现出强大的泛化能力。然而，一些方法通过显式生成语言推理链或预测未来视觉状态来提升动作准确性。这种显式推理带来了显著的推理延迟，限制了机器人操作所需的时间分辨率。此外，显式推理局限于语言空间，难以捕捉物理世界中难以言喻的精细属性。这导致了表示瓶颈，阻碍了对物理动态的忠实捕捉。</p>
<p><strong>2. 主要创新点/方法贡献：</strong>
为了解决上述问题，本文提出了 <strong>LaST<script type="math/tex">_{0}</script></strong>，一个创新的框架，通过 <strong>潜在时空链式思考（Latent Spatio-Temporal Chain-of-Thought, LaST CoT）</strong> 实现高效的“先推理后执行”行为。
*   <strong>潜在时空链式思考（LaST CoT）：</strong> LaST<script type="math/tex">_{0}</script> 引入了一个高效的潜在 CoT 空间，能够捕捉难以言喻的精细物理和机器人动态。该空间通过自回归方式建模未来的视觉动态、3D 结构信息和机器人本体感受状态，并将这些表示跨时间延伸，形成时间上一致的隐式推理轨迹。
*   <strong>双系统架构（Mixture-of-Transformers）：</strong> LaST<script type="math/tex">_{0}</script> 采用了一个双系统架构，由一个 <strong>推理专家（Reasoning Expert）</strong> 和一个 <strong>执行专家（Acting Expert）</strong> 组成。推理专家以低频率进行潜在推理，捕捉时空依赖性；执行专家以高频率生成动作，并以机器人导向的潜在表示为条件。两者通过共享的自注意力机制进行协调。
*   <strong>异构频率训练与部署：</strong> LaST<script type="math/tex">_{0}</script> 在训练时就考虑了异构的操作频率，使其在部署时能够自适应地切换推理和动作的频率，实现实时性。
*   <strong>高效推理机制：</strong> 通过缓存推理专家的关键值（KV）状态，执行专家在中间步骤中只需 O(1) 的时间即可访问潜在 CoT 信息，避免了重复解码，显著提升了推理效率。</p>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>性能提升：</strong> 在十个模拟任务和六个真实世界操作任务中，LaST<script type="math/tex">_{0}</script> 的平均成功率分别比现有 VLA 方法提高了 8% 和 13%。
*   <strong>效率提升：</strong> LaST<script type="math/tex">_{0}</script> 的推理速度显著快于显式 CoT 方法，在 RTX 4090 GPU 上达到了 15.4 Hz 的推理速度（1:4 快速-慢速频率比），同时保持了与 πο.5 (13.8 Hz) 相当的效率。
*   <strong>长时序鲁棒性：</strong> 在一个多步真实世界任务中，LaST<script type="math/tex">_{0}</script> 实现了近 5 倍于先前方法的成功率，表明其在长时间序列任务中保持连贯潜在表征的能力。
*   <strong>注意力机制分析：</strong> 与无 CoT 和显式 CoT 的方法相比，LaST<script type="math/tex">_{0}</script> 的注意力热图显示出更集中的模式，突显了其对时空信息的优越理解。
*   <strong>消融实验验证：</strong> 消融研究证明了多模态潜在表示（视觉、点云、本体感受）、适当的潜在 token 数量、足够的时间覆盖范围以及合理的专家协作频率对模型性能的重要性。</p>
<p><strong>4. 提及的局限性：</strong>
论文中并未明确列出局限性，但从其研究方向和方法来看，可以推测：
*   <strong>潜在空间的表示能力：</strong> 虽然 LaST CoT 能够捕捉精细动态，但其表示能力仍可能受到潜在空间维度的限制，对于极其复杂或精细的物理交互可能仍有提升空间。
*   <strong>训练数据的依赖性：</strong> 与大多数 VLA 模型一样，LaST<script type="math/tex">_{0}</script> 的性能也依赖于大规模、高质量的机器人操作数据集。
*   <strong>泛化到全新任务的能力：</strong> 虽然模型在多种任务上表现出色，但其在完全未见过的新颖任务上的泛化能力仍需进一步验证。</p>
<p><strong>5. 未来研究方向：</strong>
*   <strong>更丰富的潜在时空推理空间：</strong> 探索更具表现力和结构化的物理抽象，以实现更精细的推理。
*   <strong>高级的训练策略：</strong> 研究通过强化学习联合优化潜在推理和动作生成，以及利用延迟奖励来扩展模型在更复杂、接触性强的动态环境中的能力。
*   <strong>扩展到更复杂的场景：</strong> 将模型应用于更具挑战性的长时序操作任务，并考虑动态变化的环境。</p>
<p><strong>论文的创新性/重要性：</strong>
LaST<script type="math/tex">_{0}</script> 的核心贡献在于其 <strong>潜在时空链式思考（LaST CoT）</strong> 的概念，它成功地将 CoT 的推理能力从离散的语言空间转移到连续的、多模态的潜在空间。这不仅解决了显式 CoT 的延迟和表示瓶颈问题，而且通过双系统架构实现了高效的“先推理后执行”范式，在机器人操作领域取得了显著的性能和效率提升。该工作为构建更智能、更具适应性的机器人奠定了基础，尤其是在需要精细物理理解和时间连贯性的复杂操作任务中。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To mitigate these limitations, we propose LaST<script type="math/tex">_0</script>, a framework that enables efficient reasoning before acting through a Latent Spatio-Temporal Chain-of-Thought (CoT), capturing fine-grained physical and robotic dynamics that are often difficult to verbalize.</li>
<li>Specifically, we introduce a token-efficient latent CoT space that models future visual dynamics, 3D structural information, and robot proprioceptive states, and further extends these representations across time to enable temporally consistent implicit reasoning trajectories.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.05248v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.05248v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.05246v1'></a></p>
<h2 id="pixel-perfect-visual-geometry-estimation"><a href="https://arxiv.org/abs/2601.05246v1">Pixel-Perfect Visual Geometry Estimation</a></h2>
<p><strong>Authors:</strong> Gangwei Xu, Haotong Lin, Hongcheng Luo, Haiyang Sun, Bing Wang, Guang Chen, Sida Peng, Hangjun Ye, Xin Yang</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recovering clean and accurate geometry from images is essential for robotics and augmented reality. However, existing geometry foundation models still suffer severely from flying pixels and the loss of fine details. In this paper, we present pixel-perfect visual geometry models that can predict high-quality, flying-pixel-free point clouds by leveraging generative modeling in the pixel space. We first introduce Pixel-Perfect Depth (PPD), a monocular depth foundation model built upon pixel-space diffusion transformers (DiT). To address the high computational complexity associated with pixel-space diffusion, we propose two key designs: 1) Semantics-Prompted DiT, which incorporates semantic representations from vision foundation models to prompt the diffusion process, preserving global semantics while enhancing fine-grained visual details; and 2) Cascade DiT architecture that progressively increases the number of image tokens, improving both efficiency and accuracy. To further extend PPD to video (PPVD), we introduce a new Semantics-Consistent DiT, which extracts temporally consistent semantics from a multi-view geometry foundation model. We then perform reference-guided token propagation within the DiT to maintain temporal coherence with minimal computational and memory overhead. Our models achieve the best performance among all generative monocular and video depth estimation models and produce significantly cleaner point clouds than all other models.</p>
<p><strong>Analysis:</strong></p>
<p>作为一名计算机视觉和机器学习领域的专家，我对这篇论文的摘要进行了深入分析。以下是我的评估：</p>
<p><strong>1. 论文的主要贡献（2-3句话的简洁总结）</strong></p>
<p>该论文提出了“Pixel-Perfect”视觉几何估计模型，通过在像素空间利用生成式建模（特别是扩散模型），显著解决了现有几何基础模型中存在的“飞点”（flying pixels）和细节丢失问题。其核心贡献在于引入了Pixel-Perfect Depth (PPD) 和 Pixel-Perfect Video Depth (PPVD) 模型，能够生成高质量、无飞点且细节丰富的点云，并在单目和视频深度估计任务上取得了SOTA（State-of-the-Art）性能。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>该论文的关键创新和方法论集中在以下几个方面：</p>
<ul>
<li><strong>像素空间生成式建模（Pixel-Space Generative Modeling）：</strong> 这是最核心的创新点。不同于以往可能在特征空间或隐空间进行操作，该模型直接在像素空间利用扩散模型（Diffusion Transformers, DiT）进行深度估计。这种方法有望直接生成更精细、更符合图像像素分布的深度图。</li>
<li><strong>Semantics-Prompted DiT：</strong> 为了克服像素空间扩散模型的高计算复杂度，作者引入了语义引导。通过利用现有视觉基础模型的语义表示来“提示”扩散过程，可以在保留全局语义信息的同时，显著增强对精细几何细节的恢复能力。这是一种巧妙地结合了语义理解和几何生成的策略。</li>
<li><strong>Cascade DiT 架构：</strong> 为了进一步提升效率和精度，论文采用了级联（Cascade）的DiT架构。这种设计允许模型在不同阶段逐步增加图像Token的数量，从而在保证计算效率的同时，能够捕捉更丰富和更精细的几何信息。</li>
<li><strong>Semantics-Consistent DiT (用于视频)：</strong> 针对视频深度估计，论文提出了Semantics-Consistent DiT。其核心在于从多视图几何基础模型中提取时间上一致的语义信息，并将其用于指导视频深度估计。这确保了视频序列中几何估计的连贯性。</li>
<li><strong>Reference-Guided Token Propagation (用于视频)：</strong> 为了在视频中维持时间连贯性并最小化计算和内存开销，模型采用了参考引导的Token传播机制。这意味着模型可以利用前一帧或关键帧的信息来高效地更新当前帧的深度估计，从而实现平滑的时间过渡。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<p>这篇论文对计算机视觉领域的潜在影响是深远的，主要体现在：</p>
<ul>
<li><strong>提升几何估计的质量和鲁棒性：</strong> 通过解决“飞点”和细节丢失问题，该模型有望显著提升单目和视频深度估计的准确性和视觉质量，使其在实际应用中更可靠。</li>
<li><strong>推动生成式模型在几何任务中的应用：</strong> 该研究证明了在像素空间直接应用扩散模型进行几何估计的可行性和优越性，可能会激发更多研究者探索生成式模型在其他几何任务（如3D重建、表面法线估计等）中的应用。</li>
<li><strong>为机器人和AR/VR提供更优质的几何感知：</strong> 更高质量的点云和深度图对于机器人导航、避障、SLAM（Simultaneous Localization and Mapping）以及AR/VR中的场景理解和交互至关重要。该研究的成果将直接受益于这些领域。</li>
<li><strong>促进基础模型之间的协同：</strong> 该研究巧妙地结合了视觉基础模型（用于语义引导）和几何基础模型（用于时间一致性），展示了不同类型基础模型协同工作的潜力。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>机器人学：</strong> 自动驾驶、无人机导航、服务机器人、工业自动化中的场景理解和路径规划。</li>
<li><strong>增强现实 (AR) 和虚拟现实 (VR)：</strong> 实时场景重建、物体放置、用户交互、沉浸式体验。</li>
<li><strong>3D 重建：</strong> 从单张图像或视频生成高质量的3D模型。</li>
<li><strong>计算机辅助设计 (CAD) 和制造：</strong> 从图像中提取精确的几何信息用于设计和生产。</li>
<li><strong>医学影像：</strong> 从2D医学图像中恢复3D结构信息。</li>
<li><strong>内容创作：</strong> 自动生成3D资产，为游戏、电影等行业提供支持。</li>
<li><strong>摄影和图像编辑：</strong> 智能抠图、背景替换、深度感知滤镜等。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<p>尽管摘要描绘了令人兴奋的成果，但仍可以推断出一些潜在的局限性：</p>
<ul>
<li><strong>计算复杂度：</strong> 尽管作者提出了Semantics-Prompted DiT和Cascade DiT来缓解，但像素空间扩散模型本身通常具有较高的计算和内存需求。在资源受限的设备上部署可能仍然是一个挑战。</li>
<li><strong>对视觉基础模型的依赖：</strong> Semantics-Prompted DiT的性能在一定程度上依赖于所使用的视觉基础模型的质量和泛化能力。如果基础模型在特定场景下表现不佳，可能会影响最终的几何估计结果。</li>
<li><strong>多视图几何基础模型的依赖（用于视频）：</strong> PPVD模型依赖于多视图几何基础模型来提取时间一致的语义。这意味着其性能也可能受到该多视图模型的限制，并且需要预训练或访问这样的模型。</li>
<li><strong>泛化能力（潜在）：</strong> 摘要强调了“best performance”，但并未明确说明模型在极端光照条件、纹理稀疏场景、或与训练数据分布差异较大的新场景下的泛化能力如何。</li>
<li><strong>“Pixel-Perfect”的定义：</strong> 虽然论文声称“pixel-perfect”，但实际的精度极限、对亚像素级别细节的恢复能力，以及是否能完全消除所有类型的几何伪影，仍需通过实验验证。</li>
<li><strong>训练数据需求：</strong> 训练高质量的生成式模型通常需要大量的标注数据。该模型可能也需要大规模的、高质量的几何标注数据集进行训练。</li>
</ul>
<p>总而言之，这篇论文通过在像素空间引入创新的生成式建模方法，为解决现有几何估计模型的关键痛点提供了有前景的解决方案。其技术细节，特别是语义引导和级联架构的设计，在理论上和实践上都具有很高的研究价值和应用潜力。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we present pixel-perfect visual geometry models that can predict high-quality, flying-pixel-free point clouds by leveraging generative modeling in the pixel space.</li>
<li>To address the high computational complexity associated with pixel-space diffusion, we propose two key designs: 1) Semantics-Prompted DiT, which incorporates semantic representations from vision foundation models to prompt the diffusion process, preserving global semantics while enhancing fine-grained visual details; and 2) Cascade DiT architecture that progressively increases the number of image tokens, improving both efficiency and accuracy.</li>
<li>To further extend PPD to video (PPVD), we introduce a new Semantics-Consistent DiT, which extracts temporally consistent semantics from a multi-view geometry foundation model.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.05246v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.05246v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.05244v1'></a></p>
<h2 id="grex-generalized-referring-expression-segmentation-comprehension-and-generation"><a href="https://arxiv.org/abs/2601.05244v1">GREx: Generalized Referring Expression Segmentation, Comprehension, and Generation</a></h2>
<p><strong>Authors:</strong> Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, Yu-Gang Jiang</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Referring Expression Segmentation (RES) and Comprehension (REC) respectively segment and detect the object described by an expression, while Referring Expression Generation (REG) generates an expression for the selected object. Existing datasets and methods commonly support single-target expressions only, i.e., one expression refers to one object, not considering multi-target and no-target expressions. This greatly limits the real applications of REx (RES/REC/REG). This paper introduces three new benchmarks called Generalized Referring Expression Segmentation (GRES), Comprehension (GREC), and Generation (GREG), collectively denoted as GREx, which extend the classic REx to allow expressions to identify an arbitrary number of objects. We construct the first large-scale GREx dataset gRefCOCO that contains multi-target, no-target, and single-target expressions and their corresponding images with labeled targets. GREx and gRefCOCO are designed to be backward-compatible with REx, facilitating extensive experiments to study the performance gap of the existing REx methods on GREx tasks. One of the challenges of GRES/GREC is complex relationship modeling, for which we propose a baseline ReLA that adaptively divides the image into regions with sub-instance clues and explicitly models the region-region and region-language dependencies. The proposed ReLA achieves the state-of-the-art results on the both GRES and GREC tasks. The proposed gRefCOCO dataset and method are available at https://henghuiding.github.io/GREx.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“GREx: Generalized Referring Expression Segmentation, Comprehension, and Generation”的全面中文摘要：</p>
<p><strong>论文题目：</strong> GREx: Generalized Referring Expression Segmentation, Comprehension, and Generation</p>
<p><strong>作者：</strong> Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, Yu-Gang Jiang</p>
<p><strong>摘要：</strong></p>
<p>这篇论文旨在解决现有Referring Expression (REx)任务（包括分割RES、理解REC和生成REG）在处理现实世界应用中的局限性。当前主流的REx数据集和方法主要支持<strong>单目标表达式</strong>，即一个表达式仅指向一个对象，而忽略了<strong>多目标表达式</strong>（一个表达式指向多个对象）和<strong>无目标表达式</strong>（表达式不匹配任何对象）的情况。这种局限性极大地限制了REx技术在实际场景中的应用。</p>
<p><strong>1. 主要问题或研究问题：</strong></p>
<p>论文的核心研究问题是如何克服现有REx任务在处理多目标和无目标表达式时的局限性，使其能够更灵活、更实用地应用于现实世界。具体来说，是如何扩展REx任务以支持任意数量的目标，并为此构建相应的数据集和方法。</p>
<p><strong>2. 关键创新或方法论贡献：</strong></p>
<ul>
<li><strong>提出GREx（Generalized Referring Expression）任务：</strong> 作者引入了三个新的基准任务：<strong>广义指代表达式分割 (GRES)</strong>、<strong>广义指代表达式理解 (GREC)</strong> 和 <strong>广义指代表达式生成 (GREG)</strong>。这些任务扩展了传统的REx，允许表达式指向任意数量的目标，包括多目标和无目标情况。</li>
<li><strong>构建gRefCOCO数据集：</strong> 作者构建了一个<strong>大规模的、首个支持多目标、无目标和单目标表达式的GREx数据集</strong>，名为gRefCOCO。该数据集包含带标注的目标的图像，并且与现有的RefCOCO数据集兼容，便于研究者进行实验和比较。</li>
<li><strong>提出ReLA基线方法：</strong> 针对GRES和GREC任务中复杂的<strong>关系建模</strong>挑战，作者提出了一种名为<strong>ReLA</strong>的基线方法。ReLA能够自适应地将图像划分为具有子实例线索的区域，并显式地建模<strong>区域-区域</strong>和<strong>区域-语言</strong>之间的依赖关系。该方法通过动态地聚合区域特征，提供了更灵活的建模方式。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<ul>
<li><strong>ReLA在GRES和GREC任务上取得SOTA（State-of-the-Art）结果：</strong> 作者提出的ReLA方法在GRES和GREC任务上取得了当前最优的性能。这表明其提出的关系建模方法对于处理多目标和复杂表达式至关重要。</li>
<li><strong>gRefCOCO数据集的价值：</strong> gRefCOCO数据集的发布为GREx任务的研究提供了重要的资源，促进了对更具挑战性的指代表达式理解和生成任务的研究。</li>
<li><strong>GREx任务的实用性：</strong> GREx任务的引入和gRefCOCO数据集的构建，使得指代表达式技术能够更好地应用于图像编辑、字幕生成、视频制作和人机交互等更广泛的实际应用场景。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong></p>
<ul>
<li><strong>现有REx方法在GREx任务上的不足：</strong> 论文指出，在GREx任务上，即使是针对经典REx任务训练的现有方法，在处理多目标和无目标表达式时也表现出不足。</li>
<li><strong>无目标表达式的识别挑战：</strong> 论文提到，即使在gRefCOCO数据集上，模型在识别无目标表达式方面仍有提升空间，尤其是在表达式具有欺骗性或与图像中的真实实例非常相似时。</li>
<li><strong>GREC任务中的实例区分难度：</strong> 在GREC任务中，即使模型能够正确预测目标数量，但如果预测的边界框与真实目标框的IoU阈值不匹配，仍然会导致失败。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>改进对无目标和多目标表达式的处理：</strong> 开发能够更好地理解和识别无目标表达式，以及更精细地解析多目标表达式中复杂关系和属性的模型。</li>
<li><strong>细粒度的关系建模：</strong> 进一步研究如何捕捉表达式中涉及的多个对象之间更细粒度的关系和依赖。</li>
<li><strong>鲁棒性研究：</strong> 提高模型在处理真实世界数据中的噪声、变化和不一致性方面的鲁棒性。</li>
<li><strong>长距离依赖建模：</strong> 探索更有效的方法来捕捉语言元素和视觉上下文之间的长距离依赖关系。</li>
<li><strong>计数和序数表达式的处理：</strong> 专门研究如何准确理解和响应包含计数（如“两个人”）和序数（如“左边第二个”）的表达式。</li>
<li><strong>跨模态交互和融合：</strong> 探索更创新的方法来融合视觉和语言信息，以提高理解能力。</li>
<li><strong>多语言和跨领域应用：</strong> 将GREx任务扩展到多语言和跨领域场景，以拓宽其应用范围。</li>
<li><strong>利用大型语言模型（LLMs）：</strong> 探索如何利用LLMs的常识知识和推理能力来增强对表达式的理解，尤其是在处理隐式信息和假设时。</li>
</ul>
<p>总而言之，这篇论文通过引入GREx任务、发布gRefCOCO数据集以及提出创新的ReLA方法，显著推动了指代表达式理解和生成领域的发展，使其能够更好地应对现实世界中更复杂、更多样化的场景。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This paper introduces three new benchmarks called Generalized Referring Expression Segmentation (GRES), Comprehension (GREC), and Generation (GREG), collectively denoted as GREx, which extend the classic REx to allow expressions to identify an arbitrary number of objects.</li>
<li>One of the challenges of GRES/GREC is complex relationship modeling, for which we propose a baseline ReLA that adaptively divides the image into regions with sub-instance clues and explicitly models the region-region and region-language dependencies.</li>
<li>The proposed ReLA achieves the state-of-the-art results on the both GRES and GREC tasks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.05244v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.05244v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.05243v1'></a></p>
<h2 id="generate-transfer-adapt-learning-functional-dexterous-grasping-from-a-single-human-demonstration"><a href="https://arxiv.org/abs/2601.05243v1">Generate, Transfer, Adapt: Learning Functional Dexterous Grasping from a Single Human Demonstration</a></h2>
<p><strong>Authors:</strong> Xingyi He, Adhitya Polavaram, Yunhao Cao, Om Deshmukh, Tianrui Wang, Xiaowei Zhou, Kuan Fang</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Categories:</strong> cs.RO, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Functional grasping with dexterous robotic hands is a key capability for enabling tool use and complex manipulation, yet progress has been constrained by two persistent bottlenecks: the scarcity of large-scale datasets and the absence of integrated semantic and geometric reasoning in learned models. In this work, we present CorDex, a framework that robustly learns dexterous functional grasps of novel objects from synthetic data generated from just a single human demonstration. At the core of our approach is a correspondence-based data engine that generates diverse, high-quality training data in simulation. Based on the human demonstration, our data engine generates diverse object instances of the same category, transfers the expert grasp to the generated objects through correspondence estimation, and adapts the grasp through optimization. Building on the generated data, we introduce a multimodal prediction network that integrates visual and geometric information. By devising a local-global fusion module and an importance-aware sampling mechanism, we enable robust and computationally efficient prediction of functional dexterous grasps. Through extensive experiments across various object categories, we demonstrate that CorDex generalizes well to unseen object instances and significantly outperforms state-of-the-art baselines.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Generate, Transfer, Adapt: Learning Functional Dexterous Grasping from a Single Human Demonstration”的全面中文摘要：</p>
<p><strong>论文摘要：Generate, Transfer, Adapt: Learning Functional Dexterous Grasping from a Single Human Demonstration</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>该论文旨在解决<strong>功能性灵巧抓取</strong>（functional dexterous grasping）领域面临的两个关键瓶颈：
*   <strong>大规模数据集的稀缺性：</strong> 获取包含功能性灵巧抓取标注的大规模数据集极其困难，真实世界的数据采集成本高昂且难以扩展到新物体。
*   <strong>语义与几何信息融合的不足：</strong> 现有方法往往侧重于几何推理，忽略了对物体功能至关重要的语义信息，导致模型难以在未见过的情况下生成既稳定又功能适用的抓取。</p>
<p><strong>2. 主要创新/方法贡献：</strong></p>
<p>作者提出了一个名为 <strong>CorDex</strong> 的框架，其核心创新在于：</p>
<ul>
<li>
<p><strong>基于对应关系的合成数据引擎：</strong></p>
<ul>
<li><strong>多阶段数据生成：</strong> 从单个<strong>人类演示视频</strong>出发，该引擎能够自主生成多样化、高质量的训练数据。</li>
<li><strong>多样化物体生成：</strong> 通过检索互联网图像并转换为 3D 模型，生成同一类别下具有丰富外观和形状变化的物体实例。</li>
<li><strong>跨实例抓取转移：</strong> 利用新颖的<strong>2D-3D 对应关系管道</strong>，将演示中的专家抓取（以 3D 指尖接触点表示）转移到生成的物体实例上，克服了直接 3D 匹配的局限性。</li>
<li><strong>物理信息引导的抓取适应：</strong> 通过<strong>物理模拟优化</strong>，调整转移的抓取姿态，使其同时满足功能性和稳定性要求，确保生成数据的质量。</li>
</ul>
</li>
<li>
<p><strong>多模态预测网络：</strong></p>
<ul>
<li><strong>融合视觉与几何信息：</strong> 引入一个预测模型，能够有效整合来自 RGB 图像的<strong>语义信息</strong>和深度传感器提供的<strong>几何信息</strong>。</li>
<li><strong>局部-全局特征融合模块：</strong> 设计了一个创新的模块，通过<strong>局部交叉注意力</strong>捕捉接触区域的细节，并通过<strong>全局自注意力</strong>编码整体物体上下文，实现对物体局部和全局信息的有效融合。</li>
<li><strong>重要性感知采样机制：</strong> 引入一种<strong>自适应采样</strong>策略，优先关注与接触相关的区域，提高计算效率和预测精度，避免被无关的表面点淹没。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>性能显著提升：</strong> CorDex 在模拟和真实世界实验中，对未见过物体实例和类别的功能性灵巧抓取任务上，均取得了<strong>显著优于</strong>现有最先进方法的性能。在真实世界测试中，成功率达到了 <strong>69%</strong>。</li>
<li><strong>数据生成的可扩展性：</strong> 该框架能够从单个演示视频生成大规模（900 个物体，1.08 百万张图像，11 百万个图像-抓取对）的功能性抓取数据集，大大降低了数据采集的成本和难度。</li>
<li><strong>泛化能力强：</strong> CorDex 能够很好地泛化到<strong>未见过的新物体实例</strong>，并且其数据引擎可以轻松扩展到新任务，而无需额外的训练。</li>
<li><strong>对现有方法的改进：</strong> 实验表明，即使是基于现有方法的改进版本（如在 CorDex 数据集上训练的 D(R,O)），其性能也远不及 CorDex，突显了 CorDex 的模型设计和数据生成方法的有效性。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>对深度输入的敏感性：</strong> 尽管训练中注入了深度噪声，模型在真实世界中对严重损坏或位移的深度输入仍然敏感，反映了合成与真实世界深度感知之间的领域差距。</li>
<li><strong>类别特定训练：</strong> 该框架目前仍专注于<strong>类别特定</strong>的训练，尚未完全实现对<strong>开放集场景</strong>（open-set scenarios）的泛化。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>扩展任务多样性：</strong> 未来工作应探索如何扩展任务的多样性，以实现更广泛的应用。</li>
<li><strong>开发通用模型：</strong> 目标是开发能够展现出对未见物体和任务的<strong>涌现式泛化能力</strong>（emergent generalization）的通用模型。</li>
</ul>
<p>总而言之，CorDex 框架通过创新的数据引擎和多模态预测网络，有效地解决了功能性灵巧抓取领域的数据稀缺和语义-几何融合难题，为机器人实现更高级别的工具使用和操作能力奠定了坚实的基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we present CorDex, a framework that robustly learns dexterous functional grasps of novel objects from synthetic data generated from just a single human demonstration.</li>
<li>At the core of our approach is a correspondence-based data engine that generates diverse, high-quality training data in simulation.</li>
<li>Building on the generated data, we introduce a multimodal prediction network that integrates visual and geometric information.</li>
<li>Through extensive experiments across various object categories, we demonstrate that CorDex generalizes well to unseen object instances and significantly outperforms state-of-the-art baselines.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.05243v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.05243v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.05241v1'></a></p>
<h2 id="robovip-multi-view-video-generation-with-visual-identity-prompting-augments-robot-manipulation"><a href="https://arxiv.org/abs/2601.05241v1">RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation</a></h2>
<p><strong>Authors:</strong> Boyang Wang, Haoran Zhang, Shujie Zhang, Jinkun Hao, Mingda Jia, Qi Lv, Yucheng Mao, Zhaoyang Lyu, Jia Zeng, Xudong Xu, Jiangmiao Pang</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation”的全面中文摘要，其中包含了您要求的各个部分：</p>
<p><strong>论文题目：</strong> RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation</p>
<p><strong>作者：</strong> Boyang Wang, Haoran Zhang, Shujie Zhang, Jinkun Hao, Mingda Jia, Qi Lv, Yucheng Mao, Zhaoyang Lyu, Jia Zeng, Xudong Xu, Jiangmiao Pang</p>
<hr />
<p><strong>全面摘要</strong></p>
<p><strong>1. 主要问题/研究问题：</strong></p>
<p>训练有效的机器人策略（policy）需要多样化、数量充足且高质量的操作数据。然而，由于硬件和物理设置的限制，大规模真实世界操作数据的收集在多样化的环境中难以扩展。现有方法利用文本提示条件下的图像扩散模型来增强操作数据，通过改变背景和桌面物体来丰富视觉观测。然而，这些方法忽略了最先进策略模型所需的<strong>多视图（multi-view）</strong>和<strong>时间连贯性（temporally coherent）</strong>观测的重要性。此外，纯文本提示难以精确指定场景设置。</p>
<p><strong>2. 关键创新/方法贡献：</strong></p>
<p>本文提出了 <strong>RoboVIP</strong>，一个多视图视频生成增强框架，其核心创新在于引入了<strong>视觉身份提示（visual identity prompting）</strong>。</p>
<ul>
<li><strong>视觉身份提示：</strong> 该方法使用示例图像作为条件输入，指导扩散模型生成期望的场景设置，从而提供比文本提示更精细、更具语义一致性的视觉引导。</li>
<li><strong>多视图视频生成：</strong> RoboVIP 专注于生成时间连贯的多视图视频序列，以满足现代策略模型对丰富空间信息的需求。</li>
<li><strong>自动化分割流水线：</strong> 为了实现多视图视频的增强，论文开发了一个自动化的分割流水线，能够准确分割出机器人手臂和交互物体。该流水线利用动作信息来克服现有模型在定位目标物体时的困难，尤其是在手腕摄像头视角下。</li>
<li><strong>大规模视觉身份库构建：</strong> 为了实现“即插即用”（plug-and-play）的视觉身份提示，论文构建了一个可扩展的流水线，从大型机器人数据集中自动策划和过滤，构建了一个包含数百万个视觉身份的库，无需人工干预。</li>
<li><strong>多视图视频扩散模型：</strong> RoboVIP 集成了多视图视频扩散模型，该模型能够处理多视图输入，并结合文本提示和视觉身份提示进行条件生成。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<ul>
<li><strong>性能提升：</strong> 使用 RoboVIP 生成的增强数据训练下游的视觉-语言-动作（VLA）和视觉运动（visuomotor）策略模型，在<strong>模拟环境</strong>和<strong>真实机器人</strong>设置中均取得了<strong>一致的性能提升</strong>。</li>
<li><strong>在模拟环境中：</strong> 在 SimplerEnv 模拟器上，RoboVIP 增强的数据显著提高了 Octo 和 πο 这两个主流 VLA 模型在各种任务上的成功率，尤其是在更具挑战性的“放置”（Put）阶段，显示出更强的闭环控制能力和任务完成可靠性。RoboVIP 增强的数据甚至在某些情况下超越了使用真实数据进行微调的效果。</li>
<li><strong>在真实机器人环境中：</strong> 在真实机器人实验中，RoboVIP 增强的数据显著提高了 Diffusion Policy 模型在<strong>杂乱场景（cluttered scene）</strong>下的鲁棒性和泛化能力，成功率远超其他基线方法，证明了其在应对真实世界视觉干扰方面的有效性。</li>
<li><strong>视觉质量和一致性：</strong> 通过用户研究和可视化结果表明，RoboVIP 生成的视频在<strong>身份保持</strong>和<strong>场景丰富度</strong>方面表现出色，能够生成更具挑战性、更逼真的桌面内容。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>现有工具的局限性：</strong> 论文指出，尽管 RoboVIP 实现了大规模数据增强的自动化，但仍受限于当前工具的能力。例如，最先进的视频分割模型在抓手定位和闪烁方面仍有困难；视觉语言模型（VLM）在识别交互物体方面可能失败；开放词汇分割模型可能产生不一致的掩码。</li>
<li><strong>模拟环境的限制：</strong> 虽然论文在 SimplerEnv 模拟器上进行了评估，但该模拟器仅支持单视图输入，未能充分评估多视图一致性训练的全部优势。</li>
<li><strong>数据预处理的挑战：</strong> 对于长视频数据，需要进行时间分割以避免扩散模型因输入过长而出现分割失败。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>更强大的分割和 VLM 模型：</strong> 进一步提升视频分割和视觉语言模型的能力，以更准确地识别交互物体和处理复杂的场景。</li>
<li><strong>更广泛的模拟环境评估：</strong> 开发或利用支持多视图输入的模拟环境，以更全面地评估多视图一致性训练的效益。</li>
<li><strong>长时序策略的增强：</strong> RoboVIP 的视频生成能力为未来需要长时序上下文的 VLA 训练提供了新的方向。</li>
<li><strong>更精细的视觉身份控制：</strong> 探索更精细的视觉身份控制机制，以实现更具创造性和多样性的场景生成。</li>
</ul>
<p><strong>论文对计算机视觉领域的新颖性/重要性：</strong></p>
<p>RoboVIP 的主要贡献在于将<strong>视觉身份提示</strong>这一概念引入到机器人操作数据的增强中，并成功地将其与<strong>多视图视频生成</strong>相结合。这解决了现有方法在生成数据时对<strong>时间连贯性</strong>和<strong>多视图信息</strong>的忽视，以及纯文本提示的局限性。通过构建自动化的视觉身份库和高效的视频扩散模型，RoboVIP 提供了一种可扩展、即插即用的数据增强解决方案，显著提升了机器人策略的学习效果，尤其是在复杂和多样化的真实世界场景中。这为解决机器人数据稀疏性问题提供了一个有前景的方向，并对计算机视觉在机器人领域的应用具有重要意义。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models.</li>
<li>To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.05241v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.05241v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.05239v1'></a></p>
<h2 id="plenoptic-video-generation"><a href="https://arxiv.org/abs/2601.05239v1">Plenoptic Video Generation</a></h2>
<p><strong>Authors:</strong> Xiao Fu, Shitao Tang, Min Shi, Xian Liu, Jinwei Gu, Ming-Yu Liu, Dahua Lin, Chen-Hsuan Lin</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/</p>
<p><strong>Analysis:</strong></p>
<p>None</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory.</li>
<li>Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation).</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.05239v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.05239v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.05237v1'></a></p>
<h2 id="objectforesight-predicting-future-3d-object-trajectories-from-human-videos"><a href="https://arxiv.org/abs/2601.05237v1">ObjectForesight: Predicting Future 3D Object Trajectories from Human Videos</a></h2>
<p><strong>Authors:</strong> Rustin Soraki, Homanga Bharadhwaj, Ali Farhadi, Roozbeh Mottaghi</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Humans can effortlessly anticipate how objects might move or change through interaction--imagining a cup being lifted, a knife slicing, or a lid being closed. We aim to endow computational systems with a similar ability to predict plausible future object motions directly from passive visual observation. We introduce ObjectForesight, a 3D object-centric dynamics model that predicts future 6-DoF poses and trajectories of rigid objects from short egocentric video sequences. Unlike conventional world or dynamics models that operate in pixel or latent space, ObjectForesight represents the world explicitly in 3D at the object level, enabling geometrically grounded and temporally coherent predictions that capture object affordances and trajectories. To train such a model at scale, we leverage recent advances in segmentation, mesh reconstruction, and 3D pose estimation to curate a dataset of 2 million plus short clips with pseudo-ground-truth 3D object trajectories. Through extensive experiments, we show that ObjectForesight achieves significant gains in accuracy, geometric consistency, and generalization to unseen objects and scenes, establishing a scalable framework for learning physically grounded, object-centric dynamics models directly from observation. objectforesight.github.io</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“ObjectForesight: Predicting Future 3D Object Trajectories from Human Videos”的全面中文摘要，重点突出其研究问题、创新贡献、主要结果、局限性和未来方向：</p>
<p><strong>论文题目：</strong> ObjectForesight: Predicting Future 3D Object Trajectories from Human Videos
<strong>作者：</strong> Rustin Soraki, Homanga Bharadhwaj, Ali Farhadi, Roozbeh Mottaghi</p>
<p><strong>摘要：</strong></p>
<p>这篇论文旨在解决一个核心问题：<strong>如何让计算系统能够像人类一样，仅通过被动观察日常人类活动视频，就能预测出物体在三维空间中的未来运动轨迹。</strong> 换句话说，研究者们希望赋予机器“预见”物体如何移动和互动（例如，杯子被拿起、刀具被滑动、盖子被合上）的能力，而无需直接建模人类的动作。</p>
<p><strong>核心创新与方法贡献：</strong></p>
<ol>
<li><strong>提出3D物体动力学预测任务：</strong> 论文正式定义并形式化了从人类视频中预测3D物体动力学的任务，为该领域的研究建立了一个标准化的设置。</li>
<li><strong>ObjectForesight模型：</strong> 提出了一个名为ObjectForesight的<strong>3D物体中心（object-centric）的向前动力学模型</strong>。该模型的核心在于：<ul>
<li><strong>显式的3D物体表示：</strong> 与许多在像素或潜在空间操作的模型不同，ObjectForesight在3D空间中显式地表示物体，并以物体为中心进行推理。这使得模型能够生成几何上合理且时间上连贯的预测，并捕捉物体的“可供性”（affordances）。</li>
<li><strong>6-DoF轨迹预测：</strong> 模型能够预测刚性物体未来6自由度（6-DoF）的位姿（pose）和轨迹。</li>
<li><strong>基于扩散的Transformer架构：</strong> 模型结合了一个<strong>几何感知的三维点编码器（PointTransformerV3）</strong>来理解场景和物体几何，以及一个<strong>基于扩散的Transformer（DiT）</strong>来生成多样化、物理上一致的未来轨迹。这种架构能够处理多模态预测，即一个输入可能对应多种可能的未来运动。</li>
<li><strong>深度归一化位姿表示：</strong> 为了提高数值稳定性和训练效率，模型将位姿表示为深度归一化的9D位姿（pose）token。</li>
</ul>
</li>
<li><strong>大规模数据集的构建：</strong> 为了训练如此复杂的模型，研究者们面临数据稀缺的挑战。他们开发了一个<strong>自动化的数据策管流程</strong>，从大量的（200万+）<strong>EPIC-Kitchens</strong>视频片段中提取了伪地面真实（pseudo-ground-truth）的3D物体轨迹。该流程利用了先进的分割、网格重建和3D位姿估计技术，将普通的视频转化为具有丰富语义和物理约束的训练数据。</li>
</ol>
<p><strong>主要结果与意义：</strong></p>
<ul>
<li><strong>显著的性能提升：</strong> ObjectForesight在Epic-Kitchens和HOT3D-Clips数据集上，在多种3D轨迹预测指标（包括平移和旋转误差）上都取得了显著的性能提升，<strong>大幅优于</strong>其非扩散的自回归基线模型（ObjectForesight-AR）。</li>
<li><strong>超越视频生成基线：</strong> 与先进的视频生成模型（如Luma AI Ray3）相比，ObjectForesight在预测物体轨迹方面表现出<strong>更强的鲁棒性、一致性和准确性</strong>。这突显了直接进行3D推理的优势，而非仅仅合成图像。</li>
<li><strong>泛化能力：</strong> 模型在<strong>未见过（unseen）的物体和场景</strong>上表现出良好的泛化能力，证明了其学习到的物理动力学知识的普适性。</li>
<li><strong>物理一致性：</strong> 模型生成的轨迹不仅在几何上准确，而且在物理上也是<strong>连贯且可信的</strong>，能够捕捉到真实的物体互动动态。</li>
<li><strong>可扩展性：</strong> 该研究建立了一个<strong>可扩展的框架</strong>，能够从被动观察中学习物理上合理的、物体中心的动力学模型。</li>
</ul>
<p><strong>论文中提到的局限性：</strong></p>
<ul>
<li><strong>刚性物体假设：</strong> 当前的ObjectForesight模型主要关注<strong>刚性物体</strong>的预测。对于柔性、可变形或关节式物体，其能力受到限制。</li>
<li><strong>短视界预测：</strong> 模型在预测<strong>短时间跨度</strong>（例如，几秒钟）的轨迹方面表现最佳。对于更长远的预测，误差会累积，性能会下降。</li>
<li><strong>对数据质量的依赖：</strong> 虽然论文构建了大规模数据集，但其质量依赖于自动化流程的准确性，尤其是在处理遮挡、模糊等复杂情况时。</li>
</ul>
<p><strong>潜在的未来研究方向：</strong></p>
<ul>
<li><strong>扩展到非刚性物体：</strong> 将模型的能力扩展到处理<strong>柔性、可变形或关节式物体</strong>，例如衣物、绳索或机器人手臂。</li>
<li><strong>更长视界的预测：</strong> 进一步研究如何提高模型在<strong>更长预测视界</strong>下的准确性和稳定性，可能需要更先进的长期依赖建模技术。</li>
<li><strong>更精细的交互建模：</strong> 探索更精细的<strong>人与物体交互</strong>的建模，例如预测人类的意图、抓取点或更复杂的操纵行为。</li>
<li><strong>集成到机器人控制：</strong> 将ObjectForesight模型集成到<strong>机器人控制系统</strong>中，实现更智能、更具适应性的机器人操作。</li>
<li><strong>更鲁棒的数据处理：</strong> 开发更先进的技术来处理<strong>低质量或不完整</strong>的视频数据，以进一步扩大训练数据的覆盖范围。</li>
</ul>
<p>总而言之，ObjectForesight是3D物体动力学预测领域的一项重要进展，它通过创新的模型架构和大规模数据集的构建，显著提升了从视频中预测物体未来运动的能力，为实现更具智能的视觉理解和机器人交互奠定了坚实的基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce ObjectForesight, a 3D object-centric dynamics model that predicts future 6-DoF poses and trajectories of rigid objects from short egocentric video sequences.</li>
<li>Through extensive experiments, we show that ObjectForesight achieves significant gains in accuracy, geometric consistency, and generalization to unseen objects and scenes, establishing a scalable framework for learning physically grounded, object-centric dynamics models directly from observation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.05237v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.05237v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.05230v1'></a></p>
<h2 id="learning-latent-action-world-models-in-the-wild"><a href="https://arxiv.org/abs/2601.05230v1">Learning Latent Action World Models In The Wild</a></h2>
<p><strong>Authors:</strong> Quentin Garrido, Tushar Nagarajan, Basile Terver, Nicolas Ballas, Yann LeCun, Michael Rabbat</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Categories:</strong> cs.AI, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Agents capable of reasoning and planning in the real world require the ability of predicting the consequences of their actions. While world models possess this capability, they most often require action labels, that can be complex to obtain at scale. This motivates the learning of latent action models, that can learn an action space from videos alone. Our work addresses the problem of learning latent actions world models on in-the-wild videos, expanding the scope of existing works that focus on simple robotics simulations, video games, or manipulation data. While this allows us to capture richer actions, it also introduces challenges stemming from the video diversity, such as environmental noise, or the lack of a common embodiment across videos. To address some of the challenges, we discuss properties that actions should follow as well as relevant architectural choices and evaluations. We find that continuous, but constrained, latent actions are able to capture the complexity of actions from in-the-wild videos, something that the common vector quantization does not. We for example find that changes in the environment coming from agents, such as humans entering the room, can be transferred across videos. This highlights the capability of learning actions that are specific to in-the-wild videos. In the absence of a common embodiment across videos, we are mainly able to learn latent actions that become localized in space, relative to the camera. Nonetheless, we are able to train a controller that maps known actions to latent ones, allowing us to use latent actions as a universal interface and solve planning tasks with our world model with similar performance as action-conditioned baselines. Our analyses and experiments provide a step towards scaling latent action models to the real world.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：Learning Latent Action World Models In The Wild</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话的简洁总结）</strong></p>
<p>这篇论文提出了一种在野外视频（in-the-wild videos）中学习潜在动作世界模型的方法。该方法能够从无标签的视频数据中学习动作表示，克服了传统世界模型依赖显式动作标签的局限性，并成功捕捉了复杂、多样化场景下的动作信息，为实现更通用的智能体规划能力迈出了重要一步。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<ul>
<li><strong>在野外视频上学习潜在动作：</strong> 这是最核心的创新点。以往的工作多集中在受控的模拟环境、游戏或特定机器人操作数据上。而“in-the-wild videos”意味着数据来源极其多样，包含各种环境噪声、视角变化、非标准化的动作以及不同主体（如人类）的出现，这极大地增加了学习的难度，但也使得模型更具普适性。</li>
<li><strong>学习动作空间而非依赖预定义标签：</strong> 论文的核心在于“latent action models”，即模型自主学习动作的潜在表示，而不是依赖于人类提供的、可能难以获取且不完整的动作标签。这使得模型能够发现视频中隐藏的、更精细的动作模式。</li>
<li><strong>连续但受限的潜在动作表示：</strong> 论文发现，使用连续的、但具有一定约束的潜在动作表示比离散的向量量化（Vector Quantization）更能捕捉到野外视频中动作的复杂性。这暗示了动作的连续性和平滑性在现实世界中是重要的。</li>
<li><strong>跨视频的动作迁移能力：</strong> 论文展示了模型能够将一个视频中观察到的环境变化（例如，有人进入房间）迁移到另一个视频中，这表明学习到的潜在动作具有一定的泛化性和对环境动态的理解能力。</li>
<li><strong>相机相对的局部化动作表示：</strong> 由于缺乏共同的具身（embodiment），论文承认学习到的潜在动作在空间上是相对于相机进行定位的。这是一个重要的观察，也指出了未来研究的方向。</li>
<li><strong>控制器将已知动作映射到潜在动作：</strong> 为了解决潜在动作的通用性问题，论文提出训练一个控制器，将已知的、具体的动作映射到学习到的潜在动作上。这使得潜在动作可以作为一种“通用接口”，用于规划任务。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<ul>
<li><strong>降低世界模型训练门槛：</strong> 解决了获取大量动作标签的难题，使得构建更强大的世界模型成为可能，尤其是在需要处理海量真实世界视频数据的场景下。</li>
<li><strong>提升智能体在真实世界中的规划和推理能力：</strong> 能够从无监督的视频数据中学习动作的因果关系，将极大地增强智能体在复杂、动态的真实环境中进行预测和规划的能力。</li>
<li><strong>推动通用人工智能（AGI）的发展：</strong> 学习能够理解和预测真实世界动态的“世界模型”是实现AGI的关键一步。这项工作通过处理更具挑战性的数据，为这一目标贡献了重要力量。</li>
<li><strong>促进跨领域迁移学习：</strong> 学习到的通用动作表示可能有助于将知识从一个领域迁移到另一个领域，减少对特定领域数据的依赖。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>机器人学：</strong> 机器人可以通过学习野外视频中的动作来理解和模仿人类行为，从而实现更自然的交互和更复杂的任务执行。</li>
<li><strong>自动驾驶：</strong> 预测其他车辆、行人或其他动态物体的行为是自动驾驶的关键。这项研究可以帮助模型从海量交通视频中学习这些行为模式。</li>
<li><strong>视频理解和内容生成：</strong> 更好地理解视频中的动作和因果关系，可以用于更智能的视频搜索、摘要、推荐，甚至生成更逼真的视频内容。</li>
<li><strong>人机交互：</strong> 智能助手或虚拟角色可以更好地理解用户的意图和行为，从而提供更个性化和有效的服务。</li>
<li><strong>行为分析和监控：</strong> 在安防、医疗等领域，可以用于分析和预测人群或个体的行为模式。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<ul>
<li><strong>动作的局部化：</strong> 论文明确指出，由于缺乏共同的具身，学习到的潜在动作主要在空间上相对于相机进行定位。这意味着模型可能难以理解绝对的空间关系或跨不同视角下的同一动作。</li>
<li><strong>对“环境变化”的理解可能受限于视频内容：</strong> 虽然提到了“环境变化”，但这种变化是来自“agents，such as humans entering the room”。这意味着模型对环境变化的理解可能主要集中在由动态物体（尤其是人类）引起的变化，而对其他类型的环境变化（如天气变化、物体被移除等）的捕捉能力可能有限。</li>
<li><strong>“已知动作”到“潜在动作”的映射：</strong> 尽管提出了控制器来解决通用性问题，但这种映射的有效性和鲁棒性在多大程度上依赖于“已知动作”的质量和覆盖范围，以及控制器本身的泛化能力，这在摘要中并未深入说明。</li>
<li><strong>对“in-the-wild”视频多样性的完全处理：</strong> 尽管论文声称扩展了范围，但“in-the-wild”视频的挑战是巨大的，例如极端的视角变化、遮挡、低分辨率、模糊等。摘要中提到的“environmental noise”和“lack of a common embodiment”是部分挑战，但可能还有其他未提及的挑战。</li>
<li><strong>评估的全面性：</strong> 摘要提到了“relevant architectural choices and evaluations”，但具体评估指标和任务的全面性（例如，是否进行了长时序预测、因果推理等）需要阅读全文才能了解。</li>
</ul>
<p><strong>总结一下，这篇论文的价值在于它将世界模型的学习从受控环境推向了更具挑战性的“in-the-wild”视频场景，并提出了一种从无标签数据中学习通用动作表示的方法。这对于构建更具普适性和智能性的AI系统具有重要的理论和实践意义。</strong></p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Nonetheless, we are able to train a controller that maps known actions to latent ones, allowing us to use latent actions as a universal interface and solve planning tasks with our world model with similar performance as action-conditioned baselines.</li>
<li>Our analyses and experiments provide a step towards scaling latent action models to the real world.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.05230v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.05230v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2026-01-09 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
