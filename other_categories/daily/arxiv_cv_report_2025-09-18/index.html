<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-09-18 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DEâ«¶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Birdâs-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">è®ºæéè¯»</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-Î±
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-09-17/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-09-19/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-09-18">Arxiv Computer Vision Papers - 2025-09-18</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#sail-vl2-technical-report" class="nav-link">SAIL-VL2 Technical Report</a>
                </li>
                <li class="nav-item">
                    <a href="#panorama-the-rise-of-omnidirectional-vision-in-the-embodied-ai-era" class="nav-link">PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era</a>
                </li>
                <li class="nav-item">
                    <a href="#mars2-2025-challenge-on-multimodal-reasoning-datasets-methods-results-discussion-and-outlook" class="nav-link">MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods, Results, Discussion, and Outlook</a>
                </li>
                <li class="nav-item">
                    <a href="#wan-animate-unified-character-animation-and-replacement-with-holistic-replication" class="nav-link">Wan-Animate: Unified Character Animation and Replacement with Holistic Replication</a>
                </li>
                <li class="nav-item">
                    <a href="#noise-level-diffusion-guidance-well-begun-is-half-done" class="nav-link">Noise-Level Diffusion Guidance: Well Begun is Half Done</a>
                </li>
                <li class="nav-item">
                    <a href="#map-end-to-end-autonomous-driving-with-map-assisted-planning" class="nav-link">MAP: End-to-End Autonomous Driving with Map-Assisted Planning</a>
                </li>
                <li class="nav-item">
                    <a href="#evhand-fpv-efficient-event-based-3d-hand-tracking-from-first-person-view" class="nav-link">EvHand-FPV: Efficient Event-Based 3D Hand Tracking from First-Person View</a>
                </li>
                <li class="nav-item">
                    <a href="#edits-enhancing-dataset-distillation-with-implicit-textual-semantics" class="nav-link">EDITS: Enhancing Dataset Distillation with Implicit Textual Semantics</a>
                </li>
                <li class="nav-item">
                    <a href="#ndlpnet-a-location-aware-nighttime-deraining-network-and-a-real-world-benchmark-dataset" class="nav-link">NDLPNet: A Location-Aware Nighttime Deraining Network and a Real-World Benchmark Dataset</a>
                </li>
                <li class="nav-item">
                    <a href="#cross-modal-full-mode-fine-grained-alignment-for-text-to-image-person-retrieval" class="nav-link">Cross-modal Full-mode Fine-grained Alignment for Text-to-Image Person Retrieval</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-09-18">Arxiv Computer Vision Papers - 2025-09-18</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>å¥½çï¼è¿æ¯ä¸ä»½éå¯¹2025å¹´9æ17æ¥Arxivè®¡ç®æºè§è§è®ºæçæ§è¡æè¦ï¼æ¨å¨å¸®å©å¿ç¢çç ç©¶äººåå¿«éäºè§£ææ°è¿å±ã</p>
<hr />
<p><strong>Arxiv è®¡ç®æºè§è§æ¯æ¥æ¥åæ§è¡æè¦ (2025-09-17)</strong></p>
<p><strong>æ¦è¿°ï¼</strong>
ä»æ¥Arxivè®¡ç®æºè§è§è®ºæåç°åºå¤æ¨¡æå­¦ä¹ ãå·èº«æºè½ãçææ¨¡ååå®éåºç¨åºæ¯ï¼å¦èªå¨é©¾é©¶ãå¤é´å»é¨ï¼çå¼ºå²åå±è¶å¿ãç¹å«æ¯ï¼å¤§åè§è§è¯­è¨æ¨¡åï¼VLMsï¼çè¿æ­¥ãå¨åè§è§å¨å·èº«AIä¸­çå´èµ·ä»¥åé«ææ°æ®å¤çåæ¨¡åè®­ç»æ¹æ³æ¯æ ¸å¿ä¸»é¢ã</p>
<p><strong>ä¸»è¦ä¸»é¢ä¸è¶å¿ï¼</strong></p>
<ol>
<li><strong>å¤æ¨¡æä¸å¤§åæ¨¡åï¼</strong> å¤ç¯è®ºæèç¦äºè§è§è¯­è¨æ¨¡åï¼VLMsï¼çæ©å±ãè¯ä¼°ååºç¨ï¼å¼ºè°å¶å¨å¤ææ¨çä»»å¡ä¸­çæ½åã</li>
<li><strong>å·èº«æºè½ä¸å¨åè§è§ï¼</strong> å·èº«AIé¢åæç»­åæ¸©ï¼å¨åè§è§ä½ä¸ºå³é®æç¥æ¨¡å¼åå°é«åº¦å³æ³¨ï¼æ¨å¨ä¸ºæºè½ä½æä¾æ´å¨é¢çç¯å¢çè§£ã</li>
<li><strong>çææ¨¡åä¸æ©æ£æ¨¡åï¼</strong> æ©æ£æ¨¡åå¨å¾åçæåå¨ç»é¢åçåºç¨è¿ä¸æ­¥æ·±åï¼ç ç©¶äººåè´åäºæåå¶æçåæ§å¶åã</li>
<li><strong>å®éåºç¨ä¸é²æ£æ§ï¼</strong> èªå¨é©¾é©¶ãå¤é´å¾åå¤çãæé¨è¿½è¸ªç­å®éåºç¨åºæ¯æ¯ç ç©¶éç¹ï¼å¼ºè°æ¨¡åå¨å¤æçå®ä¸çæ¡ä»¶ä¸çæ§è½åé²æ£æ§ã</li>
<li><strong>æ°æ®æçä¸å¯¹é½ï¼</strong> æ°æ®è¸é¦ãè·¨æ¨¡æå¯¹é½ç­ææ¯æ¨å¨æé«æ°æ®å©ç¨æçï¼å¹¶è§£å³å¤æ¨¡ææ°æ®ä¹é´çè¯­ä¹é¸¿æ²ã</li>
</ol>
<p><strong>ç¹å«æ¾èæåæ°è®ºæï¼</strong></p>
<ul>
<li><strong>"PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era" (Xu Zheng et al.)</strong>ï¼è¿ç¯è®ºæå¯è½æ¯ä¸ç¯å·æéç¨ç¢æä¹çç»¼è¿°æåç»æ§å·¥ä½ï¼ç³»ç»æ§å°æ¢è®¨äºå¨åè§è§å¨å·èº«AIä¸­çéè¦æ§ãææåæªæ¥æ¹åãå®é¢ç¤ºçå·èº«æºè½é¢åæç¥èå¼çéå¤§è½¬åã</li>
<li><strong>"SAIL-VL2 Technical Report" (Weijie Yin et al.)</strong>ï¼ä½ä¸ºä¸ä»½ææ¯æ¥åï¼å®å¯è½è¯¦ç»ä»ç»äºæä¸ªå¤§åè§è§è¯­è¨æ¨¡åçæ¶æãè®­ç»æ¹æ³åæ§è½ï¼å¯¹äºçè§£å½åVLMsçSOTAï¼State-of-the-Artï¼è³å³éè¦ã</li>
<li><strong>"Noise-Level Diffusion Guidance: Well Begun is Half Done" (Harvey Mannering et al.)</strong>ï¼è¿ç¯è®ºæå¯è½æåºäºæ©æ£æ¨¡åè®­ç»ææ¨çè¿ç¨ä¸­çå³é®ä¼åï¼éè¿æ¹è¿åªå£°å¤çæ¥æ¾èæåçæè´¨éææçï¼å·ææ½å¨çå¹¿æ³å½±åã</li>
</ul>
<p><strong>æ°å´ç ç©¶æ¹åæææ¯ï¼</strong></p>
<ul>
<li><strong>å¨åè§è§å¨å·èº«AIä¸­çæ®åï¼</strong> ä¸åä»ä»æ¯ç¹å®ä»»å¡çè§£å³æ¹æ¡ï¼èæ¯æä¸ºå·èº«æºè½ä½æç¥ç³»ç»çæ ¸å¿ç»ä»¶ã</li>
<li><strong>å¤æ¨¡ææ¨çææçæ ååï¼</strong> "MARS2 2025 Challenge"è¡¨æç¤¾åºæ­£å¨ç§¯ææ¨å¨å¤æ¨¡ææ¨ççåºåæµè¯åæ¹æ³åæ°ã</li>
<li><strong>æ©æ£æ¨¡åçç²¾ç»åæ§å¶ä¸æçæåï¼</strong> "Noise-Level Diffusion Guidance"ç­å·¥ä½é¢ç¤ºçæ©æ£æ¨¡åå°æ´å å¯æ§ãé«æã</li>
<li><strong>äºä»¶ç¸æºå¨å·èº«æç¥ä¸­çåºç¨ï¼</strong> "EvHand-FPV"å±ç¤ºäºäºä»¶ç¸æºå¨ä½å»¶è¿ãé«å¨æèå´åºæ¯ä¸ï¼å¦æé¨è¿½è¸ªï¼çç¬ç¹ä¼å¿ã</li>
<li><strong>éå¼ææ¬è¯­ä¹å¨æ°æ®è¸é¦ä¸­çå©ç¨ï¼</strong> "EDITS"æåºäºä¸ç§æ°é¢çæ°æ®è¸é¦æ¹æ³ï¼å¯è½ä¸ºé«ææ¨¡åè®­ç»æä¾æ°æè·¯ã</li>
</ul>
<p><strong>å»ºè®®éè¯»å¨æçè®ºæï¼</strong></p>
<ol>
<li><strong>"PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era" (Xu Zheng et al.)</strong>ï¼å¯¹äºå³æ³¨å·èº«AIåæªæ¥æç¥èå¼çç ç©¶äººåï¼è¿ç¯è®ºææ¯å¿è¯»çï¼å®æä¾äºå®è§çè§è§åæ½å¨çæªæ¥ç ç©¶æ¹åã</li>
<li><strong>"SAIL-VL2 Technical Report" (Weijie Yin et al.)</strong>ï¼å¦æä½ å¯¹å¤§åè§è§è¯­è¨æ¨¡åçææ°è¿å±åææ¯ç»èæå´è¶£ï¼è¿ä»½ææ¯æ¥åå°æä¾å®è´µçæ´å¯ã</li>
<li><strong>"MAP: End-to-End Autonomous Driving with Map-Assisted Planning" (Huilin Yin et al.)</strong>ï¼å¯¹äºèªå¨é©¾é©¶é¢åçç ç©¶äººåï¼è¿ç¯è®ºæå¯è½å±ç¤ºäºç«¯å°ç«¯èªå¨é©¾é©¶ä¸é«ç²¾å°å¾ç»åçææ°è¿å±åå®éææã</li>
<li><strong>"Noise-Level Diffusion Guidance: Well Begun is Half Done" (Harvey Mannering et al.)</strong>ï¼å¦æä½ ä»äºçææ¨¡åææ©æ£æ¨¡åçç ç©¶ï¼è¿ç¯è®ºæå¯è½æä¾äºæåæ¨¡åæ§è½ææççå³é®ææ¯ã</li>
<li><strong>"MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods, Results, Discussion, and Outlook" (Peng Xu et al.)</strong>ï¼å¯¹äºå¸æäºè§£å¤æ¨¡ææ¨çé¢åææ°ææãæ°æ®éåæ¹æ³çç ç©¶äººåï¼è¿ç¯è®ºææä¾äºå¨é¢çæ¦è§ã</li>
</ol>
<hr />
<p>è¿ä»½æè¦æ¨å¨å¸®å©æ¨å¿«éç­éåºä¸æ¨ç ç©¶æ¹åæç¸å³çè®ºæï¼å¹¶äºè§£å½åè®¡ç®æºè§è§é¢åçç­ç¹ååæ²¿ã</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2509.14033v1">SAIL-VL2 Technical Report</a></li>
<li><a href="#2509.12989v1">PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era</a></li>
<li><a href="#2509.14142v1">MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods, Results, Discussion, and Outlook</a></li>
<li><a href="#2509.14055v1">Wan-Animate: Unified Character Animation and Replacement with Holistic Replication</a></li>
<li><a href="#2509.13936v1">Noise-Level Diffusion Guidance: Well Begun is Half Done</a></li>
<li><a href="#2509.13926v1">MAP: End-to-End Autonomous Driving with Map-Assisted Planning</a></li>
<li><a href="#2509.13883v1">EvHand-FPV: Efficient Event-Based 3D Hand Tracking from First-Person View</a></li>
<li><a href="#2509.13858v1">EDITS: Enhancing Dataset Distillation with Implicit Textual Semantics</a></li>
<li><a href="#2509.13766v1">NDLPNet: A Location-Aware Nighttime Deraining Network and a Real-World Benchmark Dataset</a></li>
<li><a href="#2509.13754v1">Cross-modal Full-mode Fine-grained Alignment for Text-to-Image Person Retrieval</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2509.14033v1'></a></p>
<h2 id="sail-vl2-technical-report"><a href="https://arxiv.org/abs/2509.14033v1">SAIL-VL2 Technical Report</a></h2>
<p><strong>Authors:</strong> Weijie Yin, Yongjie Ye, Fangxun Shu, Yue Liao, Zijian Kang, Hongyuan Dong, Haiyang Yu, Dingkang Yang, Jiacong Wang, Han Wang, Wenzhuo Liu, Xiao Liang, Shuicheng Yan, Chao Feng</p>
<p><strong>Published:</strong> 2025-09-17</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM)
for comprehensive multimodal understanding and reasoning. As the successor to
SAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8B
parameter scales across diverse image and video benchmarks, demonstrating
strong capabilities from fine-grained perception to complex reasoning. Three
core innovations drive its effectiveness. First, a large-scale data curation
pipeline with scoring and filtering strategies enhances both quality and
distribution across captioning, OCR, QA, and video data, improving training
efficiency. Second, a progressive training framework begins with a powerful
pre-trained vision encoder (SAIL-ViT), advances through multimodal
pre-training, and culminates in a thinking-fusion SFT-RL hybrid paradigm that
systematically strengthens model capabilities. Third, architectural advances
extend beyond dense LLMs to efficient sparse Mixture-of-Experts (MoE) designs.
With these contributions, SAIL-VL2 demonstrates competitive performance across
106 datasets and achieves state-of-the-art results on challenging reasoning
benchmarks such as MMMU and MathVista. Furthermore, on the OpenCompass
leaderboard, SAIL-VL2-2B ranks first among officially released open-source
models under the 4B parameter scale, while serving as an efficient and
extensible foundation for the open-source multimodal community.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºWeijie Yinç­äººå¨è®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åæ°åçè®ºæâSAIL-VL2 Technical Reportâçå¨é¢æè¦ã</p>
<p><strong>è®ºææè¦ï¼SAIL-VL2ææ¯æ¥å</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³å½åè§è§-è¯­è¨æ¨¡åï¼LVMï¼å¨å®ç°å¨é¢å¤æ¨¡æçè§£åæ¨çæ¹é¢çææãå°½ç®¡ç°æLVMå¨æ§è½ä¸æææåï¼ä½å¨å¤çå¤ææ¨çä»»å¡ãå®ç°ç»ç²åº¦æç¥ä»¥åå¨ä¸åæ¨¡æé´ææå¯¹é½æ¹é¢ä»å­å¨ä¸è¶³ãSAIL-VL2çç®æ æ¯å¼åä¸ä¸ªé«æãå¯æ©å±çå¼æºLVMï¼è½å¤è¶è¶ç°æææ¯ï¼å¨å¹¿æ³çå¤æ¨¡æä»»å¡ä¸­å±ç°åºåè¶çæ§è½ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
SAIL-VL2çæææ§ä¸»è¦ç±ä¸é¡¹æ ¸å¿åæ°é©±å¨ï¼</p>
<ul>
<li><strong>å¤§è§æ¨¡æ°æ®æ´çç®¡éï¼</strong> è®ºæè®¾è®¡äºä¸ä¸ªå¤§è§æ¨¡æ°æ®æ´çç®¡éï¼éè¿è¯ååè¿æ»¤ç­ç¥ï¼æåäºå¾åæ æ³¨ãåå­¦å­ç¬¦è¯å«ï¼OCRï¼ãé®ç­ï¼QAï¼åè§é¢æ°æ®çè´¨éååå¸ï¼ä»èæé«äºè®­ç»æçãè¿è§£å³äºç°ææ°æ®éä¸­å¯è½å­å¨çåªå£°ååå¸åå·®é®é¢ã</li>
<li><strong>æ¸è¿å¼è®­ç»æ¡æ¶ï¼</strong> å¼å¥äºä¸ä¸ªåé¶æ®µçè®­ç»æ¡æ¶ï¼é¦åä½¿ç¨å¼ºå¤§çé¢è®­ç»è§è§ç¼ç å¨ï¼SAIL-ViTï¼ï¼ç¶åè¿è¡å¤æ¨¡æé¢è®­ç»ï¼æç»éç¨âæè-èåâçSFT-RLï¼çç£å¾®è°-å¼ºåå­¦ä¹ ï¼æ··åèå¼ï¼ç³»ç»æ§å°å¢å¼ºäºæ¨¡åçåé¡¹è½åãè¿åæ¬ï¼<ul>
<li><strong>SAIL-ViTçæ¸è¿å¼ä¼åï¼</strong> éè¿ä¸é¶æ®µï¼ç­èº«éåºãç»ç²åº¦å¯¹é½ãä¸çç¥è¯æ³¨å¥ï¼è®­ç»ç­ç¥ï¼å°å¤ç²åº¦ç¥è¯æ³¨å¥è§è§ç¼ç å¨ï¼å®ç°ä¸LLMçå¨é¢å¯¹é½ã</li>
<li><strong>AdaLRSï¼èªéåºå­¦ä¹ çæç´¢ï¼ï¼</strong> å¨åºç¡å¤æ¨¡æé¢è®­ç»é¶æ®µå¼å¥å¨æå­¦ä¹ çè°åº¦å¨ï¼ä»¥æé«ä¼åæçåææã</li>
<li><strong>æ°æ®ééæ ·ç­ç¥ï¼</strong> å¨é¢è®­ç»é¶æ®µéç¨ä¸¤æ­¥ééæ ·ç­ç¥ï¼ä»¥ç¼è§£å¤§è§æ¨¡æ æ³¨åVQAæ°æ®ä¸­çåå¸åå·®ï¼å¢å¼ºå¤æ ·æ§ï¼å¹¶é²æ­¢æ¨¡å¼å´©æºã</li>
<li><strong>æ¨¡åæ±¤ï¼Model Soupï¼ç­ç¥ï¼</strong> å¨SFTåï¼éè¿åå¹¶åè´¨æ¨¡åæ¥è¿ä¸æ­¥æåæ¨¡åæ§è½ï¼å®ç°ç¨³å®ä¸æ¾èçæ§è½æ¹è¿ã</li>
</ul>
</li>
<li><strong>é«æçç¨çä¸å®¶æ··åï¼MoEï¼è®¾è®¡ï¼</strong> æ¶æåæ°è¶è¶äºä¼ ç»çå¯éLLMï¼éç¨äºé«æçç¨çMoEè®¾è®¡ãè¿å¨ä¿æè®¡ç®æççåæ¶ï¼å®ç°äºåæ°è§æ¨¡çæ©å±ï¼å¹¶éè¿å¹³è¡¡ä¸å®¶æ¿æ´»ãæ°æ®åå¸æç¥è°ä¼åä¸å®¶ä¸ä¸åä¿çç­ç¥ï¼ç¡®ä¿äºMoEçç¨³å®æ§åå¯æ©å±æ§ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
SAIL-VL2å¨å¤ä¸ªç»´åº¦ä¸å±ç¤ºäºåè¶çæ§è½ï¼</p>
<ul>
<li><strong>é¢åçæ§è½ï¼</strong> SAIL-VL2å¨2Bå8Båæ°è§æ¨¡ä¸ï¼å¨106ä¸ªæ°æ®éä¸å®ç°äºæåè¿çæ§è½ï¼æ¶µçäºå¾ååè§é¢åºåæµè¯ã</li>
<li><strong>ç»ç²åº¦æç¥ä¸å¤ææ¨çï¼</strong> æ¨¡åå¨ç»ç²åº¦æç¥å°å¤ææ¨çä»»å¡ä¸­é½è¡¨ç°åºå¼ºå¤§çè½åï¼å°¤å¶å¨MMMUåMathVistaç­æææ§æ¨çåºåæµè¯ä¸­åå¾äºæåè¿çç»æã</li>
<li><strong>OpenCompassæè¡æ¦è¡¨ç°ï¼</strong> SAIL-VL2-2Bå¨OpenCompassæè¡æ¦ä¸ï¼å¨4Båæ°è§æ¨¡ä»¥ä¸çå®æ¹åå¸å¼æºæ¨¡åä¸­æåç¬¬ä¸ï¼è¯æäºå¶ä½ä¸ºé«æä¸å¯æ©å±çå¼æºå¤æ¨¡æç¤¾åºåºç¡æ¨¡åçç«äºåã</li>
<li><strong>è§è§-ææ¬å¯¹é½ï¼</strong> SAIL-ViTè½å¤ææå°ç¼©å°è§è§åææ¬ç¹å¾ç©ºé´ä¹é´çå·®è·ï¼ä½¿å¾è§è§ç¹å¾åéæ´å ç´§åï¼å¹¶ä¸ææ¬ç¹å¾åéææ´å¤§çéå ã</li>
<li><strong>å¤æ¨¡æçè§£ä»»å¡ï¼</strong> å¨éç¨å¤æ¨¡æçè§£ãææ¡£å¾åçè§£åå¤å¾åä¸è§é¢çè§£ä»»å¡ä¸­ï¼SAIL-VL2ååå¾äºé¢åææåè¿çæ§è½ã</li>
</ul>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸­æªæç¡®æåå½åSAIL-VL2æ¨¡åçå·ä½å±éæ§ãç¶èï¼å®æç¤ºäºæªæ¥ç ç©¶çæ¹åï¼è¿éå¸¸ä¹åæ äºå½åæ¨¡åçæ¹è¿ç©ºé´ï¼</p>
<ul>
<li><strong>åææ°æ®ä¸­çè¯­è¨åå·®ï¼</strong> è®ºææåºï¼å°½ç®¡åææ°æ®æå©äºå¤§è§æ¨¡è®­ç»ï¼ä½LLMçæçåææ°æ®å¯è½å¼å¥è¯­è¨è¡¨è¾¾ä¸çåå¸åå·®ï¼å¯¼è´åè´¨åæªè¾åæéçåå¼æ§ã</li>
<li><strong>æ¨çä»»å¡çå¤ææ§ï¼</strong> å°½ç®¡SAIL-VL2å¨å¤ææ¨çä»»å¡ä¸è¡¨ç°åºè²ï¼ä½âæè-èåâçSFT-RLæ··åèå¼ä»æ¯æç»­å¼ºåçéç¹ï¼è¡¨æè¿äºä»»å¡ä»æè¿ä¸æ­¥æåçç©ºé´ã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºæå±æäºSAIL-VL2ç³»åçæªæ¥åå±ï¼åæ¬ï¼</p>
<ul>
<li><strong>æ´é«æçæ¶æï¼</strong> æç»­æ¢ç´¢åå¼åæ´é«æçLVMæ¶æã</li>
<li><strong>å¨é¢çé¢è®­ç»ç­ç¥ï¼</strong> è¿ä¸æ­¥ä¼ååå®åé¢è®­ç»ç­ç¥ã</li>
<li><strong>æ¹è¿çå¼ºåå­¦ä¹ èå¼ï¼</strong> æ·±å¥ç ç©¶ååºç¨æ´åè¿çå¼ºåå­¦ä¹ æ¹æ³ï¼ä»¥æç»­æåå¤æ¨¡ææºè½ã</li>
<li><strong>æ¨å¨å¼æºå¤æ¨¡æçæç³»ç»ï¼</strong> å°SAIL-VL2ä½ä¸ºé«æä¸å¯æ©å±çåºç¡æ¨¡åï¼èµè½æ´å¹¿æ³çå¼æºå¤æ¨¡æç¤¾åºã</li>
</ul>
<p>æ»èè¨ä¹ï¼SAIL-VL2ææ¯æ¥åè¯¦ç»ä»ç»äºä¸ä¸ªå¨æ°æ®æ´çãè®­ç»ç­ç¥åæ¶ææ¹é¢è¿è¡åæ°çè§è§-è¯­è¨åºç¡æ¨¡åãå®å¨å¤é¡¹åºåæµè¯ä¸­åå¾äºæ¾èçæ§è½æåï¼å°¤å¶å¨æ¨çä»»å¡ä¸­è¡¨ç°çªåºï¼å¹¶æææä¸ºå¼æºå¤æ¨¡æé¢åçéè¦æ¨å¨åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM)
for comprehensive multimodal understanding and reasoning.</li>
<li>As the successor to
SAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8B
parameter scales across diverse image and video benchmarks, demonstrating
strong capabilities from fine-grained perception to complex reasoning.</li>
<li>With these contributions, SAIL-VL2 demonstrates competitive performance across
106 datasets and achieves state-of-the-art results on challenging reasoning
benchmarks such as MMMU and MathVista.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.14033v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.14033v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.12989v1'></a></p>
<h2 id="panorama-the-rise-of-omnidirectional-vision-in-the-embodied-ai-era"><a href="https://arxiv.org/abs/2509.12989v1">PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era</a></h2>
<p><strong>Authors:</strong> Xu Zheng, Chenfei Liao, Ziqiao Weng, Kaiyu Lei, Zihao Dongfang, Haocong He, Yuanhuiyi Lyu, Lutao Jiang, Lu Qi, Li Chen, Danda Pani Paudel, Kailun Yang, Linfeng Zhang, Luc Van Gool, Xuming Hu</p>
<p><strong>Published:</strong> 2025-09-16</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Omnidirectional vision, using 360-degree vision to understand the
environment, has become increasingly critical across domains like robotics,
industrial inspection, and environmental monitoring. Compared to traditional
pinhole vision, omnidirectional vision provides holistic environmental
awareness, significantly enhancing the completeness of scene perception and the
reliability of decision-making. However, foundational research in this area has
historically lagged behind traditional pinhole vision. This talk presents an
emerging trend in the embodied AI era: the rapid development of omnidirectional
vision, driven by growing industrial demand and academic interest. We highlight
recent breakthroughs in omnidirectional generation, omnidirectional perception,
omnidirectional understanding, and related datasets. Drawing on insights from
both academia and industry, we propose an ideal panoramic system architecture
in the embodied AI era, PANORAMA, which consists of four key subsystems.
Moreover, we offer in-depth opinions related to emerging trends and
cross-community impacts at the intersection of panoramic vision and embodied
AI, along with the future roadmap and open challenges. This overview
synthesizes state-of-the-art advancements and outlines challenges and
opportunities for future research in building robust, general-purpose
omnidirectional AI systems in the embodied AI era.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾ç±Xu Zhengç­äººæ°åçè®ºæâPANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Eraâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³å¨åè§è§ï¼360åº¦è§è§ï¼å¨å·èº«AIæ¶ä»£æ¥çå¢é¿çéè¦æ§ä¸è¯¥é¢ååºç¡ç ç©¶ç¸å¯¹æ»åä¹é´çå·®è·ãä¼ ç»éå­è§è§æä¾äºç­çªçè§åºï¼èå¨åè§è§è½æä¾å¯¹ç¯å¢çæ´ä½æç¥ï¼è¿å¯¹äºæºå¨äººãå·¥ä¸æ£æµåç¯å¢çæµç­å·èº«AIä»»å¡è³å³éè¦ãç¶èï¼å¨åè§è§å¨æ°æ®ç¶é¢ãæ¨¡åè½åååºç¨ç©ºç½æ¹é¢é¢ä¸´ææï¼é»ç¢äºå¶å¨å·èº«AIé¢åçè¿ä¸æ­¥åå±ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è®ºæçæ ¸å¿è´¡ç®æ¯æåºäºä¸ä¸ªçæ³çå¨åç³»ç»æ¶æââ<strong>PANORAMA</strong>ï¼å®ç±åä¸ªå³é®å­ç³»ç»ç»æï¼æ¨å¨æ´åå¨åè§è§ä¸å·èº«AIï¼
*   <strong>å­ç³»ç»1ï¼æ°æ®ééä¸é¢å¤çï¼Data Acquisition &amp; Pre-processingï¼</strong>ï¼è´è´£æè·åå§å¨åæ°æ®å¹¶è½¬æ¢ä¸ºéåè®¡ç®å¤ççæ ¼å¼ï¼åæ¬æ°æ®æè·ãæ ¼å¼è½¬æ¢ãåæ­¥ä¸æ ¡åã
*   <strong>å­ç³»ç»2ï¼æç¥ï¼Perceptionï¼</strong>ï¼å¯¹é¢å¤çåçå¨åæ°æ®è¿è¡åºç¡åºæ¯æç¥ï¼å©ç¨éåºçé¢å ä½çæ·±åº¦å­¦ä¹ æ¨¡åæåä¸°å¯çç»æåä¿¡æ¯ï¼åæ¬ç¹å¾æååç¯å¢æç¥ï¼è¯­ä¹åå²ãç®æ æ£æµãæ·±åº¦ä¼°è®¡ï¼ã
*   <strong>å­ç³»ç»3ï¼åºç¨ï¼Applicationï¼</strong>ï¼å°æç¥æ´å¯è½¬åä¸ºå·èº«AIæºè½ä½çè¡å¨ï¼æå¡äºå¯¼èªä¸SLAMãäººæºäº¤äºãæ°å­å­ªçä¸3Déå»ºç­ä¸æ¸¸ä»»å¡ã
*   <strong>å­ç³»ç»4ï¼å éä¸é¨ç½²ï¼Acceleration &amp; Employmentï¼</strong>ï¼è§£å³å¤çé«åè¾¨çå¨åæ°æ®å¨èµæºåéç¯å¢ä¸çè®¡ç®ææï¼éè¿è½¯ä»¶å éï¼æ¨¡åéåãåªæï¼åç¡¬ä»¶é¨ç½²ï¼è¾¹ç¼è®¡ç®å¹³å°ï¼ç¡®ä¿æ´ä¸ªæµç¨çè®¡ç®å¯è¡æ§ã</p>
<p>æ­¤å¤ï¼è®ºæè¿åé¡¾äºå¨åè§è§å¨<strong>çæãæç¥ãçè§£</strong>å<strong>ç¸å³æ°æ®é</strong>æ¹é¢çææ°çªç ´ï¼å¹¶æåºäºä¸ä¸ªåé¶æ®µçæªæ¥è·¯çº¿å¾ï¼ä»¥æå»ºä¸ä¸ªçæ³çç»ä¸å¨åä»»å¡æ¨¡åã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
è®ºæéè¿æåºPANORAMAç³»ç»æ¶æï¼ä¸ºå·èº«AIæ¶ä»£çå¨åè§è§åå±æä¾äºä¸ä¸ªå¨é¢çæ¡æ¶ãå¶æä¹å¨äºï¼
*   <strong>ç³»ç»åè§£å³ææ</strong>ï¼PANORAMAæ¶æç³»ç»å°è§£å³äºå¨åè§è§å¨æ°æ®ãæ¨¡åååºç¨å±é¢çææï¼ä¸ºå®ç°éç¨åé²æ£çå·èº«æºè½å¥ å®äºåºç¡ã
*   <strong>ä¿è¿è·¨ç¤¾åºå½±å</strong>ï¼å¨åè§è§çæçè¢«è§ä¸ºä¸é¡¹åºç¡æ§ä½¿è½ææ¯ï¼è½å¤ä¿è¿æºå¨äººãèªä¸»å¯¼èªãäººæºäº¤äºãè®¤ç¥AIåèææºè½ä½ç­å¤ä¸ªé¢åçè·¨ç¤¾åºçªç ´ã
*   <strong>æææªæ¥åå±æ¹å</strong>ï¼è®ºæç»¼åäºå­¦æ¯çåå·¥ä¸ççè§è§£ï¼æåºäºä¸ä¸ªè¯¦ç»çæªæ¥è·¯çº¿å¾ï¼å­ä¸ªé¶æ®µï¼æ°æ®éæ´åãå¤æ¨¡ææ©å±ãæ¨çä¸å·èº«æ°æ®ãç»ä¸æ¨¡åé¢è®­ç»ãè¯ä¼°ä¸åºåæµè¯ãé¨ç½²ä¸æ³åï¼ï¼ä¸ºæå»ºå¨åAIç³»ç»æä¾äºæ¸æ°çè·¯å¾ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸­æå°äºå½åå¨åè§è§ç ç©¶çå ç¹å±éæ§ï¼è¿äºä¹æ¯PANORAMAç³»ç»éè¦åæçææï¼
*   <strong>æ°æ®ç¶é¢</strong>ï¼å¨åå¾åï¼å°¤å¶æ¯ç­è·æ±ç¶æå½±å¾åï¼ç±äºå ä½å¤±çåé«åè¾¨çï¼æå¨æ æ³¨ææ¬æ´é«ï¼ä¼ ç»èªå¨åæ æ³¨å·¥å·æçä½ä¸ï¼å¯¼è´ç¼ºä¹å¤§è§æ¨¡é«è´¨éæ°æ®éã
*   <strong>æ¨¡åè½å</strong>ï¼ç°æé¢è®­ç»æ¨¡åï¼å¦å·ç§¯åæ± åæä½ï¼çå½çº³åç½®ï¼å¦å¹³ç§»ä¸åæ§ï¼ä¸»è¦éå¯¹éå­å¾åè®¾è®¡ï¼é¾ä»¥çè§£å¨åå¾åçå¤±çç¹æ§ï¼å¯¼è´æ§è½æ¾èä¸éã
*   <strong>åºç¨ç©ºç½</strong>ï¼å°½ç®¡æ°ä¼ æå¨åå·èº«AIæ¶ä»£å¸¦æ¥äºæ°çåºç¨åºæ¯ï¼ä½ç±äºç¼ºä¹è·¨å­¦ç§äººæä»¥åç°æå¨åæ°æ®åæ¨¡åçä¸è¶³ï¼è®¸å¤ç¹å®åºæ¯çå­é¢åï¼å¦å¨åçäº§å®å¨æ£æ¥ãå¨åæ£®æç«ç¾æ£æµï¼ä»ç¼ºä¹ååæ¢ç´¢ã
*   <strong>æ³åæ§åé²æ£æ§</strong>ï¼å¤§å¤æ°ç°ææ¨¡åä»ä¸æ³¨äºç¹å®åºæ¯ææå½±æ¹æ³ï¼é¾ä»¥æ³åå°ä¸åçå¨åä¼ æå¨è§æ ¼ãåºç¨åºæ¯åæå½±æ¹æ³ã
*   <strong>å¨æå¤±çå¤ç</strong>ï¼å½åæ¹æ³å°å¨åå¾åçå¤±çè§ä¸ºä¸å¸§æ å³çå ä½é®é¢ï¼ä½ç°å®ä¸çåºæ¯ä¸­çå¤±çæ¬è´¨ä¸æ¯å¨æçï¼ç¼ºä¹å¯¹å¤±çå¨å¨åè§é¢åºåä¸­æ¶é´ä¸è´æ§åæ¼åçèèã
*   <strong>å¯æ©å±åç»ä¸çæ¶æ</strong>ï¼ç¼ºä¹ä¸é¨ä¸ºå¨åè§è§è®¾è®¡çç»ä¸ãå¤ä»»å¡åºç¡æ¨¡åï¼ç°ææ¨¡åæçä½ä¸ä¸ä»»å¡ç¹å®ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºææåºäºä»¥ä¸æªæ¥ç ç©¶æ¹åï¼
*   <strong>æ°æ®éåå»º</strong>ï¼è§åååå¸å¤§è§æ¨¡ãå¤ä»»å¡å¨åæ°æ®éï¼æ¶µççå®ä¸çåºæ¯çå¤ææ§ï¼åæ¬å®¤åå¤ãéç¨åå·èº«æºè½åºæ¯ã
*   <strong>ç®æ³ç ç©¶</strong>ï¼è¶è¶åºäºéå­æ¨¡åçç®åééï¼åå»ºå·æå¨åä¿¡æ¯çæ°é¢æ¶æåå¨æå­¦ä¹ èå¼ï¼ä»¥åºå¯¹å¨åè§è§çç¬ç¹ææã
*   <strong>åºç¨å·¥ç¨</strong>ï¼æ¢ç´¢åå±ç¤ºå¨åæç¥å¨çå®ä¸çæºå¨äººåäº¤äºç³»ç»ä¸­çä¼å¿ï¼å¼¥åå®éªå®¤ç ç©¶ä¸å®éåºç¨ä¹é´çé¸¿æ²ã
*   <strong>æå½±æ å³è¡¨ç¤ºå­¦ä¹ </strong>ï¼å¼åè½å¤ä»æ æ æ³¨å¨åä¿¡æ¯ï¼åæ¬å¾ååè§é¢æµï¼ä¸­å­¦ä¹ ä¸åç¹å¾çæå½±æ å³è¡¨ç¤ºåèªçç£å­¦ä¹ ææ¯ã
*   <strong>å¨ä½æç¥è¡¨ç¤ºå­¦ä¹ </strong>ï¼ä½¿æ¨¡åè½å¤å­¦ä¹ å¨åå¾åä¸­çå¨ä½å¯¼åè¡¨ç¤ºï¼å°å¨åè§è§çç¬ç¹ä¼å¿æ´åå°ä¸æ¸¸æ§å¶ç­ç¥ä¸­ï¼ä»¥å®ç°æ´ææåé«æçæºå¨äººå³ç­ã
*   <strong>ç»ä¸åºç¡æ¨¡å</strong>ï¼é¢è®­ç»ä¸é¨ä¸ºå¨åè§è§è®¾è®¡çç»ä¸ãå¤ä»»å¡åºç¡æ¨¡åï¼ä»¥ææå¨åå ä½åè¯­ä¹çåºæ¬çè§£ï¼ä»èæé«æ§è½åæ³åè½åï¼å¹¶åå°å¯¹å¤§éä»»å¡ç¹å®æ°æ®éçéæ±ã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>Drawing on insights from
both academia and industry, we propose an ideal panoramic system architecture
in the embodied AI era, PANORAMA, which consists of four key subsystems.</li>
<li>This overview
synthesizes state-of-the-art advancements and outlines challenges and
opportunities for future research in building robust, general-purpose
omnidirectional AI systems in the embodied AI era.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.12989v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.12989v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.14142v1'></a></p>
<h2 id="mars2-2025-challenge-on-multimodal-reasoning-datasets-methods-results-discussion-and-outlook"><a href="https://arxiv.org/abs/2509.14142v1">MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods, Results, Discussion, and Outlook</a></h2>
<p><strong>Authors:</strong> Peng Xu, Shengwu Xiong, Jiajun Zhang, Yaxiong Chen, Bowen Zhou, Chen Change Loy, David A. Clifton, Kyoung Mu Lee, Luc Van Gool, Ruiming He, Ruilin Yao, Xinwei Long, Jirui Huang, Kai Tian, Sa Yang, Yihua Shao, Jin Feng, Yue Zhong, Jiakai Zhou, Cheng Tang, Tianyu Zou, Yifang Zhang, Junming Liang, Guoyou Li, Zhaoxiang Wang, Qiang Zhou, Yichen Zhao, Shili Xiong, Hyeongjin Nam, Jaerin Lee, Jaeyoung Chung, JoonKyu Park, Junghun Oh, Kanggeon Lee, Wooseok Lee, Juneyoung Ro, Turghun Osman, Can Hu, Chaoyang Liao, Cheng Chen, Chengcheng Han, Chenhao Qiu, Chong Peng, Cong Xu, Dailin Li, Feiyu Wang, Feng Gao, Guibo Zhu, Guopeng Tang, Haibo Lu, Han Fang, Han Qi, Hanxiao Wu, Haobo Cheng, Hongbo Sun, Hongyao Chen, Huayong Hu, Hui Li, Jiaheng Ma, Jiang Yu, Jianing Wang, Jie Yang, Jing He, Jinglin Zhou, Jingxuan Li, Josef Kittler, Lihao Zheng, Linnan Zhao, Mengxi Jia, Muyang Yan, Nguyen Thanh Thien, Pu Luo, Qi Li, Shien Song, Shijie Dong, Shuai Shao, Shutao Li, Taofeng Xue, Tianyang Xu, Tianyi Gao, Tingting Li, Wei Zhang, Weiyang Su, Xiaodong Dong, Xiao-Jun Wu, Xiaopeng Zhou, Xin Chen, Xin Wei, Xinyi You, Xudong Kang, Xujie Zhou, Xusheng Liu, Yanan Wang, Yanbin Huang, Yang Liu, Yang Yang, Yanglin Deng, Yashu Kang, Ye Yuan, Yi Wen, Yicen Tian, Yilin Tao, Yin Tang, Yipeng Lin, Yiqing Wang, Yiting Xi, Yongkang Yu, Yumei Li, Yuxin Qin, Yuying Chen, Yuzhe Cen, Zhaofan Zou, Zhaohong Liu, Zhehao Shen, Zhenglin Du, Zhengyang Li, Zhenni Huang, Zhenwei Shao, Zhilong Song, Zhiyong Feng, Zhiyu Wang, Zhou Yu, Ziang Li, Zihan Zhai, Zijian Zhang, Ziyang Peng, Ziyun Xiao, Zongshu Li</p>
<p><strong>Published:</strong> 2025-09-17</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>This paper reviews the MARS2 2025 Challenge on Multimodal Reasoning. We aim
to bring together different approaches in multimodal machine learning and LLMs
via a large benchmark. We hope it better allows researchers to follow the
state-of-the-art in this very dynamic area. Meanwhile, a growing number of
testbeds have boosted the evolution of general-purpose large language models.
Thus, this year's MARS2 focuses on real-world and specialized scenarios to
broaden the multimodal reasoning applications of MLLMs. Our organizing team
released two tailored datasets Lens and AdsQA as test sets, which support
general reasoning in 12 daily scenarios and domain-specific reasoning in
advertisement videos, respectively. We evaluated 40+ baselines that include
both generalist MLLMs and task-specific models, and opened up three competition
tracks, i.e., Visual Grounding in Real-world Scenarios (VG-RS), Visual Question
Answering with Spatial Awareness (VQA-SA), and Visual Reasoning in Creative
Advertisement Videos (VR-Ads). Finally, 76 teams from the renowned academic and
industrial institutions have registered and 40+ valid submissions (out of
1200+) have been included in our ranking lists. Our datasets, code sets (40+
baselines and 15+ participants' methods), and rankings are publicly available
on the MARS2 workshop website and our GitHub organization page
https://github.com/mars2workshop/, where our updates and announcements of
upcoming events will be continuously provided.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºâMARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods, Results, Discussion, and Outlookâè®ºæçå¨é¢æè¦ï¼</p>
<p><strong>è®ºææè¦ï¼MARS2 2025 å¤æ¨¡ææ¨çææèµ</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³å¤æ¨¡ææºå¨å­¦ä¹ åå¤§åè¯­è¨æ¨¡åï¼LLMsï¼å¨å¤æ¨¡ææ¨çæ¹é¢çææï¼ç¹å«æ¯å¨çå®ä¸çåä¸ä¸åºæ¯ä¸­ãæ ¸å¿é®é¢æ¯ç°ææ¨¡åå¨ç©ºé´æ¨çãç»åçè§£åæ½è±¡æ¨çï¼å°¤å¶æ¯å¨å¹¿åè§é¢ç­é¢åï¼æ¹é¢çå±éæ§ï¼ä»¥åç°æè¯ä¼°ç³»ç»æªè½æä¾ä»æç¥å°æ¨çè¿ç»­æ§çç³»ç»æ§è¡¨å¾ãææèµæ¨å¨éè¿å¤§ååºåæµè¯ï¼ä¿è¿å¤æ¨¡æLLMså¨å¤æå¤æ¨¡ææ¨çåâç³»ç»2âæ¢æèæ¹é¢çè¿æ­¥ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>å®å¶æ°æ®éï¼</strong> åå¸äºä¸¤ä¸ªä¸é¨è®¾è®¡çå¤§è§æ¨¡å¤æ¨¡ææ°æ®éââLensåAdsQAãLensç¨äºè¯ä¼°12ä¸ªæ¥å¸¸åºæ¯ä¸­çéç¨æ¨çï¼AdsQAç¨äºå¹¿åè§é¢ä¸­çé¢åç¹å®æ¨çï¼æ¨å¨æ¢ç´¢æ¨çä»»å¡é´çååæåºåééæ­¥å¤ææ¨çã
*   <strong>ç»¼ååºåï¼</strong> è¯ä¼°äº40å¤ä¸ªåºçº¿æ¨¡åï¼åæ¬éç¨MLLMsåä»»å¡ç¹å®æ¨¡åï¼ä»¥å15ä¸ªåä¸å¢éçè§£å³æ¹æ¡ï¼æ¨¡åè§æ¨¡ä»3Bå°72Bä¸ç­ï¼æ¶µçäºå¼æºååä¸æ¨¡åï¼æä¾äºå¨é¢çæ¯è¾ã
*   <strong>ä¸é¡¹ç«èµèµéï¼</strong> è®¾ç«äºä¸ä¸ªå¼æ¾å¼QAèµéï¼
    *   <strong>Track #1 çå®ä¸çåºæ¯ä¸­çè§è§å®ä½ (VG-RS)ï¼</strong> è¯ä¼°æ¨¡åå¨å¤æåºæ¯ä¸­çåºæ¯æç¥ãç©ä½å®ä½åç©ºé´æ¨çè½åã
    *   <strong>Track #2 ç©ºé´æç¥è§è§é®ç­ (VQA-SA)ï¼</strong> è¯ä¼°æ¨¡åæ ¹æ®ç¨æ·æä»¤ï¼åºäºå·ä½ç©çåå®¹è¿è¡ç©ºé´ãå¸¸è¯ååäºå®æ¨ççè½åã
    *   <strong>Track #3 åæå¹¿åè§é¢ä¸­çè§è§æ¨ç (VR-Ads)ï¼</strong> æ¢ç´¢æ¨¡åå¨å¹¿åè§é¢ä¸­çè§£éå«ãéç©çåæ½è±¡è§è§æ¦å¿µçè®¤ç¥æ¨çè½åã
*   <strong>å¼æ¾æºä»£ç å¯å¤ç°æ§ï¼</strong> æææ°æ®éãä»£ç åæååå¨MARS2å·¥ä½åç½ç«åGitHubç»ç»é¡µé¢ä¸å¬å¼ï¼ç¡®ä¿äºç ç©¶çå¯å¤ç°æ§ã
*   <strong>åèµå¢éçæ¹æ³è®ºï¼</strong> è®¸å¤å¢ééç¨äºåè¿çMLLMsä½ä¸ºåºç¡æ¨¡åï¼å¹¶éè¿çç£å¾®è°ï¼SFTï¼åå¼ºåå­¦ä¹ ï¼RLï¼è¿è¡å¤æ­¥å¯¹é½ï¼ä»¥è§£å³å¤ææ¨çä»»å¡ãå¸¸è§çç­ç¥åæ¬éæå­¦ä¹ ãæ°æ®å¢å¼ºãæç¤ºå·¥ç¨åæ¨¡ååä½ï¼éç¨æ¨¡åä¸ä¸å®¶æ¨¡åç»åï¼ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æææ§å¸æ¾ï¼</strong> ç»æè¡¨æï¼å³ä½¿ä½¿ç¨å¼ºå¤§çLLMsä½ä¸ºåºç¡æ¨¡åï¼å¨å¤æåºæ¯åä¸ä¸é¢åè¿è¡å¤æ¨¡ææ¨çä»ç¶å·ææææ§ãä¾å¦ï¼VG-RSä»»å¡çè·èè§£å³æ¹æ¡å¾åæªè¶è¿70%ï¼VR-Adsèµéçæä½³åç¡®çï¼56%ï¼ä¸äººç±»è¡¨ç°ï¼çº¦70%ï¼ä»æææ¾å·®è·ã
*   <strong>æ¨¡åå±éæ§ï¼</strong> æéæ¡ä¾åææ­ç¤ºäºå½åå¤æ¨¡æå¤§åè¯­è¨æ¨¡åå¨ç»ç²åº¦å¾åçè§£ï¼å¦æ··æ·ç¸ä¼¼ææãè¯å«è¯­ä¹ç®æ ï¼åè§è§é®ç­ä¸­çè§è§çè§£åå·®ï¼è¿åº¦ä¾èµç¬¬ä¸äººç§°è§è§åéªãè¯¯è§£âè·ç¦»âç­æ¦å¿µï¼æ¹é¢çå±éæ§ã
*   <strong>ååæåºåééæ­¥æ¨çï¼</strong> ææèµæåå¸å¼äºç¤¾åºå¯¹æ¨çä»»å¡ååæåºåééæ­¥å¤ææ¨çé®é¢çå³æ³¨ã
*   <strong>ä¿è¿ç ç©¶ï¼</strong> ææèµä¸ºå¤æ¨¡ææ¨çé¢åæä¾äºä¸ä¸ªå¨é¢ãå¤æ ·åçåºåï¼æ¨å¨äºæ°ä¸ä»£MLLMsæ¨çè½åçåå±ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>å¯é æ§åæ³åè½åä¸è¶³ï¼</strong> æäº¤çè§£å³æ¹æ¡è½ç¶æ§è½æææåï¼ä½ä»ç¼ºä¹å¯é æ§åæ³åè½åãä¾å¦ï¼å¼ºåå­¦ä¹ çå¥å±å½æ°ï¼å¦IoUåæ°ï¼å¯è½æ¬¡ä¼ï¼å¯¼è´æ¨¡åå¯è½å¤±å»å¶ä»è½åã
*   <strong>ç»ç²åº¦å¾åçè§£çææï¼</strong> æ¨¡åå¨ç»ç²åº¦å¾åçè§£ä¸­å®¹ææ··æ·ç¸ä¼¼æææé¾ä»¥è¯å«è¯­ä¹ç®æ ã
*   <strong>è§è§é®ç­ä¸­çåå·®ï¼</strong> æ¨¡åå¨è§è§çè§£æ¹é¢å­å¨æ¾èåå·®ï¼è¿åº¦ä¾èµç¬¬ä¸äººç§°è§è§åéªï¼å¹¶è¯¯è§£âè·ç¦»âç­æ¦å¿µï¼åæ åºç¼ºä¹ç©çå¸¸è¯ã
*   <strong>æ½è±¡æ¨ççææï¼</strong> å¹¿åè§é¢ä¸­çæ½è±¡æ¨çï¼å¦ææåæãè¥éé»è¾ãè¯´æç­ç¥ï¼å¯¹æ¨¡åæåºäºæ´é«çè¦æ±ï¼ç°ææ¨¡åä»é¾ä»¥ææå¤çã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æåæ¨¡åå¯é æ§åæ³åè½åï¼</strong> æ¢ç´¢æ´ä¼çå¥å±å½æ°åå¯¹é½ææ¯ï¼ä»¥æé«MLLMsçå¯é æ§åæ³åè½åï¼é¿åå ä»»å¡ç¹å®ä¼åèæå¤±å¶ä»éç¨è½åã
*   <strong>æ·±åç»ç²åº¦çè§£åå¸¸è¯æ¨çï¼</strong> è¿ä¸æ­¥ç ç©¶å¦ä½ä½¿MLLMsæ´å¥½å°çè§£ç»ç²åº¦è§è§ä¿¡æ¯ï¼å¹¶èå¥ç©çå¸¸è¯ï¼ä»¥åå°è¯¯è§£ååå·®ã
*   <strong>å å¼ºæ½è±¡åè®¤ç¥æ¨çï¼</strong> éå¯¹å¹¿åè§é¢ç­ä¸ä¸é¢åï¼å¼åæ´ææçå¤æ¨¡æèåãæ¶é´å»ºæ¨¡åå¤é¨ç¥è¯æ´åæ¹æ³ï¼ä»¥æ¯ææ´é«å±æ¬¡çè®¤ç¥æ¨çã
*   <strong>æ©å±åºåååºç¨åºæ¯ï¼</strong> ç»§ç»­æä¾æ°çåºç¨åºæ¯åé«è´¨éæ°æ®ï¼åçº§æµè¯éè§æ¨¡ï¼å¹¶æä¾æ´å¤èµéï¼ä»¥ä¿è¿å¤æ¨¡ææ¨çé¢åçå¼æ¾æºä»£ç ç¤¾åºåå±ã
*   <strong>éç¨ä¸ä¸ä¸æ¨¡ååä½ï¼</strong> è¿ä¸æ­¥æ¢ç´¢éç¨æ¨¡åä¸ä¸å®¶æ¨¡ååä½çæ½åï¼ä»¥åºå¯¹å¤æå¤æ¨¡ææ¨çä»»å¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We hope it better allows researchers to follow the
state-of-the-art in this very dynamic area.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.14142v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.14142v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.14055v1'></a></p>
<h2 id="wan-animate-unified-character-animation-and-replacement-with-holistic-replication"><a href="https://arxiv.org/abs/2509.14055v1">Wan-Animate: Unified Character Animation and Replacement with Holistic Replication</a></h2>
<p><strong>Authors:</strong> Gang Cheng, Xin Gao, Li Hu, Siqi Hu, Mingyang Huang, Chaonan Ji, Ju Li, Dechao Meng, Jinwei Qi, Penchong Qiao, Zhen Shen, Yafei Song, Ke Sun, Linrui Tian, Feng Wang, Guangyuan Wang, Qi Wang, Zhongjian Wang, Jiayu Xiao, Sheng Xu, Bang Zhang, Peng Zhang, Xindi Zhang, Zhe Zhang, Jingren Zhou, Lian Zhuo</p>
<p><strong>Published:</strong> 2025-09-17</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We introduce Wan-Animate, a unified framework for character animation and
replacement. Given a character image and a reference video, Wan-Animate can
animate the character by precisely replicating the expressions and movements of
the character in the video to generate high-fidelity character videos.
Alternatively, it can integrate the animated character into the reference video
to replace the original character, replicating the scene's lighting and color
tone to achieve seamless environmental integration. Wan-Animate is built upon
the Wan model. To adapt it for character animation tasks, we employ a modified
input paradigm to differentiate between reference conditions and regions for
generation. This design unifies multiple tasks into a common symbolic
representation. We use spatially-aligned skeleton signals to replicate body
motion and implicit facial features extracted from source images to reenact
expressions, enabling the generation of character videos with high
controllability and expressiveness. Furthermore, to enhance environmental
integration during character replacement, we develop an auxiliary Relighting
LoRA. This module preserves the character's appearance consistency while
applying the appropriate environmental lighting and color tone. Experimental
results demonstrate that Wan-Animate achieves state-of-the-art performance. We
are committed to open-sourcing the model weights and its source code.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Gang Chengç­æ°åçè®ºæâWan-Animate: Unified Character Animation and Replacement with Holistic Replicationâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼Wan-Animate: Unified Character Animation and Replacement with Holistic Replication</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢</strong>
å½åè®¡ç®æºè§è§é¢åå¨è§è²å¨ç»åæ¿æ¢æ¹é¢å­å¨ä¸ä¸ªå³é®ç©ºç½ï¼ç¼ºä¹ä¸ä¸ªè½å¤ç»ä¸æ§å¶è¿å¨ãè¡¨æåç¯å¢äº¤äºï¼å¹¶å®ç°é«ä¿çåº¦çæ´ä½è§£å³æ¹æ¡ãç°æçå¼æºæ¡æ¶å¨æ§è½åå®æ´æ§ä¸å­å¨æ¾èä¸è¶³ï¼å°¤å¶æ¯å¨å¨é¢å¤å¶å¯æè¡¨ç°åçé¢é¨å¨æä¸èº«ä½è¿å¨ç¸ç»åï¼ä»¥åå°è§è²å¨ç»ä¸ç¯å¢èæ¯ï¼å³è§è²æ¿æ¢ï¼æ ç¼éææ¹é¢ãæ¬ç ç©¶æ¨å¨è§£å³è¿äºææï¼æä¾ä¸ä¸ªç»ä¸ä¸é«æ§è½çè§è²å¨ç»åæ¿æ¢æ¡æ¶ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®</strong>
Wan-AnimateåºäºWanæ¨¡åæå»ºï¼å¹¶å¼å¥äºå¤é¡¹å³é®åæ°ï¼
*   <strong>ç»ä¸æ¡æ¶ä¸è¾å¥èå¼ï¼</strong> Wan-Animateæåºäºä¸ä¸ªç»ä¸çæ¡æ¶ï¼è½å¤å¤çè§è²å¨ç»åè§è²æ¿æ¢ä¸¤ç§æ ¸å¿åè½ãéè¿ä¿®æ¹åçè¾å¥èå¼ï¼å®è½å¤åºååèæ¡ä»¶åçæåºåï¼å°å¤ä»»å¡ç»ä¸ä¸ºå±åçç¬¦å·è¡¨ç¤ºï¼ä»èå¨ä¸å¼å¥æ¾èåå¸åç§»çæåµä¸ï¼é«æå°è¿è¡åè®­ç»ã
*   <strong>è§£è¦æ§å¶ä¿¡å·ï¼</strong> ä¸ºäºå®ç°æ´ä½è§è²æ§å¶ï¼æ¨¡åå°æ§å¶ä¿¡å·è§£è¦ä¸ºèº«ä½è¿å¨åé¢é¨è¡¨æã
    *   <strong>èº«ä½è¿å¨æ§å¶ï¼</strong> éç¨ç©ºé´å¯¹é½çéª¨æ¶ä¿¡å·æ¥å¤å¶èº«ä½è¿å¨ï¼å¹³è¡¡äºåç¡®æ§åéç¨æ§ãè¿äºä¿¡å·éè¿æ·»å å°åå§åªå£°æ½å¨åéä¸­è¿è¡æ³¨å¥ã
    *   <strong>é¢é¨è¡¨ææ§å¶ï¼</strong> ç´æ¥ä½¿ç¨åèè§é¢ä¸­çåå§é¢é¨å¾åä½ä¸ºé©±å¨ä¿¡å·ï¼ä»¥ä¿çæå¤§ç»èãè¿äºé¢é¨å¾åè¢«ç¼ç ä¸ºæ½å¨åéï¼ä»¥è§£è¦è¡¨æä¿¡æ¯åèº«ä»½å±æ§ï¼å¹¶éè¿è·¨æ³¨æåæºå¶æ³¨å¥æ¨¡åã
*   <strong>è¾å©éåLoRAï¼Relighting LoRAï¼ï¼</strong> ä¸ºäºå¢å¼ºè§è²æ¿æ¢æ¶çç¯å¢éæï¼Wan-Animateå¼åäºä¸ä¸ªè¾å©çRelighting LoRAæ¨¡åãè¯¥æ¨¡åå¨åºç¨éå½çç¯å¢åç§åè²è°çåæ¶ï¼ä¿æäºè§è²å¤è§çä¸è´æ§ï¼ç¡®ä¿äºæ¿æ¢è§è²ä¸æ°ç¯å¢çæ ç¼èåã
*   <strong>æ¸è¿å¼è®­ç»ç­ç¥ï¼</strong> è®­ç»è¿ç¨åä¸ºå¤ä¸ªé¶æ®µï¼åæ¬èº«ä½æ§å¶è®­ç»ãé¢é¨æ§å¶è®­ç»ãèåæ§å¶è®­ç»åèåæ¨¡å¼è®­ç»ï¼æåæ¯Relighting LoRAè®­ç»ãè¿ç§æ¸è¿å¼æ¹æ³æå©äºæ¨¡åå¿«éæ¶æï¼å¹¶ææå­¦ä¹ å¤æçæ§å¶ä»»å¡ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹</strong>
å®éªç»æè¡¨æï¼Wan-Animateå¨è§è²å¨ç»åæ¿æ¢ä»»å¡ä¸­åå¾äºæåè¿çæ§è½ï¼
*   <strong>é«ä¿çåº¦ä¸è¡¨ç°åï¼</strong> æ¨¡åè½å¤ç²¾ç¡®å¤å¶åèè§é¢ä¸­è§è²çè¡¨æåå¨ä½ï¼çæé«ä¿çåº¦çè§è²è§é¢ï¼å·æé«åº¦çå¯æ§æ§åè¡¨ç°åã
*   <strong>æ ç¼ç¯å¢éæï¼</strong> å¨è§è²æ¿æ¢æ¨¡å¼ä¸ï¼éè¿Relighting LoRAï¼å¨ç»è§è²è½å¤æ ç¼èå¥åèè§é¢çç¯å¢ï¼å¤å¶åºæ¯çåç§åè²è°ã
*   <strong>è¶è¶ç°æå¼æºåé­æºæ¹æ¡ï¼</strong> å®éè¯ä¼°ï¼SSIMãLPIPSãFVDï¼æ¾ç¤ºWan-Animateå¨æ§è½ä¸ä¼äºå¤§å¤æ°ç°æå¼æºæ¡æ¶ãäººç±»è¯ä¼°ç»æä¹è¡¨æï¼ä¸Runway Act-twoåDreamActor-M1ç­é­æºSOTAè§£å³æ¹æ¡ç¸æ¯ï¼Wan-Animateå¨è§é¢çæè´¨éãèº«ä»½ä¸è´æ§ãè¿å¨åç¡®æ§åè¡¨æåç¡®æ§æ¹é¢è¡¨ç°åºä¼è¶æ§ã
*   <strong>éç¨æ§ä¸é²æ£æ§ï¼</strong> æ¨¡åè½å¤å¾å¥½å°æ³åå°åç§äººå½¢è§è²ï¼å¨èåãåèº«åå¨èº«éå¤´ç­å¤ç§åºæ¯ä¸è¡¨ç°åºå¼ºå¤§çé²æ£æ§ã</p>
<p>è¿äºç»æçæä¹å¨äºï¼Wan-Animateä¸ºé«ä¿çè§è²å¨ç»åæ¿æ¢æä¾äºä¸ä¸ªå¨é¢ä¸é«æ§è½çè§£å³æ¹æ¡ï¼æ¾èæåäºè¯¥é¢åçç°æææ¯æ°´å¹³ï¼å¹¶ææå éè§è²å¾åå¨ç»ææ¯çåå±åå®éåºç¨ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§</strong>
*   <strong>éª¨æ¶å§¿æéå®åçå±éæ§ï¼</strong> å¨è§è²æ¿æ¢æ¨¡å¼ä¸ï¼ä¸ºäºé¿åç ´åè§è²ä¸ç¯å¢çç¹å®äº¤äºå³ç³»ï¼è®ºæä¸å»ºè®®ä½¿ç¨å§¿æéå®åãè¿å¯¼è´å¨æ¿æ¢èº«ä½å½¢ç¶å·®å¼æ¾èçè§è²æ¶ï¼å¯è½ä¼åºç°ä¸äºåå½¢ã
*   <strong>ææ¬æ§å¶çéæ ¸å¿æ§ï¼</strong> å°½ç®¡Wan-Animateæ¯æä¸å®ç¨åº¦çææ¬æ§å¶ï¼ä½è¿å¨ä¿¡å·æ¯ä¸»è¦çæ§å¶å ç´ ï¼ææ¬æ§å¶è¢«è§ä¸ºéæ ¸å¿åè½ï¼å»ºè®®ä½¿ç¨é»è®¤ææ¬æç¤ºã
*   <strong>å¯¹SMPLå½¢ç¶çä¾èµï¼é´æ¥æåï¼ï¼</strong> è®ºæå¨è®¨è®ºèº«ä½æ§å¶ä¿¡å·æ¶æå°ï¼æ¸²æçSMPLå¾ååå«è§è²å½¢ç¶ä¿¡æ¯ï¼å¦æSMPLå½¢ç¶ä¸åç¡®ï¼å¯è½ä¼ä½¿æ¨¡åä¾èµå½¢ç¶çº¿ç´¢æ¥æå¯¼çæï¼ä»èå½±åèº«ä»½ä¸è´æ§ãè½ç¶Wan-Animateéæ©äºéª¨æ¶è¡¨ç¤ºæ¥é¿åè¿ä¸ªé®é¢ï¼ä½è¿ä¹æç¤ºäº3Då½¢ç¶è¡¨ç¤ºå¯è½å¸¦æ¥çææã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å</strong>
è®ºæä¸­å¹¶æªæç¡®æåºæªæ¥çç ç©¶æ¹åï¼ä½ä»å¶è´¡ç®åå±éæ§ä¸­å¯ä»¥æ¨æ­åºä»¥ä¸å ç¹ï¼
*   <strong>æ´ç²¾ç»çå§¿æéå®åï¼</strong> è§£å³è§è²æ¿æ¢ä¸­å§¿æéå®åçå±éæ§ï¼å¼åæ´æºè½çéå®åæ¹æ³ï¼ä½¿å¶å¨ä¸ç ´åç¯å¢äº¤äºçåæä¸ï¼è½å¤å¤çèº«ä½å½¢ç¶å·®å¼æ¾èçè§è²ï¼åå°åå½¢ã
*   <strong>å¢å¼ºææ¬æ§å¶è½åï¼</strong> å°½ç®¡ç®åè¿å¨ä¿¡å·æ¯ä¸»å¯¼ï¼ä½æªæ¥å¯ä»¥æ¢ç´¢å¦ä½æ´ææå°æ´åææ¬æç¤ºï¼ä½¿å¶æä¸ºä¸ä¸ªæ´æ ¸å¿ãæ´å¼ºå¤§çæ§å¶å ç´ ï¼å®ç°æ´çµæ´»çåä½ã
*   <strong>å¤æ¨¡æèåä¸äº¤äºï¼</strong> è¿ä¸æ­¥æ¢ç´¢è§è²ä¸ç¯å¢ãè§è²ä¸ç©ä½ä¹é´æ´å¤æçäº¤äºï¼ä¾å¦ï¼é¤äºåç§åè²è°ï¼è¿è½å¤å¶ç©çå±æ§ãæè´¨ç­ï¼å®ç°æ´æ·±å±æ¬¡çåºæ¯èåã
*   <strong>å®æ¶æ§è½ä¼åï¼</strong> å°½ç®¡è®ºæå¼ºè°äºé«ææ¨çï¼ä½å¯¹äºæ´é¿çè§é¢åæ´é«åè¾¨çççæï¼è¿ä¸æ­¥ä¼åæ¨¡åçå®æ¶æ§è½åè®¡ç®æçä»æ¯éè¦çæ¹åã
*   <strong>æ´å¹¿æ³çåºç¨åºæ¯ï¼</strong> æ¢ç´¢Wan-Animateå¨èæç°å®ãæ¸¸æãæ°å­äººãçµå½±ç¹æç­æ´å¹¿æ³é¢åçåºç¨ï¼å¹¶æ ¹æ®ç¹å®éæ±è¿è¡å®å¶åå¼åã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce Wan-Animate, a unified framework for character animation and
replacement.</li>
<li>Furthermore, to enhance environmental
integration during character replacement, we develop an auxiliary Relighting
LoRA.</li>
<li>Experimental
results demonstrate that Wan-Animate achieves state-of-the-art performance.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.14055v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.14055v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.13936v1'></a></p>
<h2 id="noise-level-diffusion-guidance-well-begun-is-half-done"><a href="https://arxiv.org/abs/2509.13936v1">Noise-Level Diffusion Guidance: Well Begun is Half Done</a></h2>
<p><strong>Authors:</strong> Harvey Mannering, Zhiwu Huang, Adam Prugel-Bennett</p>
<p><strong>Published:</strong> 2025-09-17</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Diffusion models have achieved state-of-the-art image generation. However,
the random Gaussian noise used to start the diffusion process influences the
final output, causing variations in image quality and prompt adherence.
Existing noise-level optimization approaches generally rely on extra dataset
construction, additional networks, or backpropagation-based optimization,
limiting their practicality. In this paper, we propose Noise Level Guidance
(NLG), a simple, efficient, and general noise-level optimization approach that
refines initial noise by increasing the likelihood of its alignment with
general guidance - requiring no additional training data, auxiliary networks,
or backpropagation. The proposed NLG approach provides a unified framework
generalizable to both conditional and unconditional diffusion models,
accommodating various forms of diffusion-level guidance. Extensive experiments
on five standard benchmarks demonstrate that our approach enhances output
generation quality and input condition adherence. By seamlessly integrating
with existing guidance methods while maintaining computational efficiency, our
method establishes NLG as a practical and scalable enhancement to diffusion
models. Code can be found at
https://github.com/harveymannering/NoiseLevelGuidance.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Harvey Manneringç­äººæ°åçè®ºæâNoise-Level Diffusion Guidance: Well Begun is Half Doneâçå¨é¢æè¦ã</p>
<hr />
<h3 id="noise-level-diffusion-guidance-well-begun-is-half-done_1">è®ºææè¦ï¼Noise-Level Diffusion Guidance: Well Begun is Half Done</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
æ©æ£æ¨¡åå¨å¾åçææ¹é¢åå¾äºæåè¿çæ§è½ï¼ä½å¶çæè´¨éåå¯¹æç¤ºçä¾ä»æ§åå°åå§éæºé«æ¯åªå£°çæ¾èå½±åãç°æçåªå£°çº§å«ä¼åï¼NLOï¼æ¹æ³éå¸¸ä¾èµäºé¢å¤çæ°æ®éæå»ºãè¾å©ç½ç»æåºäºååä¼ æ­çä¼åï¼è¿éå¶äºå®ä»¬çå®ç¨æ§åå¯æ©å±æ§ãå æ­¤ï¼è¯¥ç ç©¶æ¨å¨è§£å³å¦ä½å¼åä¸ç§ç®åãé«æãéç¨çåªå£°çº§å«ä¼åæ¹æ³ï¼ä»¥æé«æ©æ£æ¨¡åçè¾åºè´¨éåæ¡ä»¶ä¾ä»æ§ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
æ¬ææåºäº<strong>åªå£°çº§å«å¼å¯¼ï¼Noise Level Guidance, NLGï¼</strong>ï¼è¿æ¯ä¸ç§æ°é¢çåªå£°çº§å«ä¼åæ¹æ³ï¼å¶æ ¸å¿åæ°å¨äºï¼
*   <strong>ç®åé«æï¼</strong> NLGéè¿å¢å åå§åªå£°ä¸éç¨å¼å¯¼ï¼å¦ææ¬æç¤ºãç±»å«æ ç­¾æè´¨éåº¦éï¼å¯¹é½çå¯è½æ§æ¥ä¼ååå§åªå£°ï¼èæ éé¢å¤è®­ç»æ°æ®ãè¾å©ç½ç»æååä¼ æ­ãå®éè¿å¯¹æ©æ£æ¨¡åå¨ä¸åæ¡ä»¶ä¸çè¾åºè¿è¡ç®åçº¿æ§ç»åæ¥æ¨å¯¼åºç¼è¾æ¹åï¼ç¶åè¿­ä»£å°åºç¨äºåå§åªå£°ã
*   <strong>éç¨æ§ï¼</strong> NLGæä¾äºä¸ä¸ªç»ä¸çæ¡æ¶ï¼å¯æ¨å¹¿å°æ¡ä»¶åæ æ¡ä»¶æ©æ£æ¨¡åï¼å¹¶éåºåç§å½¢å¼çæ©æ£çº§å«å¼å¯¼ï¼å¦Classifier-Free Guidance (CFG) å Autoguidance (AutoG)ï¼ã
*   <strong>é¿ååå¸å¤éè¯¯ï¼</strong> ä¸ºäºé²æ­¢åªå£°å¨è¿­ä»£åºç¨ç¼è¾æ¹åæ¶åç¦»æ åæ­£æåå¸ï¼NLGéç¨äºæ¹åè£åªãæ·»å å°éé«æ¯åªå£°åå½ä¸åç­ç­ç¥ï¼ä»¥ä¿æåªå£°çåå¸åç¹æ§ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æ§è½æåï¼</strong> å¨äºä¸ªæ ååºåä¸çå¤§éå®éªè¡¨æï¼NLGæ¹æ³æ¾èæé«äºè¾åºçæè´¨éåè¾å¥æ¡ä»¶ä¾ä»æ§ã
*   <strong>è®¡ç®æçï¼</strong> NLGä¸ç°æå¼å¯¼æ¹æ³æ ç¼éæï¼åæ¶ä¿æäºè®¡ç®æçãä¸ç«äºæ¹æ³InitNOç¸æ¯ï¼NLGçéåº¦å¿«äº4åï¼åå­ä½¿ç¨éåå°äº3åã
*   <strong>æ³åè½åï¼</strong> NLGå¨æ æ¡ä»¶åæ¡ä»¶å¾åçæä»»å¡ä¸­è¡¨ç°åºåè¶çæ³åè½åï¼éç¨äºä¸åç±»åçæ©æ£æ¨¡åï¼å¦Stable Diffusion v2.1, v1.5, v3.5, FLUX.1-devåEDM2ï¼ã
*   <strong>ç¨æ·ç ç©¶ï¼</strong> ç¨æ·ç ç©¶ç»æè¡¨æï¼å¨æ CFGè®¾ç½®ä¸ï¼NLGå¨å¾åçå®æåææ¬æç¤ºå¯¹é½æ¹é¢åä¼äºé«æ¯åªå£°åºçº¿ï¼å·æç»è®¡å­¦æä¹ä¸çæ¾èåå¥½ãå¨CFGå¯ç¨æ¶ï¼NLGä¹ä¿æäºæ´é«çèçã
*   <strong>åºç¨æ¢ç´¢ï¼</strong> NLGè½å¤æææ¹åä¸è¾å¥æç¤ºå¯¹é½ä¸ä½³çå¾åï¼å¹¶è½éè¿å¨åªå£°å¯¹é½é¶æ®µä½¿ç¨ç¹å®æç¤ºæ¥ä¸ºåå§åªå£°æ·»å ç»æï¼ä»èå¨çæé¶æ®µå¡«åå¾åè¯­ä¹ãå®è¿å±ç¤ºäºè·¨æ¨¡ååªå£°å¯¹é½åæ¨ççè½åã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>é«CFGå¼å¯¼ä¸çææï¼</strong> è®ºææåºï¼å¨é«CFGå¼å¯¼è®¾ç½®ä¸ï¼NLGççå¤æ¯æå°çï¼è¿ä¸ä¹åçä¸äºåªå£°çº§å«ä¼åå·¥ä½ç¸ä¼¼ãç¶èï¼å¯¹äºä½CLIPåæ°ï¼å³å¯¹é½ä¸ä½³ï¼çå¾åï¼å³ä½¿å¨é«å¼å¯¼ä¸ï¼NLGä»ç¶å¯è½æ¯æççã
*   <strong>åæ­¥æ©æ£æ¨¡åçéå¶ï¼</strong> è®ºææå°ï¼ReNOç­æ¹æ³åéäºåæ­¥æ©æ£æ¨¡åï¼å ä¸ºæ¯ä¸ªæ©æ£æ­¥éª¤é½å¿é¡»è¿è¡ååä¼ æ­ãè½ç¶NLGä¸åæ­¤éå¶ï¼ä½å¯¹äºåSD-Turboè¿æ ·ç»è¿è¸é¦çåæ­¥æ©æ£æ¨¡åï¼NLGå¯è½ä¸ä¼å¸¦æ¥æ¾èçå¾åæ¹è¿ï¼è¿å¯è½æ¯å ä¸ºè¿äºæ¨¡åå¨æ¨çæ¶ä¸éè¦CFGã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ¢ç´¢æ´å¤ä¼å¿ï¼</strong> æªæ¥çå·¥ä½å°ä¾§éäºæ¢ç´¢NLGæ¹æ³å¨åå§åªå£°ä¼åæ¹é¢çå¶ä»ä¼å¿ï¼ä¾å¦å¨æ´é«æ©æ£çº§å«å¼å¯¼ä¸ï¼ä»¥åå¨ç»æååå§åªå£°åè·¨æ©æ£æ¨¡åç­æ°åè½æ¹é¢çè¿ä¸æ­¥åºç¨ã
*   <strong>è¶åæ°è°ä¼ï¼</strong> è®ºææåºï¼å¨EDM2å®éªä¸­ï¼NLGçåæ°è®¾ç½®ä»æè¿ä¸æ­¥æ¹è¿çæ½åï¼å¯ä»¥éè¿è¶åæ°è°ä¼æ¥ä¼åæ§è½ã</p>
<hr />
<p>è¿ç¯è®ºæéè¿æåºNLGï¼ä¸ºæ©æ£æ¨¡åé¢åæä¾äºä¸ä¸ªå®ç¨ä¸é«æçåªå£°ä¼åè§£å³æ¹æ¡ï¼åæäºç°ææ¹æ³çè®¡ç®åæ°æ®ä¾èµæ§éå¶ï¼æ¾èæåäºå¾åçæè´¨éåæ¡ä»¶ä¾ä»æ§ï¼ä¸ºæ©æ£æ¨¡åçè¿ä¸æ­¥åå±å¥ å®äºåºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Diffusion models have achieved state-of-the-art image generation.</li>
<li>In this paper, we propose Noise Level Guidance
(NLG), a simple, efficient, and general noise-level optimization approach that
refines initial noise by increasing the likelihood of its alignment with
general guidance - requiring no additional training data, auxiliary networks,
or backpropagation.</li>
<li>Extensive experiments
on five standard benchmarks demonstrate that our approach enhances output
generation quality and input condition adherence.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.13936v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.13936v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.13926v1'></a></p>
<h2 id="map-end-to-end-autonomous-driving-with-map-assisted-planning"><a href="https://arxiv.org/abs/2509.13926v1">MAP: End-to-End Autonomous Driving with Map-Assisted Planning</a></h2>
<p><strong>Authors:</strong> Huilin Yin, Yiming Kan, Daniel Watzenig</p>
<p><strong>Published:</strong> 2025-09-17</p>
<p><strong>Categories:</strong> cs.RO, cs.AI, cs.CV, I.2.9; I.2.10</p>
<p><strong>Abstract:</strong></p>
<p>In recent years, end-to-end autonomous driving has attracted increasing
attention for its ability to jointly model perception, prediction, and planning
within a unified framework. However, most existing approaches underutilize the
online mapping module, leaving its potential to enhance trajectory planning
largely untapped. This paper proposes MAP (Map-Assisted Planning), a novel
map-assisted end-to-end trajectory planning framework. MAP explicitly
integrates segmentation-based map features and the current ego status through a
Plan-enhancing Online Mapping module, an Ego-status-guided Planning module, and
a Weight Adapter based on current ego status. Experiments conducted on the
DAIR-V2X-seq-SPD dataset demonstrate that the proposed method achieves a 16.6%
reduction in L2 displacement error, a 56.2% reduction in off-road rate, and a
44.5% improvement in overall score compared to the UniV2X baseline, even
without post-processing. Furthermore, it achieves top ranking in Track 2 of the
End-to-End Autonomous Driving through V2X Cooperation Challenge of MEIS
Workshop @CVPR2025, outperforming the second-best model by 39.5% in terms of
overall score. These results highlight the effectiveness of explicitly
leveraging semantic map features in planning and suggest new directions for
improving structure design in end-to-end autonomous driving systems. Our code
is available at https://gitee.com/kymkym/map.git</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Huilin Yin, Yiming Kan, Daniel Watzenigæ°åçè®ºæâMAP: End-to-End Autonomous Driving with Map-Assisted Planningâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼MAP: End-to-End Autonomous Driving with Map-Assisted Planning</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
ç«¯å°ç«¯èªå¨é©¾é©¶ç³»ç»å å¶è½å¤å¨ä¸ä¸ªç»ä¸æ¡æ¶åèåå»ºæ¨¡æç¥ãé¢æµåè§åèå¤åå³æ³¨ãç¶èï¼ç°æçå¤§å¤æ°æ¹æ³æªè½ååå©ç¨å¨çº¿å°å¾æ¨¡åçæ½åï¼å¯¼è´å¶å¨è½¨è¿¹è§åæ¹é¢çå¢å¼ºä½ç¨æªè¢«ååææãæ¬ç ç©¶æ¨å¨è§£å³å¦ä½æææ´åè¯­ä¹å°å¾ç¹å¾ä»¥æ¾èæåç«¯å°ç«¯èªå¨é©¾é©¶ç³»ç»çè½¨è¿¹è§åæ§è½ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³å­¦è´¡ç®ï¼</strong>
æ¬è®ºææåºäºMAPï¼Map-Assisted Planningï¼ï¼ä¸ä¸ªæ°é¢çå°å¾è¾å©ç«¯å°ç«¯è½¨è¿¹è§åæ¡æ¶ï¼å¶ä¸»è¦åæ°ç¹å¨äºï¼
*   <strong>æ¾å¼æ´åå°å¾ç¹å¾ï¼</strong> MAPéè¿ä¸ä¸ª<strong>è§åå¢å¼ºå¨çº¿å°å¾æ¨¡åï¼Plan-enhancing Online Mapping moduleï¼</strong>ï¼æ¾å¼å°å°åºäºåå²çå°å¾ç¹å¾ä¸å½åèªè½¦ç¶æç¸ç»åãè¿è§£å³äºä¼ ç»æ¹æ³ä¸­å°å¾ä¿¡æ¯å©ç¨ä¸è¶³çé®é¢ã
*   <strong>èªè½¦ç¶æå¼å¯¼çè§åï¼</strong> å¼å¥äº<strong>èªè½¦ç¶æå¼å¯¼è§åæ¨¡åï¼Ego-status-guided Planning moduleï¼</strong>ï¼è¿ä¸æ­¥ç»åäºè§åè¿ç¨ï¼ä½¿å¶æ´è´´åå½åè½¦è¾çå®æ¶ç¶æã
*   <strong>æéééå¨ï¼</strong> åºäºå½åèªè½¦ç¶æï¼è®¾è®¡äºä¸ä¸ª<strong>æéééå¨ï¼Weight Adapterï¼</strong>ï¼ç¨äºå¨æèåæ¥èªå¨çº¿å°å¾æ¨¡ååèªè½¦ç¶æå¼å¯¼è§åæ¨¡åçè¾åºï¼ä»¥çææç»çè½¨è¿¹è§åã
*   <strong>UniV2Xåºçº¿çæ©å±ï¼</strong> MAPæ¯UniV2Xåºçº¿çä¸ä¸ªæ©å±ï¼éè¿å¼å¥ä¸è¿°æ¨¡åï¼æ¾èæåäºå¶æ§è½ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
å¨DAIR-V2X-seq-SPDæ°æ®éä¸è¿è¡çå®éªç»æè¡¨æï¼MAPæ¹æ³åå¾äºæ¾èçæ§è½æåï¼
*   <strong>L2ä½ç§»è¯¯å·®ï¼L2 displacement errorï¼éä½16.6%</strong>ã
*   <strong>åç¦»éè·¯çï¼off-road rateï¼éä½56.2%</strong>ã
*   <strong>æ´ä½å¾åï¼overall scoreï¼æå44.5%</strong>ã
è¿äºæ¹è¿æ¯å¨æ²¡æåå¤ççæåµä¸å®ç°çï¼å¸æ¾äºè¯¥æ¹æ³çå¼ºå¤§æ§è½ã</p>
<p>æ­¤å¤ï¼MAPå¨CVPR2025 MEIS WorkshopçV2Xååææèµç«¯å°ç«¯èªå¨é©¾é©¶èµé2ä¸­è·å¾äº<strong>ç¬¬ä¸å</strong>ï¼å¶æ´ä½å¾åæ¯ç¬¬äºåæ¨¡åé«åº39.5%ã</p>
<p>è¿äºç»æå¼ºè°äºæ¾å¼å©ç¨è¯­ä¹å°å¾ç¹å¾å¨è§åä¸­çæææ§ï¼å¹¶ä¸ºç«¯å°ç«¯èªå¨é©¾é©¶ç³»ç»çç»æè®¾è®¡æä¾äºæ°çæ¹åã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸­å¹¶æªæç¡®æåå·ä½çå±éæ§ãç¶èï¼ä»å¶å¼ºè°âæ¾å¼å©ç¨è¯­ä¹å°å¾ç¹å¾âåâç»æè®¾è®¡çæ°æ¹åâæ¥çï¼å¯ä»¥æ¨æ­åºå½åç«¯å°ç«¯ç³»ç»å¨å°å¾ä¿¡æ¯å©ç¨åæ¨¡ååè®¾è®¡æ¹é¢ä»ææ¹è¿ç©ºé´ãMAPçæåä¹æç¤ºäºç°ææ¹æ³å¯è½å¨å¤æåºæ¯ä¸å¯¹å°å¾ä¿¡æ¯çä¾èµæ§ä¸è¶³ï¼æèèåæ¹å¼ä¸å¤ä¼åã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ´å¤æçå°å¾ç¹å¾èåï¼</strong> æ¢ç´¢é¤äºåå²ä¹å¤ï¼å¦ä½èåæ´å¤æ ·åçå°å¾ç¹å¾ï¼å¦ææä¿¡æ¯ãè¯­ä¹å³ç³»ç­ï¼ï¼ä»¥è¿ä¸æ­¥æåè§åçé²æ£æ§ååç¡®æ§ã
*   <strong>å¨ææéééçä¼åï¼</strong> æ·±å¥ç ç©¶æéééå¨ï¼ä½¿å¶è½å¤æ ¹æ®æ´å¤æçç¯å¢ä¸ä¸æåé©¾é©¶æå¾è¿è¡å¨æè°æ´ï¼èéä»ä»åºäºå½åèªè½¦ç¶æã
*   <strong>å¯è§£éæ§ä¸å®å¨æ§ï¼</strong> éçç«¯å°ç«¯ç³»ç»å¤ææ§çå¢å ï¼å¦ä½æé«å¶å³ç­çå¯è§£éæ§åå®å¨æ§å°æ¯éè¦çç ç©¶æ¹åã
*   <strong>å¤æ¨¡ææç¥ä¸å°å¾çååï¼</strong> æ¢ç´¢å¦ä½å°å°å¾ä¿¡æ¯ä¸é·è¾¾ãæ¿åé·è¾¾ç­å¶ä»ä¼ æå¨æ°æ®è¿è¡æ´æ·±å±æ¬¡çèåï¼ä»¥åºå¯¹æ¶å£å¤©æ°åä¼ æå¨å¤±æç­ææã
*   <strong>æ³åè½åï¼</strong> å¨æ´å¤æ ·åçæ°æ®éåçå®ä¸çåºæ¯ä¸­éªè¯MAPçæ³åè½åï¼å¹¶æ¢ç´¢å¦ä½ä½¿å¶éåºä¸åå°çåºååäº¤éè§åã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>This paper proposes MAP (Map-Assisted Planning), a novel
map-assisted end-to-end trajectory planning framework.</li>
<li>These results highlight the effectiveness of explicitly
leveraging semantic map features in planning and suggest new directions for
improving structure design in end-to-end autonomous driving systems.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.13926v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.13926v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.13883v1'></a></p>
<h2 id="evhand-fpv-efficient-event-based-3d-hand-tracking-from-first-person-view"><a href="https://arxiv.org/abs/2509.13883v1">EvHand-FPV: Efficient Event-Based 3D Hand Tracking from First-Person View</a></h2>
<p><strong>Authors:</strong> Zhen Xu, Guorui Lu, Chang Gao, Qinyu Chen</p>
<p><strong>Published:</strong> 2025-09-17</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Hand tracking holds great promise for intuitive interaction paradigms, but
frame-based methods often struggle to meet the requirements of accuracy, low
latency, and energy efficiency, especially in resource-constrained settings
such as Extended Reality (XR) devices. Event cameras provide <script type="math/tex">\mu</script>s-level
temporal resolution at mW-level power by asynchronously sensing brightness
changes. In this work, we present EvHand-FPV, a lightweight framework for
egocentric First-Person-View 3D hand tracking from a single event camera. We
construct an event-based FPV dataset that couples synthetic training data with
3D labels and real event data with 2D labels for evaluation to address the
scarcity of egocentric benchmarks. EvHand-FPV also introduces a wrist-based
region of interest (ROI) that localizes the hand region via geometric cues,
combined with an end-to-end mapping strategy that embeds ROI offsets into the
network to reduce computation without explicit reconstruction, and a multi-task
learning strategy with an auxiliary geometric feature head that improves
representations without test-time overhead. On our real FPV test set,
EvHand-FPV improves 2D-AUCp from 0.77 to 0.85 while reducing parameters from
11.2M to 1.2M by 89% and FLOPs per inference from 1.648G to 0.185G by 89%. It
also maintains a competitive 3D-AUCp of 0.84 on synthetic data. These results
demonstrate accurate and efficient egocentric event-based hand tracking
suitable for on-device XR applications. The dataset and code are available at
https://github.com/zen5x5/EvHand-FPV.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºâEvHand-FPV: Efficient Event-Based 3D Hand Tracking from First-Person Viewâè®ºæçä¸­ææè¦ï¼æ¶µçäºæ¨è¦æ±çææè¦ç¹ï¼</p>
<p><strong>è®ºææè¦ï¼EvHand-FPVï¼åºäºäºä»¶çç¬¬ä¸äººç§°è§è§é«æ3Dæé¨è¿½è¸ª</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
æé¨è¿½è¸ªå¯¹äºç´è§äº¤äºèå¼å·æå·¨å¤§æ½åï¼ä½ä¼ ç»çåºäºå¸§çæ¹æ³å¨æ»¡è¶³æ©å±ç°å®ï¼XRï¼è®¾å¤ç­èµæºåéç¯å¢å¯¹ç²¾åº¦ãä½å»¶è¿åè½æçè¦æ±æ¹é¢é¢ä¸´ææãäºä»¶ç¸æºéè¿å¼æ­¥æç¥äº®åº¦ååæä¾å¾®ç§çº§æ¶é´åè¾¨çåæ¯«ç¦çº§åèï¼ä¸ºè§£å³è¿äºé®é¢æä¾äºæ°çéå¾ãæ¬ç ç©¶æ¨å¨å¼åä¸ç§è½»éçº§ãé«æçåºäºäºä»¶çç¬¬ä¸äººç§°è§è§ï¼FPVï¼3Dæé¨è¿½è¸ªæ¡æ¶ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è´¡ç®ï¼</strong>
*   <strong>äºä»¶FPVæ°æ®éæå»ºï¼</strong> è®ºææå»ºäºä¸ä¸ªäºä»¶FPVæ°æ®éï¼ç»åäºå¸¦æ3Dæ ç­¾çåæè®­ç»æ°æ®åå¸¦æ2Dæ ç­¾ççå®äºä»¶æ°æ®ï¼ä»¥è§£å³ç¬¬ä¸äººç§°è§è§åºåç¨ç¼ºçé®é¢ã
*   <strong>èé¨ROIå®ä½ä¸ç«¯å°ç«¯æ å°ç­ç¥ï¼</strong> å¼å¥äºä¸ç§åºäºèé¨çå´è¶£åºåï¼ROIï¼å®ä½æ¹æ³ï¼éè¿å ä½çº¿ç´¢å±é¨åæé¨åºåãç»åç«¯å°ç«¯æ å°ç­ç¥ï¼å°ROIåç§»åµå¥ç½ç»ä¸­ï¼ä»èå¨æ éæ¾å¼éå»ºçæåµä¸åå°è®¡ç®éã
*   <strong>å¤ä»»å¡å­¦ä¹ ç­ç¥ï¼</strong> éç¨å¸¦æè¾å©å ä½ç¹å¾å¤´çå¤ä»»å¡å­¦ä¹ ç­ç¥ï¼å¨ä¸å¢å æµè¯æ¶å¼éçæåµä¸ï¼æé«äºè¡¨å¾è½åã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   å¨çå®FPVæµè¯éä¸ï¼EvHand-FPVå°2D-AUCpä»0.77æé«å°0.85ï¼åæ¶å°åæ°éä»11.2Måå°å°1.2Mï¼éä½89%ï¼ï¼æ¯æ¬¡æ¨ççFLOPsä»1.648Gåå°å°0.185Gï¼éä½89%ï¼ã
*   å¨åææ°æ®ä¸ï¼EvHand-FPVä¿æäº0.84çç«äºæ§3D-AUCpã
*   è¿äºç»æè¡¨æï¼EvHand-FPVå®ç°äºåç¡®é«æçäºä»¶åºç¬¬ä¸äººç§°è§è§æé¨è¿½è¸ªï¼éå¸¸éåå¨XRè®¾å¤ä¸è¿è¡é¨ç½²ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸­æ²¡ææç¡®æåå·ä½çå±éæ§ï¼ä½ä»å¶åæ°ç¹åæªæ¥å·¥ä½æ¹åå¯ä»¥æ¨æ­åºä¸äºæ½å¨çæ¹é¢ãä¾å¦ï¼è½ç¶æå»ºäºæ°æ®éï¼ä½äºä»¶æ°æ®æ¬èº«çç¹æ§ï¼å¦ç¨çæ§ãåªå£°ï¼å¯è½ä»ç¶æ¯ææãæ­¤å¤ï¼è½ç¶å®ç°äºé«ææ§ï¼ä½è¿ä¸æ­¥ä¼åä»¥éåºæ´å¹¿æ³çXRè®¾å¤ååºç¨å¯è½ä»éåªåã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ´å¹¿æ³çæ³åæ§ï¼</strong> æ¢ç´¢å¦ä½å°EvHand-FPVæ¨å¹¿å°æ´å¤æ ·åçç¨æ·ãæé¨å§¿æåç¯å¢æ¡ä»¶ã
*   <strong>å¤æ¨¡æèåï¼</strong> ç»åäºä»¶æ°æ®ä¸å¶ä»ä¼ æå¨ï¼å¦æ¯æ§æµéååIMUï¼çæ°æ®ï¼ä»¥è¿ä¸æ­¥æé«è¿½è¸ªçé²æ£æ§åç²¾åº¦ã
*   <strong>å®æ¶é¨ç½²ä¼åï¼</strong> è¿ä¸æ­¥ä¼åæ¨¡ååç®æ³ï¼ä»¥å®ç°æ´ä½çå»¶è¿åæ´é«çå¸§çï¼æ»¡è¶³æ´ä¸¥æ ¼çå®æ¶XRåºç¨éæ±ã
*   <strong>æ°æ®éæ©å±ï¼</strong> æç»­æ©å±äºä»¶FPVæ°æ®éï¼åå«æ´å¤åºæ¯ãæ´å¤æ ·åçæé¨å¨ä½åæ´ç²¾ç»çæ æ³¨ï¼ä»¥æ¯ææ´å¤æçæ¨¡åè®­ç»ã
*   <strong>äºä»¶æ°æ®å¤çï¼</strong> æ¢ç´¢æ´åè¿çäºä»¶æ°æ®å¤çåç¹å¾æåæ¹æ³ï¼ä»¥æ´å¥½å°å©ç¨äºä»¶ç¸æºçç¬ç¹ä¼å¿ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we present EvHand-FPV, a lightweight framework for
egocentric First-Person-View 3D hand tracking from a single event camera.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.13883v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.13883v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.13858v1'></a></p>
<h2 id="edits-enhancing-dataset-distillation-with-implicit-textual-semantics"><a href="https://arxiv.org/abs/2509.13858v1">EDITS: Enhancing Dataset Distillation with Implicit Textual Semantics</a></h2>
<p><strong>Authors:</strong> Qianxin Xia, Jiawei Du, Guoming Lu, Zhiyong Shu, Jielei Wang</p>
<p><strong>Published:</strong> 2025-09-17</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Dataset distillation aims to synthesize a compact dataset from the original
large-scale one, enabling highly efficient learning while preserving
competitive model performance. However, traditional techniques primarily
capture low-level visual features, neglecting the high-level semantic and
structural information inherent in images. In this paper, we propose EDITS, a
novel framework that exploits the implicit textual semantics within the image
data to achieve enhanced distillation. First, external texts generated by a
Vision Language Model (VLM) are fused with image features through a Global
Semantic Query module, forming the prior clustered buffer. Local Semantic
Awareness then selects representative samples from the buffer to construct
image and text prototypes, with the latter produced by guiding a Large Language
Model (LLM) with meticulously crafted prompt. Ultimately, Dual Prototype
Guidance strategy generates the final synthetic dataset through a diffusion
model. Extensive experiments confirm the effectiveness of our method.Source
code is available in: https://github.com/einsteinxia/EDITS.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾å¯¹Qianxin Xiaç­äººæ°åçè®ºæâEDITS: Enhancing Dataset Distillation with Implicit Textual Semanticsâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼EDITS: Enhancing Dataset Distillation with Implicit Textual Semantics</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢</strong>
æ°æ®éè¸é¦ï¼Dataset Distillation, DDï¼æ¨å¨ä»å¤§è§æ¨¡åå§æ°æ®éä¸­åæä¸ä¸ªç´§åçæ°æ®éï¼ä»¥å®ç°é«æå­¦ä¹ å¹¶ä¿ææç«äºåçæ¨¡åæ§è½ãç¶èï¼ä¼ ç»DDæ¹æ³ä¸»è¦å³æ³¨ä½çº§è§è§ç¹å¾ï¼å¾å¾å¿½ç¥äºå¾åä¸­åºæçé«çº§è¯­ä¹åç»æä¿¡æ¯ãè¿å¯¼è´è¸é¦åºçæ°æ®éå¨è¯­ä¹ä¸°å¯æ§æ¹é¢ä¸è¶³ï¼å½±åäºå¶å¨è·¨æ¶ææ³ååä¿¡æ¯æå¤±æ¹é¢çæ§è½ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®</strong>
æ¬ææåºäºä¸ä¸ªåä¸ºEDITSï¼Enhancing Dataset Distillation with Implicit Textual Semanticsï¼çæ°é¢æ¡æ¶ï¼éè¿å©ç¨å¾åæ°æ®ä¸­éå«çææ¬è¯­ä¹æ¥å¢å¼ºæ°æ®éè¸é¦ãå¶å³é®åæ°åè´¡ç®åæ¬ï¼</p>
<ul>
<li><strong>å©ç¨å¤é¨ææ¬è¯­ä¹å¢å¼ºè¸é¦ï¼</strong> EDITSæ¯é¦ä¸ªå©ç¨å¤é¨ææ¬ä¿¡å·å®ç°å¢å¼ºDDçæ¡æ¶ï¼è§£å³äºä¼ ç»æ¹æ³ä»ä¾èµè§è§ç¹å¾çå±éæ§ã</li>
<li><strong>å¨å±è¯­ä¹æ¥è¯¢ï¼Global Semantic Query, GSQï¼ï¼</strong> è¯¥æ¨¡åéè¿è§è§è¯­è¨æ¨¡åï¼VLMï¼å¦LLaVAï¼çæçå¤é¨ææ¬ä¸å¾åç¹å¾è¿è¡èåãå®è®¡ç®æ¯ä¸ªææ¬æè¿°å¯¹å¾åçå½±ååæ°ï¼æå»ºä¸ä¸ªåæ­¥çèç±»ç¼å²åºï¼ä»èå¨é¢å°å°ææ¬è¯­ä¹èå¥å¾åç¹å¾ï¼å¢å¼ºåç»­ååè¡¨ç¤ºè½åã</li>
<li><strong>å±é¨è¯­ä¹æç¥ï¼Local Semantic Awareness, LSAï¼ï¼</strong> LSAä»èç±»ç¼å²åºä¸­éæ©å·æä»£è¡¨æ§çåéæ ·æ¬ï¼ä»¥æå»ºå¾ååææ¬ååãå¾åååéè¿VAEç¼ç å¨çæï¼èææ¬åååéè¿ç²¾å¿è®¾è®¡çæç¤ºï¼promptï¼å¼å¯¼å¤§åè¯­è¨æ¨¡åï¼LLMï¼å¦DeepSeekï¼è¿è¡æ»ç»ï¼è§£å³äºç´æ¥ä½¿ç¨èç±»ä¸­å¿ä½ä¸ºååæåºæçè¯­ä¹ä¸è¶³é®é¢ã</li>
<li><strong>åååæå¯¼ï¼Dual-Prototype Guidance, DPGï¼ï¼</strong> æç»ï¼éè¿æ©æ£æ¨¡åï¼Latent Diffusion Modelï¼ç»åå¾ååææ¬ååï¼çææç»çåææ°æ®éï¼ç¡®ä¿äºè¸é¦æ°æ®çå¤æ ·æ§åä»£è¡¨æ§ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹</strong>
å¹¿æ³çå®éªè¯å®äºEDITSæ¹æ³çæææ§ï¼</p>
<ul>
<li><strong>æ§è½æåï¼</strong> å¨ImageNetå­éï¼ImageWoofãImageNetteãImageIDCï¼ä¸ï¼EDITSå¨ä¸åIPCï¼æ¯ç±»å¾åæ°ï¼åæ¶æï¼ConvNet-6ãResNetAP-10ãResNet-18ï¼è®¾ç½®ä¸ï¼å§ç»æ¯ç°ææåè¿çæ¹æ³é«åºçº¦1%-3%ãè¿ååè¯æäºå¶å¨ä¸åæ°æ®éåæ¶æä¸çé²æ£æ§åæ³åè½åã</li>
<li><strong>ä½åè¾¨çæ°æ®éè¡¨ç°ï¼</strong> å¨CIFAR-10åCIFAR-100ç­ä½åè¾¨çæ°æ®éä¸ï¼EDITSä¹è¶è¶äºä»å©ç¨ææ¬ååçVLCPæ¹æ³ï¼è¿ä¸æ­¥è¯å®äºå¶å¨é¢è¯­ä¹å¢å¼ºçæææ§ã</li>
<li><strong>ååè´¨éï¼</strong> å®éªç»æè¡¨æï¼EDITSçæçååå¨è¯­ä¹ä¸æ´å·è§£éæ§ï¼è½å¤æ´å¥½å°ææå¾åçé«çº§è¯­ä¹ä¿¡æ¯ï¼èéä»ä»æ¯ä½çº§è§è§çº¹çã</li>
</ul>
<p>è¿äºç»æè¡¨æï¼éè¿æ´åéå¼ææ¬è¯­ä¹ï¼EDITSè½å¤çææ´é«è´¨éãæ´å·è¯­ä¹ä»£è¡¨æ§çè¸é¦æ°æ®éï¼æ¾èæåäºæ°æ®éè¸é¦çæ§è½åæ³åè½åã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§</strong>
è®ºæä¸­æå°äºä»¥ä¸å ç¹å±éæ§ï¼</p>
<ul>
<li><strong>LLMè¾å¥éå¶ï¼</strong> LLMçè¾å¥åéäºtokenæ°éï¼è¿å¤çè¾å¥ä¼å¢å çæææ¬ååçæ¶é´å¼éã</li>
<li><strong>ææ¬ä¿¡æ¯éï¼</strong> è¿å¤çææ¬ä¿¡æ¯å¯è½åç¦»å¾åååçè¯­ä¹è¡¨ç¤ºï¼å®éªè§å¯å°ææ¬éå¢å ä¼å¯¼è´æ§è½ä¸éãè¿è¡¨æéè¦å¹³è¡¡ææ¬ä¿¡æ¯çä¸°å¯æ§åç¸å³æ§ã</li>
<li><strong>è®¡ç®ææ¬ï¼</strong> è½ç¶æ°æ®éè¸é¦æ¨å¨æé«æçï¼ä½VLMåLLMçä½¿ç¨ï¼ä»¥åæ©æ£æ¨¡åççæè¿ç¨ï¼å¯è½ä»ç¶æ¶åä¸å®çè®¡ç®ææ¬ã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å</strong>
åºäºä¸è¿°å·¥ä½åå±éæ§ï¼è®ºææç¤ºæå¯ä»¥æ¨æ­åºä»¥ä¸æªæ¥ç ç©¶æ¹åï¼</p>
<ul>
<li><strong>æ´é«æçè¯­ä¹æåï¼</strong> æ¢ç´¢æ´è½»éçº§ææ´é«æçVLMåLLMï¼ä»¥åå°è¯­ä¹æååååçæçæ¶é´åè®¡ç®ææ¬ã</li>
<li><strong>èªéåºææ¬ä¿¡æ¯éï¼</strong> ç ç©¶å¦ä½æ ¹æ®å¾ååå®¹åç±»å«èªéåºå°ç¡®å®æä½³çææ¬ä¿¡æ¯éï¼ä»¥é¿åä¿¡æ¯è¿è½½æä¸è¶³ã</li>
<li><strong>å¤æ¨¡æååèåä¼åï¼</strong> è¿ä¸æ­¥ä¼åå¾ååææ¬ååçèåç­ç¥ï¼ä¾å¦æ¢ç´¢æ´å¤æçæ³¨æåæºå¶æå¤æ¨¡æTransformerç»æï¼ä»¥æ´å¥½å°ææè·¨æ¨¡æçç»å¾®è¯­ä¹å³èã</li>
<li><strong>è·¨é¢ååé¶æ ·æ¬è¸é¦ï¼</strong> æ¢ç´¢EDITSæ¡æ¶å¨æ´å¹¿æ³çé¢ååé¶æ ·æ¬å­¦ä¹ åºæ¯ä¸çåºç¨ï¼ä»¥éªè¯å¶å¨æ´å·æææ§ç¯å¢ä¸çæ³åè½åã</li>
<li><strong>å¯è§£éæ§å¢å¼ºï¼</strong> è¿ä¸æ­¥ç ç©¶å¦ä½éååå¯è§åææ¬è¯­ä¹å¯¹è¸é¦è¿ç¨åæç»æ¨¡åæ§è½çå½±åï¼ä»¥æé«æ¹æ³çå¯è§£éæ§ã</li>
<li><strong>ç»åå¶ä»è¸é¦èå¼ï¼</strong> æ¢ç´¢å°EDITSçè¯­ä¹å¢å¼ºææ³ä¸å¹éåºDDæ¹æ³æå¶ä»çæåºDDæ¹æ³ç¸ç»åï¼ä»¥æè¾¾å°æ´ä¼çæ§è½ã</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we propose EDITS, a
novel framework that exploits the implicit textual semantics within the image
data to achieve enhanced distillation.</li>
<li>Extensive experiments confirm the effectiveness of our method.Source
code is available in: https://github.com/einsteinxia/EDITS.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.13858v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.13858v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.13766v1'></a></p>
<h2 id="ndlpnet-a-location-aware-nighttime-deraining-network-and-a-real-world-benchmark-dataset"><a href="https://arxiv.org/abs/2509.13766v1">NDLPNet: A Location-Aware Nighttime Deraining Network and a Real-World Benchmark Dataset</a></h2>
<p><strong>Authors:</strong> Huichun Liu, Xiaosong Li, Yang Liu, Xiaoqi Cheng, Haishu Tan</p>
<p><strong>Published:</strong> 2025-09-17</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Visual degradation caused by rain streak artifacts in low-light conditions
significantly hampers the performance of nighttime surveillance and autonomous
navigation. Existing image deraining techniques are primarily designed for
daytime conditions and perform poorly under nighttime illumination due to the
spatial heterogeneity of rain distribution and the impact of light-dependent
stripe visibility. In this paper, we propose a novel Nighttime Deraining
Location-enhanced Perceptual Network(NDLPNet) that effectively captures the
spatial positional information and density distribution of rain streaks in
low-light environments. Specifically, we introduce a Position Perception Module
(PPM) to capture and leverage spatial contextual information from input data,
enhancing the model's capability to identify and recalibrate the importance of
different feature channels. The proposed nighttime deraining network can
effectively remove the rain streaks as well as preserve the crucial background
information. Furthermore, We construct a night scene rainy (NSR) dataset
comprising 900 image pairs, all based on real-world nighttime scenes, providing
a new benchmark for nighttime deraining task research. Extensive qualitative
and quantitative experimental evaluations on both existing datasets and the NSR
dataset consistently demonstrate our method outperform the state-of-the-art
(SOTA) methods in nighttime deraining tasks. The source code and dataset is
available at https://github.com/Feecuin/NDLPNet.</p>
<p><strong>Analysis:</strong></p>
<p>ä»¥ä¸æ¯Huichun Liu, Xiaosong Li, Yang Liu, Xiaoqi Cheng, Haishu Tanæ°åçè®ºæâNDLPNet: A Location-Aware Nighttime Deraining Network and a Real-World Benchmark Datasetâçå¨é¢æè¦ï¼</p>
<p><strong>1. è§£å³çä¸»è¦é®é¢æç ç©¶é®é¢</strong>
è¯¥è®ºææ¨å¨è§£å³å¤é´ä½åç§æ¡ä»¶ä¸é¨çº¹å¯¹è§è§ç³»ç»ï¼å¦å¤é´çæ§åèªå¨é©¾é©¶ï¼é æçä¸¥éå¾åéåé®é¢ãç°æçå»é¨ææ¯ä¸»è¦éå¯¹ç½å¤©åºæ¯è®¾è®¡ï¼å¨å¤é´ç±äºé¨æ°´åå¸çç©ºé´å¼è´¨æ§ååç§ä¾èµçé¨çº¹å¯è§æ§å·®å¼èè¡¨ç°ä¸ä½³ãå æ­¤ï¼æ ¸å¿é®é¢æ¯å¦ä½å¨å¤é´å¤æçåç§æ¡ä»¶ä¸ï¼ææå»é¤é¨çº¹å¹¶ä¿çèæ¯ç»èã</p>
<p><strong>2. å³é®åæ°ææ¹æ³å­¦è´¡ç®</strong>
*   <strong>NDLPNetæ¨¡åï¼</strong> æåºäºä¸ç§æ°é¢çå¤é´å»é¨ä½ç½®å¢å¼ºæç¥ç½ç»ï¼Nighttime Deraining Location-enhanced Perceptual Network, NDLPNetï¼ãè¯¥ç½ç»è½å¤æææè·ä½åç§ç¯å¢ä¸­é¨çº¹çç©ºé´ä½ç½®ä¿¡æ¯åå¯åº¦åå¸ã
*   <strong>ä½ç½®æç¥æ¨¡åï¼Position Perception Module, PPMï¼ï¼</strong> å¼å¥PPMæ¨¡åï¼ç¨äºæè·åå©ç¨è¾å¥æ°æ®çç©ºé´ä¸ä¸æä¿¡æ¯ï¼å¢å¼ºæ¨¡åè¯å«åéæ°æ ¡åä¸åç¹å¾éééè¦æ§çè½åãPPMç»åäºç©ºé´ä½ç½®ç¼ç ï¼Spatial Position Coding, SPCï¼åé«æééæ³¨æåï¼Efficient Channel Attention, ECAï¼ã
    *   <strong>ç©ºé´ä½ç½®ç¼ç ï¼SPCï¼ï¼</strong> éå¯¹å¤é´é¨çº¹çç¹ç¹ï¼æåºäºä¸ç§æ´ç²¾ç»åèªéåºçä½ç½®ç¼ç æ¨¡åï¼ä¸ä»èèäºç»´ç©ºé´åæ ï¼è¿å å¥äºé¨ç²å¯åº¦ä¿¡æ¯ï¼ä»¥å¸®å©æ¨¡åçè§£åå©ç¨å¤ç©ºé´æ¹åä¸çä½ç½®ä¿¡æ¯ååå¸ã
    *   <strong>é«æééæ³¨æåï¼ECAï¼ï¼</strong> å¨ä½ç½®ç¼ç ä¹åï¼éè¿ECAæ¨¡åè¿ä¸æ­¥ä¼åç¹å¾æåï¼åå°åä½ä¿¡æ¯çå½±åï¼å¹¶æ ¹æ®åç´ ä½ç½®çç¹å®ç¼ç è°æ´ééæéï¼ä»èæ´å¥½å°ä¿çå¾åç»èã
*   <strong>å¤é´åºæ¯é¨å¤©ï¼NSRï¼æ°æ®éï¼</strong> æå»ºäºä¸ä¸ªåå«900å¯¹å¾åçåçå®å¤é´åºæ¯é¨å¤©æ°æ®éãè¯¥æ°æ®éåºäºçå®çå¤é´åºæ¯ï¼ä¸ºå¤é´å»é¨ä»»å¡ç ç©¶æä¾äºæ°çåºåï¼å¼¥è¡¥äºç°æå¤é´é¨å¤©æ°æ®éæ°éåå¤æ ·æ§ä¸è¶³çç¼ºç¹ï¼å¹¶åå«äºå¤é´åºæ¯å¸¸è§çåªå£°ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹</strong>
*   <strong>åè¶çå»é¨æ§è½ï¼</strong> å¨ç°ææ°æ®éï¼å¦GTAV-NightRainï¼åæ°æå»ºçNSRæ°æ®éä¸è¿è¡çå¹¿æ³å®æ§åå®éå®éªè¯ä¼°è¡¨æï¼NDLPNetå¨å¤é´å»é¨ä»»å¡ä¸­å§ç»ä¼äºæåè¿ï¼SOTAï¼æ¹æ³ãä¾å¦ï¼å¨GTAV-NightRainæ°æ®éä¸ï¼NDLPNetçPSNRæ¯RLPé«åº2.49dBãå¨NSRæ°æ®éä¸ï¼NDLPNetä¹åå¾äºæé«çPSNR/SSIMå¼ã
*   <strong>èæ¯ä¿¡æ¯ä¿çï¼</strong> æåºçç½ç»è½å¤ææå»é¤é¨çº¹ï¼åæ¶ä¿çå³é®çèæ¯ä¿¡æ¯ï¼é¿åäºç°ææ¹æ³ä¸­å¸¸è§çæ¨¡ç³åå¤±çé®é¢ã
*   <strong>æ³åè½åï¼</strong> å¨ç½å¤©åºæ¯æ°æ®éï¼å¦Rain200LåRain200Hï¼ä¸çå®éªä¹éªè¯äºè¯¥æ¹æ³å·æè¾å¼ºçæ³åè½åï¼å¨Rain200Hæ°æ®éä¸åå¾äºæä½³æ§è½ã
*   <strong>æ¨¡åæææ§ï¼</strong> æ¶èå®éªè¯å®äºPPMæ¨¡åï¼åæ¬SPCåECAï¼çæææ§ï¼è¡¨æä¸ç»´ç©ºé´ä½ç½®ä¿¡æ¯å¯¹ææå»é¨è³å³éè¦ï¼å¹¶ä¸ECAæ¨¡åæå©äºæ¨¡åæ´å¥½å°å³æ³¨ç¸å³ä¿¡æ¯å¹¶åå°åä½æ°æ®çå½±åã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§</strong>
*   <strong>æ¨¡åæçï¼</strong> æåºçæ¹æ³å¨æ¨¡åæçæ¹é¢å­å¨å±éæ§ã
*   <strong>ç½å¤©åºæ¯æ³åè½åï¼</strong> å°½ç®¡å¨ç½å¤©åºæ¯ä¸­è¡¨ç°è¯å¥½ï¼ä½å¶æ³åè½åä»éè¿ä¸æ­¥æ¹è¿ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å</strong>
*   <strong>æ¨¡åä¼åï¼</strong> æªæ¥çå·¥ä½å°ä¸æ³¨äºä¼åæ¨¡åçæçã
*   <strong>æ³åè½åæåï¼</strong> è¿ä¸æ­¥æåæ¨¡åå¨ç½å¤©åºæ¯ä¸­çæ³åè½åã
*   <strong>ä»»å¡æ©å±ï¼</strong> å°å»é¨ä»»å¡æ©å±å°é¨æ»´å»é¤ã</p>
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæéè¿å¼å¥NDLPNetæ¨¡ååPPMæ¨¡åï¼å¹¶æå»ºäºNSRæ°æ®éï¼ä¸ºå¤é´å¾åå»é¨é¢åååºäºéè¦è´¡ç®ï¼ææè§£å³äºå¤é´å¤æåç§æ¡ä»¶ä¸é¨çº¹å»é¤çææï¼å¹¶ä¸ºæªæ¥çç ç©¶æä¾äºåå®çåºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we propose a novel Nighttime Deraining
Location-enhanced Perceptual Network(NDLPNet) that effectively captures the
spatial positional information and density distribution of rain streaks in
low-light environments.</li>
<li>Specifically, we introduce a Position Perception Module
(PPM) to capture and leverage spatial contextual information from input data,
enhancing the model's capability to identify and recalibrate the importance of
different feature channels.</li>
<li>Furthermore, We construct a night scene rainy (NSR) dataset
comprising 900 image pairs, all based on real-world nighttime scenes, providing
a new benchmark for nighttime deraining task research.</li>
<li>Extensive qualitative
and quantitative experimental evaluations on both existing datasets and the NSR
dataset consistently demonstrate our method outperform the state-of-the-art
(SOTA) methods in nighttime deraining tasks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.13766v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.13766v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.13754v1'></a></p>
<h2 id="cross-modal-full-mode-fine-grained-alignment-for-text-to-image-person-retrieval"><a href="https://arxiv.org/abs/2509.13754v1">Cross-modal Full-mode Fine-grained Alignment for Text-to-Image Person Retrieval</a></h2>
<p><strong>Authors:</strong> Hao Yin, Xin Man, Feiyu Chen, Jie Shao, Heng Tao Shen</p>
<p><strong>Published:</strong> 2025-09-17</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Text-to-Image Person Retrieval (TIPR) is a cross-modal matching task that
aims to retrieve the most relevant person images based on a given text query.
The key challenge in TIPR lies in achieving effective alignment between textual
and visual modalities within a common latent space. To address this challenge,
prior approaches incorporate attention mechanisms for implicit cross-modal
local alignment. However, they lack the ability to verify whether all local
features are correctly aligned. Moreover, existing methods primarily focus on
hard negative samples during model updates, with the goal of refining
distinctions between positive and negative pairs, often neglecting incorrectly
matched positive pairs. To alleviate these issues, we propose FMFA, a
cross-modal Full-Mode Fine-grained Alignment framework, which enhances global
matching through explicit fine-grained alignment and existing implicit
relational reasoning -- hence the term ``full-mode" -- without requiring
additional supervision. Specifically, we design an Adaptive Similarity
Distribution Matching (A-SDM) module to rectify unmatched positive sample
pairs. A-SDM adaptively pulls the unmatched positive pairs closer in the joint
embedding space, thereby achieving more precise global alignment. Additionally,
we introduce an Explicit Fine-grained Alignment (EFA) module, which makes up
for the lack of verification capability of implicit relational reasoning. EFA
strengthens explicit cross-modal fine-grained interactions by sparsifying the
similarity matrix and employs a hard coding method for local alignment. Our
proposed method is evaluated on three public datasets, achieving
state-of-the-art performance among all global matching methods. Our code is
available at https://github.com/yinhao1102/FMFA.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Hao Yin, Xin Man, Feiyu Chen, Jie Shao, Heng Tao Shenæ°åçè®ºæâCross-modal Full-mode Fine-grained Alignment for Text-to-Image Person Retrievalâçå¨é¢æè¦ã</p>
<hr />
<h3 id="_1">è®ºææè¦ï¼è·¨æ¨¡æå¨æ¨¡å¼ç»ç²åº¦å¯¹é½ç¨äºææ¬å°å¾åäººç©æ£ç´¢</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
ææ¬å°å¾åäººç©æ£ç´¢ï¼TIPRï¼çæ ¸å¿ææå¨äºå¦ä½å¨å±åçæ½å¨ç©ºé´ä¸­å®ç°ææ¬åè§è§æ¨¡æä¹é´çææå¯¹é½ï¼ä»¥æ ¹æ®ææ¬æ¥è¯¢æ£ç´¢æç¸å³çäººç©å¾åãç°ææ¹æ³ä¸»è¦éè¿éå¼è·¨æ¨¡æå±é¨å¯¹é½æ¥è§£å³ï¼ä½å®ä»¬ç¼ºä¹éªè¯ææå±é¨ç¹å¾æ¯å¦æ­£ç¡®å¯¹é½çè½åãæ­¤å¤ï¼ç°ææ¹æ³å¨æ¨¡åæ´æ°æ¶ä¸»è¦å³æ³¨å°é¾è´æ ·æ¬ï¼æ¨å¨ç»åæ­£è´æ ·æ¬ä¹é´çåºå«ï¼å´å¾å¾å¿½ç¥äºéè¯¯å¹éçæ­£æ ·æ¬ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ä¸ºäºè§£å³ä¸è¿°é®é¢ï¼ä½èæåºäº<strong>FMFAï¼Full-Mode Fine-grained Alignmentï¼</strong>æ¡æ¶ï¼éè¿æ¾å¼ç»ç²åº¦å¯¹é½åç°æéå¼å³ç³»æ¨çæ¥å¢å¼ºå¨å±å¹éï¼èæ éé¢å¤çç£ãå¶ä¸»è¦åæ°åæ¬ï¼</p>
<ul>
<li><strong>èªéåºç¸ä¼¼åº¦åå¸å¹éï¼A-SDMï¼æ¨¡åï¼</strong> æ¨å¨çº æ­£æªå¹éçæ­£æ ·æ¬å¯¹ãA-SDMå¨èååµå¥ç©ºé´ä¸­èªéåºå°å°æªå¹éçæ­£æ ·æ¬å¯¹æè¿ï¼ä»èå®ç°æ´ç²¾ç¡®çå¨å±å¯¹é½ãå®æ ¹æ®æªå¹éæ­£æ ·æ¬å¯¹çç¸å¯¹è·ç¦»èªéåºå°è°æ´æåã</li>
<li><strong>æ¾å¼ç»ç²åº¦å¯¹é½ï¼EFAï¼æ¨¡åï¼</strong> å¼¥è¡¥äºéå¼å³ç³»æ¨çç¼ºä¹éªè¯è½åçä¸è¶³ãEFAéè¿ç¨çåç¸ä¼¼åº¦ç©éµå¹¶éç¨ç¡¬ç¼ç æ¹æ³è¿è¡å±é¨å¯¹é½ï¼ä»èå å¼ºäºæ¾å¼è·¨æ¨¡æç»ç²åº¦äº¤äºãè¿ç§è®¾è®¡éè¿æ¾å¼èåç¨çç¸ä¼¼åº¦ç©éµï¼å¹¶å¯¹èååçå¤æ¨¡æè¡¨ç¤ºä¸å¶åå§è§è§åææ¬è¡¨ç¤ºè¿è¡ç¡¬ç¼ç å¯¹é½ï¼ä»¥æå°ååä½ä¿¡æ¯åè®¡ç®ææ¬ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
FMFAå¨ä¸ä¸ªå¬å¼æ°æ®éï¼CUHK-PEDESãICFG-PEDESåRSTPReidï¼ä¸è¿è¡äºè¯ä¼°ï¼å¹¶å¨ææå¨å±å¹éæ¹æ³ä¸­åå¾äºæåè¿çæ§è½ã</p>
<ul>
<li><strong>CUHK-PEDESæ°æ®éï¼</strong> å¨ä¸ä½¿ç¨ReIDåé¢è®­ç»çVL-Backbonesæ¶ï¼FMFAå¨Rank-1åç¡®çåmAPä¸ä¼äºææç°æå¨å±å¹éæ¹æ³ãå¨ä½¿ç¨ReIDåé¢è®­ç»çVL-Backbonesæ¶ï¼FMFAä¹ä¿æäºä¼è¶æ§ã</li>
<li><strong>RSTPReidæ°æ®éï¼</strong> FMFAåå¾äºå·æç«äºåçæ§è½ï¼å¨Rank-1åRank-5ä¸åä¼äºåºçº¿IRRAã</li>
<li><strong>ICFG-PEDESæ°æ®éï¼</strong> FMFAå¨ææè¯ä¼°ææ ä¸ååå¾äºæä½³æ§è½ï¼Rank-1åmAPåææ¾èæåã</li>
<li><strong>æ¶èç ç©¶ï¼</strong> è¯å®äºA-SDMåEFAæ¨¡åå¯¹æ´ä½æ§è½çè´¡ç®ï¼å®ä»¬å±åä½ç¨ï¼ç¸äºè¡¥åã</li>
<li><strong>æ¨çéåº¦ï¼</strong> ä½ä¸ºä¸ç§å¨å±å¹éæ¹æ³ï¼FMFAå¨æ¨çé¶æ®µä»è®¡ç®å¨å±ç¹å¾ï¼å æ­¤æ¯å±é¨å¹éæ¹æ³å·ææ´é«çæ¨çéåº¦ï¼å°¤å¶æ¯å¨æµè¯éè§æ¨¡å¢å¤§æ¶ã</li>
</ul>
<p>è¿äºç»æè¡¨æFMFAå·æè¯å¥½çæ³åæ§åé²æ£æ§ï¼è½å¤ææè§£å³TIPRä¸­çå¯¹é½ææã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
è®ºææåºï¼EFAæ¨¡åä¸­ç¨çè¿ç¨çåºå®éå¼ä»ä¿çæç¸å³çå¾ååï¼è¿å¯è½å¯¼è´è¯­ä¹ä¿¡æ¯çä¸¢å¤±ï¼å¹¶éå¶å±é¨ç¹å¾çææèåï¼ä»èå½±åæ¨¡åçæ´ä½æ§è½ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
ä¸ºäºåæä¸è¿°å±éæ§ï¼æªæ¥çç ç©¶æ¹åå¯ä»¥åæ¬ï¼
*   æ´åè½å¤æè·å®æ´è¯­ä¹ä¿¡æ¯çèªéåºæ¹æ³ï¼ä¾å¦æ å½¢Transformerï¼tree transformer [43]ï¼ï¼ä»¥è¿ä¸æ­¥å¢å¼ºæ¨¡åæ§è½ã</p>
<hr />
<p>è¿ä»½æè¦çªåºäºè®ºæçæ ¸å¿è´¡ç®ï¼å³éè¿ç»åèªéåºç¸ä¼¼åº¦åå¸å¹éåæ¾å¼ç»ç²åº¦å¯¹é½ï¼è§£å³äºææ¬å°å¾åäººç©æ£ç´¢ä¸­è·¨æ¨¡æå¯¹é½çææï¼å¹¶å¨å¤ä¸ªåºåæµè¯ä¸­åå¾äºé¢åçæ§è½ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To alleviate these issues, we propose FMFA, a
cross-modal Full-Mode Fine-grained Alignment framework, which enhances global
matching through explicit fine-grained alignment and existing implicit
relational reasoning -- hence the term ``full-mode" -- without requiring
additional supervision.</li>
<li>Additionally,
we introduce an Explicit Fine-grained Alignment (EFA) module, which makes up
for the lack of verification capability of implicit relational reasoning.</li>
<li>Our
proposed method is evaluated on three public datasets, achieving
state-of-the-art performance among all global matching methods.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.13754v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.13754v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-09-18 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
