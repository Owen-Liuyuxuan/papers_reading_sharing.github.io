<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-09-18 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-09-17/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-09-19/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-09-18">Arxiv Computer Vision Papers - 2025-09-18</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#sail-vl2-technical-report" class="nav-link">SAIL-VL2 Technical Report</a>
                </li>
                <li class="nav-item">
                    <a href="#panorama-the-rise-of-omnidirectional-vision-in-the-embodied-ai-era" class="nav-link">PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era</a>
                </li>
                <li class="nav-item">
                    <a href="#mars2-2025-challenge-on-multimodal-reasoning-datasets-methods-results-discussion-and-outlook" class="nav-link">MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods, Results, Discussion, and Outlook</a>
                </li>
                <li class="nav-item">
                    <a href="#wan-animate-unified-character-animation-and-replacement-with-holistic-replication" class="nav-link">Wan-Animate: Unified Character Animation and Replacement with Holistic Replication</a>
                </li>
                <li class="nav-item">
                    <a href="#noise-level-diffusion-guidance-well-begun-is-half-done" class="nav-link">Noise-Level Diffusion Guidance: Well Begun is Half Done</a>
                </li>
                <li class="nav-item">
                    <a href="#map-end-to-end-autonomous-driving-with-map-assisted-planning" class="nav-link">MAP: End-to-End Autonomous Driving with Map-Assisted Planning</a>
                </li>
                <li class="nav-item">
                    <a href="#evhand-fpv-efficient-event-based-3d-hand-tracking-from-first-person-view" class="nav-link">EvHand-FPV: Efficient Event-Based 3D Hand Tracking from First-Person View</a>
                </li>
                <li class="nav-item">
                    <a href="#edits-enhancing-dataset-distillation-with-implicit-textual-semantics" class="nav-link">EDITS: Enhancing Dataset Distillation with Implicit Textual Semantics</a>
                </li>
                <li class="nav-item">
                    <a href="#ndlpnet-a-location-aware-nighttime-deraining-network-and-a-real-world-benchmark-dataset" class="nav-link">NDLPNet: A Location-Aware Nighttime Deraining Network and a Real-World Benchmark Dataset</a>
                </li>
                <li class="nav-item">
                    <a href="#cross-modal-full-mode-fine-grained-alignment-for-text-to-image-person-retrieval" class="nav-link">Cross-modal Full-mode Fine-grained Alignment for Text-to-Image Person Retrieval</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-09-18">Arxiv Computer Vision Papers - 2025-09-18</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，这是一份针对2025年9月17日Arxiv计算机视觉论文的执行摘要，旨在帮助忙碌的研究人员快速了解最新进展。</p>
<hr />
<p><strong>Arxiv 计算机视觉每日报告执行摘要 (2025-09-17)</strong></p>
<p><strong>概述：</strong>
今日Arxiv计算机视觉论文呈现出多模态学习、具身智能、生成模型和实际应用场景（如自动驾驶、夜间去雨）的强劲发展趋势。特别是，大型视觉语言模型（VLMs）的进步、全向视觉在具身AI中的崛起以及高效数据处理和模型训练方法是核心主题。</p>
<p><strong>主要主题与趋势：</strong></p>
<ol>
<li><strong>多模态与大型模型：</strong> 多篇论文聚焦于视觉语言模型（VLMs）的扩展、评估和应用，强调其在复杂推理任务中的潜力。</li>
<li><strong>具身智能与全向视觉：</strong> 具身AI领域持续升温，全向视觉作为关键感知模式受到高度关注，旨在为智能体提供更全面的环境理解。</li>
<li><strong>生成模型与扩散模型：</strong> 扩散模型在图像生成和动画领域的应用进一步深化，研究人员致力于提升其效率和控制力。</li>
<li><strong>实际应用与鲁棒性：</strong> 自动驾驶、夜间图像处理、手部追踪等实际应用场景是研究重点，强调模型在复杂真实世界条件下的性能和鲁棒性。</li>
<li><strong>数据效率与对齐：</strong> 数据蒸馏、跨模态对齐等技术旨在提高数据利用效率，并解决多模态数据之间的语义鸿沟。</li>
</ol>
<p><strong>特别显著或创新论文：</strong></p>
<ul>
<li><strong>"PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era" (Xu Zheng et al.)</strong>：这篇论文可能是一篇具有里程碑意义的综述或前瞻性工作，系统性地探讨了全向视觉在具身AI中的重要性、挑战和未来方向。它预示着具身智能领域感知范式的重大转变。</li>
<li><strong>"SAIL-VL2 Technical Report" (Weijie Yin et al.)</strong>：作为一份技术报告，它可能详细介绍了某个大型视觉语言模型的架构、训练方法和性能，对于理解当前VLMs的SOTA（State-of-the-Art）至关重要。</li>
<li><strong>"Noise-Level Diffusion Guidance: Well Begun is Half Done" (Harvey Mannering et al.)</strong>：这篇论文可能提出了扩散模型训练或推理过程中的关键优化，通过改进噪声处理来显著提升生成质量或效率，具有潜在的广泛影响。</li>
</ul>
<p><strong>新兴研究方向或技术：</strong></p>
<ul>
<li><strong>全向视觉在具身AI中的普及：</strong> 不再仅仅是特定任务的解决方案，而是成为具身智能体感知系统的核心组件。</li>
<li><strong>多模态推理挑战的标准化：</strong> "MARS2 2025 Challenge"表明社区正在积极推动多模态推理的基准测试和方法创新。</li>
<li><strong>扩散模型的精细化控制与效率提升：</strong> "Noise-Level Diffusion Guidance"等工作预示着扩散模型将更加可控、高效。</li>
<li><strong>事件相机在具身感知中的应用：</strong> "EvHand-FPV"展示了事件相机在低延迟、高动态范围场景下（如手部追踪）的独特优势。</li>
<li><strong>隐式文本语义在数据蒸馏中的利用：</strong> "EDITS"提出了一种新颖的数据蒸馏方法，可能为高效模型训练提供新思路。</li>
</ul>
<p><strong>建议阅读全文的论文：</strong></p>
<ol>
<li><strong>"PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era" (Xu Zheng et al.)</strong>：对于关注具身AI和未来感知范式的研究人员，这篇论文是必读的，它提供了宏观的视角和潜在的未来研究方向。</li>
<li><strong>"SAIL-VL2 Technical Report" (Weijie Yin et al.)</strong>：如果你对大型视觉语言模型的最新进展和技术细节感兴趣，这份技术报告将提供宝贵的洞察。</li>
<li><strong>"MAP: End-to-End Autonomous Driving with Map-Assisted Planning" (Huilin Yin et al.)</strong>：对于自动驾驶领域的研究人员，这篇论文可能展示了端到端自动驾驶与高精地图结合的最新进展和实际效果。</li>
<li><strong>"Noise-Level Diffusion Guidance: Well Begun is Half Done" (Harvey Mannering et al.)</strong>：如果你从事生成模型或扩散模型的研究，这篇论文可能提供了提升模型性能或效率的关键技术。</li>
<li><strong>"MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods, Results, Discussion, and Outlook" (Peng Xu et al.)</strong>：对于希望了解多模态推理领域最新挑战、数据集和方法的研究人员，这篇论文提供了全面的概览。</li>
</ol>
<hr />
<p>这份摘要旨在帮助您快速筛选出与您研究方向最相关的论文，并了解当前计算机视觉领域的热点和前沿。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2509.14033v1">SAIL-VL2 Technical Report</a></li>
<li><a href="#2509.12989v1">PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era</a></li>
<li><a href="#2509.14142v1">MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods, Results, Discussion, and Outlook</a></li>
<li><a href="#2509.14055v1">Wan-Animate: Unified Character Animation and Replacement with Holistic Replication</a></li>
<li><a href="#2509.13936v1">Noise-Level Diffusion Guidance: Well Begun is Half Done</a></li>
<li><a href="#2509.13926v1">MAP: End-to-End Autonomous Driving with Map-Assisted Planning</a></li>
<li><a href="#2509.13883v1">EvHand-FPV: Efficient Event-Based 3D Hand Tracking from First-Person View</a></li>
<li><a href="#2509.13858v1">EDITS: Enhancing Dataset Distillation with Implicit Textual Semantics</a></li>
<li><a href="#2509.13766v1">NDLPNet: A Location-Aware Nighttime Deraining Network and a Real-World Benchmark Dataset</a></li>
<li><a href="#2509.13754v1">Cross-modal Full-mode Fine-grained Alignment for Text-to-Image Person Retrieval</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2509.14033v1'></a></p>
<h2 id="sail-vl2-technical-report"><a href="https://arxiv.org/abs/2509.14033v1">SAIL-VL2 Technical Report</a></h2>
<p><strong>Authors:</strong> Weijie Yin, Yongjie Ye, Fangxun Shu, Yue Liao, Zijian Kang, Hongyuan Dong, Haiyang Yu, Dingkang Yang, Jiacong Wang, Han Wang, Wenzhuo Liu, Xiao Liang, Shuicheng Yan, Chao Feng</p>
<p><strong>Published:</strong> 2025-09-17</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM)
for comprehensive multimodal understanding and reasoning. As the successor to
SAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8B
parameter scales across diverse image and video benchmarks, demonstrating
strong capabilities from fine-grained perception to complex reasoning. Three
core innovations drive its effectiveness. First, a large-scale data curation
pipeline with scoring and filtering strategies enhances both quality and
distribution across captioning, OCR, QA, and video data, improving training
efficiency. Second, a progressive training framework begins with a powerful
pre-trained vision encoder (SAIL-ViT), advances through multimodal
pre-training, and culminates in a thinking-fusion SFT-RL hybrid paradigm that
systematically strengthens model capabilities. Third, architectural advances
extend beyond dense LLMs to efficient sparse Mixture-of-Experts (MoE) designs.
With these contributions, SAIL-VL2 demonstrates competitive performance across
106 datasets and achieves state-of-the-art results on challenging reasoning
benchmarks such as MMMU and MathVista. Furthermore, on the OpenCompass
leaderboard, SAIL-VL2-2B ranks first among officially released open-source
models under the 4B parameter scale, while serving as an efficient and
extensible foundation for the open-source multimodal community.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于Weijie Yin等人在计算机视觉和机器学习领域撰写的论文“SAIL-VL2 Technical Report”的全面摘要。</p>
<p><strong>论文摘要：SAIL-VL2技术报告</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决当前视觉-语言模型（LVM）在实现全面多模态理解和推理方面的挑战。尽管现有LVM在性能上有所提升，但在处理复杂推理任务、实现细粒度感知以及在不同模态间有效对齐方面仍存在不足。SAIL-VL2的目标是开发一个高效、可扩展的开源LVM，能够超越现有技术，在广泛的多模态任务中展现出卓越的性能。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
SAIL-VL2的有效性主要由三项核心创新驱动：</p>
<ul>
<li><strong>大规模数据整理管道：</strong> 论文设计了一个大规模数据整理管道，通过评分和过滤策略，提升了图像标注、光学字符识别（OCR）、问答（QA）和视频数据的质量和分布，从而提高了训练效率。这解决了现有数据集中可能存在的噪声和分布偏差问题。</li>
<li><strong>渐进式训练框架：</strong> 引入了一个分阶段的训练框架，首先使用强大的预训练视觉编码器（SAIL-ViT），然后进行多模态预训练，最终采用“思考-融合”的SFT-RL（监督微调-强化学习）混合范式，系统性地增强了模型的各项能力。这包括：<ul>
<li><strong>SAIL-ViT的渐进式优化：</strong> 通过三阶段（热身适应、细粒度对齐、世界知识注入）训练策略，将多粒度知识注入视觉编码器，实现与LLM的全面对齐。</li>
<li><strong>AdaLRS（自适应学习率搜索）：</strong> 在基础多模态预训练阶段引入动态学习率调度器，以提高优化效率和效果。</li>
<li><strong>数据重采样策略：</strong> 在预训练阶段采用两步重采样策略，以缓解大规模标注和VQA数据中的分布偏差，增强多样性，并防止模式崩溃。</li>
<li><strong>模型汤（Model Soup）策略：</strong> 在SFT后，通过合并同质模型来进一步提升模型性能，实现稳定且显著的性能改进。</li>
</ul>
</li>
<li><strong>高效的稀疏专家混合（MoE）设计：</strong> 架构创新超越了传统的密集LLM，采用了高效的稀疏MoE设计。这在保持计算效率的同时，实现了参数规模的扩展，并通过平衡专家激活、数据分布感知调优和专家专业化保留策略，确保了MoE的稳定性和可扩展性。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
SAIL-VL2在多个维度上展示了卓越的性能：</p>
<ul>
<li><strong>领先的性能：</strong> SAIL-VL2在2B和8B参数规模下，在106个数据集上实现了最先进的性能，涵盖了图像和视频基准测试。</li>
<li><strong>细粒度感知与复杂推理：</strong> 模型在细粒度感知到复杂推理任务中都表现出强大的能力，尤其在MMMU和MathVista等挑战性推理基准测试中取得了最先进的结果。</li>
<li><strong>OpenCompass排行榜表现：</strong> SAIL-VL2-2B在OpenCompass排行榜上，在4B参数规模以下的官方发布开源模型中排名第一，证明了其作为高效且可扩展的开源多模态社区基础模型的竞争力。</li>
<li><strong>视觉-文本对齐：</strong> SAIL-ViT能够有效地缩小视觉和文本特征空间之间的差距，使得视觉特征向量更加紧凑，并与文本特征向量有更大的重叠。</li>
<li><strong>多模态理解任务：</strong> 在通用多模态理解、文档图像理解和多图像与视频理解任务中，SAIL-VL2均取得了领先或最先进的性能。</li>
</ul>
<p><strong>4. 论文中提及的局限性：</strong>
论文中未明确提及当前SAIL-VL2模型的具体局限性。然而，它暗示了未来研究的方向，这通常也反映了当前模型的改进空间：</p>
<ul>
<li><strong>合成数据中的语言偏差：</strong> 论文指出，尽管合成数据有助于大规模训练，但LLM生成的合成数据可能引入语言表达上的分布偏差，导致同质化措辞和有限的变异性。</li>
<li><strong>推理任务的复杂性：</strong> 尽管SAIL-VL2在复杂推理任务上表现出色，但“思考-融合”的SFT-RL混合范式仍是持续强化的重点，表明这些任务仍有进一步提升的空间。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
论文展望了SAIL-VL2系列的未来发展，包括：</p>
<ul>
<li><strong>更高效的架构：</strong> 持续探索和开发更高效的LVM架构。</li>
<li><strong>全面的预训练策略：</strong> 进一步优化和完善预训练策略。</li>
<li><strong>改进的强化学习范式：</strong> 深入研究和应用更先进的强化学习方法，以持续提升多模态智能。</li>
<li><strong>推动开源多模态生态系统：</strong> 将SAIL-VL2作为高效且可扩展的基础模型，赋能更广泛的开源多模态社区。</li>
</ul>
<p>总而言之，SAIL-VL2技术报告详细介绍了一个在数据整理、训练策略和架构方面进行创新的视觉-语言基础模型。它在多项基准测试中取得了显著的性能提升，尤其在推理任务中表现突出，并有望成为开源多模态领域的重要推动力。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM)
for comprehensive multimodal understanding and reasoning.</li>
<li>As the successor to
SAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8B
parameter scales across diverse image and video benchmarks, demonstrating
strong capabilities from fine-grained perception to complex reasoning.</li>
<li>With these contributions, SAIL-VL2 demonstrates competitive performance across
106 datasets and achieves state-of-the-art results on challenging reasoning
benchmarks such as MMMU and MathVista.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.14033v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.14033v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.12989v1'></a></p>
<h2 id="panorama-the-rise-of-omnidirectional-vision-in-the-embodied-ai-era"><a href="https://arxiv.org/abs/2509.12989v1">PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era</a></h2>
<p><strong>Authors:</strong> Xu Zheng, Chenfei Liao, Ziqiao Weng, Kaiyu Lei, Zihao Dongfang, Haocong He, Yuanhuiyi Lyu, Lutao Jiang, Lu Qi, Li Chen, Danda Pani Paudel, Kailun Yang, Linfeng Zhang, Luc Van Gool, Xuming Hu</p>
<p><strong>Published:</strong> 2025-09-16</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Omnidirectional vision, using 360-degree vision to understand the
environment, has become increasingly critical across domains like robotics,
industrial inspection, and environmental monitoring. Compared to traditional
pinhole vision, omnidirectional vision provides holistic environmental
awareness, significantly enhancing the completeness of scene perception and the
reliability of decision-making. However, foundational research in this area has
historically lagged behind traditional pinhole vision. This talk presents an
emerging trend in the embodied AI era: the rapid development of omnidirectional
vision, driven by growing industrial demand and academic interest. We highlight
recent breakthroughs in omnidirectional generation, omnidirectional perception,
omnidirectional understanding, and related datasets. Drawing on insights from
both academia and industry, we propose an ideal panoramic system architecture
in the embodied AI era, PANORAMA, which consists of four key subsystems.
Moreover, we offer in-depth opinions related to emerging trends and
cross-community impacts at the intersection of panoramic vision and embodied
AI, along with the future roadmap and open challenges. This overview
synthesizes state-of-the-art advancements and outlines challenges and
opportunities for future research in building robust, general-purpose
omnidirectional AI systems in the embodied AI era.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供由Xu Zheng等人撰写的论文“PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era”的全面摘要。</p>
<hr />
<p><strong>论文摘要：PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决全向视觉（360度视觉）在具身AI时代日益增长的重要性与该领域基础研究相对滞后之间的差距。传统针孔视觉提供了狭窄的视场，而全向视觉能提供对环境的整体感知，这对于机器人、工业检测和环境监测等具身AI任务至关重要。然而，全向视觉在数据瓶颈、模型能力和应用空白方面面临挑战，阻碍了其在具身AI领域的进一步发展。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
论文的核心贡献是提出了一个理想的全向系统架构——<strong>PANORAMA</strong>，它由四个关键子系统组成，旨在整合全向视觉与具身AI：
*   <strong>子系统1：数据采集与预处理（Data Acquisition &amp; Pre-processing）</strong>：负责捕获原始全向数据并转换为适合计算处理的格式，包括数据捕获、格式转换、同步与校准。
*   <strong>子系统2：感知（Perception）</strong>：对预处理后的全向数据进行基础场景感知，利用适应球面几何的深度学习模型提取丰富的结构化信息，包括特征提取和环境感知（语义分割、目标检测、深度估计）。
*   <strong>子系统3：应用（Application）</strong>：将感知洞察转化为具身AI智能体的行动，服务于导航与SLAM、人机交互、数字孪生与3D重建等下游任务。
*   <strong>子系统4：加速与部署（Acceleration &amp; Employment）</strong>：解决处理高分辨率全向数据在资源受限环境下的计算挑战，通过软件加速（模型量化、剪枝）和硬件部署（边缘计算平台）确保整个流程的计算可行性。</p>
<p>此外，论文还回顾了全向视觉在<strong>生成、感知、理解</strong>和<strong>相关数据集</strong>方面的最新突破，并提出了一个分阶段的未来路线图，以构建一个理想的统一全向任务模型。</p>
<p><strong>3. 主要结果及其意义：</strong>
论文通过提出PANORAMA系统架构，为具身AI时代的全向视觉发展提供了一个全面的框架。其意义在于：
*   <strong>系统化解决挑战</strong>：PANORAMA架构系统地解决了全向视觉在数据、模型和应用层面的挑战，为实现通用和鲁棒的具身智能奠定了基础。
*   <strong>促进跨社区影响</strong>：全向视觉的成熟被视为一项基础性使能技术，能够促进机器人、自主导航、人机交互、认知AI和虚拟智能体等多个领域的跨社区突破。
*   <strong>指明未来发展方向</strong>：论文综合了学术界和工业界的见解，提出了一个详细的未来路线图（六个阶段：数据集整合、多模态扩展、推理与具身数据、统一模型预训练、评估与基准测试、部署与泛化），为构建全向AI系统提供了清晰的路径。</p>
<p><strong>4. 论文中提及的局限性：</strong>
论文中提到了当前全向视觉研究的几点局限性，这些也是PANORAMA系统需要克服的挑战：
*   <strong>数据瓶颈</strong>：全向图像（尤其是等距柱状投影图像）由于几何失真和高分辨率，手动标注成本更高，传统自动化标注工具效率低下，导致缺乏大规模高质量数据集。
*   <strong>模型能力</strong>：现有预训练模型（如卷积和池化操作）的归纳偏置（如平移不变性）主要针对针孔图像设计，难以理解全向图像的失真特性，导致性能显著下降。
*   <strong>应用空白</strong>：尽管新传感器和具身AI时代带来了新的应用场景，但由于缺乏跨学科人才以及现有全向数据和模型的不足，许多特定场景的子领域（如全向生产安全检查、全向森林火灾检测）仍缺乏充分探索。
*   <strong>泛化性和鲁棒性</strong>：大多数现有模型仍专注于特定场景或投影方法，难以泛化到不同的全向传感器规格、应用场景和投影方法。
*   <strong>动态失真处理</strong>：当前方法将全向图像的失真视为与帧无关的几何问题，但现实世界场景中的失真本质上是动态的，缺乏对失真在全向视频序列中时间一致性和演变的考虑。
*   <strong>可扩展和统一的架构</strong>：缺乏专门为全向视觉设计的统一、多任务基础模型，现有模型效率低下且任务特定。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
论文提出了以下未来研究方向：
*   <strong>数据集创建</strong>：规划和发布大规模、多任务全向数据集，涵盖真实世界场景的复杂性，包括室内外、通用和具身智能场景。
*   <strong>算法研究</strong>：超越基于针孔模型的简单适配，创建具有全向信息的新颖架构和动态学习范式，以应对全向视觉的独特挑战。
*   <strong>应用工程</strong>：探索和展示全向感知在真实世界机器人和交互系统中的优势，弥合实验室研究与实际应用之间的鸿沟。
*   <strong>投影无关表示学习</strong>：开发能够从无标注全向信息（包括图像和视频流）中学习不变特征的投影无关表示和自监督学习技术。
*   <strong>动作感知表示学习</strong>：使模型能够学习全向图像中的动作导向表示，将全向视觉的独特优势整合到下游控制策略中，以实现更有效和高效的机器人决策。
*   <strong>统一基础模型</strong>：预训练专门为全向视觉设计的统一、多任务基础模型，以捕捉全向几何和语义的基本理解，从而提高性能和泛化能力，并减少对大量任务特定数据集的需求。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>Drawing on insights from
both academia and industry, we propose an ideal panoramic system architecture
in the embodied AI era, PANORAMA, which consists of four key subsystems.</li>
<li>This overview
synthesizes state-of-the-art advancements and outlines challenges and
opportunities for future research in building robust, general-purpose
omnidirectional AI systems in the embodied AI era.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.12989v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.12989v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.14142v1'></a></p>
<h2 id="mars2-2025-challenge-on-multimodal-reasoning-datasets-methods-results-discussion-and-outlook"><a href="https://arxiv.org/abs/2509.14142v1">MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods, Results, Discussion, and Outlook</a></h2>
<p><strong>Authors:</strong> Peng Xu, Shengwu Xiong, Jiajun Zhang, Yaxiong Chen, Bowen Zhou, Chen Change Loy, David A. Clifton, Kyoung Mu Lee, Luc Van Gool, Ruiming He, Ruilin Yao, Xinwei Long, Jirui Huang, Kai Tian, Sa Yang, Yihua Shao, Jin Feng, Yue Zhong, Jiakai Zhou, Cheng Tang, Tianyu Zou, Yifang Zhang, Junming Liang, Guoyou Li, Zhaoxiang Wang, Qiang Zhou, Yichen Zhao, Shili Xiong, Hyeongjin Nam, Jaerin Lee, Jaeyoung Chung, JoonKyu Park, Junghun Oh, Kanggeon Lee, Wooseok Lee, Juneyoung Ro, Turghun Osman, Can Hu, Chaoyang Liao, Cheng Chen, Chengcheng Han, Chenhao Qiu, Chong Peng, Cong Xu, Dailin Li, Feiyu Wang, Feng Gao, Guibo Zhu, Guopeng Tang, Haibo Lu, Han Fang, Han Qi, Hanxiao Wu, Haobo Cheng, Hongbo Sun, Hongyao Chen, Huayong Hu, Hui Li, Jiaheng Ma, Jiang Yu, Jianing Wang, Jie Yang, Jing He, Jinglin Zhou, Jingxuan Li, Josef Kittler, Lihao Zheng, Linnan Zhao, Mengxi Jia, Muyang Yan, Nguyen Thanh Thien, Pu Luo, Qi Li, Shien Song, Shijie Dong, Shuai Shao, Shutao Li, Taofeng Xue, Tianyang Xu, Tianyi Gao, Tingting Li, Wei Zhang, Weiyang Su, Xiaodong Dong, Xiao-Jun Wu, Xiaopeng Zhou, Xin Chen, Xin Wei, Xinyi You, Xudong Kang, Xujie Zhou, Xusheng Liu, Yanan Wang, Yanbin Huang, Yang Liu, Yang Yang, Yanglin Deng, Yashu Kang, Ye Yuan, Yi Wen, Yicen Tian, Yilin Tao, Yin Tang, Yipeng Lin, Yiqing Wang, Yiting Xi, Yongkang Yu, Yumei Li, Yuxin Qin, Yuying Chen, Yuzhe Cen, Zhaofan Zou, Zhaohong Liu, Zhehao Shen, Zhenglin Du, Zhengyang Li, Zhenni Huang, Zhenwei Shao, Zhilong Song, Zhiyong Feng, Zhiyu Wang, Zhou Yu, Ziang Li, Zihan Zhai, Zijian Zhang, Ziyang Peng, Ziyun Xiao, Zongshu Li</p>
<p><strong>Published:</strong> 2025-09-17</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>This paper reviews the MARS2 2025 Challenge on Multimodal Reasoning. We aim
to bring together different approaches in multimodal machine learning and LLMs
via a large benchmark. We hope it better allows researchers to follow the
state-of-the-art in this very dynamic area. Meanwhile, a growing number of
testbeds have boosted the evolution of general-purpose large language models.
Thus, this year's MARS2 focuses on real-world and specialized scenarios to
broaden the multimodal reasoning applications of MLLMs. Our organizing team
released two tailored datasets Lens and AdsQA as test sets, which support
general reasoning in 12 daily scenarios and domain-specific reasoning in
advertisement videos, respectively. We evaluated 40+ baselines that include
both generalist MLLMs and task-specific models, and opened up three competition
tracks, i.e., Visual Grounding in Real-world Scenarios (VG-RS), Visual Question
Answering with Spatial Awareness (VQA-SA), and Visual Reasoning in Creative
Advertisement Videos (VR-Ads). Finally, 76 teams from the renowned academic and
industrial institutions have registered and 40+ valid submissions (out of
1200+) have been included in our ranking lists. Our datasets, code sets (40+
baselines and 15+ participants' methods), and rankings are publicly available
on the MARS2 workshop website and our GitHub organization page
https://github.com/mars2workshop/, where our updates and announcements of
upcoming events will be continuously provided.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods, Results, Discussion, and Outlook”论文的全面摘要：</p>
<p><strong>论文摘要：MARS2 2025 多模态推理挑战赛</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决多模态机器学习和大型语言模型（LLMs）在多模态推理方面的挑战，特别是在真实世界和专业场景中。核心问题是现有模型在空间推理、组合理解和抽象推理（尤其是在广告视频等领域）方面的局限性，以及现有评估系统未能提供从感知到推理连续性的系统性表征。挑战赛旨在通过大型基准测试，促进多模态LLMs在复杂多模态推理和“系统2”慢思考方面的进步。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>定制数据集：</strong> 发布了两个专门设计的大规模多模态数据集——Lens和AdsQA。Lens用于评估12个日常场景中的通用推理，AdsQA用于广告视频中的领域特定推理，旨在探索推理任务间的协同效应和非逐步复杂推理。
*   <strong>综合基准：</strong> 评估了40多个基线模型（包括通用MLLMs和任务特定模型）以及15个参与团队的解决方案，模型规模从3B到72B不等，涵盖了开源和商业模型，提供了全面的比较。
*   <strong>三项竞赛赛道：</strong> 设立了三个开放式QA赛道：
    *   <strong>Track #1 真实世界场景中的视觉定位 (VG-RS)：</strong> 评估模型在复杂场景中的场景感知、物体定位和空间推理能力。
    *   <strong>Track #2 空间感知视觉问答 (VQA-SA)：</strong> 评估模型根据用户指令，基于具体物理内容进行空间、常识和反事实推理的能力。
    *   <strong>Track #3 创意广告视频中的视觉推理 (VR-Ads)：</strong> 探索模型在广告视频中理解隐含、非物理和抽象视觉概念的认知推理能力。
*   <strong>开放源代码可复现性：</strong> 所有数据集、代码和排名均在MARS2工作坊网站和GitHub组织页面上公开，确保了研究的可复现性。
*   <strong>参赛团队的方法论：</strong> 许多团队采用了先进的MLLMs作为基础模型，并通过监督微调（SFT）和强化学习（RL）进行多步对齐，以解决复杂推理任务。常见的策略包括集成学习、数据增强、提示工程和模型协作（通用模型与专家模型结合）。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>挑战性凸显：</strong> 结果表明，即使使用强大的LLMs作为基础模型，在复杂场景和专业领域进行多模态推理仍然具有挑战性。例如，VG-RS任务的获胜解决方案得分未超过70%，VR-Ads赛道的最佳准确率（56%）与人类表现（约70%）仍有明显差距。
*   <strong>模型局限性：</strong> 故障案例分析揭示了当前多模态大型语言模型在细粒度图像理解（如混淆相似材料、识别语义目标）和视觉问答中的视角理解偏差（过度依赖第一人称视角先验、误解“距离”等概念）方面的局限性。
*   <strong>协同效应和非逐步推理：</strong> 挑战赛成功吸引了社区对推理任务协同效应和非逐步复杂推理问题的关注。
*   <strong>促进研究：</strong> 挑战赛为多模态推理领域提供了一个全面、多样化的基准，推动了新一代MLLMs推理能力的发展。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>可靠性和泛化能力不足：</strong> 提交的解决方案虽然性能有所提升，但仍缺乏可靠性和泛化能力。例如，强化学习的奖励函数（如IoU分数）可能次优，导致模型可能失去其他能力。
*   <strong>细粒度图像理解的挑战：</strong> 模型在细粒度图像理解中容易混淆相似材料或难以识别语义目标。
*   <strong>视觉问答中的偏差：</strong> 模型在视角理解方面存在显著偏差，过度依赖第一人称视角先验，并误解“距离”等概念，反映出缺乏物理常识。
*   <strong>抽象推理的挑战：</strong> 广告视频中的抽象推理（如情感分析、营销逻辑、说服策略）对模型提出了更高的要求，现有模型仍难以有效处理。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>提升模型可靠性和泛化能力：</strong> 探索更优的奖励函数和对齐技术，以提高MLLMs的可靠性和泛化能力，避免因任务特定优化而损失其他通用能力。
*   <strong>深化细粒度理解和常识推理：</strong> 进一步研究如何使MLLMs更好地理解细粒度视觉信息，并融入物理常识，以减少误解和偏差。
*   <strong>加强抽象和认知推理：</strong> 针对广告视频等专业领域，开发更有效的多模态融合、时间建模和外部知识整合方法，以支持更高层次的认知推理。
*   <strong>扩展基准和应用场景：</strong> 继续提供新的应用场景和高质量数据，升级测试集规模，并提供更多赛道，以促进多模态推理领域的开放源代码社区发展。
*   <strong>通用与专业模型协作：</strong> 进一步探索通用模型与专家模型协作的潜力，以应对复杂多模态推理任务。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We hope it better allows researchers to follow the
state-of-the-art in this very dynamic area.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.14142v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.14142v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.14055v1'></a></p>
<h2 id="wan-animate-unified-character-animation-and-replacement-with-holistic-replication"><a href="https://arxiv.org/abs/2509.14055v1">Wan-Animate: Unified Character Animation and Replacement with Holistic Replication</a></h2>
<p><strong>Authors:</strong> Gang Cheng, Xin Gao, Li Hu, Siqi Hu, Mingyang Huang, Chaonan Ji, Ju Li, Dechao Meng, Jinwei Qi, Penchong Qiao, Zhen Shen, Yafei Song, Ke Sun, Linrui Tian, Feng Wang, Guangyuan Wang, Qi Wang, Zhongjian Wang, Jiayu Xiao, Sheng Xu, Bang Zhang, Peng Zhang, Xindi Zhang, Zhe Zhang, Jingren Zhou, Lian Zhuo</p>
<p><strong>Published:</strong> 2025-09-17</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We introduce Wan-Animate, a unified framework for character animation and
replacement. Given a character image and a reference video, Wan-Animate can
animate the character by precisely replicating the expressions and movements of
the character in the video to generate high-fidelity character videos.
Alternatively, it can integrate the animated character into the reference video
to replace the original character, replicating the scene's lighting and color
tone to achieve seamless environmental integration. Wan-Animate is built upon
the Wan model. To adapt it for character animation tasks, we employ a modified
input paradigm to differentiate between reference conditions and regions for
generation. This design unifies multiple tasks into a common symbolic
representation. We use spatially-aligned skeleton signals to replicate body
motion and implicit facial features extracted from source images to reenact
expressions, enabling the generation of character videos with high
controllability and expressiveness. Furthermore, to enhance environmental
integration during character replacement, we develop an auxiliary Relighting
LoRA. This module preserves the character's appearance consistency while
applying the appropriate environmental lighting and color tone. Experimental
results demonstrate that Wan-Animate achieves state-of-the-art performance. We
are committed to open-sourcing the model weights and its source code.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Gang Cheng等撰写的论文“Wan-Animate: Unified Character Animation and Replacement with Holistic Replication”的全面摘要。</p>
<hr />
<p><strong>论文摘要：Wan-Animate: Unified Character Animation and Replacement with Holistic Replication</strong></p>
<p><strong>1. 主要问题或研究问题</strong>
当前计算机视觉领域在角色动画和替换方面存在一个关键空白：缺乏一个能够统一控制运动、表情和环境交互，并实现高保真度的整体解决方案。现有的开源框架在性能和完整性上存在显著不足，尤其是在全面复制富有表现力的面部动态与身体运动相结合，以及将角色动画与环境背景（即角色替换）无缝集成方面。本研究旨在解决这些挑战，提供一个统一且高性能的角色动画和替换框架。</p>
<p><strong>2. 关键创新或方法论贡献</strong>
Wan-Animate基于Wan模型构建，并引入了多项关键创新：
*   <strong>统一框架与输入范式：</strong> Wan-Animate提出了一个统一的框架，能够处理角色动画和角色替换两种核心功能。通过修改后的输入范式，它能够区分参考条件和生成区域，将多任务统一为共同的符号表示，从而在不引入显著分布偏移的情况下，高效地进行后训练。
*   <strong>解耦控制信号：</strong> 为了实现整体角色控制，模型将控制信号解耦为身体运动和面部表情。
    *   <strong>身体运动控制：</strong> 采用空间对齐的骨架信号来复制身体运动，平衡了准确性和通用性。这些信号通过添加到初始噪声潜在向量中进行注入。
    *   <strong>面部表情控制：</strong> 直接使用参考视频中的原始面部图像作为驱动信号，以保留最大细节。这些面部图像被编码为潜在向量，以解耦表情信息和身份属性，并通过跨注意力机制注入模型。
*   <strong>辅助重光LoRA（Relighting LoRA）：</strong> 为了增强角色替换时的环境集成，Wan-Animate开发了一个辅助的Relighting LoRA模块。该模块在应用适当的环境光照和色调的同时，保持了角色外观的一致性，确保了替换角色与新环境的无缝融合。
*   <strong>渐进式训练策略：</strong> 训练过程分为多个阶段，包括身体控制训练、面部控制训练、联合控制训练和联合模式训练，最后是Relighting LoRA训练。这种渐进式方法有助于模型快速收敛，并有效学习复杂的控制任务。</p>
<p><strong>3. 主要结果及其意义</strong>
实验结果表明，Wan-Animate在角色动画和替换任务中取得了最先进的性能：
*   <strong>高保真度与表现力：</strong> 模型能够精确复制参考视频中角色的表情和动作，生成高保真度的角色视频，具有高度的可控性和表现力。
*   <strong>无缝环境集成：</strong> 在角色替换模式下，通过Relighting LoRA，动画角色能够无缝融入参考视频的环境，复制场景的光照和色调。
*   <strong>超越现有开源和闭源方案：</strong> 定量评估（SSIM、LPIPS、FVD）显示Wan-Animate在性能上优于大多数现有开源框架。人类评估结果也表明，与Runway Act-two和DreamActor-M1等闭源SOTA解决方案相比，Wan-Animate在视频生成质量、身份一致性、运动准确性和表情准确性方面表现出优越性。
*   <strong>通用性与鲁棒性：</strong> 模型能够很好地泛化到各种人形角色，在肖像、半身和全身镜头等多种场景下表现出强大的鲁棒性。</p>
<p>这些结果的意义在于，Wan-Animate为高保真角色动画和替换提供了一个全面且高性能的解决方案，显著提升了该领域的现有技术水平，并有望加速角色图像动画技术的发展和实际应用。</p>
<p><strong>4. 论文中提及的局限性</strong>
*   <strong>骨架姿态重定向的局限性：</strong> 在角色替换模式下，为了避免破坏角色与环境的特定交互关系，论文不建议使用姿态重定向。这导致在替换身体形状差异显著的角色时，可能会出现一些变形。
*   <strong>文本控制的非核心性：</strong> 尽管Wan-Animate支持一定程度的文本控制，但运动信号是主要的控制因素，文本控制被视为非核心功能，建议使用默认文本提示。
*   <strong>对SMPL形状的依赖（间接提及）：</strong> 论文在讨论身体控制信号时提到，渲染的SMPL图像包含角色形状信息，如果SMPL形状不准确，可能会使模型依赖形状线索来指导生成，从而影响身份一致性。虽然Wan-Animate选择了骨架表示来避免这个问题，但这也暗示了3D形状表示可能带来的挑战。</p>
<p><strong>5. 潜在的未来研究方向</strong>
论文中并未明确提出未来的研究方向，但从其贡献和局限性中可以推断出以下几点：
*   <strong>更精细的姿态重定向：</strong> 解决角色替换中姿态重定向的局限性，开发更智能的重定向方法，使其在不破坏环境交互的前提下，能够处理身体形状差异显著的角色，减少变形。
*   <strong>增强文本控制能力：</strong> 尽管目前运动信号是主导，但未来可以探索如何更有效地整合文本提示，使其成为一个更核心、更强大的控制因素，实现更灵活的创作。
*   <strong>多模态融合与交互：</strong> 进一步探索角色与环境、角色与物体之间更复杂的交互，例如，除了光照和色调，还能复制物理属性、材质等，实现更深层次的场景融合。
*   <strong>实时性能优化：</strong> 尽管论文强调了高效推理，但对于更长的视频和更高分辨率的生成，进一步优化模型的实时性能和计算效率仍是重要的方向。
*   <strong>更广泛的应用场景：</strong> 探索Wan-Animate在虚拟现实、游戏、数字人、电影特效等更广泛领域的应用，并根据特定需求进行定制化开发。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce Wan-Animate, a unified framework for character animation and
replacement.</li>
<li>Furthermore, to enhance environmental
integration during character replacement, we develop an auxiliary Relighting
LoRA.</li>
<li>Experimental
results demonstrate that Wan-Animate achieves state-of-the-art performance.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.14055v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.14055v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.13936v1'></a></p>
<h2 id="noise-level-diffusion-guidance-well-begun-is-half-done"><a href="https://arxiv.org/abs/2509.13936v1">Noise-Level Diffusion Guidance: Well Begun is Half Done</a></h2>
<p><strong>Authors:</strong> Harvey Mannering, Zhiwu Huang, Adam Prugel-Bennett</p>
<p><strong>Published:</strong> 2025-09-17</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Diffusion models have achieved state-of-the-art image generation. However,
the random Gaussian noise used to start the diffusion process influences the
final output, causing variations in image quality and prompt adherence.
Existing noise-level optimization approaches generally rely on extra dataset
construction, additional networks, or backpropagation-based optimization,
limiting their practicality. In this paper, we propose Noise Level Guidance
(NLG), a simple, efficient, and general noise-level optimization approach that
refines initial noise by increasing the likelihood of its alignment with
general guidance - requiring no additional training data, auxiliary networks,
or backpropagation. The proposed NLG approach provides a unified framework
generalizable to both conditional and unconditional diffusion models,
accommodating various forms of diffusion-level guidance. Extensive experiments
on five standard benchmarks demonstrate that our approach enhances output
generation quality and input condition adherence. By seamlessly integrating
with existing guidance methods while maintaining computational efficiency, our
method establishes NLG as a practical and scalable enhancement to diffusion
models. Code can be found at
https://github.com/harveymannering/NoiseLevelGuidance.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Harvey Mannering等人撰写的论文“Noise-Level Diffusion Guidance: Well Begun is Half Done”的全面摘要。</p>
<hr />
<h3 id="noise-level-diffusion-guidance-well-begun-is-half-done_1">论文摘要：Noise-Level Diffusion Guidance: Well Begun is Half Done</h3>
<p><strong>1. 主要问题或研究问题：</strong>
扩散模型在图像生成方面取得了最先进的性能，但其生成质量和对提示的依从性受到初始随机高斯噪声的显著影响。现有的噪声级别优化（NLO）方法通常依赖于额外的数据集构建、辅助网络或基于反向传播的优化，这限制了它们的实用性和可扩展性。因此，该研究旨在解决如何开发一种简单、高效、通用的噪声级别优化方法，以提高扩散模型的输出质量和条件依从性。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
本文提出了<strong>噪声级别引导（Noise Level Guidance, NLG）</strong>，这是一种新颖的噪声级别优化方法，其核心创新在于：
*   <strong>简单高效：</strong> NLG通过增加初始噪声与通用引导（如文本提示、类别标签或质量度量）对齐的可能性来优化初始噪声，而无需额外训练数据、辅助网络或反向传播。它通过对扩散模型在不同条件下的输出进行简单线性组合来推导出编辑方向，然后迭代地应用于初始噪声。
*   <strong>通用性：</strong> NLG提供了一个统一的框架，可推广到条件和无条件扩散模型，并适应各种形式的扩散级别引导（如Classifier-Free Guidance (CFG) 和 Autoguidance (AutoG)）。
*   <strong>避免分布外错误：</strong> 为了防止噪声在迭代应用编辑方向时偏离标准正态分布，NLG采用了方向裁剪、添加少量高斯噪声和归一化等策略，以保持噪声的分布内特性。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>性能提升：</strong> 在五个标准基准上的大量实验表明，NLG方法显著提高了输出生成质量和输入条件依从性。
*   <strong>计算效率：</strong> NLG与现有引导方法无缝集成，同时保持了计算效率。与竞争方法InitNO相比，NLG的速度快了4倍，内存使用量减少了3倍。
*   <strong>泛化能力：</strong> NLG在无条件和条件图像生成任务中表现出卓越的泛化能力，适用于不同类型的扩散模型（如Stable Diffusion v2.1, v1.5, v3.5, FLUX.1-dev和EDM2）。
*   <strong>用户研究：</strong> 用户研究结果表明，在无CFG设置下，NLG在图像真实感和文本提示对齐方面均优于高斯噪声基线，具有统计学意义上的显著偏好。在CFG启用时，NLG也保持了更高的胜率。
*   <strong>应用探索：</strong> NLG能够有效改善与输入提示对齐不佳的图像，并能通过在噪声对齐阶段使用特定提示来为初始噪声添加结构，从而在生成阶段填充图像语义。它还展示了跨模型噪声对齐和推理的能力。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>高CFG引导下的效果：</strong> 论文指出，在高CFG引导设置下，NLG的益处是最小的，这与之前的一些噪声级别优化工作相似。然而，对于低CLIP分数（即对齐不佳）的图像，即使在高引导下，NLG仍然可能是有益的。
*   <strong>单步扩散模型的限制：</strong> 论文提到，ReNO等方法受限于单步扩散模型，因为每个扩散步骤都必须进行反向传播。虽然NLG不受此限制，但对于像SD-Turbo这样经过蒸馏的单步扩散模型，NLG可能不会带来显著的图像改进，这可能是因为这些模型在推理时不需要CFG。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>探索更多优势：</strong> 未来的工作将侧重于探索NLG方法在初始噪声优化方面的其他优势，例如在更高扩散级别引导下，以及在结构化初始噪声和跨扩散模型等新功能方面的进一步应用。
*   <strong>超参数调优：</strong> 论文指出，在EDM2实验中，NLG的参数设置仍有进一步改进的潜力，可以通过超参数调优来优化性能。</p>
<hr />
<p>这篇论文通过提出NLG，为扩散模型领域提供了一个实用且高效的噪声优化解决方案，克服了现有方法的计算和数据依赖性限制，显著提升了图像生成质量和条件依从性，为扩散模型的进一步发展奠定了基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Diffusion models have achieved state-of-the-art image generation.</li>
<li>In this paper, we propose Noise Level Guidance
(NLG), a simple, efficient, and general noise-level optimization approach that
refines initial noise by increasing the likelihood of its alignment with
general guidance - requiring no additional training data, auxiliary networks,
or backpropagation.</li>
<li>Extensive experiments
on five standard benchmarks demonstrate that our approach enhances output
generation quality and input condition adherence.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.13936v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.13936v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.13926v1'></a></p>
<h2 id="map-end-to-end-autonomous-driving-with-map-assisted-planning"><a href="https://arxiv.org/abs/2509.13926v1">MAP: End-to-End Autonomous Driving with Map-Assisted Planning</a></h2>
<p><strong>Authors:</strong> Huilin Yin, Yiming Kan, Daniel Watzenig</p>
<p><strong>Published:</strong> 2025-09-17</p>
<p><strong>Categories:</strong> cs.RO, cs.AI, cs.CV, I.2.9; I.2.10</p>
<p><strong>Abstract:</strong></p>
<p>In recent years, end-to-end autonomous driving has attracted increasing
attention for its ability to jointly model perception, prediction, and planning
within a unified framework. However, most existing approaches underutilize the
online mapping module, leaving its potential to enhance trajectory planning
largely untapped. This paper proposes MAP (Map-Assisted Planning), a novel
map-assisted end-to-end trajectory planning framework. MAP explicitly
integrates segmentation-based map features and the current ego status through a
Plan-enhancing Online Mapping module, an Ego-status-guided Planning module, and
a Weight Adapter based on current ego status. Experiments conducted on the
DAIR-V2X-seq-SPD dataset demonstrate that the proposed method achieves a 16.6%
reduction in L2 displacement error, a 56.2% reduction in off-road rate, and a
44.5% improvement in overall score compared to the UniV2X baseline, even
without post-processing. Furthermore, it achieves top ranking in Track 2 of the
End-to-End Autonomous Driving through V2X Cooperation Challenge of MEIS
Workshop @CVPR2025, outperforming the second-best model by 39.5% in terms of
overall score. These results highlight the effectiveness of explicitly
leveraging semantic map features in planning and suggest new directions for
improving structure design in end-to-end autonomous driving systems. Our code
is available at https://gitee.com/kymkym/map.git</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Huilin Yin, Yiming Kan, Daniel Watzenig撰写的论文“MAP: End-to-End Autonomous Driving with Map-Assisted Planning”的全面摘要。</p>
<hr />
<p><strong>论文摘要：MAP: End-to-End Autonomous Driving with Map-Assisted Planning</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
端到端自动驾驶系统因其能够在一个统一框架内联合建模感知、预测和规划而备受关注。然而，现有的大多数方法未能充分利用在线地图模块的潜力，导致其在轨迹规划方面的增强作用未被充分挖掘。本研究旨在解决如何有效整合语义地图特征以显著提升端到端自动驾驶系统的轨迹规划性能。</p>
<p><strong>2. 关键创新或方法学贡献：</strong>
本论文提出了MAP（Map-Assisted Planning），一个新颖的地图辅助端到端轨迹规划框架，其主要创新点在于：
*   <strong>显式整合地图特征：</strong> MAP通过一个<strong>规划增强在线地图模块（Plan-enhancing Online Mapping module）</strong>，显式地将基于分割的地图特征与当前自车状态相结合。这解决了传统方法中地图信息利用不足的问题。
*   <strong>自车状态引导的规划：</strong> 引入了<strong>自车状态引导规划模块（Ego-status-guided Planning module）</strong>，进一步细化了规划过程，使其更贴合当前车辆的实时状态。
*   <strong>权重适配器：</strong> 基于当前自车状态，设计了一个<strong>权重适配器（Weight Adapter）</strong>，用于动态融合来自在线地图模块和自车状态引导规划模块的输出，以生成最终的轨迹规划。
*   <strong>UniV2X基线的扩展：</strong> MAP是UniV2X基线的一个扩展，通过引入上述模块，显著提升了其性能。</p>
<p><strong>3. 主要结果及其意义：</strong>
在DAIR-V2X-seq-SPD数据集上进行的实验结果表明，MAP方法取得了显著的性能提升：
*   <strong>L2位移误差（L2 displacement error）降低16.6%</strong>。
*   <strong>偏离道路率（off-road rate）降低56.2%</strong>。
*   <strong>整体得分（overall score）提升44.5%</strong>。
这些改进是在没有后处理的情况下实现的，凸显了该方法的强大性能。</p>
<p>此外，MAP在CVPR2025 MEIS Workshop的V2X协同挑战赛端到端自动驾驶赛道2中获得了<strong>第一名</strong>，其整体得分比第二名模型高出39.5%。</p>
<p>这些结果强调了显式利用语义地图特征在规划中的有效性，并为端到端自动驾驶系统的结构设计提供了新的方向。</p>
<p><strong>4. 论文中提及的局限性：</strong>
论文中并未明确提及具体的局限性。然而，从其强调“显式利用语义地图特征”和“结构设计的新方向”来看，可以推断出当前端到端系统在地图信息利用和模块化设计方面仍有改进空间。MAP的成功也暗示了现有方法可能在复杂场景下对地图信息的依赖性不足，或者融合方式不够优化。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>更复杂的地图特征融合：</strong> 探索除了分割之外，如何融合更多样化的地图特征（如拓扑信息、语义关系等），以进一步提升规划的鲁棒性和准确性。
*   <strong>动态权重适配的优化：</strong> 深入研究权重适配器，使其能够根据更复杂的环境上下文和驾驶意图进行动态调整，而非仅仅基于当前自车状态。
*   <strong>可解释性与安全性：</strong> 随着端到端系统复杂性的增加，如何提高其决策的可解释性和安全性将是重要的研究方向。
*   <strong>多模态感知与地图的协同：</strong> 探索如何将地图信息与雷达、激光雷达等其他传感器数据进行更深层次的融合，以应对恶劣天气和传感器失效等挑战。
*   <strong>泛化能力：</strong> 在更多样化的数据集和真实世界场景中验证MAP的泛化能力，并探索如何使其适应不同地理区域和交通规则。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>This paper proposes MAP (Map-Assisted Planning), a novel
map-assisted end-to-end trajectory planning framework.</li>
<li>These results highlight the effectiveness of explicitly
leveraging semantic map features in planning and suggest new directions for
improving structure design in end-to-end autonomous driving systems.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.13926v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.13926v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.13883v1'></a></p>
<h2 id="evhand-fpv-efficient-event-based-3d-hand-tracking-from-first-person-view"><a href="https://arxiv.org/abs/2509.13883v1">EvHand-FPV: Efficient Event-Based 3D Hand Tracking from First-Person View</a></h2>
<p><strong>Authors:</strong> Zhen Xu, Guorui Lu, Chang Gao, Qinyu Chen</p>
<p><strong>Published:</strong> 2025-09-17</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Hand tracking holds great promise for intuitive interaction paradigms, but
frame-based methods often struggle to meet the requirements of accuracy, low
latency, and energy efficiency, especially in resource-constrained settings
such as Extended Reality (XR) devices. Event cameras provide <script type="math/tex">\mu</script>s-level
temporal resolution at mW-level power by asynchronously sensing brightness
changes. In this work, we present EvHand-FPV, a lightweight framework for
egocentric First-Person-View 3D hand tracking from a single event camera. We
construct an event-based FPV dataset that couples synthetic training data with
3D labels and real event data with 2D labels for evaluation to address the
scarcity of egocentric benchmarks. EvHand-FPV also introduces a wrist-based
region of interest (ROI) that localizes the hand region via geometric cues,
combined with an end-to-end mapping strategy that embeds ROI offsets into the
network to reduce computation without explicit reconstruction, and a multi-task
learning strategy with an auxiliary geometric feature head that improves
representations without test-time overhead. On our real FPV test set,
EvHand-FPV improves 2D-AUCp from 0.77 to 0.85 while reducing parameters from
11.2M to 1.2M by 89% and FLOPs per inference from 1.648G to 0.185G by 89%. It
also maintains a competitive 3D-AUCp of 0.84 on synthetic data. These results
demonstrate accurate and efficient egocentric event-based hand tracking
suitable for on-device XR applications. The dataset and code are available at
https://github.com/zen5x5/EvHand-FPV.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“EvHand-FPV: Efficient Event-Based 3D Hand Tracking from First-Person View”论文的中文摘要，涵盖了您要求的所有要点：</p>
<p><strong>论文摘要：EvHand-FPV：基于事件的第一人称视角高效3D手部追踪</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
手部追踪对于直观交互范式具有巨大潜力，但传统的基于帧的方法在满足扩展现实（XR）设备等资源受限环境对精度、低延迟和能效的要求方面面临挑战。事件相机通过异步感知亮度变化提供微秒级时间分辨率和毫瓦级功耗，为解决这些问题提供了新的途径。本研究旨在开发一种轻量级、高效的基于事件的第一人称视角（FPV）3D手部追踪框架。</p>
<p><strong>2. 关键创新或方法贡献：</strong>
*   <strong>事件FPV数据集构建：</strong> 论文构建了一个事件FPV数据集，结合了带有3D标签的合成训练数据和带有2D标签的真实事件数据，以解决第一人称视角基准稀缺的问题。
*   <strong>腕部ROI定位与端到端映射策略：</strong> 引入了一种基于腕部的兴趣区域（ROI）定位方法，通过几何线索局部化手部区域。结合端到端映射策略，将ROI偏移嵌入网络中，从而在无需显式重建的情况下减少计算量。
*   <strong>多任务学习策略：</strong> 采用带有辅助几何特征头的多任务学习策略，在不增加测试时开销的情况下，提高了表征能力。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   在真实FPV测试集上，EvHand-FPV将2D-AUCp从0.77提高到0.85，同时将参数量从11.2M减少到1.2M（降低89%），每次推理的FLOPs从1.648G减少到0.185G（降低89%）。
*   在合成数据上，EvHand-FPV保持了0.84的竞争性3D-AUCp。
*   这些结果表明，EvHand-FPV实现了准确高效的事件基第一人称视角手部追踪，非常适合在XR设备上进行部署。</p>
<p><strong>4. 论文中提及的局限性：</strong>
论文中没有明确提及具体的局限性，但从其创新点和未来工作方向可以推断出一些潜在的方面。例如，虽然构建了数据集，但事件数据本身的特性（如稀疏性、噪声）可能仍然是挑战。此外，虽然实现了高效性，但进一步优化以适应更广泛的XR设备和应用可能仍需努力。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>更广泛的泛化性：</strong> 探索如何将EvHand-FPV推广到更多样化的用户、手部姿态和环境条件。
*   <strong>多模态融合：</strong> 结合事件数据与其他传感器（如惯性测量单元IMU）的数据，以进一步提高追踪的鲁棒性和精度。
*   <strong>实时部署优化：</strong> 进一步优化模型和算法，以实现更低的延迟和更高的帧率，满足更严格的实时XR应用需求。
*   <strong>数据集扩展：</strong> 持续扩展事件FPV数据集，包含更多场景、更多样化的手部动作和更精细的标注，以支持更复杂的模型训练。
*   <strong>事件数据处理：</strong> 探索更先进的事件数据处理和特征提取方法，以更好地利用事件相机的独特优势。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we present EvHand-FPV, a lightweight framework for
egocentric First-Person-View 3D hand tracking from a single event camera.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.13883v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.13883v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.13858v1'></a></p>
<h2 id="edits-enhancing-dataset-distillation-with-implicit-textual-semantics"><a href="https://arxiv.org/abs/2509.13858v1">EDITS: Enhancing Dataset Distillation with Implicit Textual Semantics</a></h2>
<p><strong>Authors:</strong> Qianxin Xia, Jiawei Du, Guoming Lu, Zhiyong Shu, Jielei Wang</p>
<p><strong>Published:</strong> 2025-09-17</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Dataset distillation aims to synthesize a compact dataset from the original
large-scale one, enabling highly efficient learning while preserving
competitive model performance. However, traditional techniques primarily
capture low-level visual features, neglecting the high-level semantic and
structural information inherent in images. In this paper, we propose EDITS, a
novel framework that exploits the implicit textual semantics within the image
data to achieve enhanced distillation. First, external texts generated by a
Vision Language Model (VLM) are fused with image features through a Global
Semantic Query module, forming the prior clustered buffer. Local Semantic
Awareness then selects representative samples from the buffer to construct
image and text prototypes, with the latter produced by guiding a Large Language
Model (LLM) with meticulously crafted prompt. Ultimately, Dual Prototype
Guidance strategy generates the final synthetic dataset through a diffusion
model. Extensive experiments confirm the effectiveness of our method.Source
code is available in: https://github.com/einsteinxia/EDITS.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供对Qianxin Xia等人撰写的论文“EDITS: Enhancing Dataset Distillation with Implicit Textual Semantics”的全面摘要。</p>
<hr />
<p><strong>论文摘要：EDITS: Enhancing Dataset Distillation with Implicit Textual Semantics</strong></p>
<p><strong>1. 主要问题或研究问题</strong>
数据集蒸馏（Dataset Distillation, DD）旨在从大规模原始数据集中合成一个紧凑的数据集，以实现高效学习并保持有竞争力的模型性能。然而，传统DD方法主要关注低级视觉特征，往往忽略了图像中固有的高级语义和结构信息。这导致蒸馏出的数据集在语义丰富性方面不足，影响了其在跨架构泛化和信息损失方面的性能。</p>
<p><strong>2. 关键创新或方法论贡献</strong>
本文提出了一个名为EDITS（Enhancing Dataset Distillation with Implicit Textual Semantics）的新颖框架，通过利用图像数据中隐含的文本语义来增强数据集蒸馏。其关键创新和贡献包括：</p>
<ul>
<li><strong>利用外部文本语义增强蒸馏：</strong> EDITS是首个利用外部文本信号实现增强DD的框架，解决了传统方法仅依赖视觉特征的局限性。</li>
<li><strong>全局语义查询（Global Semantic Query, GSQ）：</strong> 该模块通过视觉语言模型（VLM，如LLaVA）生成的外部文本与图像特征进行融合。它计算每个文本描述对图像的影响分数，构建一个初步的聚类缓冲区，从而全面地将文本语义融入图像特征，增强后续原型表示能力。</li>
<li><strong>局部语义感知（Local Semantic Awareness, LSA）：</strong> LSA从聚类缓冲区中选择具有代表性的候选样本，以构建图像和文本原型。图像原型通过VAE编码器生成，而文本原型则通过精心设计的提示（prompt）引导大型语言模型（LLM，如DeepSeek）进行总结，解决了直接使用聚类中心作为原型所固有的语义不足问题。</li>
<li><strong>双原型指导（Dual-Prototype Guidance, DPG）：</strong> 最终，通过扩散模型（Latent Diffusion Model）结合图像和文本原型，生成最终的合成数据集，确保了蒸馏数据的多样性和代表性。</li>
</ul>
<p><strong>3. 主要结果及其意义</strong>
广泛的实验证实了EDITS方法的有效性：</p>
<ul>
<li><strong>性能提升：</strong> 在ImageNet子集（ImageWoof、ImageNette、ImageIDC）上，EDITS在不同IPC（每类图像数）和架构（ConvNet-6、ResNetAP-10、ResNet-18）设置下，始终比现有最先进的方法高出约1%-3%。这充分证明了其在不同数据集和架构上的鲁棒性和泛化能力。</li>
<li><strong>低分辨率数据集表现：</strong> 在CIFAR-10和CIFAR-100等低分辨率数据集上，EDITS也超越了仅利用文本原型的VLCP方法，进一步证实了其全面语义增强的有效性。</li>
<li><strong>原型质量：</strong> 实验结果表明，EDITS生成的原型在语义上更具解释性，能够更好地捕捉图像的高级语义信息，而非仅仅是低级视觉纹理。</li>
</ul>
<p>这些结果表明，通过整合隐式文本语义，EDITS能够生成更高质量、更具语义代表性的蒸馏数据集，显著提升了数据集蒸馏的性能和泛化能力。</p>
<p><strong>4. 论文中提及的局限性</strong>
论文中提到了以下几点局限性：</p>
<ul>
<li><strong>LLM输入限制：</strong> LLM的输入受限于token数量，过多的输入会增加生成文本原型的时间开销。</li>
<li><strong>文本信息量：</strong> 过多的文本信息可能偏离图像原型的语义表示，实验观察到文本量增加会导致性能下降。这表明需要平衡文本信息的丰富性和相关性。</li>
<li><strong>计算成本：</strong> 虽然数据集蒸馏旨在提高效率，但VLM和LLM的使用，以及扩散模型的生成过程，可能仍然涉及一定的计算成本。</li>
</ul>
<p><strong>5. 潜在的未来研究方向</strong>
基于上述工作和局限性，论文暗示或可以推断出以下未来研究方向：</p>
<ul>
<li><strong>更高效的语义提取：</strong> 探索更轻量级或更高效的VLM和LLM，以减少语义提取和原型生成的时间和计算成本。</li>
<li><strong>自适应文本信息量：</strong> 研究如何根据图像内容和类别自适应地确定最佳的文本信息量，以避免信息过载或不足。</li>
<li><strong>多模态原型融合优化：</strong> 进一步优化图像和文本原型的融合策略，例如探索更复杂的注意力机制或多模态Transformer结构，以更好地捕捉跨模态的细微语义关联。</li>
<li><strong>跨领域和零样本蒸馏：</strong> 探索EDITS框架在更广泛的领域和零样本学习场景下的应用，以验证其在更具挑战性环境下的泛化能力。</li>
<li><strong>可解释性增强：</strong> 进一步研究如何量化和可视化文本语义对蒸馏过程和最终模型性能的影响，以提高方法的可解释性。</li>
<li><strong>结合其他蒸馏范式：</strong> 探索将EDITS的语义增强思想与匹配基DD方法或其他生成基DD方法相结合，以期达到更优的性能。</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we propose EDITS, a
novel framework that exploits the implicit textual semantics within the image
data to achieve enhanced distillation.</li>
<li>Extensive experiments confirm the effectiveness of our method.Source
code is available in: https://github.com/einsteinxia/EDITS.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.13858v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.13858v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.13766v1'></a></p>
<h2 id="ndlpnet-a-location-aware-nighttime-deraining-network-and-a-real-world-benchmark-dataset"><a href="https://arxiv.org/abs/2509.13766v1">NDLPNet: A Location-Aware Nighttime Deraining Network and a Real-World Benchmark Dataset</a></h2>
<p><strong>Authors:</strong> Huichun Liu, Xiaosong Li, Yang Liu, Xiaoqi Cheng, Haishu Tan</p>
<p><strong>Published:</strong> 2025-09-17</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Visual degradation caused by rain streak artifacts in low-light conditions
significantly hampers the performance of nighttime surveillance and autonomous
navigation. Existing image deraining techniques are primarily designed for
daytime conditions and perform poorly under nighttime illumination due to the
spatial heterogeneity of rain distribution and the impact of light-dependent
stripe visibility. In this paper, we propose a novel Nighttime Deraining
Location-enhanced Perceptual Network(NDLPNet) that effectively captures the
spatial positional information and density distribution of rain streaks in
low-light environments. Specifically, we introduce a Position Perception Module
(PPM) to capture and leverage spatial contextual information from input data,
enhancing the model's capability to identify and recalibrate the importance of
different feature channels. The proposed nighttime deraining network can
effectively remove the rain streaks as well as preserve the crucial background
information. Furthermore, We construct a night scene rainy (NSR) dataset
comprising 900 image pairs, all based on real-world nighttime scenes, providing
a new benchmark for nighttime deraining task research. Extensive qualitative
and quantitative experimental evaluations on both existing datasets and the NSR
dataset consistently demonstrate our method outperform the state-of-the-art
(SOTA) methods in nighttime deraining tasks. The source code and dataset is
available at https://github.com/Feecuin/NDLPNet.</p>
<p><strong>Analysis:</strong></p>
<p>以下是Huichun Liu, Xiaosong Li, Yang Liu, Xiaoqi Cheng, Haishu Tan撰写的论文“NDLPNet: A Location-Aware Nighttime Deraining Network and a Real-World Benchmark Dataset”的全面摘要：</p>
<p><strong>1. 解决的主要问题或研究问题</strong>
该论文旨在解决夜间低光照条件下雨纹对视觉系统（如夜间监控和自动驾驶）造成的严重图像退化问题。现有的去雨技术主要针对白天场景设计，在夜间由于雨水分布的空间异质性和光照依赖的雨纹可见性差异而表现不佳。因此，核心问题是如何在夜间复杂的光照条件下，有效去除雨纹并保留背景细节。</p>
<p><strong>2. 关键创新或方法学贡献</strong>
*   <strong>NDLPNet模型：</strong> 提出了一种新颖的夜间去雨位置增强感知网络（Nighttime Deraining Location-enhanced Perceptual Network, NDLPNet）。该网络能够有效捕获低光照环境中雨纹的空间位置信息和密度分布。
*   <strong>位置感知模块（Position Perception Module, PPM）：</strong> 引入PPM模块，用于捕获和利用输入数据的空间上下文信息，增强模型识别和重新校准不同特征通道重要性的能力。PPM结合了空间位置编码（Spatial Position Coding, SPC）和高效通道注意力（Efficient Channel Attention, ECA）。
    *   <strong>空间位置编码（SPC）：</strong> 针对夜间雨纹的特点，提出了一种更精细和自适应的位置编码模块，不仅考虑二维空间坐标，还加入了雨粒密度信息，以帮助模型理解和利用多空间方向上的位置信息和分布。
    *   <strong>高效通道注意力（ECA）：</strong> 在位置编码之后，通过ECA模块进一步优化特征提取，减少冗余信息的影响，并根据像素位置的特定编码调整通道权重，从而更好地保留图像细节。
*   <strong>夜间场景雨天（NSR）数据集：</strong> 构建了一个包含900对图像的半真实夜间场景雨天数据集。该数据集基于真实的夜间场景，为夜间去雨任务研究提供了新的基准，弥补了现有夜间雨天数据集数量和多样性不足的缺点，并包含了夜间场景常见的噪声。</p>
<p><strong>3. 主要结果及其意义</strong>
*   <strong>卓越的去雨性能：</strong> 在现有数据集（如GTAV-NightRain）和新构建的NSR数据集上进行的广泛定性和定量实验评估表明，NDLPNet在夜间去雨任务中始终优于最先进（SOTA）方法。例如，在GTAV-NightRain数据集上，NDLPNet的PSNR比RLP高出2.49dB。在NSR数据集上，NDLPNet也取得了最高的PSNR/SSIM值。
*   <strong>背景信息保留：</strong> 提出的网络能够有效去除雨纹，同时保留关键的背景信息，避免了现有方法中常见的模糊和失真问题。
*   <strong>泛化能力：</strong> 在白天场景数据集（如Rain200L和Rain200H）上的实验也验证了该方法具有较强的泛化能力，在Rain200H数据集上取得了最佳性能。
*   <strong>模块有效性：</strong> 消融实验证实了PPM模块（包括SPC和ECA）的有效性，表明三维空间位置信息对有效去雨至关重要，并且ECA模块有助于模型更好地关注相关信息并减少冗余数据的影响。</p>
<p><strong>4. 论文中提及的局限性</strong>
*   <strong>模型效率：</strong> 提出的方法在模型效率方面存在局限性。
*   <strong>白天场景泛化能力：</strong> 尽管在白天场景中表现良好，但其泛化能力仍需进一步改进。</p>
<p><strong>5. 潜在的未来研究方向</strong>
*   <strong>模型优化：</strong> 未来的工作将专注于优化模型的效率。
*   <strong>泛化能力提升：</strong> 进一步提升模型在白天场景中的泛化能力。
*   <strong>任务扩展：</strong> 将去雨任务扩展到雨滴去除。</p>
<p>总而言之，这篇论文通过引入NDLPNet模型和PPM模块，并构建了NSR数据集，为夜间图像去雨领域做出了重要贡献，有效解决了夜间复杂光照条件下雨纹去除的挑战，并为未来的研究提供了坚实的基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we propose a novel Nighttime Deraining
Location-enhanced Perceptual Network(NDLPNet) that effectively captures the
spatial positional information and density distribution of rain streaks in
low-light environments.</li>
<li>Specifically, we introduce a Position Perception Module
(PPM) to capture and leverage spatial contextual information from input data,
enhancing the model's capability to identify and recalibrate the importance of
different feature channels.</li>
<li>Furthermore, We construct a night scene rainy (NSR) dataset
comprising 900 image pairs, all based on real-world nighttime scenes, providing
a new benchmark for nighttime deraining task research.</li>
<li>Extensive qualitative
and quantitative experimental evaluations on both existing datasets and the NSR
dataset consistently demonstrate our method outperform the state-of-the-art
(SOTA) methods in nighttime deraining tasks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.13766v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.13766v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.13754v1'></a></p>
<h2 id="cross-modal-full-mode-fine-grained-alignment-for-text-to-image-person-retrieval"><a href="https://arxiv.org/abs/2509.13754v1">Cross-modal Full-mode Fine-grained Alignment for Text-to-Image Person Retrieval</a></h2>
<p><strong>Authors:</strong> Hao Yin, Xin Man, Feiyu Chen, Jie Shao, Heng Tao Shen</p>
<p><strong>Published:</strong> 2025-09-17</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Text-to-Image Person Retrieval (TIPR) is a cross-modal matching task that
aims to retrieve the most relevant person images based on a given text query.
The key challenge in TIPR lies in achieving effective alignment between textual
and visual modalities within a common latent space. To address this challenge,
prior approaches incorporate attention mechanisms for implicit cross-modal
local alignment. However, they lack the ability to verify whether all local
features are correctly aligned. Moreover, existing methods primarily focus on
hard negative samples during model updates, with the goal of refining
distinctions between positive and negative pairs, often neglecting incorrectly
matched positive pairs. To alleviate these issues, we propose FMFA, a
cross-modal Full-Mode Fine-grained Alignment framework, which enhances global
matching through explicit fine-grained alignment and existing implicit
relational reasoning -- hence the term ``full-mode" -- without requiring
additional supervision. Specifically, we design an Adaptive Similarity
Distribution Matching (A-SDM) module to rectify unmatched positive sample
pairs. A-SDM adaptively pulls the unmatched positive pairs closer in the joint
embedding space, thereby achieving more precise global alignment. Additionally,
we introduce an Explicit Fine-grained Alignment (EFA) module, which makes up
for the lack of verification capability of implicit relational reasoning. EFA
strengthens explicit cross-modal fine-grained interactions by sparsifying the
similarity matrix and employs a hard coding method for local alignment. Our
proposed method is evaluated on three public datasets, achieving
state-of-the-art performance among all global matching methods. Our code is
available at https://github.com/yinhao1102/FMFA.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Hao Yin, Xin Man, Feiyu Chen, Jie Shao, Heng Tao Shen撰写的论文“Cross-modal Full-mode Fine-grained Alignment for Text-to-Image Person Retrieval”的全面摘要。</p>
<hr />
<h3 id="_1">论文摘要：跨模态全模式细粒度对齐用于文本到图像人物检索</h3>
<p><strong>1. 主要问题或研究问题：</strong>
文本到图像人物检索（TIPR）的核心挑战在于如何在共同的潜在空间中实现文本和视觉模态之间的有效对齐，以根据文本查询检索最相关的人物图像。现有方法主要通过隐式跨模态局部对齐来解决，但它们缺乏验证所有局部特征是否正确对齐的能力。此外，现有方法在模型更新时主要关注困难负样本，旨在细化正负样本之间的区别，却往往忽略了错误匹配的正样本。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
为了解决上述问题，作者提出了<strong>FMFA（Full-Mode Fine-grained Alignment）</strong>框架，通过显式细粒度对齐和现有隐式关系推理来增强全局匹配，而无需额外监督。其主要创新包括：</p>
<ul>
<li><strong>自适应相似度分布匹配（A-SDM）模块：</strong> 旨在纠正未匹配的正样本对。A-SDM在联合嵌入空间中自适应地将未匹配的正样本对拉近，从而实现更精确的全局对齐。它根据未匹配正样本对的相对距离自适应地调整拉力。</li>
<li><strong>显式细粒度对齐（EFA）模块：</strong> 弥补了隐式关系推理缺乏验证能力的不足。EFA通过稀疏化相似度矩阵并采用硬编码方法进行局部对齐，从而加强了显式跨模态细粒度交互。这种设计通过显式聚合稀疏相似度矩阵，并对聚合后的多模态表示与其原始视觉和文本表示进行硬编码对齐，以最小化冗余信息和计算成本。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
FMFA在三个公开数据集（CUHK-PEDES、ICFG-PEDES和RSTPReid）上进行了评估，并在所有全局匹配方法中取得了最先进的性能。</p>
<ul>
<li><strong>CUHK-PEDES数据集：</strong> 在不使用ReID域预训练的VL-Backbones时，FMFA在Rank-1准确率和mAP上优于所有现有全局匹配方法。在使用ReID域预训练的VL-Backbones时，FMFA也保持了优越性。</li>
<li><strong>RSTPReid数据集：</strong> FMFA取得了具有竞争力的性能，在Rank-1和Rank-5上均优于基线IRRA。</li>
<li><strong>ICFG-PEDES数据集：</strong> FMFA在所有评估指标上均取得了最佳性能，Rank-1和mAP均有显著提升。</li>
<li><strong>消融研究：</strong> 证实了A-SDM和EFA模块对整体性能的贡献，它们共同作用，相互补充。</li>
<li><strong>推理速度：</strong> 作为一种全局匹配方法，FMFA在推理阶段仅计算全局特征，因此比局部匹配方法具有更高的推理速度，尤其是在测试集规模增大时。</li>
</ul>
<p>这些结果表明FMFA具有良好的泛化性和鲁棒性，能够有效解决TIPR中的对齐挑战。</p>
<p><strong>4. 论文中提到的局限性：</strong>
论文指出，EFA模块中稀疏过程的固定阈值仅保留最相关的图像块，这可能导致语义信息的丢失，并限制局部特征的有效聚合，从而影响模型的整体性能。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
为了克服上述局限性，未来的研究方向可以包括：
*   整合能够捕获完整语义信息的自适应方法，例如树形Transformer（tree transformer [43]），以进一步增强模型性能。</p>
<hr />
<p>这份摘要突出了论文的核心贡献，即通过结合自适应相似度分布匹配和显式细粒度对齐，解决了文本到图像人物检索中跨模态对齐的挑战，并在多个基准测试中取得了领先的性能。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To alleviate these issues, we propose FMFA, a
cross-modal Full-Mode Fine-grained Alignment framework, which enhances global
matching through explicit fine-grained alignment and existing implicit
relational reasoning -- hence the term ``full-mode" -- without requiring
additional supervision.</li>
<li>Additionally,
we introduce an Explicit Fine-grained Alignment (EFA) module, which makes up
for the lack of verification capability of implicit relational reasoning.</li>
<li>Our
proposed method is evaluated on three public datasets, achieving
state-of-the-art performance among all global matching methods.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.13754v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.13754v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-09-18 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
