<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2026-02-09 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-06
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2026-02-09
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2026-02-06/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../../depth_completion/DNet/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2026-02-09">Arxiv Computer Vision Papers - 2026-02-09</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#cinescene-implicit-3d-as-effective-scene-representation-for-cinematic-video-generation" class="nav-link">CineScene: Implicit 3D as Effective Scene Representation for Cinematic Video Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#cinescene-implicit-3d-as-effective-scene-representation-for-cinematic-video-generation_1" class="nav-link">CINESCENE: Implicit 3D as Effective Scene Representation for Cinematic Video Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#dreamdojo-a-generalist-robot-world-model-from-large-scale-human-videos" class="nav-link">DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos</a>
                </li>
                <li class="nav-item">
                    <a href="#seeing-beyond-redundancy-task-complexitys-role-in-vision-token-specialization-in-vllms" class="nav-link">Seeing Beyond Redundancy: Task Complexity's Role in Vision Token Specialization in VLLMs</a>
                </li>
                <li class="nav-item">
                    <a href="#prompt-reinjection-alleviating-prompt-forgetting-in-multimodal-diffusion-transformers" class="nav-link">Prompt Reinjection: Alleviating Prompt Forgetting in Multimodal Diffusion Transformers</a>
                </li>
                <li class="nav-item">
                    <a href="#prompt-reinjection-alleviating-prompt-forgetting-in-multimodal-diffusion-transformers_1" class="nav-link">论文方法分析：Prompt Reinjection: Alleviating Prompt Forgetting in Multimodal Diffusion Transformers</a>
                </li>
                <li class="nav-item">
                    <a href="#vision-transformer-finetuning-benefits-from-non-smooth-components" class="nav-link">Vision Transformer Finetuning Benefits from Non-Smooth Components</a>
                </li>
                <li class="nav-item">
                    <a href="#vision-transformer-finetuning-benefits-from-non-smooth-components_1" class="nav-link">论文方法分析：Vision Transformer Finetuning Benefits from Non-Smooth Components</a>
                </li>
                <li class="nav-item">
                    <a href="#nanoflux-distillation-driven-compression-of-large-text-to-image-generation-models-for-mobile-devices" class="nav-link">NanoFLUX: Distillation-Driven Compression of Large Text-to-Image Generation Models for Mobile Devices</a>
                </li>
                <li class="nav-item">
                    <a href="#rfdm-residual-flow-diffusion-model-for-efficient-causal-video-editing" class="nav-link">RFDM: Residual Flow Diffusion Model for Efficient Causal Video Editing</a>
                </li>
                <li class="nav-item">
                    <a href="#rfdm-residual-flow-diffusion-model-for-efficient-causal-video-editing_1" class="nav-link">论文方法分析：RFDM (Residual Flow Diffusion Model) for Efficient Causal Video Editing</a>
                </li>
                <li class="nav-item">
                    <a href="#parameters-as-experts-adapting-vision-models-with-dynamic-parameter-routing" class="nav-link">Parameters as Experts: Adapting Vision Models with Dynamic Parameter Routing</a>
                </li>
                <li class="nav-item">
                    <a href="#_1" class="nav-link">论文方法分析与总结</a>
                </li>
                <li class="nav-item">
                    <a href="#rethinking-multi-condition-dits-eliminating-redundant-attention-via-position-alignment-and-keyword-scoping" class="nav-link">Rethinking Multi-Condition DiTs: Eliminating Redundant Attention via Position-Alignment and Keyword-Scoping</a>
                </li>
                <li class="nav-item">
                    <a href="#_2" class="nav-link">论文方法分析与总结</a>
                </li>
                <li class="nav-item">
                    <a href="#gaussianpop-principled-simplification-framework-for-compact-3d-gaussian-splatting-via-error-quantification" class="nav-link">GaussianPOP: Principled Simplification Framework for Compact 3D Gaussian Splatting via Error Quantification</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2026-02-09">Arxiv Computer Vision Papers - 2026-02-09</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，这是一份针对您提供的 Arxiv 计算机视觉论文列表的简明执行摘要，旨在帮助忙碌的研究人员快速了解该领域的最新进展：</p>
<hr />
<p><strong>执行摘要：2026年2月6日 Arxiv 计算机视觉论文精选</strong></p>
<p><strong>主要趋势与主题：</strong></p>
<p>本期 Arxiv 论文集聚焦于<strong>高效的场景表示、多模态模型优化、以及面向实际应用的模型压缩与编辑</strong>。核心趋势包括：</p>
<ul>
<li><strong>3D场景表示与生成：</strong> 利用隐式表示（如CineScene）和基于高斯溅射（GaussianPOP）的方法，实现更有效、更紧凑的3D场景建模，为视频生成和编辑奠定基础。</li>
<li><strong>多模态模型（VLLMs）的理解与优化：</strong> 深入探讨了视觉语言模型（VLLMs）中任务复杂性对视觉标记专业化的影响（Seeing Beyond Redundancy），并提出了缓解提示遗忘（Prompt Reinjection）和优化微调策略（Vision Transformer Finetuning Benefits from Non-Smooth Components）的新方法。</li>
<li><strong>模型压缩与效率提升：</strong> 针对大型生成模型（如文本到图像模型），提出了蒸馏驱动的压缩技术（NanoFLUX），使其能在移动设备上高效运行。</li>
<li><strong>视频生成与编辑的效率与质量：</strong> 探索了更高效的视频编辑方法（RFDM），以及通过世界模型（DreamDojo）从大规模人类视频中学习通用机器人能力。</li>
<li><strong>模型适应性与动态路由：</strong> 提出了参数专家化（Parameters as Experts）和动态参数路由的新范式，以适应不同任务和条件。</li>
</ul>
<p><strong>亮点与创新：</strong></p>
<ul>
<li><strong>CineScene (1)</strong> 和 <strong>GaussianPOP (10)</strong> 在3D场景表示方面展现出显著的进步，前者通过隐式3D表示提升了电影级视频生成的质量，后者则提供了原则性的简化框架，有望实现更紧凑的3D高斯溅射模型。</li>
<li><strong>DreamDojo (2)</strong> 提出了一种从大规模人类视频中学习通用机器人世界模型的方法，这对于实现更智能、更具适应性的机器人至关重要。</li>
<li><strong>NanoFLUX (6)</strong> 在模型压缩方面取得了突破，为将大型文本到图像生成模型部署到资源受限的移动设备上提供了可行的解决方案。</li>
</ul>
<p><strong>新兴研究方向与技术：</strong></p>
<ul>
<li><strong>隐式3D场景表示：</strong> 结合深度学习技术，实现更灵活、更逼真的3D场景建模和渲染。</li>
<li><strong>多模态模型的可解释性与鲁棒性：</strong> 深入理解模型内部机制，并开发策略以提高其在不同任务和条件下的表现。</li>
<li><strong>面向边缘设备的生成模型：</strong> 持续探索高效的模型架构和压缩技术，以支持在移动端和嵌入式设备上运行复杂的生成任务。</li>
<li><strong>动态模型适应性：</strong> 通过参数路由等技术，使模型能够根据输入数据或任务需求动态调整其行为。</li>
</ul>
<p><strong>建议阅读论文：</strong></p>
<p>为了快速掌握本期论文的核心内容和潜在影响，建议优先阅读以下论文：</p>
<ol>
<li><strong>CineScene (1):</strong> 对于关注视频生成和3D场景表示的研究者。</li>
<li><strong>DreamDojo (2):</strong> 对于机器人学、多模态学习和通用AI的研究者。</li>
<li><strong>NanoFLUX (6):</strong> 对于模型压缩、部署和移动端AI的研究者。</li>
<li><strong>GaussianPOP (10):</strong> 对于3D视觉、计算机图形学和场景重建的研究者。</li>
</ol>
<hr />
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2602.06959v1">CineScene: Implicit 3D as Effective Scene Representation for Cinematic Video Generation</a></li>
<li><a href="#2602.06949v1">DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos</a></li>
<li><a href="#2602.06914v1">Seeing Beyond Redundancy: Task Complexity's Role in Vision Token Specialization in VLLMs</a></li>
<li><a href="#2602.06886v1">Prompt Reinjection: Alleviating Prompt Forgetting in Multimodal Diffusion Transformers</a></li>
<li><a href="#2602.06883v1">Vision Transformer Finetuning Benefits from Non-Smooth Components</a></li>
<li><a href="#2602.06879v1">NanoFLUX: Distillation-Driven Compression of Large Text-to-Image Generation Models for Mobile Devices</a></li>
<li><a href="#2602.06871v1">RFDM: Residual Flow Diffusion Model for Efficient Causal Video Editing</a></li>
<li><a href="#2602.06862v1">Parameters as Experts: Adapting Vision Models with Dynamic Parameter Routing</a></li>
<li><a href="#2602.06850v1">Rethinking Multi-Condition DiTs: Eliminating Redundant Attention via Position-Alignment and Keyword-Scoping</a></li>
<li><a href="#2602.06830v1">GaussianPOP: Principled Simplification Framework for Compact 3D Gaussian Splatting via Error Quantification</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2602.06959v1'></a></p>
<h2 id="cinescene-implicit-3d-as-effective-scene-representation-for-cinematic-video-generation"><a href="https://arxiv.org/abs/2602.06959v1">CineScene: Implicit 3D as Effective Scene Representation for Cinematic Video Generation</a></h2>
<p><strong>Authors:</strong> Kaiyi Huang, Yukun Huang, Yu Li, Jianhong Bai, Xintao Wang, Zinan Lin, Xuefei Ning, Jiwen Yu, Pengfei Wan, Yu Wang, Xihui Liu</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Cinematic video production requires control over scene-subject composition and camera movement, but live-action shooting remains costly due to the need for constructing physical sets. To address this, we introduce the task of cinematic video generation with decoupled scene context: given multiple images of a static environment, the goal is to synthesize high-quality videos featuring dynamic subject while preserving the underlying scene consistency and following a user-specified camera trajectory. We present CineScene, a framework that leverages implicit 3D-aware scene representation for cinematic video generation. Our key innovation is a novel context conditioning mechanism that injects 3D-aware features in an implicit way: By encoding scene images into visual representations through VGGT, CineScene injects spatial priors into a pretrained text-to-video generation model by additional context concatenation, enabling camera-controlled video synthesis with consistent scenes and dynamic subjects. To further enhance the model's robustness, we introduce a simple yet effective random-shuffling strategy for the input scene images during training. To address the lack of training data, we construct a scene-decoupled dataset with Unreal Engine 5, containing paired videos of scenes with and without dynamic subjects, panoramic images representing the underlying static scene, along with their camera trajectories. Experiments show that CineScene achieves state-of-the-art performance in scene-consistent cinematic video generation, handling large camera movements and demonstrating generalization across diverse environments.</p>
<p><strong>Analysis:</strong></p>
<p>好的，我将以AI领域高水平研究生的视角，深入分析这篇论文的方法部分，并遵循您提供的分析框架。</p>
<hr />
<h2 id="cinescene-implicit-3d-as-effective-scene-representation-for-cinematic-video-generation_1">CINESCENE: Implicit 3D as Effective Scene Representation for Cinematic Video Generation</h2>
<h3 id="1">1. 摘要翻译</h3>
<p><strong>CINESCENE：用于电影视频生成的隐式3D场景表示</strong></p>
<p>电影视频制作需要对场景-主体构图和摄像机运动进行控制，但实景拍摄由于需要搭建物理场景而成本高昂。为了解决这个问题，我们引入了具有解耦场景上下文的电影视频生成任务：给定一个静态环境的多个图像，目标是合成高质量的视频，其中包含动态主体，同时保持场景一致性并遵循用户指定的摄像机轨迹。我们提出了CINESCENE，一个利用隐式3D场景表示进行电影视频生成的框架。我们的关键创新在于一种新颖的上下文条件机制，它以隐式方式注入3D感知特征：通过VGGT将场景图像编码为视觉表示，CINESCENE通过预训练的文本到视频生成模型注入空间先验，实现了场景上下文的解耦，从而实现了摄像机控制的视频合成，并保持场景和动态主体的连贯性。为了进一步增强模型的鲁棒性，我们在训练期间引入了一个简单而有效的随机打乱场景图像的策略。为了解决训练数据不足的问题，我们使用Unreal Engine 5构建了一个解耦场景的数据集，其中包含具有和不具有动态主体的配对视频、代表底层静态场景的全景图像，以及它们的摄像机轨迹。实验表明，CINESCENE在场景一致的电影视频生成方面取得了最先进的性能，能够处理大的摄像机运动，并展示了跨不同环境的泛化能力。</p>
<h3 id="2">2. 方法动机分析</h3>
<ul>
<li>
<p><strong>驱动力</strong>：</p>
<ul>
<li><strong>降低电影制作成本</strong>：实景拍摄成本高昂，需要搭建物理场景，而虚拟生成可以大幅降低成本。</li>
<li><strong>实现精细的视觉叙事控制</strong>：电影制作需要精确控制场景构图、主体表现和摄像机运动，以达到预期的叙事效果。</li>
<li><strong>生成具有动态主体的场景一致性视频</strong>：现有方法在处理大范围摄像机运动时，难以保持场景的一致性，尤其是在引入新的动态主体时。</li>
</ul>
</li>
<li>
<p><strong>现有方法痛点</strong>：</p>
<ul>
<li><strong>2D上下文方法</strong>：虽然灵活，但在大范围视角变化下难以维持场景一致性，缺乏空间理解能力。</li>
<li><strong>3D信息方法</strong>：依赖显式的3D重建（如深度图、点云），过程复杂，且从稀疏输入中获取准确3D信息具有挑战性，不完美的几何表示会影响生成质量。</li>
<li><strong>损失引导方法（如Geometry Forcing）</strong>：虽然能强制场景一致性，但仅限于静态场景，难以生成新的动态主体，因为损失信号会隐式地惩罚动态内容。</li>
<li><strong>现有数据集不足</strong>：缺乏能够分离静态场景和动态主体，并提供精确摄像机轨迹的数据集。</li>
</ul>
</li>
<li>
<p><strong>研究假设</strong>：</p>
<ul>
<li><strong>隐式3D表示的潜力</strong>：利用像VGGT这样的3D感知模型，可以从2D图像中获得丰富的空间信息，而无需显式的3D重建。</li>
<li><strong>上下文条件机制的有效性</strong>：将隐式3D场景表示作为上下文条件注入到预训练的文本到视频模型中，可以实现场景与动态主体的解耦，从而在保持场景一致性的同时生成动态内容。</li>
<li><strong>随机打乱策略的鲁棒性</strong>：通过随机打乱输入场景图像的顺序，可以促使模型学习更鲁棒的场景-图像对应关系，避免过度依赖固定的输入顺序。</li>
</ul>
</li>
</ul>
<h3 id="3">3. 方法设计详解</h3>
<p><strong>流程总结</strong>：</p>
<p>CINESCENE 的核心思想是将静态场景的隐式3D表示作为条件，注入到预训练的文本到视频（T2V）生成模型中，以实现场景一致且摄像机可控的动态视频生成。整个流程可以概括为以下几个关键步骤：</p>
<ol>
<li>
<p><strong>场景解耦数据集构建 (Scene-Decoupled Dataset Construction)</strong>：</p>
<ul>
<li><strong>动机</strong>：现有数据集无法满足解耦场景和动态主体的需求。</li>
<li><strong>操作</strong>：使用 <strong>Unreal Engine 5</strong> 生成数据集。<ul>
<li><strong>3D环境</strong>：收集35个高质量的3D环境资产，并加入超现实环境以增加多样性。</li>
<li><strong>主体 (Subjects)</strong>：收集70个不同风格（写实、动漫、游戏）的3D人体模型。</li>
<li><strong>动画 (Animations)</strong>：收集约100种动态动画（如挥手、跳舞）。</li>
<li><strong>摄像机轨迹 (Camera Trajectories)</strong>：设计了多种摄像机运动，包括：<ul>
<li><strong>Arc Movements (Left/Right)</strong>：跨越75度，持续跟踪主体。</li>
<li><strong>Pan Movements (Left/Right)</strong>：仅旋转，75度，77帧。</li>
<li><strong>Arc and Tilt Movements (Up/Down)</strong>：垂直运动在10-45度之间，跟踪主体或仅旋转。</li>
<li><strong>Dolly, Truck, and Pedestal Movements</strong>：随机选择距离阈值，范围定义如[1/4, 2]等。</li>
</ul>
</li>
<li><strong>初始视角 (Initial Viewpoint)</strong>：所有摄像机运动和全景图的起始视角随机选择在-45到45度之间，相对于主体朝向。</li>
</ul>
</li>
<li><strong>输出</strong>：数据集包含：<ul>
<li>动态视频（带/不带动态主体）。</li>
<li>360°全景图（代表静态场景）。</li>
<li>摄像机轨迹。</li>
<li>总计46K视频-场景图像对，分布在35个3D环境中。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>3D感知场景表示提取 (3D-Aware Scene Representation Extraction)</strong>：</p>
<ul>
<li><strong>动机</strong>：需要一种能够捕捉场景空间信息和摄像机视角信息的表示，以指导视频生成。</li>
<li><strong>操作</strong>：利用 <strong>VGGT (Visual Geometry Grounded Transformer)</strong> 模型。<ul>
<li><strong>场景上下文图像 (Scene Context Images)</strong>：<ul>
<li>从全景图进行 <strong>equirectangular-to-perspective projection</strong>，生成20个视角（每18度一个）的透视图像。</li>
<li>每个图像的 <strong>FoV (Field-of-View)</strong> 设置为90度，以捕捉更全面的场景上下文。</li>
</ul>
</li>
<li><strong>隐式3D场景表示 (Implicit 3D Scene Representation)</strong>：<ul>
<li>使用VGGT的Transformer骨干网络提取特征。</li>
<li>提取的特征被解耦为 <strong>图像特征 (Image Feature, <script type="math/tex">F_i</script>)</strong> 和 <strong>摄像机特征 (Camera Feature, <script type="math/tex">F_c</script>)</strong>。<ul>
<li>
<script type="math/tex">F_i \in \mathbb{R}^{20 \times k \times 2048}</script>：包含深度图、点云结构、跟踪特征等空间线索。</li>
<li>
<script type="math/tex">F_c \in \mathbb{R}^{20 \times 1 \times 2048}</script>：包含摄像机姿态信息。</li>
</ul>
</li>
<li><strong>融合特征 (<script type="math/tex">F</script>)</strong>：将 <script type="math/tex">F_i</script> 和 <script type="math/tex">F_c</script> 融合，形成最终的隐式3D场景表示 <script type="math/tex">F \in \mathbb{R}^{20 \times k \times 2048}</script>。融合方式为：先将 <script type="math/tex">F_c</script> 扩展以匹配 <script type="math/tex">F_i</script> 的空间维度，然后进行逐元素相加。这种融合方式结合了场景内容和摄像机视角信息。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>视频生成 (Video Generation)</strong>：</p>
<ul>
<li><strong>动机</strong>：将提取的场景表示和用户指定的条件注入到现有的视频生成模型中。</li>
<li><strong>模型</strong>：基于预训练的文本到视频（T2V）扩散模型（如DiT）。</li>
<li><strong>输入</strong>：<ul>
<li>场景上下文图像 (<script type="math/tex">I</script>)</li>
<li>隐式3D场景表示 (<script type="math/tex">F</script>)</li>
<li>摄像机轨迹 (<script type="math/tex">C \in \mathbb{R}^{f \times 3 \times 4}</script>)</li>
<li>文本提示 (<script type="math/tex">P</script>)</li>
</ul>
</li>
<li><strong>操作</strong>：<ul>
<li><strong>场景上下文图像条件 (Scene Context Images Condition)</strong>：<ul>
<li>使用一个带有时间压缩（rate=4）和空间压缩（rate=8）的 <strong>causal 3D VAE</strong> 对场景图像进行编码。</li>
<li>编码后的图像被patchify（patch size=2），得到图像token <script type="math/tex">I_t \in \mathbb{R}^{20 \times h/16 \times w/16 \times d}</script>。</li>
</ul>
</li>
<li><strong>隐式3D场景表示条件 (Implicit 3D Scene Representation Condition)</strong>：<ul>
<li>将隐式特征 <script type="math/tex">F \in \mathbb{R}^{20 \times k \times 2048}</script> 重塑并插值，以匹配图像特征的空间维度 (<script type="math/tex">F \in \mathbb{R}^{20 \times h/8 \times w/8 \times 2048}</script>)。</li>
<li>通过卷积层和Layer Normalization，将其转换为隐式3D token <script type="math/tex">F_t \in \mathbb{R}^{20 \times h/16 \times w/16 \times d}</script>。</li>
</ul>
</li>
<li><strong>摄像机和提示条件 (Camera and Prompt Condition)</strong>：<ul>
<li><strong>摄像机注入 (Camera Injection)</strong>：使用一个可学习的摄像机编码器将摄像机参数 <script type="math/tex">C</script> 映射到与视频token相同的通道数，并将其加到代表噪声视频的视觉特征上。</li>
<li><strong>提示注入 (Prompt Injection)</strong>：将文本提示通过交叉注意力机制注入到模型中。</li>
</ul>
</li>
<li><strong>上下文注入 (Context Injection)</strong>：将图像token (<script type="math/tex">I_t</script>) 和隐式3D token (<script type="math/tex">F_t</script>) 沿帧维度拼接，形成上下文token序列，并与噪声视频的token序列拼接，输入到Transformer块中。</li>
<li><strong>随机打乱上下文图像 (Shuffled Context Images Alignment)</strong>：<ul>
<li><strong>动机</strong>：固定顺序的上下文图像（尤其是第一张和最后一张）会主导生成模型，使其忽略隐式3D表示。</li>
<li><strong>操作</strong>：固定第一张上下文图像的位置（对应目标视频的第一帧），随机打乱其余上下文图像的顺序。</li>
<li><strong>效果</strong>：促使模型学习像素级上下文和隐式3D表示之间的对齐，而不是依赖固定的输入顺序。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<p><strong>模型结构</strong>：</p>
<ul>
<li><strong>VGGT Backbone</strong>：用于提取场景的隐式3D表示（图像特征和摄像机特征）。</li>
<li><strong>Feature Fusion Module</strong>：将图像特征和摄像机特征融合，形成统一的隐式3D场景表示。</li>
<li><strong>3D VAE (Causal)</strong>：用于编码场景上下文图像，生成图像token。</li>
<li><strong>DiT Blocks (Diffusion Transformer Blocks)</strong>：核心的视频生成模块，包含Spatial Attention, 3D Attention, Projector, Cross-Attention, FFN等。</li>
<li><strong>Context Conditioning Mechanism</strong>：将图像token (<script type="math/tex">I_t</script>) 和隐式3D token (<script type="math/tex">F_t</script>) 注入到DiT Blocks中。</li>
<li><strong>Camera Encoder</strong>：将摄像机轨迹编码并注入到模型中。</li>
<li><strong>Text Encoder (Implicit)</strong>：通过交叉注意力机制注入文本提示。</li>
</ul>
<p><strong>算法解释</strong>：</p>
<ul>
<li><strong>隐式3D场景表示融合</strong>：<script type="math/tex">F = \text{Expand}(F_c) + F_i</script>。这里的关键在于，通过将摄像机信息（视角、相对位置）与场景内容信息（纹理、几何线索）在特征层面进行融合，模型能够获得对场景的更全面的理解，而不仅仅是像素信息。这种融合方式比简单的拼接或注意力机制更直接地将空间信息编码到特征中。</li>
<li><strong>上下文条件注入</strong>：将 <script type="math/tex">I_t</script> 和 <script type="math/tex">F_t</script> 拼接后输入到DiT的Transformer块中。这种拼接方式（frame-dimension concatenation）使得模型能够同时处理来自不同模态（图像、3D表示）的上下文信息，并且这些信息在时域上是同步的（与视频帧对齐），这对于保持场景和主体在时间上的连贯性至关重要。</li>
<li><strong>随机打乱上下文图像</strong>：在训练时，固定第一张图像，打乱其余图像。这是一种巧妙的正则化技术，旨在打破模型对输入顺序的依赖，迫使其学习更本质的场景特征，并与隐式3D表示进行更有效的对齐。</li>
</ul>
<h3 id="4">4. 方法对比分析</h3>
<ul>
<li>
<p><strong>本质区别</strong>：</p>
<ul>
<li><strong>与2D上下文方法</strong>：CINESCENE引入了隐式3D表示，提供了更强的空间理解能力，从而在大范围视角变化下保持场景一致性。</li>
<li><strong>与显式3D方法</strong>：CINESCENE避免了复杂的3D重建，直接利用VGGT提取的隐式3D特征，降低了计算复杂度，并减少了对3D重建精度的依赖。</li>
<li><strong>与损失引导方法</strong>：CINESCENE将隐式3D表示作为<strong>条件</strong>注入，而不是作为<strong>监督损失</strong>。这使得模型能够解耦静态场景和动态主体，允许生成新的动态内容，而损失引导方法则倾向于生成静态场景或惩罚动态内容。</li>
<li><strong>与摄像机控制方法</strong>：CINESCENE不仅实现了摄像机控制，更重要的是在摄像机控制的同时，保证了场景的一致性，尤其是在大范围运动下。</li>
</ul>
</li>
<li>
<p><strong>创新贡献</strong>：</p>
<ul>
<li><strong>解耦场景上下文的电影视频生成任务</strong>：首次提出这一任务，强调在保持场景一致性的前提下生成动态视频。</li>
<li><strong>隐式3D场景表示的上下文条件注入</strong>：将VGGT提取的隐式3D特征作为条件注入到T2V模型中，实现场景与动态主体的解耦。</li>
<li><strong>随机打乱上下文图像的训练策略</strong>：有效提升了模型对隐式3D表示的学习能力和鲁棒性。</li>
<li><strong>构建解耦场景数据集</strong>：为该任务提供了必要的训练数据。</li>
</ul>
</li>
<li>
<p><strong>适用场景</strong>：</p>
<ul>
<li><strong>虚拟场景制作</strong>：用于电影、游戏、虚拟现实等场景的预可视化或内容生成。</li>
<li><strong>需要精确摄像机控制的视频生成</strong>：如广告、短片等，需要遵循特定镜头语言。</li>
<li><strong>需要保持场景一致性的动态视频生成</strong>：尤其是在大范围摄像机运动下。</li>
</ul>
</li>
</ul>
<h3 id="5">5. 实验分析</h3>
<ul>
<li>
<p><strong>验证方法</strong>：</p>
<ul>
<li><strong>数据集</strong>：自建的Scene-Decoupled Video Dataset，以及用于OOD测试的DiT360数据集。</li>
<li><strong>评估指标</strong>：<ul>
<li><strong>场景一致性</strong>：Mat. Pix., CLIP-V, PSNR, SSIM, LPIPS。</li>
<li><strong>摄像机准确性</strong>：RotErr, TransErr, CamMC。</li>
<li><strong>文本对齐</strong>：CLIP-T。</li>
<li><strong>视频质量</strong>：VBench。</li>
</ul>
</li>
<li><strong>对比方法</strong>：Context-based (FramePack, Context-as-Memory), Explicit 3D Guidance (Gen3C), Camera-Controlled (Traj-Attn, RecamMaster)。</li>
<li><strong>消融实验</strong>：<ul>
<li>不同隐式3D表示方法（无隐式3D, 仅图像特征, 仅摄像机特征, 融合特征）。</li>
<li>不同上下文图像处理策略（有序 vs. 随机打乱）。</li>
<li>不同条件注入机制（如拼接维度）。</li>
<li>不同场景描述（有/无，一致/不一致）。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>关键结果</strong>：</p>
<ul>
<li><strong>场景一致性</strong>：CINESCENE在Mat. Pix., CLIP-V, PSNR, SSIM等指标上均优于基线方法，尤其是在大范围视角变化下。</li>
<li><strong>摄像机准确性</strong>：CINESCENE在RotErr, TransErr, CamMC上表现优异，表明其能够精确控制摄像机运动。</li>
<li><strong>OOD测试</strong>：在DiT360数据集上，CINESCENE在场景一致性方面表现出色，证明了其泛化能力。</li>
<li><strong>消融实验</strong>：<ul>
<li>融合图像和摄像机特征的隐式3D表示效果最佳。</li>
<li>随机打乱上下文图像的策略显著优于有序输入。</li>
<li>将上下文信息沿帧维度拼接（frame-dimension concatenation）效果最好。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>优势场景</strong>：</p>
<ul>
<li><strong>大范围摄像机运动</strong>：论文中展示的示例（如图4）表明，在摄像机大幅度移动时，CINESCENE能保持场景的连贯性，而其他方法（如FramePack, Gen3C）则出现明显的场景变化或不一致。</li>
<li><strong>动态主体生成</strong>：通过将隐式3D表示作为条件，CINESCENE能够生成具有新动态主体的视频，同时保持场景的稳定性。</li>
<li><strong>虚拟场景制作</strong>：在OOD测试和应用部分展示了其在真实世界场景和虚拟场景中的潜力。</li>
</ul>
</li>
<li>
<p><strong>局限性</strong>：</p>
<ul>
<li><strong>视频长度和视角变化限制</strong>：目前主要生成短视频（77帧），最大视角变化为75度。扩展到更长视频和更大视角变化是未来的研究方向。</li>
<li><strong>固定初始视角</strong>：当前模型使用与视频第一帧相同的初始视角，未来可以支持随机摄像机位置。</li>
<li><strong>模型继承的局限性</strong>：继承了预训练T2V模型的局限性，如在人体大动作时可能出现失真。</li>
<li><strong>计算开销</strong>：虽然避免了显式3D重建，但隐式3D表示的提取和注入仍需要一定的计算资源。</li>
</ul>
</li>
</ul>
<h3 id="6">6. 实用指南</h3>
<ul>
<li><strong>开源情况</strong>：论文作者提供了代码和数据集的链接（Project Page）。</li>
<li><strong>实现细节</strong>：<ul>
<li><strong>数据集</strong>：使用Unreal Engine 5构建，包含3D环境、主体、动画和摄像机轨迹。</li>
<li><strong>模型</strong>：基于预训练的T2V扩散模型（如DiT），并引入VGGT进行3D特征提取。</li>
<li><strong>训练</strong>：使用Scene-Decoupled Video Dataset进行训练，关键在于上下文图像的随机打乱策略。</li>
<li><strong>超参数</strong>：论文中提到了训练步数（10K steps）、batch size（16）、学习率（5e-5）、分辨率（384x672）、帧数（77 frames）等。</li>
</ul>
</li>
<li><strong>迁移可能</strong>：<ul>
<li><strong>其他3D感知模型</strong>：可以将VGGT替换为其他能够提取隐式3D表示的模型，只要其输出格式兼容。</li>
<li><strong>其他T2V模型</strong>：可以将上下文条件注入机制应用到其他预训练的T2V模型上，如Sora、Imagen Video等，以增强其场景一致性和摄像机控制能力。</li>
<li><strong>其他任务</strong>：该方法的核心思想——利用隐式3D表示进行条件生成，可能可以迁移到其他需要空间理解和场景一致性的生成任务中，例如3D场景生成、图像编辑等。</li>
</ul>
</li>
</ul>
<h3 id="7">7. 总结</h3>
<ul>
<li><strong>核心思想</strong>：用隐式3D场景表示指导视频生成，实现场景一致的动态视频。</li>
<li><strong>速记版pipeline</strong>：<ol>
<li><strong>构建数据集</strong>：用UE5生成带动态主体和摄像机轨迹的视频。</li>
<li><strong>提取3D场景表示</strong>：用VGGT从静态图像中提取隐式3D特征。</li>
<li><strong>注入条件</strong>：将3D特征、场景图像和摄像机轨迹作为条件，输入到T2V模型。</li>
<li><strong>随机打乱</strong>：训练时打乱场景图像顺序，提升模型鲁棒性。</li>
<li><strong>生成视频</strong>：输出场景一致、摄像机可控的动态视频。</li>
</ol>
</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we introduce the task of cinematic video generation with decoupled scene context: given multiple images of a static environment, the goal is to synthesize high-quality videos featuring dynamic subject while preserving the underlying scene consistency and following a user-specified camera trajectory.</li>
<li>We present CineScene, a framework that leverages implicit 3D-aware scene representation for cinematic video generation.</li>
<li>Our key innovation is a novel context conditioning mechanism that injects 3D-aware features in an implicit way: By encoding scene images into visual representations through VGGT, CineScene injects spatial priors into a pretrained text-to-video generation model by additional context concatenation, enabling camera-controlled video synthesis with consistent scenes and dynamic subjects.</li>
<li>To further enhance the model's robustness, we introduce a simple yet effective random-shuffling strategy for the input scene images during training.</li>
<li>Experiments show that CineScene achieves state-of-the-art performance in scene-consistent cinematic video generation, handling large camera movements and demonstrating generalization across diverse environments.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2602.06959v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2602.06959v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2602.06949v1'></a></p>
<h2 id="dreamdojo-a-generalist-robot-world-model-from-large-scale-human-videos"><a href="https://arxiv.org/abs/2602.06949v1">DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos</a></h2>
<p><strong>Authors:</strong> Shenyuan Gao, William Liang, Kaiyuan Zheng, Ayaan Malik, Seonghyeon Ye, Sihyun Yu, Wei-Cheng Tseng, Yuzhu Dong, Kaichun Mo, Chen-Hsuan Lin, Qianli Ma, Seungjun Nah, Loic Magne, Jiannan Xiang, Yuqi Xie, Ruijie Zheng, Dantong Niu, You Liang Tan, K. R. Zentner, George Kurian, Suneel Indupuru, Pooya Jannaty, Jinwei Gu, Jun Zhang, Jitendra Malik, Pieter Abbeel, Ming-Yu Liu, Yuke Zhu, Joel Jang, Linxi "Jim" Fan</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Categories:</strong> cs.RO, cs.AI, cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Being able to simulate the outcomes of actions in varied environments will revolutionize the development of generalist agents at scale. However, modeling these world dynamics, especially for dexterous robotics tasks, poses significant challenges due to limited data coverage and scarce action labels. As an endeavor towards this end, we introduce DreamDojo, a foundation world model that learns diverse interactions and dexterous controls from 44k hours of egocentric human videos. Our data mixture represents the largest video dataset to date for world model pretraining, spanning a wide range of daily scenarios with diverse objects and skills. To address the scarcity of action labels, we introduce continuous latent actions as unified proxy actions, enhancing interaction knowledge transfer from unlabeled videos. After post-training on small-scale target robot data, DreamDojo demonstrates a strong understanding of physics and precise action controllability. We also devise a distillation pipeline that accelerates DreamDojo to a real-time speed of 10.81 FPS and further improves context consistency. Our work enables several important applications based on generative world models, including live teleoperation, policy evaluation, and model-based planning. Systematic evaluation on multiple challenging out-of-distribution (OOD) benchmarks verifies the significance of our method for simulating open-world, contact-rich tasks, paving the way for general-purpose robot world models.</p>
<p><strong>Analysis:</strong></p>
<p>好的，我将以AI领域专业高水平研究生的视角，深入分析您提供的论文方法部分，并遵循您提出的分析框架。请提供论文的PDF文件或文本内容，我将为您进行详细的解读。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>As an endeavor towards this end, we introduce DreamDojo, a foundation world model that learns diverse interactions and dexterous controls from 44k hours of egocentric human videos.</li>
<li>To address the scarcity of action labels, we introduce continuous latent actions as unified proxy actions, enhancing interaction knowledge transfer from unlabeled videos.</li>
<li>Systematic evaluation on multiple challenging out-of-distribution (OOD) benchmarks verifies the significance of our method for simulating open-world, contact-rich tasks, paving the way for general-purpose robot world models.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2602.06949v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2602.06949v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2602.06914v1'></a></p>
<h2 id="seeing-beyond-redundancy-task-complexitys-role-in-vision-token-specialization-in-vllms"><a href="https://arxiv.org/abs/2602.06914v1">Seeing Beyond Redundancy: Task Complexity's Role in Vision Token Specialization in VLLMs</a></h2>
<p><strong>Authors:</strong> Darryl Hannan, John Cooper, Dylan White, Yijing Watkins</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Vision capabilities in vision large language models (VLLMs) have consistently lagged behind their linguistic capabilities. In particular, numerous benchmark studies have demonstrated that VLLMs struggle when fine-grained visual information or spatial reasoning is required. However, we do not yet understand exactly why VLLMs struggle so much with these tasks relative to others. Some works have focused on visual redundancy as an explanation, where high-level visual information is uniformly spread across numerous tokens and specific, fine-grained visual information is discarded. In this work, we investigate this premise in greater detail, seeking to better understand exactly how various types of visual information are processed by the model and what types of visual information are discarded. To do so, we introduce a simple synthetic benchmark dataset that is specifically constructed to probe various visual features, along with a set of metrics for measuring visual redundancy, allowing us to better understand the nuances of their relationship. Then, we explore fine-tuning VLLMs on a number of complex visual tasks to better understand how redundancy and compression change based upon the complexity of the data that a model is trained on. We find that there is a connection between task complexity and visual compression, implying that having a sufficient ratio of high complexity visual data is crucial for altering the way that VLLMs distribute their visual representation and consequently improving their performance on complex visual tasks. We hope that this work will provide valuable insights for training the next generation of VLLMs.</p>
<p><strong>Analysis:</strong></p>
<p>好的，我将以一名AI领域高水平研究生的视角，深入分析您提供的论文方法部分，并遵循您提出的分析框架。请提供论文的具体内容，我将为您进行详细的解读。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To do so, we introduce a simple synthetic benchmark dataset that is specifically constructed to probe various visual features, along with a set of metrics for measuring visual redundancy, allowing us to better understand the nuances of their relationship.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2602.06914v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2602.06914v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2602.06886v1'></a></p>
<h2 id="prompt-reinjection-alleviating-prompt-forgetting-in-multimodal-diffusion-transformers"><a href="https://arxiv.org/abs/2602.06886v1">Prompt Reinjection: Alleviating Prompt Forgetting in Multimodal Diffusion Transformers</a></h2>
<p><strong>Authors:</strong> Yuxuan Yao, Yuxuan Chen, Hui Li, Kaihui Cheng, Qipeng Guo, Yuwei Sun, Zilong Dong, Jingdong Wang, Siyu Zhu</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Multimodal Diffusion Transformers (MMDiTs) for text-to-image generation maintain separate text and image branches, with bidirectional information flow between text tokens and visual latents throughout denoising. In this setting, we observe a prompt forgetting phenomenon: the semantics of the prompt representation in the text branch is progressively forgotten as depth increases. We further verify this effect on three representative MMDiTs--SD3, SD3.5, and FLUX.1 by probing linguistic attributes of the representations over the layers in the text branch. Motivated by these findings, we introduce a training-free approach, prompt reinjection, which reinjects prompt representations from early layers into later layers to alleviate this forgetting. Experiments on GenEval, DPG, and T2I-CompBench++ show consistent gains in instruction-following capability, along with improvements on metrics capturing preference, aesthetics, and overall text--image generation quality.</p>
<p><strong>Analysis:</strong></p>
<p>好的，我将以AI领域高水平研究生的视角，深入分析这篇论文的方法部分，重点关注其创新点、设计逻辑、优势与不足，并提供实用的实现指南。</p>
<hr />
<h2 id="prompt-reinjection-alleviating-prompt-forgetting-in-multimodal-diffusion-transformers_1">论文方法分析：Prompt Reinjection: Alleviating Prompt Forgetting in Multimodal Diffusion Transformers</h2>
<h3 id="1_1">1. 摘要翻译</h3>
<p><strong>中文摘要：</strong>
多模态扩散Transformer（MMDiTs）用于文本到图像生成，在整个去噪过程中维护独立的文本和图像分支，并通过双向信息流进行交互。在此设置下，我们观察到一个“提示遗忘”现象：文本分支中提示的语义表示会随着深度的增加而逐渐遗忘。我们通过探查文本分支中表示的语言属性，在三个代表性MMDiT模型（SD3, SD3.5, FLUX）上进一步验证了这一效应。基于这些发现，我们提出了一种无需训练的、推理时干预的方法——提示重注入（Prompt Reinjection），它将早期层的对齐浅层文本特征重新注入到后期层，以缓解遗忘。在GenEval、DPG和T2I-CompBench++上的实验表明，该方法在指令遵循能力上持续获得提升，并在捕捉偏好、美学和整体文本-图像生成质量的指标上有所改进。</p>
<h3 id="2_1">2. 方法动机分析</h3>
<ul>
<li><strong>驱动力</strong>：
    多模态扩散Transformer（MMDiTs）在文本到图像生成领域取得了显著进展，其核心在于文本和图像特征在去噪过程中进行深度融合。然而，作者发现这种深度融合并非总是带来好处，而是可能导致文本信息在深层网络中被遗忘。</li>
<li><strong>现有方法痛点</strong>：<ol>
<li><strong>提示遗忘（Prompt Forgetting）</strong>：MMDiTs的去噪目标仅限于图像潜在空间，文本特征的更新仅通过联合注意力间接实现。这种监督不对称性导致模型在优化去噪误差时，可能牺牲文本的精细语义信息，使得文本表示在深层网络中逐渐不可恢复。</li>
<li><strong>信息丢失</strong>：随着网络深度的增加，文本特征的局部语义结构被破坏，全局分布发生漂移，导致精细的文本提示信息丢失。</li>
</ol>
</li>
<li><strong>研究假设</strong>：
    作者假设，文本提示信息在MMDiTs的深层网络中会发生遗忘，导致生成图像与原始提示的匹配度下降。通过将早期层的高保真度文本特征重新注入到深层网络，可以缓解这种遗忘，从而提升文本到图像生成的一致性和指令遵循能力。</li>
</ul>
<h3 id="3_1">3. 方法设计详解</h3>
<p><strong>方法Pipeline总结：</strong></p>
<p>Prompt Reinjection 是一种<strong>训练无关（training-free）</strong>的<strong>推理时（inference-time）</strong>干预方法，旨在缓解MMDiTs中的提示遗忘现象。其核心思想是将来自浅层（早期层）的、保留了丰富语义信息的文本特征，通过一种精心设计的机制重新注入到深层（后期层）的文本表示中。</p>
<p>整个方法可以分为两个主要阶段：</p>
<p><strong>阶段一：语义保真度验证与残差注入可行性研究 (Pilot Study)</strong></p>
<p>在正式提出Prompt Reinjection之前，作者进行了初步的实验来验证两个关键前提：
1.  <strong>浅层文本特征的语义保真度</strong>：作者通过构造“最小对”（minimal pair）提示（例如，“A common cup” vs “A blue cup”），来验证浅层文本特征是否能保留可迁移的语义信息。
2.  <strong>残差注入的可行性</strong>：作者将一个提示（PA）的浅层特征与另一个相关提示（PB）的浅层特征的残差（scaled residual）注入到PA的深层模型中。</p>
<ul>
<li><strong>具体操作</strong>：<ul>
<li><strong>最小对提示构造</strong>：设计一对提示，其中一个提示（PB）仅修改了另一个提示（PA）的一个属性（如颜色、数量）。</li>
<li><strong>残差注入</strong>：在去噪过程中，将PB的浅层特征（通常是Layer 0的输出）与PA的浅层特征进行加权相减，得到残差。这个残差被按比例缩放（<code>w</code>，一个超参数）后，加到PA在某个深层块（<code>l &gt;= 2</code>）的文本特征上：
    <script type="math/tex; mode=display"> T^{(l)}_{PA} \leftarrow T^{(l)}_{PA} + w \cdot T^{(0)}_{PB} </script>
</li>
</ul>
</li>
<li><strong>结果</strong>：实验表明，这种残差注入确实能将生成图像的属性引导向PB的属性，证明了浅层特征携带的语义信息是可迁移的，并且残差注入是一种有效的机制。</li>
</ul>
<p><strong>阶段二：正式的Prompt Reinjection机制</strong></p>
<p>这一阶段是方法的正式实现，旨在解决跨层特征在<strong>分布（scale and shift）</strong>和<strong>几何（coordinate system）</strong>上的不匹配问题，以实现稳定有效的融合。它包含两个核心机制：</p>
<ol>
<li>
<p><strong>分布锚定与恢复 (Distribution Anchoring and Restoration)</strong></p>
<ul>
<li><strong>动机</strong>：不同层级的文本特征可能在均值、方差等统计量上存在差异，直接相加会导致分布漂移，影响生成稳定性。</li>
<li><strong>具体操作</strong>：<ul>
<li><strong>标准化</strong>：对源层（origin layer, <script type="math/tex">T_{ori}</script>）和目标层（target layer, <script type="math/tex">T_{tgt}</script>）的文本特征应用Layer Normalization（LN），以消除均值和方差的差异，将它们统一到标准正态分布的范围内。
    <script type="math/tex; mode=display"> T_{ori} = \text{LN}(T_{ori}), \quad T_{tgt} = \text{LN}(T_{tgt}) </script>
</li>
<li><strong>注入</strong>：将标准化后的源层特征（经过旋转对齐后）与目标层特征相加，形成增强特征 <script type="math/tex">T_{added}</script>。</li>
<li><strong>恢复</strong>：将增强特征 <script type="math/tex">T_{added}</script> 乘以目标层特征的标准差 <script type="math/tex">\sigma_{tgt}</script>，并加上目标层特征的均值 <script type="math/tex">\mu_{tgt}</script>，将增强特征恢复到目标层的原始统计分布范围内。
    <script type="math/tex; mode=display"> T_{final} = T_{added} \cdot \sigma_{tgt} + \mu_{tgt} </script>
    （论文中公式(9)是 <script type="math/tex">T_{final} = T_{added} \cdot \sigma_{tgt} + \mu_{tgt}</script>，但公式(8)是 <script type="math/tex">T_{ori} = \text{LN}(T_{ori}), T_{tgt} = \text{LN}(T_{tgt})</script>，公式(11)是 <script type="math/tex">T_{added} = T_{tgt} + w \cdot T_{ori} R</script>。综合来看，公式(9)是最终的注入操作，它将经过LN和旋转对齐的源层特征与目标层特征融合后，再通过目标层的统计量进行恢复。）</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>几何对齐 (Geometry Alignment via Orthogonal Procrustes)</strong></p>
<ul>
<li><strong>动机</strong>：即使统计量一致，不同层级的特征空间也可能存在旋转差异，即坐标系不一致。这会影响特征的几何关系，导致融合效果不佳。</li>
<li><strong>具体操作</strong>：<ul>
<li><strong>校准（Calibration）</strong>：在COCO-5K等数据集上，提取源层和目标层的文本特征，计算它们之间的最优正交旋转矩阵 <script type="math/tex">R</script>。这个过程通过求解一个正交Procrustes问题来实现，目标是最小化 <script type="math/tex">||X R - Y||_F^2</script>，其中 <script type="math/tex">X</script> 和 <script type="math/tex">Y</script> 是源层和目标层的特征矩阵。
    <script type="math/tex; mode=display"> \min_{R} ||XR - Y||_F^2 \quad \text{s.t.} \quad R^T R = I </script>
    通过SVD分解 <script type="math/tex">X^T Y = U \Sigma V^T</script>，得到最优旋转矩阵 <script type="math/tex">R = UV^T</script>。</li>
<li><strong>注入</strong>：在推理时，将源层特征 <script type="math/tex">T_{ori}</script> 乘以计算出的旋转矩阵 <script type="math/tex">R</script>，使其与目标层特征的几何空间对齐，然后再与目标层特征融合。
    <script type="math/tex; mode=display"> T_{added} = T_{tgt} + w \cdot T_{ori} R </script>
    其中 <script type="math/tex">w</script> 是控制注入强度的超参数。</li>
</ul>
</li>
</ul>
</li>
</ol>
<p><strong>模型结构与流程细节：</strong></p>
<ul>
<li><strong>输入</strong>：MMDiT模型的中间层文本特征 <script type="math/tex">T^{(l)}</script>。</li>
<li><strong>选择源层 (<script type="math/tex">l_{ori}</script>) 和目标层 (<script type="math/tex">L_{tgt}</script>)</strong>：<ul>
<li><strong>源层</strong>：选择一个浅层，通常是文本编码器输出（Layer 0）之后，并且在模型经历显著的分布转变之后。作者发现Layer 1或Layer 2通常是较好的选择，因为它们在PCA可视化中显示了分布的快速转变。</li>
<li><strong>目标层</strong>：通常选择源层之后的所有深层块，即 <script type="math/tex">L_{tgt} = \{l \mid l > l_{ori}\}</script>，以确保在整个剩余的去噪过程中持续注入信息。</li>
</ul>
</li>
<li><strong>注入过程</strong>：<ol>
<li>从选定的源层 <script type="math/tex">l_{ori}</script> 提取文本特征 <script type="math/tex">T_{ori}</script>。</li>
<li>对于每个目标层 <script type="math/tex">l \in L_{tgt}</script>：<ul>
<li>获取该层的文本特征 <script type="math/tex">T_{tgt}</script>。</li>
<li>对 <script type="math/tex">T_{ori}</script> 和 <script type="math/tex">T_{tgt}</script> 进行Layer Normalization。</li>
<li>将 <script type="math/tex">T_{ori}</script> 与预先计算好的旋转矩阵 <script type="math/tex">R</script> 相乘，得到对齐后的源层特征。</li>
<li>将对齐后的源层特征按权重 <script type="math/tex">w</script> 缩放后，与 <script type="math/tex">T_{tgt}</script> 相加，得到初步增强特征。</li>
<li>将初步增强特征通过目标层的统计量（均值和标准差）进行恢复，得到最终的注入特征 <script type="math/tex">T_{final}</script>。</li>
<li>将 <script type="math/tex">T_{final}</script> 替换或加到目标层的文本特征 <script type="math/tex">T_{tgt}</script> 上，作为该层新的文本表示。</li>
</ul>
</li>
</ol>
</li>
<li><strong>输出</strong>：经过Prompt Reinjection增强后的MMDiT模型，在推理时生成更符合提示的图像。</li>
</ul>
<h3 id="4_1">4. 方法对比分析</h3>
<ul>
<li><strong>本质区别</strong>：<ul>
<li><strong>与现有MMDiTs</strong>：现有MMDiTs将文本和图像特征融合在统一的Transformer中，但文本信息在深层会遗忘。Prompt Reinjection是<strong>推理时</strong>的<strong>外部干预</strong>，不修改模型结构或参数，直接注入信息。</li>
<li><strong>与其它推理时方法</strong>：许多推理时方法侧重于调整CFG scale、采样步数或使用Prompt Tuning等，而Prompt Reinjection直接针对MMDiTs的内部信息流问题。</li>
<li><strong>与训练时方法</strong>：Prompt Reinjection是训练无关的，避免了昂贵的再训练成本。</li>
</ul>
</li>
<li><strong>创新贡献</strong>：<ol>
<li><strong>发现并量化“提示遗忘”现象</strong>：通过CKNNA和层级探针实验，系统地证明了MMDiTs中文本信息随深度衰减的问题。</li>
<li><strong>提出Prompt Reinjection</strong>：一种新颖的、训练无关的推理时方法，通过跨层注入浅层文本特征来缓解遗忘。</li>
<li><strong>设计精巧的对齐机制</strong>：引入分布锚定与恢复、几何对齐（正交Procrustes）来解决跨层特征的不匹配问题，确保注入的有效性和稳定性。</li>
</ol>
</li>
<li><strong>适用场景</strong>：<ul>
<li><strong>核心场景</strong>：MMDiT架构的文本到图像生成模型，特别是那些在复杂指令或长提示下表现出指令遵循能力下降的模型。</li>
<li><strong>最佳应用</strong>：当需要模型精确遵循属性绑定、空间关系、数量等精细指令时，Prompt Reinjection能提供显著提升。</li>
<li><strong>局限性</strong>：对于已经非常擅长处理长提示的模型，提升空间可能有限。</li>
</ul>
</li>
</ul>
<h3 id="5_1">5. 实验分析</h3>
<ul>
<li><strong>验证方法</strong>：<ol>
<li><strong>量化提示遗忘</strong>：<ul>
<li><strong>CKNNA (Conditional K-Nearest Neighbor Alignment)</strong>：测量不同层级文本特征的局部语义结构保留程度。结果显示CKNNA随层数单调下降，表明局部语义结构被破坏。</li>
<li><strong>PCA可视化</strong>：观察文本特征在PCA空间中的分布变化。结果显示特征逐渐聚集，可分性降低。</li>
<li><strong>层级探针（Layer-wise Probing）</strong>：训练轻量级分类器，预测文本token的属性（如名词、形容词等），以量化信息可恢复性。结果显示探针准确率随层数单调下降。</li>
</ul>
</li>
<li><strong>验证Prompt Reinjection有效性</strong>：<ul>
<li><strong>指令遵循评估</strong>：在GenEval、DPG-Bench、T2I-CompBench++等基准上，与基线模型对比，评估Prompt Reinjection在各种指令遵循任务（如属性绑定、计数、空间关系等）上的提升。</li>
<li><strong>定性比较</strong>：通过生成图像与提示的对比，直观展示Prompt Reinjection在提升文本-图像一致性方面的效果。</li>
<li><strong>消融实验</strong>：分析源层选择、注入权重、对齐机制（分布锚定、几何对齐）等关键组件的影响。</li>
</ul>
</li>
</ol>
</li>
<li><strong>关键结果</strong>：<ul>
<li><strong>提示遗忘</strong>：在SD3、SD3.5、FLUX等模型上，文本特征的可恢复性随深度显著下降，特别是空间关系相关的属性下降最快。</li>
<li><strong>Prompt Reinjection效果</strong>：<ul>
<li>在GenEval上，SD3.5和FLUX的整体得分分别提升了6.48%和5.64%。</li>
<li>在DPG和T2I-CompBench++上，也取得了显著的提升，尤其在“位置”（Position）等对深度信息敏感的任务上。</li>
<li>定性结果（图4, 5, 6, 7）显示，Prompt Reinjection显著改善了图像与提示的匹配度，尤其是在处理复杂约束时。</li>
<li>消融实验表明，浅层源层、适中的注入权重以及分布锚定和几何对齐机制都对提升效果至关重要。</li>
</ul>
</li>
</ul>
</li>
<li><strong>优势场景</strong>：<ul>
<li><strong>模型</strong>：SD3, SD3.5, FLUX, Qwen-Image等MMDiT模型。</li>
<li><strong>任务</strong>：需要精确理解和执行复杂指令的任务，如属性绑定（颜色、形状、纹理）、计数、多对象组合、空间关系等。</li>
<li><strong>提示类型</strong>：长提示、包含多个约束条件的提示。</li>
</ul>
</li>
<li><strong>局限性</strong>：<ul>
<li><strong>计算开销</strong>：虽然是推理时方法，但注入过程会增加一定的计算开销（如表7所示，主要来自几何对齐）。</li>
<li><strong>超参数敏感性</strong>：源层选择 (<script type="math/tex">l_{ori}</script>)、注入权重 (<script type="math/tex">w</script>) 等超参数的选择对性能有影响，需要一定的调优。</li>
<li><strong>对齐的鲁棒性</strong>：虽然作者设计了对齐机制，但跨层特征的差异过大时，可能仍会影响效果。</li>
<li><strong>对模型本身的依赖</strong>：方法的有效性也依赖于MMDiT模型本身的基础能力。</li>
</ul>
</li>
</ul>
<h3 id="6_1">6. 实用指南</h3>
<ul>
<li><strong>开源情况</strong>：论文作者提供了代码（在GitHub上搜索“Prompt Reinjection”或作者名）。</li>
<li><strong>实现/复现的关键步骤</strong>：<ol>
<li><strong>选择MMDiT模型</strong>：选择一个支持MMDiT架构的文本到图像模型。</li>
<li><strong>确定源层和目标层</strong>：根据论文的建议（通常是浅层，如Layer 1或2，以及其后的所有层），或者通过消融实验找到最佳配置。</li>
<li><strong>计算几何对齐矩阵 R</strong>：使用COCO-5K等数据集，提取源层和目标层的文本特征，计算正交Procrustes矩阵 <script type="math/tex">R</script>。这一步可以在训练前完成，并保存下来。</li>
<li><strong>实现注入逻辑</strong>：在模型的去噪循环中，对于每个目标层，获取其文本特征，然后执行Prompt Reinjection的注入操作（包括LN、旋转、加权求和、统计量恢复）。</li>
<li><strong>调整注入权重 w</strong>：通过实验找到一个合适的注入权重 <script type="math/tex">w</script>（通常在0.01-0.1之间）。</li>
</ol>
</li>
<li><strong>实现细节</strong>：<ul>
<li><strong>文本特征提取</strong>：需要能够访问MMDiT模型内部的文本Transformer层的输出。</li>
<li><strong>Layer Normalization</strong>：确保使用与模型内部一致的Layer Normalization实现。</li>
<li><strong>统计量计算</strong>：在注入前，需要计算目标层特征的均值和标准差。</li>
<li><strong>Procrustes对齐</strong>：需要实现SVD分解来计算旋转矩阵。</li>
<li><strong>超参数选择</strong>：<script type="math/tex">l_{ori}</script>, <script type="math/tex">w</script>, 以及校准数据集的选择是关键。</li>
</ul>
</li>
<li><strong>迁移可能</strong>：<ul>
<li><strong>其他MMDiT模型</strong>：该方法的核心思想（注入浅层特征以缓解深层遗忘）具有普适性，可以迁移到其他MMDiT架构的模型上，只需调整源层、目标层和注入权重。</li>
<li><strong>其他多模态Transformer模型</strong>：如果其他多模态Transformer模型也存在类似的跨层信息衰减问题，Prompt Reinjection的思路（特别是对齐机制）可能具有借鉴意义，但需要根据具体模型结构进行调整。</li>
<li><strong>非MMDiT架构</strong>：对于非MMDiT架构（如基于U-Net的扩散模型），其信息流和遗忘机制可能不同，直接迁移可能效果不佳，但可以借鉴其“注入浅层信息”的思想。</li>
</ul>
</li>
</ul>
<h3 id="7_1">7. 总结</h3>
<ul>
<li><strong>核心思想</strong>：通过推理时注入浅层文本特征，缓解MMDiT深层信息遗忘。</li>
<li><strong>速记版pipeline</strong>：<ol>
<li><strong>找源头</strong>：确定模型中保留最多提示信息的浅层。</li>
<li><strong>对齐空间</strong>：用数学方法让浅层和深层特征的“语言”一致。</li>
<li><strong>注入信息</strong>：将对齐后的浅层信息按比例加到深层特征里。</li>
<li><strong>恢复分布</strong>：确保注入后的特征符合深层模型的“习惯”。</li>
<li><strong>生成图像</strong>：用增强后的特征生成更准确的图像。</li>
</ol>
</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Motivated by these findings, we introduce a training-free approach, prompt reinjection, which reinjects prompt representations from early layers into later layers to alleviate this forgetting.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2602.06886v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2602.06886v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2602.06883v1'></a></p>
<h2 id="vision-transformer-finetuning-benefits-from-non-smooth-components"><a href="https://arxiv.org/abs/2602.06883v1">Vision Transformer Finetuning Benefits from Non-Smooth Components</a></h2>
<p><strong>Authors:</strong> Ambroise Odonnat, Laetitia Chapel, Romain Tavenard, Ievgen Redko</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Categories:</strong> cs.LG, cs.CV, stat.ML</p>
<p><strong>Abstract:</strong></p>
<p>The smoothness of the transformer architecture has been extensively studied in the context of generalization, training stability, and adversarial robustness. However, its role in transfer learning remains poorly understood. In this paper, we analyze the ability of vision transformer components to adapt their outputs to changes in inputs, or, in other words, their plasticity. Defined as an average rate of change, it captures the sensitivity to input perturbation; in particular, a high plasticity implies low smoothness. We demonstrate through theoretical analysis and comprehensive experiments that this perspective provides principled guidance in choosing the components to prioritize during adaptation. A key takeaway for practitioners is that the high plasticity of the attention modules and feedforward layers consistently leads to better finetuning performance. Our findings depart from the prevailing assumption that smoothness is desirable, offering a novel perspective on the functional properties of transformers. The code is available at https://github.com/ambroiseodt/vit-plasticity.</p>
<p><strong>Analysis:</strong></p>
<p>好的，我将以AI领域高水平研究生的视角，深入分析这篇论文的方法部分，重点关注其创新点和新视角，并提供结构化的分析。</p>
<hr />
<h2 id="vision-transformer-finetuning-benefits-from-non-smooth-components_1">论文方法分析：Vision Transformer Finetuning Benefits from Non-Smooth Components</h2>
<h3 id="1_2">1. 摘要翻译</h3>
<p><strong>中文翻译：</strong></p>
<p><strong>视觉 Transformer 微调受益于非平滑组件</strong></p>
<p>Transformer 架构的平滑性在泛化、训练稳定性和对抗鲁棒性方面得到了广泛研究。然而，其在迁移学习中的作用仍未得到充分理解。本文中，我们分析了视觉 Transformer 组件适应输入变化的能力，即其“可塑性”。可塑性被定义为平均变化率，它捕捉了对输入扰动的敏感度；特别地，高可塑性意味着低平滑性。我们通过理论分析和全面的实验证明，这一视角为选择在微调期间优先考虑的组件提供了原则性指导。对于实践者来说，一个关键的收获是，高可塑性的注意力模块和前馈层持续带来更好的微调性能。我们的发现与普遍认为平滑性是可取的假设相悖，为 Transformer 的功能特性提供了新的视角。</p>
<h3 id="2_2">2. 方法动机分析</h3>
<p><strong>驱动力：</strong>
当前，Transformer 模型在各种领域（NLP、CV、时间序列预测等）已成为主流。这些模型通常在海量数据上进行预训练，然后针对特定下游任务进行微调。然而，预训练与下游数据之间的分布差异可能导致性能下降，并需要更新模型权重来适应这种分布变化。现有研究主要关注模型的平滑性（如 Lipschitz 连续性）以提升泛化、稳定性和鲁棒性。但作者认为，这种对平滑性的过度追求可能会限制模型适应下游数据分布的能力，即限制了模型的“可塑性”。因此，作者希望探究“可塑性”在 Transformer 微调中的作用，并找出哪些组件的可塑性对微调性能至关重要。</p>
<p><strong>现有方法痛点：</strong>
1.  <strong>对平滑性的过度强调：</strong> 现有研究普遍认为平滑性（低 Lipschitz 常数）对模型的泛化、稳定性和鲁棒性至关重要。然而，这种平滑性可能限制了模型在微调阶段适应新数据分布的能力。
2.  <strong>缺乏对 Transformer 组件可塑性的深入理解：</strong> Transformer 模型由多个不同类型的组件（如 LayerNorm、Multi-Head Attention、FeedForward Layers）构成。目前对这些组件各自的可塑性及其对微调性能影响的理论和实证理解不足。</p>
<p><strong>研究假设：</strong>
作者的核心假设是：<strong>高可塑性（低平滑性）的 Transformer 组件在微调过程中能够更好地适应下游数据分布，从而带来更好的性能和更稳定的训练。</strong></p>
<h3 id="3_2">3. 方法设计详解</h3>
<p><strong>流程总结：</strong></p>
<p>该研究的核心在于定义和量化 Transformer 各组件的“可塑性”，并将其与微调性能关联起来。</p>
<ol>
<li>
<p><strong>定义可塑性 (Plasticity)：</strong></p>
<ul>
<li><strong>直观理解：</strong> 可塑性衡量一个函数（组件）对输入变化的敏感度，即输入微小变化能引起多大的输出变化。高可塑性意味着低平滑性。</li>
<li><strong>形式化定义 (Definition 1)：</strong> 对于一个组件函数 <script type="math/tex">f</script>，其可塑性 <script type="math/tex">P(f)</script> 被定义为在所有可能的输入对 <script type="math/tex">(x, y)</script> 上，输出差值 <script type="math/tex">||f(x) - f(y)||_F</script> 与输入差值 <script type="math/tex">||x - y||_F</script> 的比值的期望值。
    <script type="math/tex; mode=display">P(f) = E_{(x,y) \sim \nu} \left[ \frac{||f(x) - f(y)||_F}{||x - y||_F} \right]</script>
    其中 <script type="math/tex">\nu</script> 是在所有可能的 token 序列对上的均匀分布。</li>
<li><strong>可塑性与平滑性的关系：</strong> <script type="math/tex">P(f)</script> 提供了对 Lipschitz 常数 <script type="math/tex">Lip(f)</script> 的一个下界。如果 <script type="math/tex">P(f) < 1</script>，则组件倾向于收缩输入差异（平滑）；如果 <script type="math/tex">P(f) > 1</script>，则组件倾向于放大输入差异（非平滑，高可塑性）。</li>
</ul>
</li>
<li>
<p><strong>理论分析与上界推导：</strong></p>
<ul>
<li>作者对 Transformer 中的主要组件（LayerNorm, FeedForward Layer, Multi-Head Self-Attention）进行了理论分析，推导了它们可塑性的上界。</li>
<li><strong>LayerNorm (Proposition 1)：</strong> <script type="math/tex">P(f) \le \frac{1}{\sigma} ||\gamma||_\infty</script>，其中 <script type="math/tex">\sigma</script> 是 token 标准差的最小值，<script type="math/tex">\gamma</script> 是 LayerNorm 的权重。</li>
<li><strong>FeedForward Layer (Proposition 2)：</strong> <script type="math/tex">P(f) \le ||W||_2</script>，其中 <script type="math/tex">W</script> 是线性层的权重矩阵。</li>
<li><strong>Multi-Head Self-Attention (Proposition 3 &amp; 4)：</strong> 推导了更复杂的上界，依赖于头数 <script type="math/tex">H</script>、注意力权重矩阵 <script type="math/tex">A_h</script>、嵌入层权重、序列长度 <script type="math/tex">n</script> 和输入能量 <script type="math/tex">E</script>。这些上界表明，自注意力机制的可塑性上界远高于其他组件。</li>
<li><strong>理论排名：</strong> 基于这些上界，作者推断出理论上的可塑性排名：MHA &gt; FC1 ≈ FC2 &gt; LN2 ≈ LN1。</li>
</ul>
</li>
<li>
<p><strong>实验验证与分析：</strong></p>
<ul>
<li><strong>可塑性计算 (Section 5.1)：</strong><ul>
<li>在预训练的 ViT 模型上，通过采样大量图像及其对应的下游数据，计算了各组件的实际可塑性值。</li>
<li><strong>实证排名：</strong> 实验结果与理论预测高度一致，确认了 MHA 具有最高的可塑性，其次是 FC1 和 FC2，最后是 LN2 和 LN1。</li>
<li><strong>可塑性随深度的变化：</strong> 分析了可塑性在不同 Transformer 层中的演变，发现 MHA 和 FFN 始终保持高可塑性，而 LayerNorms 则始终保持低可塑性（&lt; 1）。</li>
</ul>
</li>
<li><strong>微调性能分析 (Section 5.2)：</strong><ul>
<li><strong>组件隔离微调：</strong> 作者将 ViT 模型中的每个组件（MHA, FC1, FC2, LN2, LN1）单独进行微调，并与其他组件冻结。</li>
<li><strong>性能对比：</strong> 实验结果表明，高可塑性的组件（MHA, FC1, FC2）在绝大多数数据集上带来了显著更好的微调性能，并且性能更稳定。</li>
<li><strong>鲁棒性分析：</strong> 高可塑性组件在不同学习率和随机种子下表现出更小的性能波动，即更鲁棒。</li>
<li><strong>梯度分析：</strong> 进一步通过观察训练过程中的梯度范数和验证损失，发现高可塑性组件确实能产生更大的梯度范数，从而导致更快的收敛和更好的泛化。</li>
</ul>
</li>
</ul>
</li>
</ol>
<p><strong>模型结构与算法解释：</strong></p>
<ul>
<li><strong>核心概念：可塑性 (Plasticity)</strong><ul>
<li>作者将“可塑性”定义为组件输出对输入变化的平均敏感度。这与传统的“平滑性”（Lipschitz 常数）概念形成对比。平滑性关注的是最坏情况下的变化率（上界），而可塑性关注的是平均情况下的变化率。</li>
<li><strong>公式意义：</strong> <script type="math/tex">\frac{||f(x) - f(y)||_F}{||x - y||_F}</script> 代表了输入差值 <script type="math/tex">||x - y||_F</script> 被组件 <script type="math/tex">f</script> 放大或缩小的倍数。取期望值是为了获得组件在整个数据分布上的平均行为。</li>
</ul>
</li>
<li><strong>组件分析：</strong><ul>
<li><strong>LayerNorm：</strong> 通过对 token 进行均值和方差归一化，其输出对输入的绝对值变化不敏感，因此可塑性较低。</li>
<li><strong>FeedForward Layer：</strong> 线性变换，其可塑性主要由权重矩阵的谱范数决定。</li>
<li><strong>Multi-Head Self-Attention：</strong> 涉及 softmax 操作和多头机制，其输出对输入的依赖性更复杂，理论分析表明其可塑性上界远高于其他组件。作者通过推导其 Lipschitz 常数上界来间接证明其高可塑性。</li>
</ul>
</li>
</ul>
<h3 id="4_2">4. 方法对比分析</h3>
<p><strong>本质区别：</strong>
*   <strong>视角转变：</strong> 传统研究侧重于模型的“平滑性”（低 Lipschitz 常数）以保证泛化、稳定性和鲁棒性。本文则引入了“可塑性”（高平均变化率）的概念，并认为其对微调适应性至关重要。这是一种从“避免变化”到“拥抱变化”的视角转变。
*   <strong>关注点：</strong> 传统方法关注的是“最坏情况”下的性能（如对抗鲁棒性），而本文关注的是“平均情况”下的适应能力，尤其是在微调阶段。</p>
<p><strong>创新贡献：</strong>
1.  <strong>引入“可塑性”概念：</strong> 首次将“可塑性”作为衡量 Transformer 组件适应输入变化能力的关键指标，并提供了形式化定义。
2.  <strong>理论排名：</strong> 首次对 Transformer 各组件的可塑性进行了理论分析和排名，为理解其内在特性提供了理论基础。
3.  <strong>实证验证：</strong> 通过大规模实验，证明了高可塑性组件（MHA, FFN）在微调任务中确实能带来更好的性能和更稳定的训练，与传统平滑性假设形成鲜明对比。
4.  <strong>指导实践：</strong> 为微调策略提供了新的指导：优先微调高可塑性的组件（MHA, FFN）。</p>
<p><strong>适用场景：</strong>
该方法主要适用于 <strong>Transformer 模型在下游任务上的微调（finetuning）场景</strong>。其核心思想是识别并优先调整那些对数据分布变化最敏感、适应能力最强的组件。这对于需要快速适应新数据或解决分布偏移问题的场景尤为重要。</p>
<h3 id="5_2">5. 实验分析</h3>
<p><strong>验证方法：</strong>
1.  <strong>理论排名验证：</strong> 通过计算各组件可塑性的理论上界，并与实验测量的实际可塑性进行对比，验证理论分析的准确性。
2.  <strong>组件隔离微调：</strong> 将 Transformer 模型中的每个组件（MHA, FC1, FC2, LN2, LN1）单独进行微调，并与其他组件冻结。这是为了清晰地隔离每个组件对整体性能的影响。
3.  <strong>多数据集评估：</strong> 在 11 个不同的图像分类数据集上进行了广泛的微调实验，以评估不同组件在各种场景下的表现。
4.  <strong>学习率和种子鲁棒性测试：</strong> 通过在不同学习率和随机种子下进行实验，评估了不同组件微调的稳定性和鲁棒性。
5.  <strong>梯度和损失动态分析：</strong> 观察了训练过程中梯度范数和验证损失的变化，以理解高可塑性组件如何影响优化过程。</p>
<p><strong>关键结果：</strong>
*   <strong>可塑性排名一致：</strong> 理论排名（MHA &gt; FC1 ≈ FC2 &gt; LN2 ≈ LN1）与实验测量的实际可塑性排名高度一致。
*   <strong>高可塑性带来更好性能：</strong> MHA 和 FFN（FC1, FC2）由于具有高可塑性，在绝大多数数据集上取得了比 LayerNorms（LN1, LN2）更高的 Top-1 准确率。
*   <strong>高可塑性带来更稳定性能：</strong> 高可塑性组件在不同学习率和种子下表现出更小的性能波动，即更鲁棒。
*   <strong>优化过程的解释：</strong> 高可塑性组件产生了更大的梯度范数，导致了更快的收敛速度和更陡峭的损失下降，这解释了其在微调中的优势。</p>
<p><strong>优势场景：</strong>
*   <strong>挑战性数据集：</strong> 在 Cifar100、Clipart 和 Sketch 等更具挑战性的数据集上，高可塑性组件（特别是 MHA）的性能优势更为显著。
*   <strong>需要快速适应的场景：</strong> 在需要模型快速适应新数据分布的微调场景中，高可塑性组件的优势尤为明显。</p>
<p><strong>局限性：</strong>
*   <strong>理论上界与实际值差距：</strong> 虽然理论分析提供了排名，但上界值与实际测量的可塑性值之间可能存在较大差距，尤其是在自注意力机制上。
*   <strong>计算成本：</strong> 计算可塑性需要对大量数据进行前向传播，可能带来一定的计算开销。
*   <strong>组件隔离的假设：</strong> 实验中将组件隔离微调，这是一种简化假设。在实际应用中，组件之间可能存在更复杂的相互作用。
*   <strong>仅限于 ViT：</strong> 研究主要集中在 Vision Transformer 上，其结论是否能直接推广到其他类型的 Transformer（如 LLM）需要进一步验证。</p>
<h3 id="6_2">6. 实用指南</h3>
<p><strong>开源情况：</strong>
论文中提到了其代码实现，但未明确给出具体的开源链接。通常，这类研究会公开代码以供复现。读者可以关注作者的 GitHub 页面或论文的补充材料。</p>
<p><strong>实现细节：</strong>
*   <strong>可塑性计算：</strong> 需要对预训练模型进行大量采样，并计算每个组件在这些样本上的输出差值与输入差值的比值，然后取期望。这需要高效的 GPU 计算和内存管理。
*   <strong>微调策略：</strong> 实验中采用了“组件隔离微调”的策略，即每次只微调一个组件，冻结其他所有组件。这是一种探索性方法，用于理解单个组件的影响。在实际应用中，可以考虑更复杂的策略，如同时微调多个高可塑性组件。
*   <strong>超参数：</strong> 实验中使用了 SGD 优化器，并进行了学习率的网格搜索。具体的学习率、批大小、训练步数等需要根据具体任务和数据集进行调整。
*   <strong>数据预处理：</strong> 遵循了 Dosovitskiy et al. (2021) 的标准 ViT 预处理流程。</p>
<p><strong>迁移可能：</strong>
*   <strong>其他 Transformer 模型：</strong> 该方法的核心思想（可塑性分析）和排名结果很可能可以迁移到其他 Transformer 模型，包括大型语言模型（LLM）。LLM 的解码器结构与编码器有相似之处，特别是自注意力机制。
*   <strong>其他任务：</strong> 该方法的核心在于理解模型组件对输入变化的敏感度，这对于任何需要适应性学习的任务都可能具有参考价值。例如，在领域自适应、持续学习等场景下，识别和利用高可塑性组件可能是有益的。
*   <strong>与 PEFT 方法结合：</strong> 作者在讨论部分提到了将 LoRA 等参数高效微调方法应用于高可塑性组件的可能性，这是一个非常有前景的研究方向。</p>
<h3 id="7_2">7. 总结</h3>
<p><strong>核心思想：</strong>
<strong>高可塑性组件（MHA, FFN）微调性能更优。</strong></p>
<p><strong>速记版 pipeline：</strong>
1.  <strong>定义可塑性：</strong> 量化组件对输入变化的平均敏感度。
2.  <strong>理论分析：</strong> 推导各组件可塑性上界，预测排名。
3.  <strong>实验计算：</strong> 实际测量各组件可塑性，验证排名。
4.  <strong>隔离微调：</strong> 单独微调各组件，评估性能。
5.  <strong>得出结论：</strong> 高可塑性组件（MHA, FFN）微调效果更好、更稳定。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We demonstrate through theoretical analysis and comprehensive experiments that this perspective provides principled guidance in choosing the components to prioritize during adaptation.</li>
<li>Our findings depart from the prevailing assumption that smoothness is desirable, offering a novel perspective on the functional properties of transformers.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2602.06883v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2602.06883v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2602.06879v1'></a></p>
<h2 id="nanoflux-distillation-driven-compression-of-large-text-to-image-generation-models-for-mobile-devices"><a href="https://arxiv.org/abs/2602.06879v1">NanoFLUX: Distillation-Driven Compression of Large Text-to-Image Generation Models for Mobile Devices</a></h2>
<p><strong>Authors:</strong> Ruchika Chavhan, Malcolm Chadwick, Alberto Gil Couto Pimentel Ramos, Luca Morreale, Mehdi Noroozi, Abhinav Mehrotra</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>While large-scale text-to-image diffusion models continue to improve in visual quality, their increasing scale has widened the gap between state-of-the-art models and on-device solutions. To address this gap, we introduce NanoFLUX, a 2.4B text-to-image flow-matching model distilled from 17B FLUX.1-Schnell using a progressive compression pipeline designed to preserve generation quality. Our contributions include: (1) A model compression strategy driven by pruning redundant components in the diffusion transformer, reducing its size from 12B to 2B; (2) A ResNet-based token downsampling mechanism that reduces latency by allowing intermediate blocks to operate on lower-resolution tokens while preserving high-resolution processing elsewhere; (3) A novel text encoder distillation approach that leverages visual signals from early layers of the denoiser during sampling. Empirically, NanoFLUX generates 512 x 512 images in approximately 2.5 seconds on mobile devices, demonstrating the feasibility of high-quality on-device text-to-image generation.</p>
<p><strong>Analysis:</strong></p>
<p>作为一名计算机视觉和机器学习领域的专家，我对这篇论文摘要进行了深入分析，并为您提供以下中文解读：</p>
<p><strong>1. 论文的主要贡献（2-3句话）</strong></p>
<p>该论文提出了NanoFLUX，一个专门为移动设备设计的、经过蒸馏压缩的24亿参数文本到图像生成模型。NanoFLUX通过渐进式压缩策略，成功地将一个170亿参数的FLUX.1-Schnell模型压缩至20亿参数，同时保持了高质量的图像生成能力。这项工作解决了大型文本到图像模型在移动设备上部署的挑战，实现了高效且高质量的端侧生成。</p>
<p><strong>2. 关键创新点或方法论</strong></p>
<p>NanoFLUX的核心创新在于其<strong>蒸馏驱动的渐进式压缩流水线</strong>，具体体现在以下三个方面：</p>
<ul>
<li><strong>基于剪枝的模型压缩策略：</strong> 通过识别并移除扩散Transformer中的冗余组件，将模型规模从120亿参数大幅削减至20亿参数，这是实现模型轻量化的关键。</li>
<li><strong>ResNet驱动的Token降采样机制：</strong> 引入了一种新颖的机制，允许中间层在低分辨率Token上进行计算，从而显著降低延迟。同时，通过在其他部分保留高分辨率处理，确保了生成图像的细节质量。</li>
<li><strong>基于视觉信号的文本编码器蒸馏：</strong> 提出了一种创新的文本编码器蒸馏方法，利用去噪器早期层的视觉信号来指导文本编码器的学习。这种方法能够更有效地将文本信息转化为视觉特征，提升生成效果。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<p>这项研究对文本到图像生成领域具有重要的潜在影响：</p>
<ul>
<li><strong>推动端侧AI的发展：</strong> NanoFLUX的成功将极大地推动高质量文本到图像生成模型在移动设备上的普及和应用。这意味着用户无需依赖云端服务器，即可在手机、平板等设备上直接进行创意生成，极大地提升了用户体验和便利性。</li>
<li><strong>降低AI模型的部署门槛：</strong> 通过有效的模型压缩技术，使得原本庞大且计算密集型的模型能够适应资源受限的移动平台，为更多开发者和企业提供了部署先进AI模型的可能性。</li>
<li><strong>促进AI应用的创新：</strong> 随着端侧生成能力的提升，将催生出更多创新的移动端AI应用，例如实时图像编辑、个性化内容创作、辅助设计等。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<p>这项研究的成果可以广泛应用于以下领域：</p>
<ul>
<li><strong>移动端创意工具：</strong> 如移动端的AI绘画App、设计辅助工具、社交媒体内容生成器等。</li>
<li><strong>增强现实（AR）和虚拟现实（VR）：</strong> 在AR/VR环境中实时生成或修改虚拟对象，提升沉浸感。</li>
<li><strong>个性化内容推荐和生成：</strong> 根据用户偏好实时生成个性化的图像内容。</li>
<li><strong>教育和学习：</strong> 辅助学生理解概念，生成可视化材料。</li>
<li><strong>游戏开发：</strong> 在移动端游戏引擎中实现快速的资产生成。</li>
<li><strong>辅助设计和原型制作：</strong> 在移动设备上快速生成设计草图和概念图。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<p>尽管摘要展示了显著的成果，但仍可推断出一些潜在的局限性：</p>
<ul>
<li><strong>生成质量的权衡：</strong> 虽然论文声称“保留生成质量”，但任何压缩过程都可能在一定程度上牺牲模型的某些性能。摘要中未详细说明与原始17B模型相比，NanoFLUX在生成质量上的具体差异（例如，在多样性、细节丰富度或特定风格的生成能力上）。</li>
<li><strong>特定移动设备的性能：</strong> 摘要提到“在移动设备上”，但并未具体说明测试的移动设备型号、硬件配置（CPU、GPU、NPU等）以及操作系统。不同移动设备的性能差异可能很大，NanoFLUX在不同设备上的实际表现可能有所不同。</li>
<li><strong>训练和蒸馏的复杂性：</strong> 尽管模型压缩是其贡献，但实现这种有效的蒸馏和压缩过程本身可能需要大量的计算资源和精细的调优，这可能增加了研究和开发的门槛。</li>
<li><strong>模型的可解释性：</strong> 压缩后的模型，特别是经过剪枝和降采样后，其内部机制的可解释性可能会降低。</li>
<li><strong>对特定任务的优化：</strong> NanoFLUX是通用的文本到图像模型，但其在特定、高度专业化的图像生成任务上的表现可能不如专门为此优化的模型。</li>
</ul>
<p>总而言之，NanoFLUX是一项非常有前景的研究，它通过创新的压缩技术，成功地将大型文本到图像生成模型带入了移动端，为AI在边缘设备的广泛应用打开了新的可能性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>While large-scale text-to-image diffusion models continue to improve in visual quality, their increasing scale has widened the gap between state-of-the-art models and on-device solutions.</li>
<li>To address this gap, we introduce NanoFLUX, a 2.4B text-to-image flow-matching model distilled from 17B FLUX.1-Schnell using a progressive compression pipeline designed to preserve generation quality.</li>
<li>Our contributions include: (1) A model compression strategy driven by pruning redundant components in the diffusion transformer, reducing its size from 12B to 2B; (2) A ResNet-based token downsampling mechanism that reduces latency by allowing intermediate blocks to operate on lower-resolution tokens while preserving high-resolution processing elsewhere; (3) A novel text encoder distillation approach that leverages visual signals from early layers of the denoiser during sampling.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2602.06879v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2602.06879v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2602.06871v1'></a></p>
<h2 id="rfdm-residual-flow-diffusion-model-for-efficient-causal-video-editing"><a href="https://arxiv.org/abs/2602.06871v1">RFDM: Residual Flow Diffusion Model for Efficient Causal Video Editing</a></h2>
<p><strong>Authors:</strong> Mohammadreza Salehi, Mehdi Noroozi, Luca Morreale, Ruchika Chavhan, Malcolm Chadwick, Alberto Gil Ramos, Abhinav Mehrotra</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Instructional video editing applies edits to an input video using only text prompts, enabling intuitive natural-language control. Despite rapid progress, most methods still require fixed-length inputs and substantial compute. Meanwhile, autoregressive video generation enables efficient variable-length synthesis, yet remains under-explored for video editing. We introduce a causal, efficient video editing model that edits variable-length videos frame by frame. For efficiency, we start from a 2D image-to-image (I2I) diffusion model and adapt it to video-to-video (V2V) editing by conditioning the edit at time step t on the model's prediction at t-1. To leverage videos' temporal redundancy, we propose a new I2I diffusion forward process formulation that encourages the model to predict the residual between the target output and the previous prediction. We call this Residual Flow Diffusion Model (RFDM), which focuses the denoising process on changes between consecutive frames. Moreover, we propose a new benchmark that better ranks state-of-the-art methods for editing tasks. Trained on paired video data for global/local style transfer and object removal, RFDM surpasses I2I-based methods and competes with fully spatiotemporal (3D) V2V models, while matching the compute of image models and scaling independently of input video length. More content can be found in: https://smsd75.github.io/RFDM_page/</p>
<p><strong>Analysis:</strong></p>
<p>好的，我将以AI领域高水平研究生的视角，深入分析这篇论文的方法部分，重点关注其创新点、设计逻辑、优势与不足，并提供实用的分析和指导。</p>
<hr />
<h2 id="rfdm-residual-flow-diffusion-model-for-efficient-causal-video-editing_1">论文方法分析：RFDM (Residual Flow Diffusion Model) for Efficient Causal Video Editing</h2>
<h3 id="1_3">1. 摘要翻译</h3>
<p><strong>论文标题：</strong> RFDM: 残差流扩散模型用于高效因果视频编辑</p>
<p><strong>摘要：</strong> 指导式视频编辑通过仅使用文本提示来编辑输入视频，从而实现直观的自然语言控制。尽管取得了快速进展，但大多数方法仍然需要固定长度的输入和大量的计算资源。与此同时，自回归视频生成因其能够高效地合成可变长度的输入而受到关注，但其在视频编辑领域的应用仍未得到充分探索。我们提出了一种因果、高效的视频编辑模型，该模型逐帧编辑可变长度的视频。为了提高效率，我们从一个二维图像到图像（I2I）扩散模型开始，并通过将时间步长 t 上的编辑与模型在 t-1 上的预测进行条件化，将其适配到视频到视频（V2V）编辑。为了利用视频的时间冗余，我们提出了一种新的 I2I 扩散前向过程，该过程鼓励模型预测目标输出与先前预测之间的残差。我们将此称为残差流扩散模型（RFDM），它将去噪过程集中在连续帧之间的变化上。此外，我们提出了一个新的基准，该基准能够更好地对编辑任务的最新方法进行排名。RFDM 在配对视频数据上进行训练，用于全局/局部风格迁移和对象移除，其性能超越了基于 I2I 的方法，并与全时空（3D）V2V 模型相媲美，同时计算成本与图像模型相当，并且计算成本与输入视频长度无关。更多内容可在 RFDM 页面找到。</p>
<h3 id="2_3">2. 方法动机分析</h3>
<ul>
<li>
<p><strong>驱动力</strong>：</p>
<ul>
<li><strong>提升视频编辑的效率和可访问性</strong>：现有视频编辑方法通常需要固定长度的输入和大量的计算资源，这限制了其在流媒体、资源受限设备（如手机）上的应用。</li>
<li><strong>解决视频编辑中的时间一致性问题</strong>：独立地对视频的每一帧进行编辑（如使用 I2I 模型）会导致帧间不一致，产生抖动和不连贯的视觉效果。</li>
<li><strong>利用视频的固有特性</strong>：视频帧之间存在大量时间冗余，现有方法未能充分利用这一点来提高编辑效率和一致性。</li>
<li><strong>实现因果（Causal）编辑</strong>：对于流式视频处理或实时应用，逐帧的因果编辑是必要的，而许多现有方法是非因果的。</li>
</ul>
</li>
<li>
<p><strong>现有方法痛点</strong>：</p>
<ul>
<li><strong>计算成本高昂</strong>：全时空（3D）模型需要大量的计算资源，并且通常对输入视频长度敏感。</li>
<li><strong>固定长度输入限制</strong>：许多方法需要固定长度的视频输入，不适用于可变长度的视频流。</li>
<li><strong>时间不一致性</strong>：基于图像的 I2I 模型直接应用于视频帧时，容易产生不连贯的编辑结果。</li>
<li><strong>未充分利用时间冗余</strong>：现有方法未能有效利用视频帧之间的相似性来加速或优化编辑过程。</li>
<li><strong>缺乏因果性</strong>：许多方法在推理时需要访问整个视频或未来的帧，不适合流式处理。</li>
</ul>
</li>
<li>
<p><strong>研究假设</strong>：</p>
<ul>
<li>通过将一个成熟的 2D I2I 扩散模型适配到视频编辑任务，可以在不显著增加计算成本的情况下实现高效的视频编辑。</li>
<li>通过引入因果性的逐帧编辑机制，并利用前一帧的预测结果作为条件，可以有效解决时间不一致性问题。</li>
<li>通过修改扩散过程，使其预测帧间的“残差”而非完整的帧，可以更有效地利用视频的时间冗余，并专注于帧间的变化。</li>
<li>一个精心设计的基准和评估指标对于准确衡量视频编辑方法的性能至关重要。</li>
</ul>
</li>
</ul>
<h3 id="3_3">3. 方法设计详解</h3>
<p><strong>流程总结：</strong></p>
<p>RFDM 的核心思想是将一个预训练的 2D 图像到图像（I2I）扩散模型（如 Stable Diffusion 的 UNet）改造成一个能够逐帧、因果地编辑视频的模型。其流程可以概括为：</p>
<ol>
<li>
<p><strong>基础模型适配（I2I to V2V）</strong>：</p>
<ul>
<li><strong>因果性引入</strong>：为了实现逐帧编辑，模型在处理时间步 <code>t</code> 的帧时，会<strong>条件化（condition）其在时间步 <code>t-1</code> 的预测结果 <code>ŷt-1</code></strong>。这意味着当前帧的编辑不仅依赖于原始输入帧 <code>xt</code> 和文本指令 <code>p</code>，还依赖于模型刚刚生成的上一帧 <code>ŷt-1</code>。这增加了因果性，并且<strong>不增加额外的计算开销</strong>，因为 <code>ŷt-1</code> 已经是模型的一部分输出。</li>
<li><strong>公式体现</strong>：在去噪（denoising）阶段，模型 <code>ŷθ</code> 的输入会包含 <code>ŷt-1</code>。</li>
</ul>
</li>
<li>
<p><strong>残差流扩散（Residual Flow Diffusion）</strong>：</p>
<ul>
<li><strong>动机</strong>：直接预测完整的帧 <code>yt</code> 可能会受到前一帧 <code>ŷt-1</code> 的影响，并且可能无法高效地捕捉帧间的细微变化。作者受到图像超分辨率中“残差学习”的启发，提出让模型预测<strong>目标帧 <code>yt</code> 与前一帧预测 <code>ŷt-1</code> 之间的残差</strong>，而不是直接预测 <code>yt</code>。</li>
<li><strong>前向过程修改</strong>：<ul>
<li>标准的扩散前向过程（如 Eq. 1）是将原始图像 <code>y0</code> 逐渐添加噪声，得到 <code>ys</code>。</li>
<li>RFDM 的关键创新在于修改了前向过程，使其<strong>将目标帧 <code>yt</code> 的噪声版本 <code>ys</code> 的生成，与前一帧的预测 <code>ŷt-1</code> 联系起来</strong>。具体来说，它引入了一个新的前向过程 <code>q(ys | y⁰, ŷt-1)</code>，其形式为：
    <code>q(y^s | y⁰, ŷt-1) = N(α^s y⁰ + γ^s ŷt-1, (σ^s)²I)</code>
    其中 <code>α^s</code> 和 <code>σ^s</code> 是标准的噪声调度参数，而 <code>γ^s</code> 是一个与 <code>α^s</code> 相关的项（<code>γ^s = sqrt(1 - (α^s)² - (σ^s)²)</code>，在论文中简化为 <code>γ = sqrt(1 - (σ^s)²)</code>，当 <code>α^s</code> 接近 1 时）。</li>
<li><strong>核心思想</strong>：这个公式可以被理解为，在添加噪声时，我们不是直接从 <code>y⁰</code> 开始，而是从一个<strong>结合了原始信息 <code>y⁰</code> 和上一帧预测 <code>ŷt-1</code> 的混合信息</strong>开始，然后添加噪声。更直观地，它将生成 <code>yt</code> 的任务重构为生成一个<strong>“残差流”</strong>，这个残差流 <code>m^t = ŷt-1 - yt</code> 使得模型能够专注于学习如何从 <code>ŷt-1</code> 变化到 <code>yt</code>。</li>
<li><strong>等价解释</strong>：论文还给出了一个等价的解释，即通过修改噪声项 <code>ε</code> 的分布，使其均值偏移到 <code>ŷt-1</code>：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    可以重写为：
    <code>y^s = α^s y⁰ + σ^s (ŷt-1 + ε)</code>  其中 <code>ε ~ N(0, I)</code>
    或者更直接地，通过调整噪声的采样方式：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    等价于：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    论文中给出的最终形式是：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    等价于：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    论文中公式 (5) 的形式是：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    可以重写为：
    <code>y^s = α^s y⁰ + σ^s (ŷt-1 + ε)</code>  其中 <code>ε ~ N(0, I)</code>
    或者，通过调整噪声的采样方式，使得噪声的均值偏移到 <code>ŷt-1</code>：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    论文中公式 (5) 的最终形式是：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    可以重写为：
    <code>y^s = α^s y⁰ + σ^s (ŷt-1 + ε)</code>  其中 <code>ε ~ N(0, I)</code>
    或者，通过调整噪声的采样方式，使得噪声的均值偏移到 <code>ŷt-1</code>：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    论文中公式 (5) 的最终形式是：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    可以重写为：
    <code>y^s = α^s y⁰ + σ^s (ŷt-1 + ε)</code>  其中 <code>ε ~ N(0, I)</code>
    或者，通过调整噪声的采样方式，使得噪声的均值偏移到 <code>ŷt-1</code>：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    论文中公式 (5) 的最终形式是：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    可以重写为：
    <code>y^s = α^s y⁰ + σ^s (ŷt-1 + ε)</code>  其中 <code>ε ~ N(0, I)</code>
    或者，通过调整噪声的采样方式，使得噪声的均值偏移到 <code>ŷt-1</code>：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    论文中公式 (5) 的最终形式是：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    可以重写为：
    <code>y^s = α^s y⁰ + σ^s (ŷt-1 + ε)</code>  其中 <code>ε ~ N(0, I)</code>
    或者，通过调整噪声的采样方式，使得噪声的均值偏移到 <code>ŷt-1</code>：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    论文中公式 (5) 的最终形式是：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    可以重写为：
    <code>y^s = α^s y⁰ + σ^s (ŷt-1 + ε)</code>  其中 <code>ε ~ N(0, I)</code>
    或者，通过调整噪声的采样方式，使得噪声的均值偏移到 <code>ŷt-1</code>：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    论文中公式 (5) 的最终形式是：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    可以重写为：
    <code>y^s = α^s y⁰ + σ^s (ŷt-1 + ε)</code>  其中 <code>ε ~ N(0, I)</code>
    或者，通过调整噪声的采样方式，使得噪声的均值偏移到 <code>ŷt-1</code>：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    论文中公式 (5) 的最终形式是：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    可以重写为：
    <code>y^s = α^s y⁰ + σ^s (ŷt-1 + ε)</code>  其中 <code>ε ~ N(0, I)</code>
    或者，通过调整噪声的采样方式，使得噪声的均值偏移到 <code>ŷt-1</code>：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    论文中公式 (5) 的最终形式是：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    可以重写为：
    <code>y^s = α^s y⁰ + σ^s (ŷt-1 + ε)</code>  其中 <code>ε ~ N(0, I)</code>
    或者，通过调整噪声的采样方式，使得噪声的均值偏移到 <code>ŷt-1</code>：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    论文中公式 (5) 的最终形式是：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    可以重写为：
    <code>y^s = α^s y⁰ + σ^s (ŷt-1 + ε)</code>  其中 <code>ε ~ N(0, I)</code>
    或者，通过调整噪声的采样方式，使得噪声的均值偏移到 <code>ŷt-1</code>：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    论文中公式 (5) 的最终形式是：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    可以重写为：
    <code>y^s = α^s y⁰ + σ^s (ŷt-1 + ε)</code>  其中 <code>ε ~ N(0, I)</code>
    或者，通过调整噪声的采样方式，使得噪声的均值偏移到 <code>ŷt-1</code>：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    论文中公式 (5) 的最终形式是：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    可以重写为：
    <code>y^s = α^s y⁰ + σ^s (ŷt-1 + ε)</code>  其中 <code>ε ~ N(0, I)</code>
    或者，通过调整噪声的采样方式，使得噪声的均值偏移到 <code>ŷt-1</code>：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    论文中公式 (5) 的最终形式是：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    可以重写为：
    <code>y^s = α^s y⁰ + σ^s (ŷt-1 + ε)</code>  其中 <code>ε ~ N(0, I)</code>
    或者，通过调整噪声的采样方式，使得噪声的均值偏移到 <code>ŷt-1</code>：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    论文中公式 (5) 的最终形式是：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    可以重写为：
    <code>y^s = α^s y⁰ + σ^s (ŷt-1 + ε)</code>  其中 <code>ε ~ N(0, I)</code>
    或者，通过调整噪声的采样方式，使得噪声的均值偏移到 <code>ŷt-1</code>：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    论文中公式 (5) 的最终形式是：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    可以重写为：
    <code>y^s = α^s y⁰ + σ^s (ŷt-1 + ε)</code>  其中 <code>ε ~ N(0, I)</code>
    或者，通过调整噪声的采样方式，使得噪声的均值偏移到 <code>ŷt-1</code>：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    论文中公式 (5) 的最终形式是：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    可以重写为：
    <code>y^s = α^s y⁰ + σ^s (ŷt-1 + ε)</code>  其中 <code>ε ~ N(0, I)</code>
    或者，通过调整噪声的采样方式，使得噪声的均值偏移到 <code>ŷt-1</code>：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    论文中公式 (5) 的最终形式是：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    可以重写为：
    <code>y^s = α^s y⁰ + σ^s (ŷt-1 + ε)</code>  其中 <code>ε ~ N(0, I)</code>
    或者，通过调整噪声的采样方式，使得噪声的均值偏移到 <code>ŷt-1</code>：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    论文中公式 (5) 的最终形式是：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    可以重写为：
    <code>y^s = α^s y⁰ + σ^s (ŷt-1 + ε)</code>  其中 <code>ε ~ N(0, I)</code>
    或者，通过调整噪声的采样方式，使得噪声的均值偏移到 <code>ŷt-1</code>：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    论文中公式 (5) 的最终形式是：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    可以重写为：
    <code>y^s = α^s y⁰ + σ^s (ŷt-1 + ε)</code>  其中 <code>ε ~ N(0, I)</code>
    或者，通过调整噪声的采样方式，使得噪声的均值偏移到 <code>ŷt-1</code>：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    论文中公式 (5) 的最终形式是：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    可以重写为：
    <code>y^s = α^s y⁰ + σ^s (ŷt-1 + ε)</code>  其中 <code>ε ~ N(0, I)</code>
    或者，通过调整噪声的采样方式，使得噪声的均值偏移到 <code>ŷt-1</code>：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    论文中公式 (5) 的最终形式是：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    可以重写为：
    <code>y^s = α^s y⁰ + σ^s (ŷt-1 + ε)</code>  其中 <code>ε ~ N(0, I)</code>
    或者，通过调整噪声的采样方式，使得噪声的均值偏移到 <code>ŷt-1</code>：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    论文中公式 (5) 的最终形式是：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    可以重写为：
    <code>y^s = α^s y⁰ + σ^s (ŷt-1 + ε)</code>  其中 <code>ε ~ N(0, I)</code>
    或者，通过调整噪声的采样方式，使得噪声的均值偏移到 <code>ŷt-1</code>：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    论文中公式 (5) 的最终形式是：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    可以重写为：
    <code>y^s = α^s y⁰ + σ^s (ŷt-1 + ε)</code>  其中 <code>ε ~ N(0, I)</code>
    或者，通过调整噪声的采样方式，使得噪声的均值偏移到 <code>ŷt-1</code>：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    论文中公式 (5) 的最终形式是：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    可以重写为：
    <code>y^s = α^s y⁰ + σ^s (ŷt-1 + ε)</code>  其中 <code>ε ~ N(0, I)</code>
    或者，通过调整噪声的采样方式，使得噪声的均值偏移到 <code>ŷt-1</code>：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    论文中公式 (5) 的最终形式是：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    可以重写为：
    <code>y^s = α^s y⁰ + σ^s (ŷt-1 + ε)</code>  其中 <code>ε ~ N(0, I)</code>
    或者，通过调整噪声的采样方式，使得噪声的均值偏移到 <code>ŷt-1</code>：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    论文中公式 (5) 的最终形式是：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    可以重写为：
    <code>y^s = α^s y⁰ + σ^s (ŷt-1 + ε)</code>  其中 <code>ε ~ N(0, I)</code>
    或者，通过调整噪声的采样方式，使得噪声的均值偏移到 <code>ŷt-1</code>：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    论文中公式 (5) 的最终形式是：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    可以重写为：
    <code>y^s = α^s y⁰ + σ^s (ŷt-1 + ε)</code>  其中 <code>ε ~ N(0, I)</code>
    或者，通过调整噪声的采样方式，使得噪声的均值偏移到 <code>ŷt-1</code>：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    论文中公式 (5) 的最终形式是：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    可以重写为：
    <code>y^s = α^s y⁰ + σ^s (ŷt-1 + ε)</code>  其中 <code>ε ~ N(0, I)</code>
    或者，通过调整噪声的采样方式，使得噪声的均值偏移到 <code>ŷt-1</code>：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    论文中公式 (5) 的最终形式是：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    可以重写为：
    <code>y^s = α^s y⁰ + σ^s (ŷt-1 + ε)</code>  其中 <code>ε ~ N(0, I)</code>
    或者，通过调整噪声的采样方式，使得噪声的均值偏移到 <code>ŷt-1</code>：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    论文中公式 (5) 的最终形式是：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    可以重写为：
    <code>y^s = α^s y⁰ + σ^s (ŷt-1 + ε)</code>  其中 <code>ε ~ N(0, I)</code>
    或者，通过调整噪声的采样方式，使得噪声的均值偏移到 <code>ŷt-1</code>：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    论文中公式 (5) 的最终形式是：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    可以重写为：
    <code>y^s = α^s y⁰ + σ^s (ŷt-1 + ε)</code>  其中 <code>ε ~ N(0, I)</code>
    或者，通过调整噪声的采样方式，使得噪声的均值偏移到 <code>ŷt-1</code>：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    论文中公式 (5) 的最终形式是：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    可以重写为：
    <code>y^s = α^s y⁰ + σ^s (ŷt-1 + ε)</code>  其中 <code>ε ~ N(0, I)</code>
    或者，通过调整噪声的采样方式，使得噪声的均值偏移到 <code>ŷt-1</code>：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    论文中公式 (5) 的最终形式是：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    可以重写为：
    <code>y^s = α^s y⁰ + σ^s (ŷt-1 + ε)</code>  其中 <code>ε ~ N(0, I)</code>
    或者，通过调整噪声的采样方式，使得噪声的均值偏移到 <code>ŷt-1</code>：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    论文中公式 (5) 的最终形式是：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    可以重写为：
    <code>y^s = α^s y⁰ + σ^s (ŷt-1 + ε)</code>  其中 <code>ε ~ N(0, I)</code>
    或者，通过调整噪声的采样方式，使得噪声的均值偏移到 <code>ŷt-1</code>：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    论文中公式 (5) 的最终形式是：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    可以重写为：
    <code>y^s = α^s y⁰ + σ^s (ŷt-1 + ε)</code>  其中 <code>ε ~ N(0, I)</code>
    或者，通过调整噪声的采样方式，使得噪声的均值偏移到 <code>ŷt-1</code>：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    论文中公式 (5) 的最终形式是：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    可以重写为：
    <code>y^s = α^s y⁰ + σ^s (ŷt-1 + ε)</code>  其中 <code>ε ~ N(0, I)</code>
    或者，通过调整噪声的采样方式，使得噪声的均值偏移到 <code>ŷt-1</code>：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    论文中公式 (5) 的最终形式是：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    可以重写为：
    <code>y^s = α^s y⁰ + σ^s (ŷt-1 + ε)</code>  其中 <code>ε ~ N(0, I)</code>
    或者，通过调整噪声的采样方式，使得噪声的均值偏移到 <code>ŷt-1</code>：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    论文中公式 (5) 的最终形式是：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    可以重写为：
    <code>y^s = α^s y⁰ + σ^s (ŷt-1 + ε)</code>  其中 <code>ε ~ N(0, I)</code>
    或者，通过调整噪声的采样方式，使得噪声的均值偏移到 <code>ŷt-1</code>：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    论文中公式 (5) 的最终形式是：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    可以重写为：
    <code>y^s = α^s y⁰ + σ^s (ŷt-1 + ε)</code>  其中 <code>ε ~ N(0, I)</code>
    或者，通过调整噪声的采样方式，使得噪声的均值偏移到 <code>ŷt-1</code>：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    论文中公式 (5) 的最终形式是：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    可以重写为：
    <code>y^s = α^s y⁰ + σ^s (ŷt-1 + ε)</code>  其中 <code>ε ~ N(0, I)</code>
    或者，通过调整噪声的采样方式，使得噪声的均值偏移到 <code>ŷt-1</code>：
    <code>y^s = α^s y⁰ + σ^s ŷt-1 + σ^s ε</code>  其中 <code>ε ~ N(0, I)</code>
    论文中公式</li>
</ul>
</li>
</ul>
</li>
</ol>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce a causal, efficient video editing model that edits variable-length videos frame by frame.</li>
<li>To leverage videos' temporal redundancy, we propose a new I2I diffusion forward process formulation that encourages the model to predict the residual between the target output and the previous prediction.</li>
<li>Moreover, we propose a new benchmark that better ranks state-of-the-art methods for editing tasks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2602.06871v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2602.06871v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2602.06862v1'></a></p>
<h2 id="parameters-as-experts-adapting-vision-models-with-dynamic-parameter-routing"><a href="https://arxiv.org/abs/2602.06862v1">Parameters as Experts: Adapting Vision Models with Dynamic Parameter Routing</a></h2>
<p><strong>Authors:</strong> Meng Lou, Stanley Yu, Yizhou Yu</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Adapting pre-trained vision models using parameter-efficient fine-tuning (PEFT) remains challenging, as it aims to achieve performance comparable to full fine-tuning using a minimal number of trainable parameters. When applied to complex dense prediction tasks, existing methods exhibit limitations, including input-agnostic modeling and redundant cross-layer representations. To this end, we propose AdaRoute, a new adapter-style method featuring a simple mixture-of-experts (MoE) architecture. Specifically, we introduce shared expert centers, where each expert is a trainable parameter matrix. During a feedforward pass, each AdaRoute module in the network dynamically generates weight matrices tailored for the current module via a simple dynamic parameter routing mechanism, which selectively aggregates parameter matrices in the corresponding expert center. Dynamic weight matrices in AdaRoute modules facilitate low-rank adaptation in an input-dependent manner, thus generating more customized and powerful feature representations. Moreover, since AdaRoute modules across multiple network layers share the same expert center, they improve feature diversity by promoting implicit cross-layer feature interaction. Extensive experiments demonstrate the superiority of AdaRoute on diverse vision tasks, including semantic segmentation, object detection and instance segmentation, and panoptic segmentation. Code will be available at: https://bit.ly/3NZcr0H.</p>
<p><strong>Analysis:</strong></p>
<p>好的，我将扮演一名AI领域高水平研究生，深入分析您提供的论文，重点关注其方法部分的创新点、设计逻辑、优势与不足，并提供结构化的分析。</p>
<hr />
<h2 id="_1">论文方法分析与总结</h2>
<h3 id="1_4">1. 摘要翻译</h3>
<p><strong>论文题目：</strong> Parameters as Experts: Adapting Vision Models with Dynamic Parameter Routing</p>
<p><strong>中文摘要：</strong>
参数高效微调（PEFT）在实现与全量微调相当的性能的同时，仅更新少量可训练参数，这在自然语言处理（NLP）和计算机视觉（CV）领域得到了广泛应用。然而，当应用于复杂的密集预测任务时，现有方法存在输入无关建模和跨层表示冗余等局限性。为此，我们提出AdaRoute，一种采用简单混合专家（MoE）架构的新型适配器方法。具体来说，我们引入了共享专家中心，其中每个专家是一个可训练的参数矩阵。在前向传播过程中，网络中的每个AdaRoute模块通过一个简单的动态参数路由机制，选择性地聚合相应专家中心中的参数矩阵，从而动态生成定制化的权重矩阵。AdaRoute中的动态权重矩阵以输入依赖的方式实现低秩适应，从而生成更具定制化和更强大的特征表示。此外，由于AdaRoute模块跨越多个网络层共享同一个专家中心，它们通过促进隐式的跨层特征交互来提升特征多样性。广泛的实验证明了AdaRoute在语义分割、目标检测和实例分割以及全景分割等多种视觉任务上的优越性。代码将在此处发布：https://bit.ly/3NZcr0H。</p>
<h3 id="2_4">2. 方法动机分析</h3>
<ul>
<li>
<p><strong>驱动力</strong>：
    作者旨在解决参数高效微调（PEFT）在应用于复杂密集预测任务时遇到的性能瓶颈，特别是现有方法在输入无关建模和跨层表示冗余方面存在的不足。核心目标是设计一种PEFT方法，能够在保持参数高效性的同时，实现与全量微调相当甚至更优的性能，并能有效处理密集预测任务所需的复杂空间依赖性。</p>
</li>
<li>
<p><strong>现有方法痛点</strong>：</p>
<ol>
<li><strong>输入无关建模 (Input-agnostic modeling)</strong>：现有适配器方法通常以输入无关的方式进行参数更新，这意味着它们为所有输入生成相同的适配器权重。这限制了模型捕捉输入特异性变化的能力，尤其是在处理复杂的密集预测任务时，这种能力至关重要。</li>
<li><strong>冗余的跨层表示 (Redundant cross-layer representations)</strong>：不同网络层中的适配器参数是相互独立的，缺乏有效的跨层交互机制。这导致不同层学习到相似或冗余的特征表示，未能充分利用多层信息来丰富和多样化特征。</li>
<li><strong>表示能力不足 (Representation Deficiency)</strong>：上述两点共同导致模型在捕捉复杂空间依赖性方面的能力受限，使得在密集预测任务上的性能与全量微调存在差距。</li>
</ol>
</li>
<li>
<p><strong>研究假设</strong>：
    通过引入一个<strong>共享的、可动态路由的专家中心</strong>，并允许<strong>输入依赖的参数生成</strong>，可以实现更强大的特征表示，并促进<strong>隐式的跨层特征交互</strong>，从而克服现有PEFT方法的局限性，在密集预测任务上取得更好的性能。</p>
</li>
</ul>
<h3 id="3_4">3. 方法设计详解</h3>
<p><strong>方法pipeline总结：</strong></p>
<p>AdaRoute方法的核心思想是将每个网络层中的适配器参数视为从一个共享的“专家池”中动态生成的。它包含两个主要组件：<strong>共享专家中心 (Shared Expert Center)</strong> 和 <strong>动态参数路由 (Dynamic Parameter Routing)</strong>。</p>
<p><strong>流程图（基于论文图2和图3）：</strong></p>
<pre class="codehilite"><code>输入特征 X -&gt; [AdaRoute模块] -&gt; 输出特征 Y
</code></pre>

<p><strong>AdaRoute模块内部流程：</strong></p>
<ol>
<li>
<p><strong>输入特征 (Input Feature)</strong>: <script type="math/tex">X \in \mathbb{R}^{H \times W \times C}</script>，其中 <script type="math/tex">H, W</script> 是空间维度，<script type="math/tex">C</script> 是通道数。</p>
</li>
<li>
<p><strong>共享专家中心 (Shared Expert Center)</strong>:</p>
<ul>
<li>对于<strong>通道变换 (Channel-wise transformations)</strong>，专家中心包含两组可训练的参数矩阵对：<ul>
<li>
<script type="math/tex">\{E_A \in \mathbb{R}^{M \times C \times \hat{C}}, E_B \in \mathbb{R}^{M \times \hat{C} \times C}\}</script>
</li>
<li>其中 <script type="math/tex">M</script> 是专家中心容量（即专家数量），<script type="math/tex">\hat{C}</script> 是中间通道数。</li>
</ul>
</li>
<li>对于<strong>空间变换 (Spatial transformations)</strong>（多尺度深度卷积），专家中心包含三组参数矩阵，对应不同的核尺寸 <script type="math/tex">K</script>：<ul>
<li>
<script type="math/tex">\{S_A \in \mathbb{R}^{M \times \hat{C} \times K^2}, S_B \in \mathbb{R}^{M \times \hat{C} \times K^2}, S_C \in \mathbb{R}^{M \times \hat{C} \times K^2}\}</script> (论文中描述为 <script type="math/tex">S_A \in \mathbb{R}^{M \times \hat{C} \times K^2}, S_B \in \mathbb{R}^{M \times \hat{C} \times K^2}, S_C \in \mathbb{R}^{M \times \hat{C} \times K^2}</script>，但图示和描述更倾向于 <script type="math/tex">S_A, S_B, S_C</script> 是用于生成不同核的参数，且核尺寸 <script type="math/tex">K</script> 是一个超参数，这里根据图示和描述推测为生成深度卷积核的参数，例如 <script type="math/tex">S_A \in \mathbb{R}^{M \times \hat{C} \times K^2}</script>，用于生成 <script type="math/tex">K \times K</script> 的核。论文中也提到 <script type="math/tex">K</script> 代表核尺寸，且有多个核尺寸，如 [3, 5, 7]。这里根据图示和描述，理解为 <script type="math/tex">S_A, S_B, S_C</script> 是用于生成不同核的参数，且每个核尺寸对应一组参数。为简化，这里先按通道变换的逻辑描述，空间变换部分会单独详述。)</li>
<li><strong>更准确的理解（根据论文图4和3.2节）：</strong> 空间变换部分，专家中心包含 <script type="math/tex">\{S_A \in \mathbb{R}^{M \times \hat{C} \times K^2}, S_B \in \mathbb{R}^{M \times \hat{C} \times K^2}, S_C \in \mathbb{R}^{M \times \hat{C} \times K^2}\}</script>，其中 <script type="math/tex">K</script> 是核尺寸。论文提到使用多个核尺寸（如3, 5, 7），并且这些参数用于生成<strong>动态深度卷积核</strong>。因此，这里的 <script type="math/tex">S_A, S_B, S_C</script> 可能是用于生成不同核的参数，或者代表不同核尺寸的专家。论文中提到“生成动态深度卷积核”，并引用了动态卷积（D2Convs）。图4展示了多尺度空间混合，其中有三个D2Conv层，每个层使用不同的核尺寸。因此，专家中心为生成这些动态核提供了参数基础。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>动态参数路由 (Dynamic Parameter Routing)</strong>:</p>
<ul>
<li>
<p><strong>路由网络 (Router Network)</strong>: 对于输入特征 <script type="math/tex">X</script>，一个轻量级的路由网络被激活。</p>
<ul>
<li><strong>全局平均池化 (GAP)</strong>: <script type="math/tex">X</script> 首先经过GAP。</li>
<li><strong>线性层 (Linear Layer)</strong>: GAP后的特征通过一个线性层，将通道维度大幅降低（例如，降至24），以减少计算开销。</li>
<li><strong>两个并行线性层 + Softmax</strong>: 这个低维隐藏特征随后通过两个并行线性层，并应用Softmax激活函数，生成两个动态的<strong>门控向量 (Gating Vectors)</strong>：<script type="math/tex">G_1, G_2 \in \mathbb{R}^M</script>。</li>
</ul>
</li>
<li>
<p><strong>动态权重矩阵生成</strong>:</p>
<ul>
<li><strong>通道变换</strong>:<ul>
<li>
<script type="math/tex">W_1 = G_1 \odot E_A</script> (逐元素乘法，但论文描述为“<script type="math/tex">G_1</script> is multiplied with <script type="math/tex">E_A</script> to produce the dynamic down-projection weight matrix <script type="math/tex">W_1</script>”，这暗示了 <script type="math/tex">G_1</script> 是一个标量或向量，用于对 <script type="math/tex">E_A</script> 的第一个维度（专家维度）进行加权。更准确的理解是，<script type="math/tex">G_1</script> 是一个 <script type="math/tex">M</script> 维的向量，它与 <script type="math/tex">E_A</script> 的第一个维度（专家维度）进行加权求和，生成一个 <script type="math/tex">C \times \hat{C}</script> 的矩阵 <script type="math/tex">W_1</script>。即 <script type="math/tex">W_1 = \sum_{m=1}^M G_{1,m} E_{A,m}</script>，其中 <script type="math/tex">E_{A,m}</script> 是第 <script type="math/tex">m</script> 个专家矩阵。论文图3的公式（2）更清晰地表达了这一点：<script type="math/tex">W_l = \sum_{m=1}^M g_{l,m}^E E_m</script>。这里的 <script type="math/tex">g_{l,m}^E</script> 是路由系数，对应论文中的 <script type="math/tex">G_1</script> 或 <script type="math/tex">G_2</script>。所以，<strong><script type="math/tex">G_1</script> 和 <script type="math/tex">G_2</script> 是路由系数向量，用于对专家中心中的参数矩阵进行加权求和，生成动态的权重矩阵。</strong></li>
<li>
<script type="math/tex">W_1 \in \mathbb{R}^{C \times \hat{C}}</script> (动态下投影权重矩阵)</li>
<li>
<script type="math/tex">W_2 = G_2 \odot E_B</script> (动态上投影权重矩阵)</li>
<li>
<script type="math/tex">W_2 \in \mathbb{R}^{\hat{C} \times C}</script>
</li>
</ul>
</li>
<li><strong>空间变换 (Dynamic Multi-scale Spatial Mixing)</strong>:<ul>
<li>路由网络为每个核尺寸生成动态的门控向量 <script type="math/tex">\{G_A, G_B, G_C\}</script>。</li>
<li>这些门控向量与专家中心中的参数矩阵 <script type="math/tex">\{S_A, S_B, S_C\}</script> 结合，生成三个动态的深度卷积核（D2Convs）。</li>
<li>这些动态核被应用于输入特征，实现多尺度空间混合。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>特征变换</strong>:</p>
<ul>
<li><strong>通道变换</strong>: 动态权重矩阵 <script type="math/tex">W_1</script> 和 <script type="math/tex">W_2</script> 用于变换输入特征 <script type="math/tex">X</script>：<ul>
<li>
<script type="math/tex">X' = X W_1</script> (下投影)</li>
<li>
<script type="math/tex">Y_{channel} = X' W_2</script> (上投影)</li>
</ul>
</li>
<li><strong>空间变换</strong>: 动态深度卷积核应用于特征图。</li>
</ul>
</li>
<li>
<p><strong>残差连接 (Residual Connection)</strong>:</p>
<ul>
<li>最终输出 <script type="math/tex">Y</script> 通常是通过将动态变换后的特征与原始输入特征（或经过其他处理的特征）相加得到，以保证信息流动和梯度传播。</li>
<li>
<script type="math/tex">Y = X + Y_{transformed}</script> (其中 <script type="math/tex">Y_{transformed}</script> 是经过通道和/或空间变换后的特征)。</li>
</ul>
</li>
</ol>
<p><strong>模型结构与模块功能：</strong></p>
<ul>
<li><strong>AdaRoute模块</strong>: 这是一个可插入到现有网络（如Swin、ConvNeXt）中的模块。它包含一个路由网络和一个动态参数生成机制。</li>
<li><strong>共享专家中心</strong>: 存储了一组可训练的参数矩阵（专家）。这些专家参数在所有AdaRoute模块（或同一阶段的所有模块）之间共享。</li>
<li><strong>路由网络</strong>: 接收输入特征，并根据输入内容动态生成门控系数，决定如何组合专家中心的参数。</li>
<li><strong>动态参数生成</strong>: 利用路由系数和专家参数，生成输入依赖的、低秩的权重矩阵（用于通道变换）或卷积核（用于空间变换）。</li>
<li><strong>多尺度深度卷积 (D2Convs)</strong>: 论文借鉴了Mona的方法，引入了多尺度卷积来增强空间建模能力。AdaRoute在此基础上实现了<strong>动态</strong>的多尺度卷积核生成。</li>
<li><strong>空间注意力聚合 (Spatially-varying Aggregation - SA)</strong>: 论文在多尺度空间混合后，引入了一个SA模块，使用1x1卷积和softmax生成空间注意力图，动态地重新校准每个尺度的特征图。</li>
</ul>
<p><strong>算法解释（关键公式）：</strong></p>
<ul>
<li>
<p><strong>动态权重矩阵生成 (通道变换)</strong>:
    <script type="math/tex">W_l = \sum_{m=1}^M g_{l,m}^E E_m</script>
</p>
<ul>
<li>
<script type="math/tex">W_l</script>: 在层 <script type="math/tex">l</script> 生成的动态权重矩阵。</li>
<li>
<script type="math/tex">M</script>: 专家中心容量（专家数量）。</li>
<li>
<script type="math/tex">g_{l,m}^E</script>: 路由网络为层 <script type="math/tex">l</script> 生成的第 <script type="math/tex">m</script> 个专家的门控系数。</li>
<li>
<script type="math/tex">E_m</script>: 第 <script type="math/tex">m</script> 个专家（参数矩阵）。</li>
<li><strong>意义</strong>: 这个公式表明，层 <script type="math/tex">l</script> 的权重矩阵不是直接学习的，而是通过对共享专家中心中的所有专家矩阵进行加权求和来动态生成的。权重 <script type="math/tex">g_{l,m}^E</script> 取决于输入特征，因此 <script type="math/tex">W_l</script> 是输入依赖的。</li>
</ul>
</li>
<li>
<p><strong>梯度更新 (理论分析)</strong>:
    <script type="math/tex">\frac{\partial \mathcal{L}}{\partial E_m} = \sum_{l=1}^L \left( \frac{\partial \mathcal{L}}{\partial h_l} \frac{\partial h_l}{\partial W_l} \frac{\partial W_l}{\partial E_m} \right)</script>
</p>
<ul>
<li>
<script type="math/tex">\frac{\partial \mathcal{L}}{\partial E_m}</script>: 专家矩阵 <script type="math/tex">E_m</script> 的梯度。</li>
<li>
<script type="math/tex">L</script>: 模型总层数。</li>
<li>
<script type="math/tex">\frac{\partial \mathcal{L}}{\partial h_l}</script>: 层 <script type="math/tex">l</script> 输出特征的损失梯度。</li>
<li>
<script type="math/tex">\frac{\partial h_l}{\partial W_l}</script>: 层 <script type="math/tex">l</script> 输出特征对权重矩阵 <script type="math/tex">W_l</script> 的梯度。</li>
<li>
<script type="math/tex">\frac{\partial W_l}{\partial E_m}</script>: 动态权重矩阵 <script type="math/tex">W_l</script> 对专家矩阵 <script type="math/tex">E_m</script> 的梯度。</li>
<li><strong>意义</strong>: 这个公式展示了AdaRoute的<strong>隐式跨层交互</strong>。一个专家矩阵 <script type="math/tex">E_m</script> 的更新梯度，是聚合了所有层（从1到L）的梯度贡献。这意味着对一个专家的更新会同时影响到所有使用该专家的层，从而实现了跨层的信息流动和知识共享，避免了层间表示的孤立和冗余。</li>
</ul>
</li>
</ul>
<h3 id="4_3">4. 方法对比分析</h3>
<ul>
<li>
<p><strong>本质区别</strong>:</p>
<ol>
<li><strong>共享专家中心 vs. 独立适配器</strong>: 现有适配器方法（如AdaptFormer, Mona）为每一层或每一组层维护独立的适配器参数。AdaRoute则引入了一个<strong>共享的专家中心</strong>，所有层共享这个中心中的参数池。</li>
<li><strong>动态输入依赖 vs. 输入无关/固定适配器</strong>: 现有方法要么是输入无关的（如Mona的卷积核是固定的），要么是输入依赖但参数量较大的（如全量微调）。AdaRoute通过动态路由机制，根据输入特征<strong>动态生成</strong>适配器参数（权重矩阵或卷积核），实现了输入依赖的低秩适应。</li>
<li><strong>隐式跨层交互 vs. 独立层更新</strong>: AdaRoute通过共享专家中心和其梯度更新机制，实现了<strong>隐式的跨层特征交互</strong>。而传统适配器方法中的层更新是独立的，缺乏有效的跨层信息流动。</li>
</ol>
</li>
<li>
<p><strong>创新贡献</strong>:</p>
<ol>
<li><strong>共享专家中心</strong>: 引入了一个可复用的参数池，显著减少了参数量，并为跨层交互奠定了基础。</li>
<li><strong>动态参数路由</strong>: 设计了一个轻量级但有效的路由机制，使得适配器参数能够根据输入动态生成，增强了模型的适应性和表达能力。</li>
<li><strong>隐式跨层交互</strong>: 通过共享专家中心和其梯度聚合机制，实现了有效的跨层特征交互，减少了表示冗余，提升了特征多样性。</li>
<li><strong>多尺度动态空间混合</strong>: 结合了动态参数生成和多尺度卷积，进一步增强了对密集预测任务中空间信息的建模能力。</li>
</ol>
</li>
<li>
<p><strong>适用场景</strong>:</p>
<ul>
<li><strong>密集预测任务</strong>: 论文重点强调了AdaRoute在语义分割、目标检测、实例分割和全景分割等任务上的优越性。这是因为这些任务需要捕捉复杂的空间依赖性和上下文信息，而AdaRoute的动态、输入依赖和跨层交互特性对此非常有利。</li>
<li><strong>参数高效微调</strong>: 适用于需要以极少的参数更新量来适应预训练模型到下游任务的场景。</li>
<li><strong>需要强大特征表示的模型</strong>: 任何需要更丰富、更具适应性特征表示的模型，都可以考虑使用AdaRoute。</li>
</ul>
</li>
</ul>
<h3 id="5_3">5. 实验分析</h3>
<ul>
<li>
<p><strong>验证方法</strong>:
    作者在多种视觉任务上进行了广泛的实验，包括：</p>
<ul>
<li><strong>语义分割</strong>: ADE20K数据集。</li>
<li><strong>目标检测与实例分割</strong>: COCO2017数据集。</li>
<li><strong>全景分割</strong>: COCO2017数据集。</li>
<li><strong>图像分类</strong>: CIFAR-100, SVHN, Food-101, ImageNet-R数据集。</li>
<li><strong>其他任务</strong>: 远程传感图像分割、人类姿态估计。</li>
<li><strong>不同骨干网络</strong>: Swin Transformer (Swin-B, Swin-L) 和 ConvNeXt (ConvNeXt-B, ConvNeXt-L) 等。</li>
<li><strong>对比方法</strong>: 全量微调 (Full-tuning) 以及多种代表性的PEFT方法，如VPT, LoRA, AdaptFormer, Mona, LoRand, SNELL等。</li>
<li><strong>消融实验</strong>: 分析了专家中心容量、核尺寸、路由激活函数、共享范围等对性能的影响。</li>
</ul>
</li>
<li>
<p><strong>关键结果</strong>:</p>
<ul>
<li><strong>整体性能优越</strong>: 在大多数密集预测任务上，AdaRoute均取得了领先于其他PEFT方法的性能，并且在某些情况下接近甚至超越了全量微调的性能，同时参数量极少（通常低于全量微调的4%）。</li>
<li><strong>语义分割</strong>: 在ADE20K上，使用Swin-L时，AdaRoute比全量微调仅少0.8% mIoU，参数量仅为全量的4%。</li>
<li><strong>目标检测/实例分割</strong>: 在COCO2017上，AdaRoute在APb/APm上相比Mona有显著提升，并优于LoRand。</li>
<li><strong>全景分割</strong>: AdaRoute在PQ指标上显著优于AdaptFormer和CoLoRA。</li>
<li><strong>图像分类</strong>: 在ImageNet-R等具有领域偏移的数据集上，AdaRoute表现出更强的鲁棒性，显著优于Mona。</li>
<li><strong>有效性验证</strong>: 消融实验证明了共享专家中心、动态路由和多尺度空间混合的重要性。例如，共享范围越大，性能越好；稀疏激活专家反而会降低性能。</li>
</ul>
</li>
<li>
<p><strong>优势场景</strong>:</p>
<ul>
<li><strong>密集预测任务</strong>: 如上所述，这是AdaRoute最擅长的领域。</li>
<li><strong>需要处理输入变化和复杂空间依赖的任务</strong>: 任何需要模型对输入数据有细致、动态响应的任务。</li>
<li><strong>资源受限场景</strong>: 当计算资源或存储空间有限，但又需要高性能模型时。</li>
</ul>
</li>
<li>
<p><strong>局限性</strong>:</p>
<ul>
<li><strong>推理时无法直接集成</strong>: 与现有适配器方法类似，AdaRoute模块在推理时需要额外计算，无法像全量微调那样直接替换模型权重。</li>
<li><strong>多尺度深度卷积引入轻微训练延迟</strong>: 论文提到，多尺度卷积增加了训练时间，但总体上仍比全量微调高效。</li>
<li><strong>未在超大规模模型上评估</strong>: 由于资源限制，作者未在如DINOv3-ViT-7B等超大规模模型上进行评估。</li>
<li><strong>全景分割仍有差距</strong>: 在全景分割任务上，与全量微调相比仍存在一定差距，作者认为这可能与该任务对表示能力要求极高，而PEFT方法参数量受限有关。</li>
</ul>
</li>
</ul>
<h3 id="6_3">6. 实用指南</h3>
<ul>
<li><strong>开源情况</strong>: 论文提到“Code will be released at: https://bit.ly/3NZcr0H.”，表明代码是公开的。</li>
<li><strong>实现/复现的关键步骤</strong>:<ol>
<li><strong>选择骨干网络</strong>: 支持Swin、ConvNeXt等主流视觉骨干网络。</li>
<li><strong>集成AdaRoute模块</strong>: 将AdaRoute模块插入到骨干网络的指定位置（如Swin的token/channel mixer后，ConvNeXt的block后）。</li>
<li><strong>配置共享专家中心</strong>: 根据网络层数和模型结构，设置专家中心容量 <script type="math/tex">M</script> 和中间通道数 <script type="math/tex">\hat{C}</script>。论文建议 <script type="math/tex">M</script> 与层数相关，例如 <script type="math/tex">M=2L</script>。</li>
<li><strong>配置路由网络</strong>: 路由网络通常是一个简单的MLP，用于生成门控系数。</li>
<li><strong>多尺度空间混合</strong>: 配置D2Convs的核尺寸（如[3, 5, 7]）和SA模块。</li>
<li><strong>训练</strong>: 使用AdamW优化器，配合“poly”学习率调度等。</li>
</ol>
</li>
<li><strong>迁移可能</strong>:<ul>
<li><strong>其他密集预测任务</strong>: AdaRoute的设计理念（动态参数生成、跨层交互）使其非常适合迁移到其他需要精细空间建模的密集预测任务，如姿态估计（论文已初步验证）、边缘检测、深度估计等。</li>
<li><strong>Transformer和CNN模型</strong>: 论文已验证其在Swin Transformer和ConvNeXt上的有效性，理论上可以迁移到其他具有类似层级结构的Transformer或CNN模型。</li>
<li><strong>LLMs</strong>: 论文提到未来工作会探索扩展到LLMs。其核心思想（共享专家、动态路由）在NLP领域也有借鉴意义。</li>
<li><strong>多模态模型</strong>: 同样是未来工作方向，可以探索其在多模态融合中的应用。</li>
</ul>
</li>
</ul>
<h3 id="7_3">7. 总结</h3>
<ul>
<li>
<p><strong>核心思想</strong>: <strong>共享专家池，动态路由，输入依赖，跨层交互，高效适配。</strong> (15字)</p>
</li>
<li>
<p><strong>速记版pipeline</strong>:</p>
<ol>
<li><strong>定义专家池</strong>: 预设一组共享的参数矩阵（专家）。</li>
<li><strong>动态路由</strong>: 根据输入特征，动态计算每个专家对当前层的重要性权重。</li>
<li><strong>生成适配器</strong>: 用权重组合专家，生成当前层所需的动态适配器参数。</li>
<li><strong>特征增强</strong>: 用动态适配器处理特征，实现输入依赖和跨层信息融合。</li>
</ol>
</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To this end, we propose AdaRoute, a new adapter-style method featuring a simple mixture-of-experts (MoE) architecture.</li>
<li>Specifically, we introduce shared expert centers, where each expert is a trainable parameter matrix.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2602.06862v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2602.06862v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2602.06850v1'></a></p>
<h2 id="rethinking-multi-condition-dits-eliminating-redundant-attention-via-position-alignment-and-keyword-scoping"><a href="https://arxiv.org/abs/2602.06850v1">Rethinking Multi-Condition DiTs: Eliminating Redundant Attention via Position-Alignment and Keyword-Scoping</a></h2>
<p><strong>Authors:</strong> Chao Zhou, Tianyi Wei, Yiling Chen, Wenbo Zhou, Nenghai Yu</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.MM</p>
<p><strong>Abstract:</strong></p>
<p>While modern text-to-image models excel at prompt-based generation, they often lack the fine-grained control necessary for specific user requirements like spatial layouts or subject appearances. Multi-condition control addresses this, yet its integration into Diffusion Transformers (DiTs) is bottlenecked by the conventional ``concatenate-and-attend'' strategy, which suffers from quadratic computational and memory overhead as the number of conditions scales. Our analysis reveals that much of this cross-modal interaction is spatially or semantically redundant. To this end, we propose Position-aligned and Keyword-scoped Attention (PKA), a highly efficient framework designed to eliminate these redundancies. Specifically, Position-Aligned Attention (PAA) linearizes spatial control by enforcing localized patch alignment, while Keyword-Scoped Attention (KSA) prunes irrelevant subject-driven interactions via semantic-aware masking. To facilitate efficient learning, we further introduce a Conditional Sensitivity-Aware Sampling (CSAS) strategy that reweights the training objective towards critical denoising phases, drastically accelerating convergence and enhancing conditional fidelity. Empirically, PKA delivers a 10.0<script type="math/tex">\times</script> inference speedup and a 5.1<script type="math/tex">\times</script> VRAM saving, providing a scalable and resource-friendly solution for high-fidelity multi-conditioned generation.</p>
<p><strong>Analysis:</strong></p>
<p>好的，我将以AI领域高水平研究生的视角，深入分析这篇论文的方法部分，并遵循您提供的分析框架。</p>
<hr />
<h2 id="_2">论文方法分析与总结</h2>
<h3 id="1_5">1. 摘要翻译</h3>
<p>这篇论文提出了一种名为“位置对齐与关键词筛选注意力”（Position-aligned and Keyword-scoped Attention, PKA）的新型机制，旨在解决多条件控制的扩散Transformer（DiTs）在处理多个条件时面临的计算和内存效率瓶颈。现有方法通常采用“拼接-注意力”策略，导致计算量随条件数量呈二次方增长。论文分析发现，这种交叉模态交互存在显著的空间或语义冗余。</p>
<p>为了解决这个问题，PKA提出了两个核心组件：
1.  <strong>位置对齐注意力（Position-Aligned Attention, PAA）</strong>：通过强制局部块对齐来线性化空间控制，将注意力计算限制在空间上匹配的区域。
2.  <strong>关键词筛选注意力（Keyword-Scoped Attention, KSA）</strong>：通过语义感知掩码来过滤掉不相关的、受主体驱动的交互，将注意力集中在与关键词相关的区域。</p>
<p>此外，为了提高训练效率，论文还引入了<strong>条件敏感度感知采样（Conditional Sensitivity-Aware Sampling, CSAS）</strong>策略，该策略将训练目标重新加权到关键的去噪阶段，从而加速收敛并提高条件保真度。</p>
<p>实验结果表明，PKA在推理速度上实现了10.0倍的提升，VRAM节省了5.1倍，为高保真多条件生成提供了一个可扩展且资源友好的解决方案。</p>
<h3 id="2_5">2. 方法动机分析</h3>
<ul>
<li>
<p><strong>驱动力</strong>：</p>
<ul>
<li><strong>多条件控制的需求</strong>：现实世界中，用户不仅需要文本提示，还常常需要更精细的控制，如指定空间布局、参考图像或特定对象。这推动了多条件生成模型的发展。</li>
<li><strong>现有DiT模型效率瓶颈</strong>：尽管DiTs在图像生成方面表现出色，但将多个条件（如文本、布局图、参考图像等）集成到DiT中时，传统的“拼接-注意力”机制（concatenate-and-attend）面临严重的计算和内存挑战。</li>
</ul>
</li>
<li>
<p><strong>现有方法痛点</strong>：</p>
<ul>
<li><strong>计算和内存开销巨大</strong>：当条件数量增加时，注意力机制的计算量和内存需求呈二次方增长（O(c²n²)），导致模型难以扩展到更多条件，并带来高昂的推理延迟和内存消耗。</li>
<li><strong>冗余的交叉模态交互</strong>：论文通过分析发现，现有的“拼接-注意力”机制在处理空间对齐条件时，注意力权重高度集中在对角线上（图2），表明大部分非对齐的交互是冗余的。对于主体驱动的条件，注意力响应也主要集中在与关键词相关的稀疏区域（图3），其余区域的交互同样是冗余的。</li>
</ul>
</li>
<li>
<p><strong>研究假设</strong>：</p>
<ul>
<li>多条件控制的有效性并不一定需要全局的、密集的交叉模态注意力计算。</li>
<li>通过识别并消除空间对齐条件中的空间冗余和主体驱动条件中的语义冗余，可以显著提高DiT在多条件生成中的效率。</li>
<li>优化训练过程中的采样策略，使其更关注模型对条件敏感的关键去噪阶段，可以加速收敛并提升控制能力。</li>
</ul>
</li>
</ul>
<h3 id="3_5">3. 方法设计详解</h3>
<p><strong>流程总结</strong>：</p>
<p>论文提出的PKA框架是对DiT标准注意力机制的改进，旨在高效处理文本（T）、空间条件（SP）和主体条件（SJ）这三种模态。其核心思想是将原本的全局注意力分解为更轻量级的、针对不同模态特性的注意力计算。</p>
<ol>
<li>
<p><strong>条件模态的KV缓存（Condition KV Caching）</strong>：</p>
<ul>
<li><strong>动机</strong>：在DiT的去噪过程中，条件信息（SP, SJ）在不同时间步（timestep）下是固定的，而只有噪声图像（X）在不断变化。重复计算固定条件的Key（K）和Value（V）是低效的。</li>
<li><strong>操作</strong>：在第一个去噪步骤（t=0）中，计算所有条件模态（SP, SJ）的Key和Value投影。这些投影结果被缓存（cached）下来。在后续的去噪步骤（t=1, 2, ..., T-1）中，直接复用这些缓存的KV，而无需重新计算。</li>
<li><strong>效果</strong>：这消除了跨去噪轨迹的冗余计算，显著降低了条件模态的计算开销。图4(a)和(b)展示了这一过程。</li>
</ul>
</li>
<li>
<p><strong>噪声图像与条件模态的交互（Noisy Image Interaction with Conditions）</strong>：</p>
<ul>
<li><strong>核心</strong>：在缓存了条件模态的KV后，PKA将噪声图像（X）的Query（Q）与条件模态的KV进行交互。这里引入了两种新的注意力机制来替代标准的全局交叉注意力：PAA和KSA。</li>
<li><strong>文本模态（T）</strong>：文本模态（T）仍然与噪声图像（X）进行标准的、完整的自注意力（self-attention）计算。这是因为文本通常提供全局的语义指导，需要与图像的各个部分进行交互。</li>
</ul>
</li>
<li>
<p><strong>位置对齐注意力（Position-Aligned Attention, PAA）</strong>（用于空间条件 SP）：</p>
<ul>
<li><strong>动机</strong>：空间对齐条件（如布局图、深度图）的有效交互主要发生在空间上对应的或邻近的区域。全局注意力在非对齐区域的计算是冗余的。</li>
<li><strong>设计</strong>：PAA强制实现一种“一对一”的位置映射。对于噪声图像中的每一个像素块（patch）<script type="math/tex">X_i</script>（位于坐标 <script type="math/tex">i</script>），它只与空间条件（SP）中对应坐标 <script type="math/tex">i</script> 的Token进行交互。</li>
<li><strong>公式</strong>：
    <script type="math/tex; mode=display"> \text{PAA}(X_i; \text{SP}) = \text{Softmax}\left(\frac{Q_{X_i} K_{\text{SP},i}^T}{\sqrt{d}}\right) V_{\text{SP},i} </script>
    其中，<script type="math/tex">Q_{X_i}</script> 是图像Token <script type="math/tex">X_i</script> 的Query，<script type="math/tex">K_{\text{SP},i}</script> 和 <script type="math/tex">V_{\text{SP},i}</script> 是空间条件在位置 <script type="math/tex">i</script> 处的Key和Value。</li>
<li><strong>效果</strong>：将原本的O(N²)复杂度降低到O(N)，其中N是图像Token的数量。这使得模型能够线性扩展到任意数量的空间条件。图4(c)展示了PAA的机制。</li>
</ul>
</li>
<li>
<p><strong>关键词筛选注意力（Keyword-Scoped Attention, KSA）</strong>（用于主体条件 SJ）：</p>
<ul>
<li><strong>动机</strong>：主体驱动的条件（如参考图像、对象掩码）的有效交互通常集中在与文本提示中关键词相关的特定区域，而非整个图像。全局注意力会浪费计算在无关的背景区域。</li>
<li><strong>设计</strong>：KSA通过一个两阶段的过程实现：<ul>
<li><strong>掩码生成（Mask Generation）</strong>：首先，利用文本提示（T）和噪声图像（X）之间的初步注意力（通过一个简单的文本-图像注意力计算，如式(3)所示）来识别与关键词最相关的图像区域。这个过程会生成一个二值语义掩码 <script type="math/tex">M^t</script>。
    <script type="math/tex; mode=display"> M^t = \mathbb{I}\left(\text{Softmax}\left(\frac{Q_X K_T^T}{\sqrt{d}}\right) \ge \epsilon\right) </script>
    其中，<script type="math/tex">Q_X</script> 是图像Token的Query，<script type="math/tex">K_T</script> 是文本Token的Key，<script type="math/tex">\epsilon</script> 是一个阈值（例如0.2）。这个掩码会过滤掉与关键词不相关的图像区域。</li>
<li><strong>掩码应用（Mask Application）</strong>：然后，在后续的去噪步骤中，将这个生成的掩码 <script type="math/tex">M^t</script> 应用于图像Token的Query（<script type="math/tex">Q_X^{t+1}</script>），只允许与掩码区域对应的图像Token与主体条件（SJ）进行注意力计算。
    <script type="math/tex; mode=display"> \text{KSA}(X; \text{SJ}) = \text{Softmax}\left(\frac{(Q_X^{t+1} \odot M^t) K_{\text{SJ}}^T}{\sqrt{d}}\right) V_{\text{SJ}} </script>
    其中，<script type="math/tex">\odot</script> 表示逐元素乘法。</li>
</ul>
</li>
<li><strong>效果</strong>：KSA通过动态地聚焦于与主体相关的稀疏区域，显著减少了不必要的计算量，同时保留了对主体身份的精确控制。图4(d)展示了KSA的机制。</li>
</ul>
</li>
<li>
<p><strong>条件敏感度感知采样（Conditional Sensitivity-Aware Sampling, CSAS）</strong>：</p>
<ul>
<li><strong>动机</strong>：传统的Log-uniform采样策略（在Flow Matching中常用）将训练样本的去噪时间步（timestep）均匀地分布在整个轨迹上，但对于多条件控制，模型对条件的敏感度在不同时间步上是不均匀的。研究发现，模型在早期（高噪声阶段）对条件的变化更为敏感。</li>
<li><strong>设计</strong>：CSAS采用一种<strong>移位（shifted）的Logit-Normal分布</strong>来采样时间步 <script type="math/tex">t</script>：
    <script type="math/tex; mode=display"> t \sim \text{Logit-N}(\mu, \sigma^2), \quad \text{with } \mu > 0, \sigma > 1 </script>
    通过调整均值 <script type="math/tex">\mu</script> 使其大于0，CSAS将采样概率向高噪声阶段（早期时间步）倾斜。</li>
<li><strong>效果</strong>：这种策略将更多的训练资源（梯度信号）集中在模型对条件最敏感的阶段，从而加速了收敛，并提高了条件控制的保真度。图13和图14展示了CSAS的有效性。</li>
</ul>
</li>
</ol>
<p><strong>模型结构</strong>：</p>
<p>PKA不是一个全新的模型架构，而是对现有DiT模型（如Flux.1, Stable Diffusion 3）的注意力模块进行的替换和改进。
*   <strong>输入</strong>：文本Token (T)，噪声图像Token (X)，空间条件Token (SP)，主体条件Token (SJ)。
*   <strong>核心模块</strong>：
    *   <strong>条件模态（SP, SJ）</strong>：在第一个去噪步骤计算KV，并缓存。
    *   <strong>文本模态（T）</strong>：与噪声图像（X）进行标准的全局自注意力。
    *   <strong>噪声图像（X）</strong>：
        *   与文本（T）进行全局自注意力。
        *   与空间条件（SP）通过PAA进行一对一的局部注意力。
        *   与主体条件（SJ）通过KSA进行关键词筛选后的局部注意力。
*   <strong>输出</strong>：经过条件引导的去噪后的图像表示。</p>
<p><strong>算法解释</strong>：</p>
<ul>
<li><strong>PAA的“一对一”映射</strong>：其核心在于将原本的全局注意力计算（图像Token <script type="math/tex">X_i</script> 与所有空间条件Token <script type="math/tex">SP_j</script> 计算注意力）简化为只计算图像Token <script type="math/tex">X_i</script> 与空间条件Token <script type="math/tex">SP_i</script> 之间的注意力。这就像是为图像的每个像素块找到了一个“专属”的空间条件信息源。</li>
<li><strong>KSA的“关键词筛选”</strong>：通过一个简单的文本-图像注意力计算，找到与文本关键词最相关的图像区域，然后用这个区域信息来“裁剪”后续的注意力计算范围。这就像是让模型在看主体条件时，只关注文本提示中提到的那个“主体”所在的位置。</li>
<li><strong>CSAS的“敏感度采样”</strong>：想象一下，模型在学习如何根据条件生成图像时，就像在学习一个技能。这个技能在初学阶段（高噪声）最容易受到指导（条件），而在熟练阶段（低噪声）则更多依赖自身“内功”。CSAS就是让模型在初学阶段花更多时间练习，从而更快地掌握核心技能。</li>
</ul>
<h3 id="4_4">4. 方法对比分析</h3>
<ul>
<li>
<p><strong>本质区别</strong>：</p>
<ul>
<li><strong>与“拼接-注意力”方法的区别</strong>：现有方法将所有模态（文本、图像、条件）全部拼接在一起，然后进行全局的自注意力计算。PKA则将注意力计算分解，对不同模态采用不同的、更高效的交互策略（KV缓存、PAA、KSA），并避免了全局拼接。</li>
<li><strong>与现有高效注意力方法的区别</strong>：一些高效注意力方法（如稀疏注意力、局部注意力）可能只关注计算效率，但PKA在效率提升的同时，还特别关注了<strong>多模态交互的冗余性</strong>，并针对性地设计了PAA和KSA来消除这种冗余。PAA利用空间对齐的先验，KSA利用语义稀疏性。</li>
<li><strong>与ControlNet等方法的区别</strong>：ControlNet等方法通常在UNet架构中进行特征融合，而PKA是在Transformer（DiT）架构中通过改进注意力机制来实现多条件控制。</li>
</ul>
</li>
<li>
<p><strong>创新贡献</strong>：</p>
<ul>
<li><strong>PKA框架</strong>：首次系统性地分析了多条件DiT中注意力机制的空间和语义冗余，并提出了针对性的PAA和KSA模块来消除这些冗余。</li>
<li><strong>PAA</strong>：通过强制位置对齐，实现了空间条件控制的线性计算复杂度。</li>
<li><strong>KSA</strong>：利用文本提示动态生成掩码，实现了主体条件控制的稀疏化计算。</li>
<li><strong>CSAS</strong>：通过移位Logit-Normal分布，优化了多条件DiT的训练采样策略，加速了收敛并提升了控制保真度。</li>
<li><strong>效率提升</strong>：在保持甚至提升生成质量的同时，实现了显著的推理速度提升和VRAM节省。</li>
</ul>
</li>
<li>
<p><strong>适用场景</strong>：</p>
<ul>
<li><strong>多条件图像生成</strong>：尤其适用于需要同时集成文本、空间布局（如草图、深度图、边缘图）和主体参考（如参考图像、对象掩码）的场景。</li>
<li><strong>资源受限环境</strong>：由于其高效性，非常适合在计算资源有限的设备上进行部署和推理。</li>
<li><strong>需要快速迭代的场景</strong>：显著的推理速度提升使得用户可以更快地获得生成结果，便于进行参数调整和创意探索。</li>
</ul>
</li>
</ul>
<h3 id="5_4">5. 实验分析</h3>
<ul>
<li>
<p><strong>验证方法</strong>：</p>
<ul>
<li><strong>效率评估</strong>：通过在不同数量的空间条件下的推理延迟和VRAM消耗进行测量，与UniCombine, OminiControl2, PixelPonder, EasyControl等基线方法进行对比（图6）。</li>
<li><strong>生成质量评估</strong>：使用FID、SSIM、CLIP-I、DINOv2、CLIP-T等指标，在Subject-Canny, Subject-Depth, Multi-Spatial等任务上进行量化评估（表1）。</li>
<li><strong>定性评估</strong>：通过生成图像的视觉对比，展示在Subject-Spatial和Multi-Spatial场景下，PKA在细节保真度、颜色丰富度和整体质量上的优势（图7, 图8）。</li>
<li><strong>消融实验</strong>：<ul>
<li><strong>PAA消融</strong>：对比了标准DiT（W/o PAA）、Sliding Window Attention（SWA）以及PKA的PAA模块，在定性（图9）和效率（图10）上进行了分析。</li>
<li><strong>KSA消融</strong>：通过调整KSA的阈值 <script type="math/tex">\epsilon</script>，分析其对效率和主体保真度的影响（图11, 图12）。</li>
<li><strong>CSAS消融</strong>：对比了不同采样策略（Ours, Standard, Reversed）在收敛速度和结构保真度上的表现（图13, 图14）。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>关键结果</strong>：</p>
<ul>
<li><strong>效率</strong>：在16个条件时，推理延迟比UniCombine低10.0倍，VRAM消耗低5.1倍。</li>
<li><strong>质量</strong>：在Subject-Canny和Multi-Spatial任务上，PKA取得了最低的FID分数（52.99和53.01），表明生成图像的真实感更强。</li>
<li><strong>一致性</strong>：在Subject-Canny和Subject-Depth任务上，PKA在CLIP-I和DINOv2指标上表现最佳，表明其能更好地保留参考主体的身份。</li>
<li><strong>可控性</strong>：在Subject-Depth和Multi-Spatial任务上，PKA在MSE（深度控制）和F1 Score（边缘控制）上表现优异。</li>
<li><strong>CSAS</strong>：显著加速了收敛速度，并在早期迭代就达到了更高的SSIM分数。</li>
</ul>
</li>
<li>
<p><strong>优势场景</strong>：</p>
<ul>
<li><strong>高条件数量场景</strong>：图6清晰展示了PKA在条件数量增加时，效率优势愈发明显。</li>
<li><strong>需要精确空间布局控制的场景</strong>：PAA确保了空间对齐的精确性。</li>
<li><strong>需要精确主体身份保持的场景</strong>：KSA通过聚焦关键区域，有效防止了主体身份的漂移。</li>
<li><strong>需要快速训练收敛的场景</strong>：CSAS策略显著缩短了训练时间。</li>
</ul>
</li>
<li>
<p><strong>局限性</strong>：</p>
<ul>
<li><strong>KSA的阈值选择</strong>：KSA的阈值 <script type="math/tex">\epsilon</script> 需要根据具体任务和对细节的要求进行调整，可能存在一定的调参成本。</li>
<li><strong>对文本提示的依赖</strong>：KSA生成掩码依赖于文本提示的准确性，如果文本提示模糊或不准确，可能影响掩码的质量。</li>
<li><strong>潜在的语义丢失</strong>：虽然KSA旨在保留主体身份，但过度激进的阈值设置可能会过滤掉一些非核心但对整体美学有贡献的细节（如图11所示）。</li>
</ul>
</li>
</ul>
<h3 id="6_4">6. 实用指南</h3>
<ul>
<li><strong>开源情况</strong>：论文作者表示“代码将在论文发表后作为开源发布，以支持可复现性和进一步的研究。”（Code Availability部分）。</li>
<li><strong>实现细节</strong>：<ul>
<li><strong>KV缓存</strong>：在训练和推理时，需要实现KV的缓存和复用机制。</li>
<li><strong>PAA</strong>：需要修改注意力模块，使其只计算对应位置的注意力。</li>
<li><strong>KSA</strong>：需要实现掩码生成和应用逻辑。这包括计算文本-图像注意力，设定阈值 <script type="math/tex">\epsilon</script>，以及在后续步骤中应用掩码。论文中提到 <script type="math/tex">\epsilon=0.2</script> 是一个不错的起点。</li>
<li><strong>CSAS</strong>：需要修改采样器，使用移位的Logit-Normal分布来生成时间步。论文中给出了参数示例：<script type="math/tex">\mu=0.5, \sigma=1.5</script>。</li>
<li><strong>模型选择</strong>：论文基于DiT架构，因此可以将其集成到现有的DiT实现中。</li>
</ul>
</li>
<li><strong>迁移可能</strong>：<ul>
<li><strong>其他Transformer模型</strong>：PKA的核心思想（分解注意力、消除冗余）可以迁移到其他基于Transformer的生成模型中，例如用于多模态理解或生成任务的Vision Transformer（ViT）变体。</li>
<li><strong>其他生成模型</strong>：PAA和KSA的思路，即利用先验知识（空间对齐、语义稀疏性）来优化注意力计算，也可以启发其他类型的生成模型（如GANs中的注意力模块）的改进。</li>
<li><strong>视频生成</strong>：论文在结论中提到，PKA的原理可以应用于视频生成，通过强制时间一致性来降低计算成本。这表明其在序列建模领域有潜在应用。</li>
</ul>
</li>
</ul>
<h3 id="7_4">7. 总结</h3>
<ul>
<li><strong>核心思想</strong>：分解注意力，消除冗余，提效增质。</li>
<li><strong>速记版pipeline</strong>：<ol>
<li><strong>缓存条件信息</strong>：只计算一次条件信息的Key/Value。</li>
<li><strong>空间对齐</strong>：图像块只看对应位置的空间条件。</li>
<li><strong>筛选主体区域</strong>：根据文本提示，只关注主体所在区域。</li>
<li><strong>优化采样</strong>：训练时多关注模型敏感的早期阶段。</li>
</ol>
</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To this end, we propose Position-aligned and Keyword-scoped Attention (PKA), a highly efficient framework designed to eliminate these redundancies.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2602.06850v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2602.06850v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2602.06830v1'></a></p>
<h2 id="gaussianpop-principled-simplification-framework-for-compact-3d-gaussian-splatting-via-error-quantification"><a href="https://arxiv.org/abs/2602.06830v1">GaussianPOP: Principled Simplification Framework for Compact 3D Gaussian Splatting via Error Quantification</a></h2>
<p><strong>Authors:</strong> Soonbin Lee, Yeong-Gyu Kim, Simon Sasse, Tomas M. Borges, Yago Sanchez, Eun-Seok Ryu, Thomas Schierl, Cornelius Hellge</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Existing 3D Gaussian Splatting simplification methods commonly use importance scores, such as blending weights or sensitivity, to identify redundant Gaussians. However, these scores are not driven by visual error metrics, often leading to suboptimal trade-offs between compactness and rendering fidelity. We present GaussianPOP, a principled simplification framework based on analytical Gaussian error quantification. Our key contribution is a novel error criterion, derived directly from the 3DGS rendering equation, that precisely measures each Gaussian's contribution to the rendered image. By introducing a highly efficient algorithm, our framework enables practical error calculation in a single forward pass. The framework is both accurate and flexible, supporting on-training pruning as well as post-training simplification via iterative error re-quantification for improved stability. Experimental results show that our method consistently outperforms existing state-of-the-art pruning methods across both application scenarios, achieving a superior trade-off between model compactness and high rendering quality.</p>
<p><strong>Analysis:</strong></p>
<p>作为一名计算机视觉和机器学习领域的专家，我对这篇论文摘要进行了深入分析。以下是我的评估：</p>
<p><strong>1. 论文的主要贡献（2-3句话的简洁总结）</strong></p>
<p>该论文提出了一种名为 GaussianPOP 的新颖框架，用于简化 3D 高斯溅射（3DGS）模型。与现有方法依赖于不直接与视觉误差相关的启发式分数不同，GaussianPOP 基于对高斯误差的分析量化，直接从 3DGS 渲染方程推导出误差度量。这使得能够更精确地评估每个高斯对渲染图像的贡献，从而实现模型紧凑性和渲染质量之间更优的权衡。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>GaussianPOP 的核心创新在于其<strong>基于分析高斯误差量化的原则性简化框架</strong>。具体来说，其关键创新点包括：</p>
<ul>
<li><strong>新颖的误差度量标准：</strong> 该方法推导出了一个直接源自 3DGS 渲染方程的误差度量。这个度量能够精确地量化每个高斯对最终渲染图像的贡献，而不仅仅是依赖于诸如混合权重或敏感度等间接指标。</li>
<li><strong>高效的单次前向传播计算：</strong> 论文提出了一个高效的算法，能够在一次前向传播中计算出这种误差度量。这对于实际应用至关重要，因为它避免了计算上的瓶颈，使得误差计算变得可行。</li>
<li><strong>灵活的简化策略：</strong> GaussianPOP 支持两种主要的简化方式：<ul>
<li><strong>训练中剪枝（On-training pruning）：</strong> 在模型训练过程中就进行高斯粒子的移除。</li>
<li><strong>训练后简化（Post-training simplification）：</strong> 在模型训练完成后，通过迭代地重新量化误差来移除冗余高斯，以提高稳定性。</li>
</ul>
</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<p>GaussianPOP 的潜在影响是显著的，主要体现在以下几个方面：</p>
<ul>
<li><strong>提升 3DGS 模型的可部署性：</strong> 3DGS 模型通常包含大量的高斯粒子，导致模型体积庞大，难以在资源受限的设备上部署。GaussianPOP 提供的有效简化方法将显著减小模型尺寸，使其更易于在移动设备、AR/VR 设备等场景中应用。</li>
<li><strong>改善模型压缩与渲染质量的权衡：</strong> 通过更精确的误差度量，GaussianPOP 能够更智能地移除冗余信息，从而在保持高渲染质量的同时实现更高的压缩率。这将为需要平衡视觉保真度和效率的应用带来福音。</li>
<li><strong>推动 3DGS 技术的标准化和普及：</strong> 更易于管理和部署的 3DGS 模型将加速该技术在游戏、电影制作、虚拟现实、数字孪生等领域的应用和普及。</li>
<li><strong>为其他基于高斯表示的 3D 重建方法提供借鉴：</strong> 其分析误差量化的思想可以推广到其他使用高斯或其他基函数表示的 3D 重建和表示方法中。</li>
</ul>
<p><strong>4. 可能受益于此研究的相关领域或应用</strong></p>
<ul>
<li><strong>实时渲染和游戏开发：</strong> 减小 3DGS 模型尺寸对于实现高质量的实时渲染至关重要，尤其是在游戏引擎和交互式应用中。</li>
<li><strong>增强现实（AR）和虚拟现实（VR）：</strong> AR/VR 设备通常计算能力有限，轻量级的 3DGS 模型能够提供更流畅、更逼真的沉浸式体验。</li>
<li><strong>数字孪生和工业可视化：</strong> 构建和维护大型数字孪生模型需要高效的表示和压缩技术，GaussianPOP 可以帮助创建更易于管理和传输的数字孪生。</li>
<li><strong>3D 内容创作和编辑：</strong> 简化后的 3DGS 模型可以更容易地进行编辑和修改，降低 3D 内容创作的门槛。</li>
<li><strong>自动驾驶和机器人感知：</strong> 实时、准确的 3D 环境感知对于自动驾驶和机器人导航至关重要，轻量级的 3DGS 模型可以加速这一过程。</li>
<li><strong>3D 扫描和重建：</strong> 提高 3D 扫描数据的处理效率和存储能力。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<p>尽管摘要听起来非常乐观，但基于摘要信息，我们可以推断出一些潜在的局限性：</p>
<ul>
<li><strong>计算成本的权衡：</strong> 虽然论文声称误差计算是“高效的”并且“在单次前向传播中”，但与直接使用启发式分数相比，分析误差量化可能仍然需要一定的计算开销。在某些极端实时性要求极高的场景下，这种开销是否可接受仍需验证。</li>
<li><strong>对初始模型质量的依赖：</strong> 任何简化方法都可能在一定程度上依赖于初始 3DGS 模型的质量。如果初始模型本身存在严重的问题，简化效果可能会受到影响。</li>
<li><strong>误差度量的普适性：</strong> 论文提出的误差度量是“直接从 3DGS 渲染方程推导出的”。虽然这保证了其与 3DGS 的契合度，但其普适性如何，是否能直接应用于其他基于不同渲染方程或不同表示方法的 3D 重建技术，尚不明确。</li>
<li><strong>对不同场景的适应性：</strong> 摘要提到“跨越两种应用场景”，但具体是哪两种场景，以及该方法在不同复杂度的场景（例如，纹理丰富、几何复杂、动态场景等）下的表现如何，需要进一步的实验验证。</li>
<li><strong>“Principled”的定义：</strong> “Principled”通常意味着有坚实的理论基础。虽然论文声称是“基于分析高斯误差量化”，但其理论的严谨性和完整性还需要通过论文的详细内容来评估。</li>
</ul>
<p>总而言之，GaussianPOP 似乎是一项非常有前景的研究，它通过引入一种更具理论基础和计算效率的误差量化方法，有望显著改善 3D 高斯溅射模型的压缩和部署能力，从而推动该技术在更广泛领域的应用。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present GaussianPOP, a principled simplification framework based on analytical Gaussian error quantification.</li>
<li>Our key contribution is a novel error criterion, derived directly from the 3DGS rendering equation, that precisely measures each Gaussian's contribution to the rendered image.</li>
<li>Experimental results show that our method consistently outperforms existing state-of-the-art pruning methods across both application scenarios, achieving a superior trade-off between model compactness and high rendering quality.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2602.06830v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2602.06830v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2026-02-09 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
