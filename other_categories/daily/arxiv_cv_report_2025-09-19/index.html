<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-09-19 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-15
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-09-18/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-09-22/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-09-19">Arxiv Computer Vision Papers - 2025-09-19</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#arxiv-2025-09-17" class="nav-link">Arxiv 计算机视觉每日报告执行摘要 (2025-09-17)</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#sail-vl2-technical-report" class="nav-link">SAIL-VL2 Technical Report</a>
                </li>
                <li class="nav-item">
                    <a href="#scalecua-scaling-open-source-computer-use-agents-with-cross-platform-data" class="nav-link">ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data</a>
                </li>
                <li class="nav-item">
                    <a href="#lightweight-and-accurate-multi-view-stereo-with-confidence-aware-diffusion-model" class="nav-link">Lightweight and Accurate Multi-View Stereo with Confidence-Aware Diffusion Model</a>
                </li>
                <li class="nav-item">
                    <a href="#out-of-sight-trajectories-tracking-fusion-and-prediction" class="nav-link">Out-of-Sight Trajectories: Tracking, Fusion, and Prediction</a>
                </li>
                <li class="nav-item">
                    <a href="#unleashing-the-potential-of-multimodal-llms-for-zero-shot-spatio-temporal-video-grounding" class="nav-link">Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding</a>
                </li>
                <li class="nav-item">
                    <a href="#a-race-bias-free-face-aging-model-for-reliable-kinship-verification" class="nav-link">A Race Bias Free Face Aging Model for Reliable Kinship Verification</a>
                </li>
                <li class="nav-item">
                    <a href="#m4diffuser-multi-view-diffusion-policy-with-manipulability-aware-control-for-robust-mobile-manipulation" class="nav-link">M4Diffuser: Multi-View Diffusion Policy with Manipulability-Aware Control for Robust Mobile Manipulation</a>
                </li>
                <li class="nav-item">
                    <a href="#roboeye-enhancing-2d-robotic-object-identification-with-selective-3d-geometric-keypoint-matching" class="nav-link">RoboEye: Enhancing 2D Robotic Object Identification with Selective 3D Geometric Keypoint Matching</a>
                </li>
                <li class="nav-item">
                    <a href="#df-llava-unlocking-mllms-potential-for-synthetic-image-detection-via-prompt-guided-knowledge-injection" class="nav-link">DF-LLaVA: Unlocking MLLM's potential for Synthetic Image Detection via Prompt-Guided Knowledge Injection</a>
                </li>
                <li class="nav-item">
                    <a href="#chain-of-thought-re-ranking-for-image-retrieval-tasks" class="nav-link">Chain-of-Thought Re-ranking for Image Retrieval Tasks</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-09-19">Arxiv Computer Vision Papers - 2025-09-19</h1>
<h2 id="executive-summary">Executive Summary</h2>
<h2 id="arxiv-2025-09-17">Arxiv 计算机视觉每日报告执行摘要 (2025-09-17)</h2>
<p><strong>概述：</strong></p>
<p>今天的 Arxiv 计算机视觉论文主要围绕<strong>多模态学习、具身智能（特别是机器人操作）、3D 视觉以及对模型鲁棒性和公平性的关注</strong>。大型语言模型（LLMs）在视觉任务中的应用持续深化，尤其是在零样本学习和知识注入方面。</p>
<p><strong>主要主题和趋势：</strong></p>
<ol>
<li><strong>多模态LLMs的融合与应用：</strong> 多个工作探索了多模态LLMs（MLLMs）在视频理解（零样本时空视频定位）、合成图像检测以及图像检索中的潜力，通过提示引导知识注入和思维链重排序等技术提升性能。</li>
<li><strong>具身智能与机器人操作：</strong> 机器人操作和感知是显著主题，包括利用多视图扩散策略进行鲁棒移动操作、通过3D几何关键点匹配增强2D对象识别，以及构建跨平台数据以扩展计算机使用代理。</li>
<li><strong>3D 视觉与重建：</strong> 3D 视觉领域有进展，如利用置信度感知扩散模型实现轻量级和准确的多视图立体，以及在跟踪、融合和预测中处理“视线外”轨迹。</li>
<li><strong>模型鲁棒性与公平性：</strong> 有论文关注模型在特定场景下的鲁棒性（如合成图像检测）和公平性（如消除人脸老化模型中的种族偏见）。</li>
<li><strong>大规模模型与数据：</strong> SAIL-VL2 和 ScaleCUA 等工作表明了构建和扩展大规模视觉语言模型及跨平台数据的持续努力。</li>
</ol>
<p><strong>特别重要或创新的论文：</strong></p>
<ul>
<li><strong>"Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding" (Zaiquan Yang et al.)：</strong> 这篇论文展示了MLLMs在复杂视频理解任务（零样本时空视频定位）中的强大能力，预示了MLLMs在更高级别视频推理中的巨大潜力。</li>
<li><strong>"M4Diffuser: Multi-View Diffusion Policy with Manipulability-Aware Control for Robust Mobile Manipulation" (Ju Dong et al.)：</strong> 将多视图扩散模型与可操作性感知控制相结合，为机器人鲁棒移动操作提供了新颖且高效的解决方案，对具身智能领域具有重要意义。</li>
<li><strong>"Lightweight and Accurate Multi-View Stereo with Confidence-Aware Diffusion Model" (Fangjinhua Wang et al.)：</strong> 引入置信度感知扩散模型来提升多视图立体的准确性和效率，为3D重建领域提供了一个有前景的轻量级方法。</li>
<li><strong>"DF-LLaVA: Unlocking MLLM's potential for Synthetic Image Detection via Prompt-Guided Knowledge Injection" (Zhuokang Shen et al.)：</strong> 巧妙地利用提示引导的知识注入来增强MLLM在合成图像检测这一关键安全任务上的能力，展示了MLLM在特定领域应用中的灵活性。</li>
</ul>
<p><strong>新兴研究方向或技术：</strong></p>
<ul>
<li><strong>MLLMs在零样本和少样本视频理解中的深化应用：</strong> 尤其是时空定位和复杂事件推理。</li>
<li><strong>扩散模型在3D视觉和机器人控制中的多功能性：</strong> 不仅用于生成，还用于提高重建精度和策略学习的鲁棒性。</li>
<li><strong>具身智能中跨平台数据和通用代理的构建：</strong> 旨在实现更广泛、更通用的机器人能力。</li>
<li><strong>通过提示工程和知识注入来定制和增强MLLMs：</strong> 以解决特定下游任务，如检测虚假信息。</li>
<li><strong>对模型公平性和偏见的持续关注：</strong> 尤其是在敏感应用如人脸识别中。</li>
</ul>
<p><strong>建议阅读全文的论文：</strong></p>
<p>对于不同兴趣的研究人员，建议阅读以下论文：</p>
<ul>
<li><strong>对多模态LLMs和视频理解感兴趣：</strong> "Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding" (Zaiquan Yang et al.)</li>
<li><strong>对机器人操作和具身智能感兴趣：</strong> "M4Diffuser: Multi-View Diffusion Policy with Manipulability-Aware Control for Robust Mobile Manipulation" (Ju Dong et al.)</li>
<li><strong>对3D重建和扩散模型感兴趣：</strong> "Lightweight and Accurate Multi-View Stereo with Confidence-Aware Diffusion Model" (Fangjinhua Wang et al.)</li>
<li><strong>对MLLMs在安全和检测任务中的应用感兴趣：</strong> "DF-LLaVA: Unlocking MLLM's potential for Synthetic Image Detection via Prompt-Guided Knowledge Injection" (Zhuokang Shen et al.)</li>
<li><strong>对大规模模型和数据扩展感兴趣：</strong> "SAIL-VL2 Technical Report" (Weijie Yin et al.) 和 "ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data" (Zhaoyang Liu et al.)</li>
</ul>
<p>这份摘要旨在帮助您快速把握今日Arxiv计算机视觉领域的关键进展，为您的研究提供有价值的参考。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2509.14033v1">SAIL-VL2 Technical Report</a></li>
<li><a href="#2509.15221v1">ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data</a></li>
<li><a href="#2509.15220v1">Lightweight and Accurate Multi-View Stereo with Confidence-Aware Diffusion Model</a></li>
<li><a href="#2509.15219v1">Out-of-Sight Trajectories: Tracking, Fusion, and Prediction</a></li>
<li><a href="#2509.15178v1">Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding</a></li>
<li><a href="#2509.15177v1">A Race Bias Free Face Aging Model for Reliable Kinship Verification</a></li>
<li><a href="#2509.14980v1">M4Diffuser: Multi-View Diffusion Policy with Manipulability-Aware Control for Robust Mobile Manipulation</a></li>
<li><a href="#2509.14966v1">RoboEye: Enhancing 2D Robotic Object Identification with Selective 3D Geometric Keypoint Matching</a></li>
<li><a href="#2509.14957v1">DF-LLaVA: Unlocking MLLM's potential for Synthetic Image Detection via Prompt-Guided Knowledge Injection</a></li>
<li><a href="#2509.14746v1">Chain-of-Thought Re-ranking for Image Retrieval Tasks</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2509.14033v1'></a></p>
<h2 id="sail-vl2-technical-report"><a href="https://arxiv.org/abs/2509.14033v1">SAIL-VL2 Technical Report</a></h2>
<p><strong>Authors:</strong> Weijie Yin, Yongjie Ye, Fangxun Shu, Yue Liao, Zijian Kang, Hongyuan Dong, Haiyang Yu, Dingkang Yang, Jiacong Wang, Han Wang, Wenzhuo Liu, Xiao Liang, Shuicheng Yan, Chao Feng</p>
<p><strong>Published:</strong> 2025-09-17</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM)
for comprehensive multimodal understanding and reasoning. As the successor to
SAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8B
parameter scales across diverse image and video benchmarks, demonstrating
strong capabilities from fine-grained perception to complex reasoning. Three
core innovations drive its effectiveness. First, a large-scale data curation
pipeline with scoring and filtering strategies enhances both quality and
distribution across captioning, OCR, QA, and video data, improving training
efficiency. Second, a progressive training framework begins with a powerful
pre-trained vision encoder (SAIL-ViT), advances through multimodal
pre-training, and culminates in a thinking-fusion SFT-RL hybrid paradigm that
systematically strengthens model capabilities. Third, architectural advances
extend beyond dense LLMs to efficient sparse Mixture-of-Experts (MoE) designs.
With these contributions, SAIL-VL2 demonstrates competitive performance across
106 datasets and achieves state-of-the-art results on challenging reasoning
benchmarks such as MMMU and MathVista. Furthermore, on the OpenCompass
leaderboard, SAIL-VL2-2B ranks first among officially released open-source
models under the 4B parameter scale, while serving as an efficient and
extensible foundation for the open-source multimodal community.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于Weijie Yin等人在计算机视觉和机器学习领域撰写的论文“SAIL-VL2 Technical Report”的全面摘要。</p>
<p><strong>论文摘要：SAIL-VL2技术报告</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决当前视觉-语言模型（LVM）在实现全面多模态理解和推理方面的挑战。尽管现有LVM在性能上有所提升，但仍面临计算效率、训练成本以及在细粒度感知和复杂推理任务中的局限性。SAIL-VL2的目标是开发一个高效、可扩展的开源LVM，能够实现最先进的性能，并推动多模态人工智能（AGI）的发展。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
SAIL-VL2的有效性主要由以下三项核心创新驱动：</p>
<ul>
<li><strong>大规模数据整理流水线：</strong> 论文设计了一个全面的数据整理流水线，通过评分和过滤策略，提升了图像、视频、OCR和问答数据的质量和分布。这显著提高了训练效率，并确保模型能够从高质量、多样化的数据中学习。</li>
<li><strong>渐进式训练框架：</strong> 训练过程分为三个阶段：<ul>
<li>首先，使用强大的预训练视觉编码器（SAIL-ViT）进行热身适应，将视觉输出粗粒度地适应到LLM域。</li>
<li>其次，通过多模态预训练进行细粒度对齐，解锁视觉编码器和适配器，以实现更深层次的对齐。</li>
<li>最后，通过思维融合（Thinking-Fusion）SFT-RL（监督微调-强化学习）混合范式，系统性地强化模型能力，使其能够进行复杂推理。</li>
</ul>
</li>
<li><strong>高效的稀疏混合专家（MoE）架构：</strong> SAIL-VL2超越了传统的密集LLM，采用了更高效的稀疏MoE设计。这使得模型能够在保持计算效率的同时，实现参数规模的扩展，并确保专家激活的平衡性和稳定性。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
SAIL-VL2在多个维度上展示了强大的性能：</p>
<ul>
<li><strong>最先进的性能：</strong> SAIL-VL2在2B和8B参数规模下，跨106个数据集实现了最先进的性能，涵盖了图像和视频基准测试，从细粒度感知到复杂推理都表现出强大的能力。</li>
<li><strong>卓越的推理能力：</strong> 在MMMU和MathVista等挑战性推理基准测试中，SAIL-VL2取得了最先进的结果。特别是，SAIL-VL2-Thinking模型在OpenCompass多模态推理基准测试中取得了领先结果，甚至超越了Gemini-2.0-Flash等强大的闭源模型。</li>
<li><strong>高效与可扩展性：</strong> SAIL-VL2-2B在OpenCompass排行榜上，在4B参数规模以下的开源模型中排名第一，证明了其作为高效且可扩展的多模态社区基础模型的潜力。</li>
<li><strong>细粒度视觉理解：</strong> 在OCR、高分辨率文档布局分析和复杂图表解释等任务中，SAIL-VL2表现出高保真度的感知能力，实现了超越同等规模模型的详细视觉基础。</li>
</ul>
<p><strong>4. 论文中提及的局限性：</strong>
论文中没有明确提及SAIL-VL2的显著局限性。然而，在数据整理部分，作者提到合成数据可能会引入语言表达上的分布偏差，因为LLM倾向于产生同质化的措辞，但他们发现模型仍能从大规模合成数据中受益。此外，在RL阶段，对于无法确定性验证真实答案的任务，需要LVM作为判断模型来提供奖励信号，这可能引入一定程度的依赖性或潜在偏差。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
论文指出，未来将继续通过以下方式进一步增强SAIL-VL系列：</p>
<ul>
<li><strong>更高效的架构：</strong> 持续探索和开发更高效的模型架构。</li>
<li><strong>全面的预训练策略：</strong> 进一步优化和完善预训练策略。</li>
<li><strong>改进的强化学习范式：</strong> 持续改进强化学习方法，以实现模型能力的持续演进。</li>
</ul>
<p>总而言之，SAIL-VL2代表了高效大型视觉-语言模型设计上的全面进步，通过在架构、训练策略和数据整理方面的创新，实现了效率和性能的持续提升，为开源多模态社区奠定了坚实的基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM)
for comprehensive multimodal understanding and reasoning.</li>
<li>As the successor to
SAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8B
parameter scales across diverse image and video benchmarks, demonstrating
strong capabilities from fine-grained perception to complex reasoning.</li>
<li>With these contributions, SAIL-VL2 demonstrates competitive performance across
106 datasets and achieves state-of-the-art results on challenging reasoning
benchmarks such as MMMU and MathVista.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.14033v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.14033v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.15221v1'></a></p>
<h2 id="scalecua-scaling-open-source-computer-use-agents-with-cross-platform-data"><a href="https://arxiv.org/abs/2509.15221v1">ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data</a></h2>
<p><strong>Authors:</strong> Zhaoyang Liu, JingJing Xie, Zichen Ding, Zehao Li, Bowen Yang, Zhenyu Wu, Xuehui Wang, Qiushi Sun, Shi Liu, Weiyun Wang, Shenglong Ye, Qingyun Li, Zeyue Tian, Gen Luo, Xiangyu Yue, Biqing Qi, Kai Chen, Bowen Zhou, Yu Qiao, Qifeng Chen, Wenhai Wang</p>
<p><strong>Published:</strong> 2025-09-18</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Vision-Language Models (VLMs) have enabled computer use agents (CUAs) that
operate GUIs autonomously, showing great potential, yet progress is limited by
the lack of large-scale, open-source computer use data and foundation models.
In this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. It
offers a large-scale dataset spanning 6 operating systems and 3 task domains,
built via a closed-loop pipeline uniting automated agents with human experts.
Trained on this scaled-up data, ScaleCUA can operate seamlessly across
platforms. Specifically, it delivers strong gains over baselines (+26.6 on
WebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-art
results (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% on
WebArena-Lite-v2). These findings underscore the power of data-driven scaling
for general-purpose computer use agents. We will release data, models, and code
to advance future research: https://github.com/OpenGVLab/ScaleCUA.</p>
<p><strong>Analysis:</strong></p>
<p>以下是关于Zhaoyang Liu等人撰写的论文“ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data”的摘要：</p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决当前计算机使用代理（CUAs）发展面临的核心挑战：缺乏大规模、开源的计算机使用数据和基础模型，这限制了视觉语言模型（VLMs）在自主操作图形用户界面（GUIs）方面的潜力。现有数据收集成本高昂、难以扩展且通用性差。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>大规模跨平台数据集：</strong> 引入ScaleCUA数据集，涵盖Windows、macOS、Linux、Android、iOS和Web六大操作系统，以及GUI理解、GUI定位和任务完成三大任务领域。
*   <strong>闭环数据收集管道：</strong> 采用自动化代理与人类专家相结合的闭环管道，高效收集高质量的原始轨迹数据，包括屏幕截图和结构化元数据，并进行标注和增强。
*   <strong>统一动作空间：</strong> 设计了一个跨平台的统一动作空间，使得代理能够以标准化方式与异构环境交互，简化了数据标注和策略学习。
*   <strong>ScaleCUA基础模型家族：</strong> 基于Qwen2.5-VL训练了一系列ScaleCUA基础代理模型，支持三种推理范式：定位模式（Grounding Mode）、直接动作模式（Direct Action Mode）和推理动作模式（Reasoned Action Mode），以实现感知、推理和动作的统一。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>卓越的性能提升：</strong> ScaleCUA在多个GUI基准测试中取得了显著的性能提升，例如在WebArena-Lite-v2上提升+26.6，在ScreenSpot-Pro上提升+10.7。
*   <strong>新的SOTA结果：</strong> 在MMBench-GUI L1-Hard上达到94.4%，在OSWorld-G上达到60.6%，在WebArena-Lite-v2上达到47.4%，均创下新的最先进（SOTA）记录。
*   <strong>数据驱动扩展的有效性：</strong> 实验结果强调了数据驱动扩展对于通用跨平台CUAs的强大作用，证明了多样化训练语料库能显著增强视觉理解能力。
*   <strong>推理模式的优势：</strong> 推理动作模式（RAM）在所有基准测试中均优于直接动作模式（DAM），尤其在复杂多步骤环境中表现突出，表明显式推理有助于维持任务连贯性并减少错误传播。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>代理收集数据质量：</strong> 自动化代理收集的数据质量仍落后于人类专家标注的数据，规则驱动的探索可能产生语义较弱的轨迹。
*   <strong>高级代理机制：</strong> 当前工作尚未整合反射、基于记忆的决策或分层规划等高级代理机制。
*   <strong>记忆机制的局限性：</strong> 当前记忆机制较为初级，将过往操作视为扁平历史，这限制了长周期推理能力。
*   <strong>跨应用泛化：</strong> 在长周期推理和跨应用泛化方面仍存在局限性。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>自改进学习循环：</strong> 有效结合自动化数据收集与迭代模型优化，形成一个自改进学习循环。
*   <strong>高级代理机制集成：</strong> 探索并整合强化学习、策略奖励模型（PRMs）、反射、记忆和分层规划等高级代理机制。
*   <strong>轻量级记忆系统：</strong> 开发能够捕获时间依赖关系的轻量级且有效的记忆系统。
*   <strong>数据混合策略：</strong> 针对通用多模态数据与GUI特定数据之间的冲突优化信号，需要更精细的数据混合策略。
*   <strong>原生桌面环境评估：</strong> 优先在原生桌面环境中执行基于Web的任务，可能需要开发新的基准测试。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. It
offers a large-scale dataset spanning 6 operating systems and 3 task domains,
built via a closed-loop pipeline uniting automated agents with human experts.</li>
<li>Specifically, it delivers strong gains over baselines (+26.6 on
WebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-art
results (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% on
WebArena-Lite-v2).</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.15221v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.15221v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.15220v1'></a></p>
<h2 id="lightweight-and-accurate-multi-view-stereo-with-confidence-aware-diffusion-model"><a href="https://arxiv.org/abs/2509.15220v1">Lightweight and Accurate Multi-View Stereo with Confidence-Aware Diffusion Model</a></h2>
<p><strong>Authors:</strong> Fangjinhua Wang, Qingshan Xu, Yew-Soon Ong, Marc Pollefeys</p>
<p><strong>Published:</strong> 2025-09-18</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>To reconstruct the 3D geometry from calibrated images, learning-based
multi-view stereo (MVS) methods typically perform multi-view depth estimation
and then fuse depth maps into a mesh or point cloud. To improve the
computational efficiency, many methods initialize a coarse depth map and then
gradually refine it in higher resolutions. Recently, diffusion models achieve
great success in generation tasks. Starting from a random noise, diffusion
models gradually recover the sample with an iterative denoising process. In
this paper, we propose a novel MVS framework, which introduces diffusion models
in MVS. Specifically, we formulate depth refinement as a conditional diffusion
process. Considering the discriminative characteristic of depth estimation, we
design a condition encoder to guide the diffusion process. To improve
efficiency, we propose a novel diffusion network combining lightweight 2D U-Net
and convolutional GRU. Moreover, we propose a novel confidence-based sampling
strategy to adaptively sample depth hypotheses based on the confidence
estimated by diffusion model. Based on our novel MVS framework, we propose two
novel MVS methods, DiffMVS and CasDiffMVS. DiffMVS achieves competitive
performance with state-of-the-art efficiency in run-time and GPU memory.
CasDiffMVS achieves state-of-the-art performance on DTU, Tanks &amp; Temples and
ETH3D. Code is available at: https://github.com/cvg/diffmvs.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Fangjinhua Wang等人撰写的论文“Lightweight and Accurate Multi-View Stereo with Confidence-Aware Diffusion Model”的全面摘要。</p>
<hr />
<h3 id="_1">论文摘要：基于置信度感知的扩散模型实现轻量级高精度多视角立体匹配</h3>
<p><strong>1. 解决的主要问题或研究问题：</strong>
该论文旨在解决多视角立体匹配（MVS）中3D几何重建的效率和精度问题。传统的MVS方法通常通过多视角深度估计和深度图融合来重建3D几何。为了提高计算效率，许多方法会先初始化一个粗略的深度图，然后逐步在高分辨率下进行细化。然而，现有的学习型MVS方法在处理光照变化、非朗伯表面和低纹理区域等挑战性场景时仍面临困难，并且在效率和精度之间存在权衡。特别是，将扩散模型引入MVS面临着如何有效利用条件信息、如何进行高效采样以及如何保持计算效率等挑战。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
该论文提出了一个新颖的MVS框架，将扩散模型引入MVS，并将其应用于深度细化任务。主要创新包括：</p>
<ul>
<li><strong>条件扩散过程的深度细化：</strong> 将深度细化任务重新定义为条件扩散过程，从随机噪声开始，通过迭代去噪逐步恢复深度图。</li>
<li><strong>条件编码器（Condition Encoder）：</strong> 设计了一个条件编码器，用于融合匹配信息、图像上下文和深度上下文特征，以指导扩散过程。这使得模型能够感知局部相似性和长距离上下文信息，从而生成准确的深度预测。</li>
<li><strong>基于置信度的采样策略（Confidence-based Sampling Strategy）：</strong> 引入了一种新颖的采样策略，根据扩散模型估计的置信度自适应地生成像素级的多个深度假设。这克服了传统方法固定采样范围的局限性，通过调整采样范围来捕获非局部一阶优化信息，从而提高去噪过程的效率和准确性。</li>
<li><strong>轻量级扩散网络（Lightweight Diffusion Network）：</strong> 提出了一种结合轻量级2D U-Net和卷积GRU的新型扩散网络。通过在单个扩散时间步内进行多迭代细化，并利用GRU捕获历史信息，显著提高了计算效率，避免了使用大型或堆叠U-Net。</li>
<li><strong>两种新型MVS方法：</strong> 基于该框架，提出了DiffMVS和CasDiffMVS。DiffMVS专注于实时应用，通过单阶段扩散模型进行深度细化；CasDiffMVS则通过两阶段级联扩散细化，旨在实现高精度重建。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
实验结果表明，该方法在多个基准测试上取得了显著的性能：</p>
<ul>
<li><strong>DiffMVS：</strong> 在运行时和GPU内存效率方面，DiffMVS达到了与现有技术相当的性能，同时在Tanks &amp; Temples和ETH3D数据集上取得了有竞争力的重建性能。</li>
<li><strong>CasDiffMVS：</strong> 在DTU、Tanks &amp; Temples和ETH3D数据集上，CasDiffMVS实现了最先进的重建性能，同时保持了高效率。</li>
<li><strong>效率优势：</strong> 相比于最先进的IterMVS方法，DiffMVS在GPU内存消耗上减少了9.13%，速度提升了69.49%。CasDiffMVS的效率与PatchmatchNet相当，但性能优于其他顶尖方法。</li>
<li><strong>泛化能力：</strong> 该方法在Tanks &amp; Temples和ETH3D等挑战性场景中表现出强大的零样本泛化能力，能够生成更完整、更准确的表面。</li>
<li><strong>消融研究：</strong> 验证了扩散机制、条件编码器（包括成本体、深度上下文和图像上下文）以及基于置信度的采样策略的有效性。结果表明，这些组件对于提高重建精度和泛化能力至关重要。</li>
</ul>
<p>这些结果的意义在于，该论文成功地将扩散模型引入MVS，并在效率和精度之间取得了新的平衡，为MVS领域提供了一个强大且轻量级的新基线。</p>
<p><strong>4. 论文中提及的局限性：</strong>
论文中没有明确提及具体的局限性，但从方法设计和实验设置中可以推断出一些潜在的方面：</p>
<ul>
<li><strong>超参数调优：</strong> 扩散模型的噪声尺度、采样步数等超参数需要仔细调优，以平衡性能和效率。虽然论文提供了默认设置，但在不同场景下可能需要进一步优化。</li>
<li><strong>训练数据依赖：</strong> 尽管在BlendedMVS上进行了微调以提高泛化能力，但作为学习型方法，其性能仍可能受到训练数据分布的影响。</li>
<li><strong>复杂场景的鲁棒性：</strong> 尽管在挑战性场景中表现良好，但极端情况（如极度低纹理、强反光等）下的鲁棒性仍有待进一步探索。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
该论文为MVS领域的未来研究开辟了多个方向：</p>
<ul>
<li><strong>更高效的扩散模型：</strong> 进一步探索更轻量级、更快的扩散模型架构，以适应更多资源受限的设备。</li>
<li><strong>自适应噪声调度：</strong> 研究更智能的噪声调度策略，使其能够根据场景特性或估计的深度不确定性自适应调整，进一步提高精度和鲁棒性。</li>
<li><strong>多模态融合：</strong> 探索将扩散模型与其他传感器数据（如LiDAR、IMU等）融合，以进一步提升MVS在复杂环境下的性能。</li>
<li><strong>实时MVS应用：</strong> 进一步优化DiffMVS，使其在实时MVS应用中发挥更大潜力，例如机器人导航、自动驾驶等。</li>
<li><strong>无监督或半监督学习：</strong> 探索在MVS中利用扩散模型进行无监督或半监督学习，以减少对大量标注数据的依赖。</li>
</ul>
<hr />
<p>这篇论文通过将扩散模型与MVS任务相结合，并引入一系列创新性的设计，为3D几何重建领域带来了新的突破，特别是在效率和精度方面取得了令人印象深刻的成果。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In
this paper, we propose a novel MVS framework, which introduces diffusion models
in MVS.</li>
<li>To improve
efficiency, we propose a novel diffusion network combining lightweight 2D U-Net
and convolutional GRU.</li>
<li>Moreover, we propose a novel confidence-based sampling
strategy to adaptively sample depth hypotheses based on the confidence
estimated by diffusion model.</li>
<li>Based on our novel MVS framework, we propose two
novel MVS methods, DiffMVS and CasDiffMVS.</li>
<li>DiffMVS achieves competitive
performance with state-of-the-art efficiency in run-time and GPU memory.</li>
<li>CasDiffMVS achieves state-of-the-art performance on DTU, Tanks &amp; Temples and
ETH3D.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.15220v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.15220v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.15219v1'></a></p>
<h2 id="out-of-sight-trajectories-tracking-fusion-and-prediction"><a href="https://arxiv.org/abs/2509.15219v1">Out-of-Sight Trajectories: Tracking, Fusion, and Prediction</a></h2>
<p><strong>Authors:</strong> Haichao Zhang, Yi Xu, Yun Fu</p>
<p><strong>Published:</strong> 2025-09-18</p>
<p><strong>Categories:</strong> cs.CV, cs.LG, cs.MA, cs.MM, cs.RO, 68T45, 68U10, 68T07, 68T40, 93C85, 93E11, 62M20, 62M10, 68U05, 94A12, F.2.2; I.2.9; I.2.10; I.4.1; I.4.8; I.4.9; I.5.4; I.3.7</p>
<p><strong>Abstract:</strong></p>
<p>Trajectory prediction is a critical task in computer vision and autonomous
systems, playing a key role in autonomous driving, robotics, surveillance, and
virtual reality. Existing methods often rely on complete and noise-free
observational data, overlooking the challenges associated with out-of-sight
objects and the inherent noise in sensor data caused by limited camera
coverage, obstructions, and the absence of ground truth for denoised
trajectories. These limitations pose safety risks and hinder reliable
prediction in real-world scenarios. In this extended work, we present
advancements in Out-of-Sight Trajectory (OST), a novel task that predicts the
noise-free visual trajectories of out-of-sight objects using noisy sensor data.
Building on our previous research, we broaden the scope of Out-of-Sight
Trajectory Prediction (OOSTraj) to include pedestrians and vehicles, extending
its applicability to autonomous driving, robotics, surveillance, and virtual
reality. Our enhanced Vision-Positioning Denoising Module leverages camera
calibration to establish a vision-positioning mapping, addressing the lack of
visual references, while effectively denoising noisy sensor data in an
unsupervised manner. Through extensive evaluations on the Vi-Fi and JRDB
datasets, our approach achieves state-of-the-art performance in both trajectory
denoising and prediction, significantly surpassing previous baselines.
Additionally, we introduce comparisons with traditional denoising methods, such
as Kalman filtering, and adapt recent trajectory prediction models to our task,
providing a comprehensive benchmark. This work represents the first initiative
to integrate vision-positioning projection for denoising noisy sensor
trajectories of out-of-sight agents, paving the way for future advances. The
code and preprocessed datasets are available at github.com/Hai-chao-Zhang/OST</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于Haichao Zhang, Yi Xu, Yun Fu撰写的论文“Out-of-Sight Trajectories: Tracking, Fusion, and Prediction”的全面摘要，涵盖了您要求的所有要点：</p>
<p><strong>论文摘要：Out-of-Sight Trajectories: Tracking, Fusion, and Prediction</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决计算机视觉和自主系统中轨迹预测面临的关键挑战，特别是当目标物体“不可见”（out-of-sight）时。现有方法通常依赖于完整、无噪声的观测数据，但现实世界中，由于摄像头覆盖范围有限、障碍物和缺乏去噪轨迹的真值，传感器数据往往存在噪声，且物体可能完全超出视野。这导致了安全风险，并阻碍了在自动驾驶、机器人、监控和虚拟现实等应用中进行可靠的轨迹预测。核心问题是如何利用有噪声的传感器数据，预测不可见物体的无噪声视觉轨迹。</p>
<p><strong>2. 关键创新或方法贡献：</strong>
该论文提出了“Out-of-Sight Trajectory (OST)”这一新颖任务，并引入了一个创新的“Vision-Positioning Denoising Module (VPD)”框架来解决它。主要贡献包括：
*   <strong>任务扩展：</strong> 将OST任务的范围从行人扩展到车辆，使其适用于更广泛的场景，如自动驾驶、机器人、监控和虚拟现实。
*   <strong>无监督去噪：</strong> 针对有噪声的传感器数据，提出了一种无监督去噪方法。由于缺乏去噪轨迹的真值，传统监督学习方法不可行。该方法通过利用视觉定位投影来构建有效的去噪监督。
*   <strong>视觉定位投影：</strong> 引入了“Vision-Positioning Projection Module (VPP)”和“Mapping Parameters Estimator (MPE)”。VPP负责将去噪后的传感器轨迹（3D世界坐标）映射到2D摄像头坐标。MPE通过分析可见物体的视觉和传感器轨迹之间的相关性，动态预测摄像头矩阵嵌入（包括内参和外参），从而解决了缺乏直接视觉参考的问题。
*   <strong>Transformer架构：</strong> 传感器去噪编码器（SDE）、映射参数估计器（MPE）和不可见物体预测解码器（OPD）均采用基于Transformer的架构，以有效捕捉轨迹数据中的时序和上下文依赖性。
*   <strong>模块化设计：</strong> 整个框架设计为模块化，包括SDE（去噪）、MPE（映射参数估计）、VPP（视觉投影）和OPD（预测），每个模块协同工作，确保对有噪声和不完整传感器数据的全面处理。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>最先进的性能：</strong> 在Vi-Fi和JRDB数据集上的广泛评估表明，该方法在轨迹去噪和预测方面均达到了最先进的性能，显著超越了现有基线。
*   <strong>去噪的重要性：</strong> 实验结果强调了鲁棒去噪在提高不可见物体轨迹预测准确性方面的关键作用，去噪性能的提升直接转化为预测准确性的提高。
*   <strong>VPD模块的有效性：</strong> 消融研究证实了VPD模块中每个组件（SDE、MPE、VPP、OPD）的必要性，它们共同促进了模型实现最优性能。
*   <strong>泛化能力：</strong> 该模型在处理不同传感器噪声场景和利用精确视觉监督方面表现出鲁棒性，在两个数据集上均表现出一致的性能。
*   <strong>开创性工作：</strong> 这是首次尝试整合视觉定位投影来去噪不可见物体的有噪声传感器轨迹，为该领域的未来发展铺平了道路。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>摄像头校准的隐式约束：</strong> 论文指出，在许多实际场景中，数据集不提供内参矩阵，且实际摄像头操作（如变焦、自动对焦）可能改变这些参数。虽然模型能够估计统一的嵌入来处理这些复杂场景，但当内参矩阵可用时，直接整合它们仍需考虑。
*   <strong>不可见物体的感知距离：</strong> 模型在处理距离非常远的物体（例如几英里外）时，性能可能会下降。目前模型在数据集范围内的距离上表现良好，但对于更远距离的性能仍需评估。极端距离的物体可能导致分布外（out-of-distribution）问题，影响性能。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>处理极端噪声：</strong> 进一步研究如何处理极端噪声情况，以提高模型在更具挑战性环境中的鲁棒性。
*   <strong>实时实现：</strong> 探索如何优化模型以实现实时轨迹预测，这对于自动驾驶等对延迟敏感的应用至关重要。
*   <strong>更远的感知距离：</strong> 评估和改进模型在处理更远距离不可见物体时的性能，以解决分布外数据问题。
*   <strong>更复杂的环境：</strong> 进一步研究在视觉观测不确定或输入信号固有噪声的复杂现实世界场景中，如何提高轨迹预测的准确性和可靠性。
*   <strong>多模态融合的扩展：</strong> 探索将更多模态（如雷达、激光雷达等）整合到框架中，以进一步增强去噪和预测能力。</p>
<p>总而言之，这篇论文通过引入OST任务和创新的VPD框架，为解决不可见物体的轨迹预测问题提供了开创性的解决方案。它通过无监督去噪和视觉定位投影，有效地处理了有噪声的传感器数据，并在多个关键应用领域取得了显著进展。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this extended work, we present
advancements in Out-of-Sight Trajectory (OST), a novel task that predicts the
noise-free visual trajectories of out-of-sight objects using noisy sensor data.</li>
<li>Through extensive evaluations on the Vi-Fi and JRDB
datasets, our approach achieves state-of-the-art performance in both trajectory
denoising and prediction, significantly surpassing previous baselines.</li>
<li>Additionally, we introduce comparisons with traditional denoising methods, such
as Kalman filtering, and adapt recent trajectory prediction models to our task,
providing a comprehensive benchmark.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.15219v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.15219v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.15178v1'></a></p>
<h2 id="unleashing-the-potential-of-multimodal-llms-for-zero-shot-spatio-temporal-video-grounding"><a href="https://arxiv.org/abs/2509.15178v1">Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding</a></h2>
<p><strong>Authors:</strong> Zaiquan Yang, Yuhao Liu, Gerhard Hancke, Rynson W. H. Lau</p>
<p><strong>Published:</strong> 2025-09-18</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Spatio-temporal video grounding (STVG) aims at localizing the spatio-temporal
tube of a video, as specified by the input text query. In this paper, we
utilize multimodal large language models (MLLMs) to explore a zero-shot
solution in STVG. We reveal two key insights about MLLMs: (1) MLLMs tend to
dynamically assign special tokens, referred to as \textit{grounding tokens},
for grounding the text query; and (2) MLLMs often suffer from suboptimal
grounding due to the inability to fully integrate the cues in the text query
(\textit{e.g.}, attributes, actions) for inference. Based on these insights, we
propose a MLLM-based zero-shot framework for STVG, which includes novel
decomposed spatio-temporal highlighting (DSTH) and temporal-augmented
assembling (TAS) strategies to unleash the reasoning ability of MLLMs. The DSTH
strategy first decouples the original query into attribute and action
sub-queries for inquiring the existence of the target both spatially and
temporally. It then uses a novel logit-guided re-attention (LRA) module to
learn latent variables as spatial and temporal prompts, by regularizing token
predictions for each sub-query. These prompts highlight attribute and action
cues, respectively, directing the model's attention to reliable spatial and
temporal related visual regions. In addition, as the spatial grounding by the
attribute sub-query should be temporally consistent, we introduce the TAS
strategy to assemble the predictions using the original video frames and the
temporal-augmented frames as inputs to help improve temporal consistency. We
evaluate our method on various MLLMs, and show that it outperforms SOTA methods
on three common STVG benchmarks.
  The code will be available at https://github.com/zaiquanyang/LLaVA_Next_STVG.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Zaiquan Yang, Yuhao Liu, Gerhard Hancke, Rynson W. H. Lau撰写的论文“Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding”的全面摘要。</p>
<hr />
<h3 id="unleashing-the-potential-of-multimodal-llms-for-zero-shot-spatio-temporal-video-grounding_1">论文摘要：Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding</h3>
<p><strong>1. 解决的主要问题或研究问题：</strong>
该论文旨在解决<strong>时空视频定位（Spatio-Temporal Video Grounding, STVG）</strong>任务，即根据给定的文本查询，在视频中定位目标对象的时空管（spatio-temporal tube）。传统的STVG方法通常依赖于昂贵的帧级标注进行全监督训练，而本文则探索利用多模态大语言模型（MLLMs）实现<strong>零样本（zero-shot）</strong>STVG解决方案，以减轻标注负担并提高泛化能力。论文特别关注MLLMs在处理复杂视频查询时，由于未能充分整合文本查询中的属性和动作线索而导致的次优定位问题。</p>
<p><strong>2. 关键创新或方法贡献：</strong>
作者基于对MLLMs的两个关键洞察（即MLLMs会动态分配“定位令牌”进行文本查询定位，但常因未能充分利用文本线索而导致定位次优），提出了一个新颖的零样本STVG框架，包含以下核心策略：</p>
<ul>
<li><strong>分解时空高亮（Decomposed Spatio-Temporal Highlighting, DSTH）策略：</strong><ul>
<li>将原始文本查询分解为<strong>属性子查询</strong>和<strong>动作子查询</strong>，分别用于在空间和时间上查询目标的存在。</li>
<li>引入<strong>对数引导重注意力（Logit-guided Re-attention, LRA）模块</strong>，通过正则化每个子查询的令牌预测，学习潜在变量作为空间和时间提示。这些提示能够分别高亮属性和动作线索，引导模型关注可靠的、与时空相关的视觉区域。</li>
</ul>
</li>
<li><strong>时序增强组装（Temporal-Augmented Assembling, TAS）策略：</strong><ul>
<li>为了提高空间定位的时序一致性（特别是属性子查询的定位），TAS策略利用原始视频帧和时序增强帧（例如，反转帧顺序）作为输入，组装不同预测以改善时序一致性。</li>
</ul>
</li>
<li><strong>定位令牌识别（Grounding Token Identification）：</strong> 论文发现MLLMs会动态分配具有高视觉激活度的特殊令牌（称为“定位令牌”），这些令牌在定位文本相关区域方面表现出色，并利用这一发现构建了零样本STVG框架。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
该方法在多种MLLMs上进行了评估，并在三个常见的STVG基准测试（HCSTVG-v1、HCSTVG-v2和VidSTG）上取得了显著优于现有SOTA方法的性能。
*   例如，基于LLaVA-Next-Video-7B模型，该方法在vIoU@0.3和vIoU@0.5指标上分别比E3M高出4.2%和1.8%。
*   与更强大的LLaVA-OneVision-7B模型结合时，性能提升更为显著，分别达到12.1%和5.7%。
*   即使在动作线索较少的VidSTG数据集上，该框架也超越了之前的SOTA方法，展现了强大的泛化能力。
*   消融实验证明了DSTH策略（包括子查询分解和LRA模块）以及TAS策略的有效性，它们能够引导MLLMs更好地关注时空相关区域，并提高空间定位的鲁棒性。</p>
<p>这些结果表明，通过有效利用MLLMs的内在能力并引入创新的高亮和组装策略，可以显著提升零样本STVG的性能，甚至超越一些弱监督方法，并接近全监督方法的水平。</p>
<p><strong>4. 论文中提及的局限性：</strong>
论文指出，该方法存在局限性。由于MLLMs的高计算消耗，它可能难以很好地处理长视频。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
未来的工作可以考虑在模型设计中整合<strong>令牌剪枝（token pruning）</strong>和<strong>关键帧选择（key frame selection）</strong>技术，以解决处理长视频时的计算效率问题。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>Based on these insights, we
propose a MLLM-based zero-shot framework for STVG, which includes novel
decomposed spatio-temporal highlighting (DSTH) and temporal-augmented
assembling (TAS) strategies to unleash the reasoning ability of MLLMs. The DSTH
strategy first decouples the original query into attribute and action
sub-queries for inquiring the existence of the target both spatially and
temporally.</li>
<li>It then uses a novel logit-guided re-attention (LRA) module to
learn latent variables as spatial and temporal prompts, by regularizing token
predictions for each sub-query.</li>
<li>In addition, as the spatial grounding by the
attribute sub-query should be temporally consistent, we introduce the TAS
strategy to assemble the predictions using the original video frames and the
temporal-augmented frames as inputs to help improve temporal consistency.</li>
<li>We
evaluate our method on various MLLMs, and show that it outperforms SOTA methods
on three common STVG benchmarks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.15178v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.15178v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.15177v1'></a></p>
<h2 id="a-race-bias-free-face-aging-model-for-reliable-kinship-verification"><a href="https://arxiv.org/abs/2509.15177v1">A Race Bias Free Face Aging Model for Reliable Kinship Verification</a></h2>
<p><strong>Authors:</strong> Ali Nazari, Bardiya Kariminia, Mohsen Ebrahimi Moghaddam</p>
<p><strong>Published:</strong> 2025-09-18</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>The age gap in kinship verification addresses the time difference between the
photos of the parent and the child. Moreover, their same-age photos are often
unavailable, and face aging models are racially biased, which impacts the
likeness of photos. Therefore, we propose a face aging GAN model, RA-GAN,
consisting of two new modules, RACEpSp and a feature mixer, to produce racially
unbiased images. The unbiased synthesized photos are used in kinship
verification to investigate the results of verifying same-age parent-child
images. The experiments demonstrate that our RA-GAN outperforms SAM-GAN on an
average of 13.14\% across all age groups, and CUSP-GAN in the 60+ age group by
9.1\% in terms of racial accuracy. Moreover, RA-GAN can preserve subjects'
identities better than SAM-GAN and CUSP-GAN across all age groups.
Additionally, we demonstrate that transforming parent and child images from the
KinFaceW-I and KinFaceW-II datasets to the same age can enhance the
verification accuracy across all age groups. The accuracy increases with our
RA-GAN for the kinship relationships of father-son and father-daughter,
mother-son, and mother-daughter, which are 5.22, 5.12, 1.63, and 0.41,
respectively, on KinFaceW-I. Additionally, the accuracy for the relationships
of father-daughter, father-son, and mother-son is 2.9, 0.39, and 1.6 on
KinFaceW-II, respectively. The code is available
at~\href{https://github.com/bardiya2254kariminia/An-Age-Transformation-whitout-racial-bias-for-Kinship-verification}{Github}</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Ali Nazari, Bardiya Kariminia, Mohsen Ebrahimi Moghaddam撰写的论文“A Race Bias Free Face Aging Model for Reliable Kinship Verification”的全面摘要。</p>
<hr />
<p><strong>论文摘要：A Race Bias Free Face Aging Model for Reliable Kinship Verification</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该研究主要解决亲属关系验证（Kinship Verification, KV）中的两大挑战：
*   <strong>年龄差距问题：</strong> 父母和子女的照片通常存在显著的年龄差异，且难以获取同龄照片，这严重影响了亲属关系验证的准确性。
*   <strong>现有面部老化模型的种族偏见：</strong> 当前的面部老化模型在生成图像时存在种族偏见，这导致合成图像与原始图像的相似度降低，进而影响亲属关系验证的可靠性。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
为了解决上述问题，作者提出了一种名为 <strong>RA-GAN (RaceAgingGAN)</strong> 的面部老化生成对抗网络模型，其核心创新包括：
*   <strong>RACEpSp 模块：</strong> 这是一个新颖的模块，旨在消除面部老化过程中产生的种族偏见，确保合成图像在种族上是无偏的。它利用预训练的ResNet34模型在Fairface数据集上进行训练，以更好地处理多样化的面部姿态和种族信息。
*   <strong>特征混合器（Feature Mixer）：</strong> 该模块用于融合年龄特征和种族特定面部特征，以找到最佳组合，从而生成在种族上无偏且身份保留的图像。
*   <strong>新数据集的构建：</strong> 作者从UTKFace数据集中收集并构建了一个新的、种族平衡的数据集，以解决现有数据集在种族属性上的不平衡问题，从而训练出更具泛化能力的模型。
*   <strong>同龄图像的亲属关系验证：</strong> 提出将父母和子女的图像转换为相同年龄，以提高亲属关系验证的准确性。</p>
<p><strong>3. 主要结果及其意义：</strong>
实验结果显著地证明了RA-GAN的有效性：
*   <strong>种族准确性提升：</strong> RA-GAN在所有年龄组的种族准确性方面平均优于SAM-GAN 13.14%，在60岁以上年龄组中优于CUSP-GAN 9.1%。这表明RA-GAN能够生成种族无偏的合成图像。
*   <strong>身份保留能力：</strong> RA-GAN在所有年龄组中比SAM-GAN和CUSP-GAN更好地保留了受试者的身份，这对于亲属关系验证至关重要。
*   <strong>亲属关系验证准确性提高：</strong> 将KinFaceW-I和KinFaceW-II数据集中的父母和子女图像转换为相同年龄后，亲属关系验证的准确性显著提高。具体而言，在KinFaceW-I数据集上，父子、父女、母子和母女关系的准确性分别提高了5.22%、5.12%、1.63%和0.41%。在KinFaceW-II数据集上，父女、父子和母子关系的准确性分别提高了2.9%、0.39%和1.6%。
*   <strong>年龄转换误差（MAE）降低：</strong> RA-GAN在年龄转换方面表现优异，除了20岁年龄组外，在所有其他年龄组中，其目标年龄转换误差均低于CUSP模型。</p>
<p>这些结果的意义在于，RA-GAN不仅解决了面部老化模型中的种族偏见问题，还通过生成高质量、身份保留且种族无偏的同龄面部图像，显著提升了亲属关系验证的准确性和可靠性，尤其是在处理真实世界中不同来源和拍摄时间照片的挑战时。</p>
<p><strong>4. 论文中提及的局限性：</strong>
论文中并未明确指出当前研究的局限性，但从其讨论和未来工作方向可以推断出一些隐含的方面：
*   <strong>数据集的年龄分布不均：</strong> 尽管作者构建了种族平衡的数据集，但仍指出如果要求数据集在种族和年龄上都均匀，图像数量会减少，这可能暗示在某些年龄段的数据量仍有待提高。
*   <strong>模型泛化能力：</strong> 尽管RA-GAN在种族和身份保留方面表现出色，但面部老化模型通常难以泛化到未充分代表的年龄组、种族或面部结构，这可能是一个持续的挑战。
*   <strong>对全脸图像的依赖：</strong> GAN模型通常需要全脸图像进行训练，而KinFaceW-I等数据集中的图像是裁剪过的，需要额外的镜像增强和自编码器转换步骤来生成全脸图像，这增加了处理的复杂性。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
论文的结论和未来工作部分指出了以下潜在研究方向：
*   <strong>进一步消除种族偏见：</strong> 作者强调其方法在融合种族特征以消除种族偏见方面具有高潜力，这暗示未来可以继续探索更先进的技术来彻底根除种族偏见，并生成与原始图像种族类别高度相似的图像。
*   <strong>更广泛的年龄范围和多样性：</strong> 尽管RA-GAN覆盖了20-80岁的年龄范围，但未来可以探索更广泛的年龄范围，并进一步提高模型在极端年龄（如儿童或老年）的性能。
*   <strong>更复杂的亲属关系验证场景：</strong> 论文主要关注父母-子女关系，未来可以扩展到其他更复杂的亲属关系（如祖父母-孙子女、兄弟姐妹等）。
*   <strong>实时应用和效率：</strong> 尽管深度学习模型在准确性上表现出色，但其计算资源需求和推理速度可能成为实时应用的瓶颈，未来可以研究更高效的模型架构和优化方法。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>Therefore, we propose a face aging GAN model, RA-GAN,
consisting of two new modules, RACEpSp and a feature mixer, to produce racially
unbiased images.</li>
<li>The experiments demonstrate that our RA-GAN outperforms SAM-GAN on an
average of 13.14\% across all age groups, and CUSP-GAN in the 60+ age group by
9.1\% in terms of racial accuracy.</li>
<li>Additionally, we demonstrate that transforming parent and child images from the
KinFaceW-I and KinFaceW-II datasets to the same age can enhance the
verification accuracy across all age groups.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.15177v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.15177v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.14980v1'></a></p>
<h2 id="m4diffuser-multi-view-diffusion-policy-with-manipulability-aware-control-for-robust-mobile-manipulation"><a href="https://arxiv.org/abs/2509.14980v1">M4Diffuser: Multi-View Diffusion Policy with Manipulability-Aware Control for Robust Mobile Manipulation</a></h2>
<p><strong>Authors:</strong> Ju Dong, Lei Zhang, Liding Zhang, Yao Ling, Yu Fu, Kaixin Bai, Zoltán-Csaba Márton, Zhenshan Bing, Zhaopeng Chen, Alois Christian Knoll, Jianwei Zhang</p>
<p><strong>Published:</strong> 2025-09-18</p>
<p><strong>Categories:</strong> cs.RO, cs.AI, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Mobile manipulation requires the coordinated control of a mobile base and a
robotic arm while simultaneously perceiving both global scene context and
fine-grained object details. Existing single-view approaches often fail in
unstructured environments due to limited fields of view, exploration, and
generalization abilities. Moreover, classical controllers, although stable,
struggle with efficiency and manipulability near singularities. To address
these challenges, we propose M4Diffuser, a hybrid framework that integrates a
Multi-View Diffusion Policy with a novel Reduced and Manipulability-aware QP
(ReM-QP) controller for mobile manipulation. The diffusion policy leverages
proprioceptive states and complementary camera perspectives with both
close-range object details and global scene context to generate task-relevant
end-effector goals in the world frame. These high-level goals are then executed
by the ReM-QP controller, which eliminates slack variables for computational
efficiency and incorporates manipulability-aware preferences for robustness
near singularities. Comprehensive experiments in simulation and real-world
environments show that M4Diffuser achieves 7 to 56 percent higher success rates
and reduces collisions by 3 to 31 percent over baselines. Our approach
demonstrates robust performance for smooth whole-body coordination, and strong
generalization to unseen tasks, paving the way for reliable mobile manipulation
in unstructured environments. Details of the demo and supplemental material are
available on our project website https://sites.google.com/view/m4diffuser.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供对Ju Dong等人撰写的论文“M4Diffuser: Multi-View Diffusion Policy with Manipulability-Aware Control for Robust Mobile Manipulation”的全面摘要。</p>
<hr />
<h3 id="m4diffuser">论文摘要：M4Diffuser: 具有可操作性感知控制的多视角扩散策略，实现鲁棒的移动操作</h3>
<p><strong>1. 解决的主要问题或研究问题：</strong>
该论文旨在解决移动操作中的核心挑战，即如何在非结构化环境中实现移动基座和机械臂的协调控制，同时感知全局场景上下文和精细物体细节。现有方法存在以下局限性：
*   <strong>单视角方法</strong>：由于视野有限、探索能力和泛化能力不足，在非结构化环境中表现不佳。
*   <strong>经典控制器</strong>：虽然稳定，但在效率和接近奇异点时的可操作性方面存在困难，通常依赖于松弛变量，增加了计算开销并降低了轨迹平滑度。
*   <strong>学习驱动方法</strong>：虽然适应性和泛化能力强，但实际部署时稳定性不足，容易因视觉输入遮挡或超出视野而失败。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
M4Diffuser 提出了一种混合框架，结合了多视角扩散策略和新型的“Reduced and Manipulability-aware QP (ReM-QP)”控制器，其主要创新点包括：</p>
<ul>
<li>
<p><strong>多视角扩散Transformer策略 (Multi-View Diffusion Transformer Policy)</strong>：</p>
<ul>
<li>该策略通过结合互补的视角和本体状态，同时捕捉局部物体细节和全局场景上下文。</li>
<li>利用Transformer编码器和条件去噪扩散过程，生成世界坐标系中任务相关的末端执行器目标。</li>
<li>多视角输入显著提高了鲁棒性和泛化能力，解决了单视角感知不足的问题。</li>
</ul>
</li>
<li>
<p><strong>Reduced and Manipulability-aware QP (ReM-QP) 控制器</strong>：</p>
<ul>
<li>在低层控制层面，ReM-QP 消除了传统QP公式中的松弛变量，从而提高了计算效率。</li>
<li>引入了基于逆条件数（ICN）的可操作性偏好，以确保在接近奇异点时的稳定性和平滑性，提高了鲁棒性。</li>
<li>通过标准不等式约束确保了安全性。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果及其重要性：</strong>
M4Diffuser 在仿真和真实世界环境中进行了广泛实验，结果表明：</p>
<ul>
<li><strong>性能提升</strong>：与基线方法相比，M4Diffuser 的成功率提高了7%到56%，碰撞率降低了3%到31%。</li>
<li><strong>鲁棒性与泛化能力</strong>：该方法在非结构化环境中实现了平滑全身协调的鲁棒性能，并对未见过的任务表现出强大的泛化能力。</li>
<li><strong>与SOTA方法的比较</strong>：M4Diffuser 显著优于传统规划方法和纯学习方法，平均成功率提高了28.4%，碰撞率降低了69%。与最先进的方法（如HoMeR）相比，成功率提高了10.0%，碰撞率减少了5.2%。</li>
<li><strong>ReM-QP的效率与平滑度</strong>：ReM-QP 将任务执行时间缩短了28%，末端执行器加加速度（jerk）降低了35%，在效率和轨迹平滑度之间取得了良好平衡。</li>
</ul>
<p>这些结果的重要性在于，M4Diffuser 为在非结构化环境中实现可靠的移动操作铺平了道路，解决了现有方法在鲁棒性、效率和泛化能力方面的关键限制。</p>
<p><strong>4. 论文中提到的局限性：</strong>
论文中并未明确列出M4Diffuser自身的局限性，但通过对现有方法的讨论，可以推断出M4Diffuser旨在克服的挑战，这些挑战在一定程度上也可能构成其未来改进的方向：</p>
<ul>
<li><strong>数据依赖性</strong>：尽管模仿学习数据效率高，但训练仍然需要专家演示数据。</li>
<li><strong>复杂环境中的探索能力</strong>：虽然多视角策略增强了感知，但机器人应对高度复杂、完全未知的环境时的探索能力仍有提升空间。</li>
<li><strong>计算资源</strong>：虽然ReM-QP提高了计算效率，但整个混合框架在实时部署时仍可能面临计算资源的需求。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
论文指出，未来的工作将集中于：</p>
<ul>
<li><strong>语言和多模态引导的移动操作 (Language- and Multimodal-guided Mobile Manipulation)</strong>：将语言指令和更多模态信息整合到移动操作中，以实现更高级别的任务理解和执行。</li>
</ul>
<hr />
<p>这份摘要清晰地概述了论文的核心内容，突出了其在移动操作领域的技术贡献和潜在影响。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address
these challenges, we propose M4Diffuser, a hybrid framework that integrates a
Multi-View Diffusion Policy with a novel Reduced and Manipulability-aware QP
(ReM-QP) controller for mobile manipulation.</li>
<li>Our approach
demonstrates robust performance for smooth whole-body coordination, and strong
generalization to unseen tasks, paving the way for reliable mobile manipulation
in unstructured environments.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.14980v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.14980v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.14966v1'></a></p>
<h2 id="roboeye-enhancing-2d-robotic-object-identification-with-selective-3d-geometric-keypoint-matching"><a href="https://arxiv.org/abs/2509.14966v1">RoboEye: Enhancing 2D Robotic Object Identification with Selective 3D Geometric Keypoint Matching</a></h2>
<p><strong>Authors:</strong> Xingwu Zhang, Guanxuan Li, Zhuocheng Zhang, Zijun Long</p>
<p><strong>Published:</strong> 2025-09-18</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>The rapidly growing number of product categories in large-scale e-commerce
makes accurate object identification for automated packing in warehouses
substantially more difficult. As the catalog grows, intra-class variability and
a long tail of rare or visually similar items increase, and when combined with
diverse packaging, cluttered containers, frequent occlusion, and large
viewpoint changes-these factors amplify discrepancies between query and
reference images, causing sharp performance drops for methods that rely solely
on 2D appearance features. Thus, we propose RoboEye, a two-stage identification
framework that dynamically augments 2D semantic features with domain-adapted 3D
reasoning and lightweight adapters to bridge training deployment gaps. In the
first stage, we train a large vision model to extract 2D features for
generating candidate rankings. A lightweight 3D-feature-awareness module then
estimates 3D feature quality and predicts whether 3D re-ranking is necessary,
preventing performance degradation and avoiding unnecessary computation. When
invoked, the second stage uses our robot 3D retrieval transformer, comprising a
3D feature extractor that produces geometry-aware dense features and a
keypoint-based matcher that computes keypoint-correspondence confidences
between query and reference images instead of conventional cosine-similarity
scoring. Experiments show that RoboEye improves Recall@1 by 7.1% over the prior
state of the art (RoboLLM). Moreover, RoboEye operates using only RGB images,
avoiding reliance on explicit 3D inputs and reducing deployment costs. The code
used in this paper is publicly available at:
https://github.com/longkukuhi/RoboEye.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Xingwu Zhang等人撰写的论文“RoboEye: Enhancing 2D Robotic Object Identification with Selective 3D Geometric Keypoint Matching”的全面摘要。</p>
<hr />
<h3 id="roboeye-3d2d">RoboEye: 通过选择性3D几何关键点匹配增强2D机器人物体识别的摘要</h3>
<p><strong>1. 主要问题或研究问题：</strong>
随着大规模电商产品目录的快速增长，仓库自动化包装中的物体识别变得越来越困难。现有方法主要依赖2D外观特征，但在面对类内变异性、长尾分布、多样化包装、杂乱容器、频繁遮挡和大幅度视角变化等挑战性场景时，其性能会急剧下降。这些因素放大了查询图像和参考图像之间的差异，使得仅基于2D特征的方法难以泛化。因此，核心研究问题是如何在不依赖显式3D输入（如点云或深度图）的情况下，利用3D几何线索来提高在复杂仓库条件下的物体识别鲁棒性，同时降低部署成本。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
RoboEye提出了一个两阶段识别框架，动态地将2D语义特征与领域适应的3D推理和轻量级适配器相结合，以弥合训练与部署之间的差距。
*   <strong>两阶段识别框架：</strong>
    *   <strong>第一阶段（2D检索）：</strong> 使用大型视觉模型提取2D特征，生成初步的候选排名。
    *   <strong>3D特征感知模块（3D-FAM）：</strong> 一个轻量级的模块，用于评估3D特征的质量，并预测是否需要进行3D重排序。这避免了不必要的计算，并在3D线索嘈杂时防止性能下降。该模块通过MRR驱动的3D感知训练（M3AT）方案进行训练，该方案识别何时3D重排序能带来实际收益。
    *   <strong>第二阶段（3D重排序）：</strong> 当3D-FAM模块被激活时，使用机器人3D检索Transformer。
*   <strong>机器人3D检索Transformer：</strong>
    *   <strong>3D特征提取器：</strong> 采用VGGT（Visual Geometry Grounded Transformer）的聚合器组件，从多视图2D图像中推断3D几何信息，生成几何感知的密集特征。
    *   <strong>关键点匹配器：</strong> 替换了传统的余弦相似度评分，通过计算查询图像和参考图像之间的关键点对应置信度来提供更鲁棒的相似性度量。该匹配器基于稀疏关键点匹配，并经过重新设计以生成置信度分数作为相似性估计。
*   <strong>基于适配器的领域适应策略：</strong> 为了弥合VGGT预训练数据与仓库特定数据集之间的领域差距，RoboEye采用了一种基于适配器的训练策略，冻结3D特征提取器，仅对匹配器进行训练，并使用轻量级知识适配器进行增强，以实现高效的领域适应。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>性能提升：</strong> RoboEye在Amazon ARMBench数据集上，Recall@1指标比现有最先进方法（RoboLLM）提高了7.1%。在多视图设置下，其性能提升更为显著，例如在容器图库检索中，Recall@1从98.0%提高到99.4%（+1.4%），在全局图库检索中，Recall@1提高了7.1%。
*   <strong>鲁棒性：</strong> RoboEye在面对大规模目录、视角和姿态变化、遮挡和包装变化等挑战时，表现出卓越的鲁棒性。
*   <strong>效率：</strong> 3D特征感知模块在平衡效率和鲁棒性方面发挥了关键作用。它仅在必要时激活3D推理，保持了接近2D运行时的速度，同时保留了几何验证的优势。
*   <strong>无需显式3D输入：</strong> RoboEye仅使用RGB图像进行操作，避免了对显式3D输入（如点云或深度图）的依赖，从而降低了部署成本。
*   <strong>模型规模与性能：</strong> 实验表明，简单地增加2D特征提取器的模型规模不足以解决仓库环境中的挑战。RoboEye通过结合3D感知组件，以可比的参数规模实现了显著的性能提升。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>计算开销：</strong> 尽管RoboEye通过3D-FAM模块减少了不必要的3D计算，但3D重排序本身仍然会带来一定的计算开销。在没有感知模块的情况下，无条件3D重排序会显著增加延迟。
*   <strong>噪声敏感性：</strong> 论文指出，在2D重排序阶段，如果包含过多低质量的候选对象，可能会引入噪声，稀释判别信号，并可能导致性能略微下降。
*   <strong>硬件限制：</strong> 论文提到，在与RoboLLM的对比中，由于硬件限制（较小的批次大小），其2D模型初始性能略低于RoboLLM，这表明对比学习通常受益于更大的批次。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
论文中没有明确提出未来的研究方向，但从其贡献和局限性来看，可以推断出以下潜在方向：
*   <strong>更轻量级的3D推理：</strong> 进一步优化3D特征提取和匹配机制，以在保持性能的同时进一步降低计算成本和推理延迟，使其更适用于资源受限的边缘设备。
*   <strong>自适应关键点选择：</strong> 探索更智能的关键点选择策略，以避免引入低质量的关键点，从而提高几何验证的准确性和稳定性。
*   <strong>多模态融合的泛化：</strong> 虽然RoboEye避免了显式3D输入，但可以研究如何在不增加部署复杂性的前提下，更有效地融合其他隐式模态信息（例如，通过物理模拟或语义知识），以进一步增强识别鲁棒性。
*   <strong>零样本/少样本学习：</strong> 鉴于电商产品目录的快速增长和长尾分布，探索如何将RoboEye框架扩展到零样本或少样本识别场景，以处理新产品或稀有产品。</p>
<hr />
<p>这篇论文通过其创新的两阶段框架和选择性3D几何关键点匹配方法，为机器人物体识别领域带来了显著的进步，特别是在具有挑战性的仓库环境中。它有效地解决了传统2D方法在复杂场景下的局限性，同时避免了对昂贵3D传感器的依赖，为实际部署提供了高效且鲁棒的解决方案。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Thus, we propose RoboEye, a two-stage identification
framework that dynamically augments 2D semantic features with domain-adapted 3D
reasoning and lightweight adapters to bridge training deployment gaps.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.14966v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.14966v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.14957v1'></a></p>
<h2 id="df-llava-unlocking-mllms-potential-for-synthetic-image-detection-via-prompt-guided-knowledge-injection"><a href="https://arxiv.org/abs/2509.14957v1">DF-LLaVA: Unlocking MLLM's potential for Synthetic Image Detection via Prompt-Guided Knowledge Injection</a></h2>
<p><strong>Authors:</strong> Zhuokang Shen, Kaisen Zhang, Bohan Jia, Yuan Fang, Zhou Yu, Shaohui Lin</p>
<p><strong>Published:</strong> 2025-09-18</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>With the increasing prevalence of synthetic images, evaluating image
authenticity and locating forgeries accurately while maintaining human
interpretability remains a challenging task. Existing detection models
primarily focus on simple authenticity classification, ultimately providing
only a forgery probability or binary judgment, which offers limited explanatory
insights into image authenticity. Moreover, while MLLM-based detection methods
can provide more interpretable results, they still lag behind expert models in
terms of pure authenticity classification accuracy. To address this, we propose
DF-LLaVA, a simple yet effective framework that unlocks the intrinsic
discrimination potential of MLLMs. Our approach first extracts latent knowledge
from MLLMs and then injects it into training via prompts. This framework allows
LLaVA to achieve outstanding detection accuracy exceeding expert models while
still maintaining the interpretability offered by MLLMs. Extensive experiments
confirm the superiority of our DF-LLaVA, achieving both high accuracy and
explainability in synthetic image detection. Code is available online at:
https://github.com/Eliot-Shen/DF-LLaVA.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Zhuokang Shen等人撰写的论文“DF-LLaVA: Unlocking MLLM's potential for Synthetic Image Detection via Prompt-Guided Knowledge Injection”的全面摘要。</p>
<hr />
<p><strong>论文摘要：DF-LLaVA: 通过提示引导的知识注入解锁MLLM在合成图像检测中的潜力</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
随着生成模型（如扩散模型）的快速发展，合成图像日益普及，对图像真实性评估和伪造定位提出了严峻挑战。现有检测模型主要关注简单的真实性二分类，提供有限的解释性洞察。虽然基于多模态大语言模型（MLLM）的检测方法能提供更具解释性的结果，但在纯粹的真实性分类准确性方面仍落后于专家模型。因此，核心研究问题是如何开发一种既能实现高检测准确性，又能提供人类可解释的伪造证据的专家模型，以应对合成图像检测的挑战。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
本文提出了DF-LLaVA，一个简单而有效的框架，旨在解锁MLLM在合成图像检测中的内在判别潜力。其关键创新和方法论贡献包括：
*   <strong>知识提取与注入框架：</strong> DF-LLaVA首先从MLLM的视觉编码器中提取潜在的判别知识（通过训练一个二分类器），然后通过提示（prompts）将其注入到训练过程中。这种方法允许LLaVA在保持MLLM解释性的同时，显著提高检测准确性。
*   <strong>视觉编码器判别潜力揭示：</strong> 论文揭示了MLLM的判别潜力主要存在于其视觉编码器中。通过在CLIP-ViT的[CLS] token上训练二分类器，并将其概率输出作为嵌入知识注入提示，DF-LLaVA能够有效利用这一潜力。
*   <strong>多视角伪影解释：</strong> DF-LLaVA能够从多个视角（如结构、失真和物理特征）识别合成图像模型产生的伪影，从而增强了对人类的解释性。
*   <strong>基于LLaVA-v1.5的架构：</strong> 框架构建在LLaVA-v1.5架构之上，包含视觉编码器（CLIP-ViT(L-14)）、视觉/语言投影器、线性头部和大型语言模型（Vicuna-v1.5-7B）。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>卓越的检测准确性：</strong> DF-LLaVA在FakeClue和LOKI数据集上实现了超越现有专家模型的检测准确性。与强大的开源模型Qwen2-VL-72B相比，DF-LLaVA在Acc和F1上平均提升了29.5%和40.1%。相对于先前的MLLM方法FakeVLM，DF-LLaVA在Acc和F1上平均提升了4.2%和3.9%。
*   <strong>强大的解释性：</strong> DF-LLaVA在CSS和ROUGE_L指标上表现出色，提供了比FakeVLM和通用MLLM更准确、可靠的伪影解释。
*   <strong>泛化能力：</strong> 在DMimage数据集上的实验结果表明，DF-LLaVA的性能与专家模型相当甚至超越，尤其在伪造图像检测方面表现最佳。
*   <strong>消融实验验证：</strong> 消融研究证实了提示引导知识注入（PGKI）框架的有效性，即使在LLaVA-FullFT设置下，PGKI也能带来进一步的性能提升，尤其在有限训练资源下，对提升LLaVA的判别能力至关重要。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>训练数据类别不平衡：</strong> 论文提到，DF-LLaVA在真实类别上的表现可能略弱于专家模型，这可能是由于训练数据中类别不平衡造成的。
*   <strong>低秩适配器（LoRA）的局限性：</strong> 实验结果表明，LLaVA-LoRA在判别任务上的性能显著低于LLaVA-FullFT，这暗示低秩适配器可能不适合此类任务。
*   <strong>FakeVLM的视觉编码器微调：</strong> 论文指出，FakeVLM通过微调视觉编码器可能扰乱了其视觉表示，导致其性能略低于LLaVA-FullFT。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>探索其他MLLM架构的判别潜力：</strong> 未来研究可以进一步探索其他MLLM架构在合成图像检测中的判别潜力。
*   <strong>增强判别能力的策略：</strong> 进一步研究增强MLLM判别能力的策略，以应对不断演变的合成图像生成技术。</p>
<hr />
<p>总而言之，DF-LLaVA通过创新的提示引导知识注入框架，成功地将MLLM的内在判别能力转化为卓越的合成图像检测性能，同时保持了高度的人类可解释性。这为计算机视觉领域在应对日益增长的合成内容挑战方面提供了一个有前景的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we propose
DF-LLaVA, a simple yet effective framework that unlocks the intrinsic
discrimination potential of MLLMs. Our approach first extracts latent knowledge
from MLLMs and then injects it into training via prompts.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.14957v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.14957v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.14746v1'></a></p>
<h2 id="chain-of-thought-re-ranking-for-image-retrieval-tasks"><a href="https://arxiv.org/abs/2509.14746v1">Chain-of-Thought Re-ranking for Image Retrieval Tasks</a></h2>
<p><strong>Authors:</strong> Shangrong Wu, Yanghong Zhou, Yang Chen, Feng Zhang, P. Y. Mok</p>
<p><strong>Published:</strong> 2025-09-18</p>
<p><strong>Categories:</strong> cs.CV, cs.IR</p>
<p><strong>Abstract:</strong></p>
<p>Image retrieval remains a fundamental yet challenging problem in computer
vision. While recent advances in Multimodal Large Language Models (MLLMs) have
demonstrated strong reasoning capabilities, existing methods typically employ
them only for evaluation, without involving them directly in the ranking
process. As a result, their rich multimodal reasoning abilities remain
underutilized, leading to suboptimal performance. In this paper, we propose a
novel Chain-of-Thought Re-Ranking (CoTRR) method to address this issue.
Specifically, we design a listwise ranking prompt that enables MLLM to directly
participate in re-ranking candidate images. This ranking process is grounded in
an image evaluation prompt, which assesses how well each candidate aligns with
users query. By allowing MLLM to perform listwise reasoning, our method
supports global comparison, consistent reasoning, and interpretable
decision-making - all of which are essential for accurate image retrieval. To
enable structured and fine-grained analysis, we further introduce a query
deconstruction prompt, which breaks down the original query into multiple
semantic components. Extensive experiments on five datasets demonstrate the
effectiveness of our CoTRR method, which achieves state-of-the-art performance
across three image retrieval tasks, including text-to-image retrieval (TIR),
composed image retrieval (CIR) and chat-based image retrieval (Chat-IR). Our
code is available at https://github.com/freshfish15/CoTRR .</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Shangrong Wu等人撰写的论文“Chain-of-Thought Re-ranking for Image Retrieval Tasks”的全面摘要。</p>
<hr />
<h3 id="chain-of-thought-re-ranking-for-image-retrieval-tasks_1">论文《Chain-of-Thought Re-ranking for Image Retrieval Tasks》摘要</h3>
<p><strong>1. 主要问题或研究问题：</strong>
图像检索是计算机视觉领域的一个基础且具有挑战性的问题。尽管多模态大型语言模型（MLLMs）在推理能力方面取得了显著进展，但现有方法通常仅将MLLMs用于评估，而未直接参与图像的排序过程。这导致MLLMs丰富的多模态推理能力未被充分利用，从而影响了图像检索的性能。本文旨在解决这一问题，即如何有效利用MLLMs的推理能力，使其直接参与图像检索的重排序过程，以提升检索准确性。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
本文提出了一种新颖的<strong>思维链重排序（Chain-of-Thought Re-Ranking, CoTRR）</strong>方法，以解决上述问题。CoTRR的核心创新和方法论贡献包括：</p>
<ul>
<li><strong>MLLM直接参与重排序：</strong> CoTRR设计了一个<strong>列表式排序提示（listwise ranking prompt）</strong>，使MLLM能够直接参与候选图像的重排序，而非仅仅进行评估。这使得MLLM能够进行全局比较和一致性推理。</li>
<li><strong>图像评估提示：</strong> 排序过程基于一个<strong>图像评估提示（image evaluation prompt）</strong>，该提示评估每个候选图像与用户查询的匹配程度。这种评估方式支持可解释的决策制定。</li>
<li><strong>查询解构提示：</strong> 为了实现结构化和细粒度的分析，CoTRR引入了一个<strong>查询解构提示（query deconstruction prompt）</strong>。该提示将原始查询分解为多个语义组件（例如，主要对象、活动、关键细节、环境、氛围），从而实现更有效和准确的匹配比较。</li>
<li><strong>统一框架：</strong> CoTRR不依赖于特定的初始检索方法，使其能够轻松、无缝地应用于多种图像检索任务，包括文本到图像检索（TIR）、组合图像检索（CIR）和基于聊天的图像检索（Chat-IR）。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
CoTRR在五个数据集上进行了广泛实验，并在三种图像检索任务（TIR、CIR和Chat-IR）上取得了<strong>最先进的性能</strong>。</p>
<ul>
<li><strong>CIR任务：</strong> 在CIRR和CIRCO数据集上，CoTRR在R@1和mAP@k等指标上显著优于基线方法（如OSrCIR和ImageScope），尤其是在R@1指标上取得了显著提升。例如，使用ViT-B/32作为骨干网络时，CoTRR在CIRR数据集上的R@1和R@5分别比ImageScope提升了12.41%和6.53%。</li>
<li><strong>TIR任务：</strong> 在Flickr30K和MSCOCO数据集上，CoTRR同样超越了原始CLIP和ImageScope，验证了其在文本到图像检索中的有效性。</li>
<li><strong>Chat-IR任务：</strong> 在VisDial数据集上，CoTRR在多个对话轮次中持续优于OpenCLIP、PlugIR和ImageScope，表明其在交互式检索场景中的强大能力。</li>
<li><strong>消融研究：</strong> 消融实验证实了CoTRR中列表式排序、查询解构和图像评估模块的互补性和有效性，每个组件都对性能提升有贡献。此外，CoTRR在不同MLLM（如Gemini 2.5 Pro、Qwen-VL-Max）上的表现稳健，其中Gemini 2.5 Pro表现最佳，这表明大型或指令对齐更好的MLLM能提供更强的基础和评估能力。</li>
</ul>
<p>这些结果表明，CoTRR通过将MLLM直接整合到重排序流程中，并利用其强大的推理能力进行细致的图像评估和列表式比较，显著提升了图像检索的准确性和鲁棒性。</p>
<p><strong>4. 论文中提到的局限性：</strong>
论文中未明确提及CoTRR方法的具体局限性。然而，从其方法论和实验设置来看，可能存在的隐性局限包括：</p>
<ul>
<li><strong>计算成本：</strong> 依赖大型MLLM进行思维链推理和列表式重排序可能带来较高的计算成本和延迟，尤其是在处理大规模候选集时。</li>
<li><strong>提示工程的敏感性：</strong> CoTRR的性能可能对提示的设计（如查询解构和图像评估提示）敏感，需要精细的提示工程来优化。</li>
<li><strong>MLLM的通用性：</strong> 尽管CoTRR在不同MLLM上表现稳健，但其性能仍受底层MLLM能力的限制。如果MLLM在特定领域或复杂推理任务上表现不佳，CoTRR的性能也可能受影响。</li>
<li><strong>可扩展性：</strong> 尽管论文提到了处理top-K候选集，但对于非常大的检索结果集，将所有候选图像输入MLLM进行列表式排序可能面临可扩展性挑战。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
基于本论文的工作，未来研究可以探索以下方向：</p>
<ul>
<li><strong>效率优化：</strong> 探索更高效的MLLM推理策略或近似方法，以降低CoTRR的计算成本和延迟，使其适用于实时或大规模检索系统。</li>
<li><strong>自适应提示：</strong> 研究如何动态生成或自适应调整查询解构和图像评估提示，以更好地适应不同查询类型、用户意图和图像内容。</li>
<li><strong>多模态反馈：</strong> 探索除了文本评估之外，如何将其他模态（如视觉注意力图、用户交互行为）的反馈整合到MLLM的重排序过程中。</li>
<li><strong>更复杂的推理：</strong> 将CoTRR扩展到更复杂的图像检索场景，例如多轮对话中涉及更深层次语义理解和上下文推理的任务。</li>
<li><strong>少样本/零样本学习：</strong> 进一步探索CoTRR在少样本或零样本场景下的性能，以及如何通过少量示例或无需额外训练来提升其泛化能力。</li>
<li><strong>用户偏好学习：</strong> 结合用户偏好学习机制，使CoTRR能够根据个体用户的历史行为和反馈进行个性化重排序。</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we propose a
novel Chain-of-Thought Re-Ranking (CoTRR) method to address this issue.</li>
<li>By allowing MLLM to perform listwise reasoning, our method
supports global comparison, consistent reasoning, and interpretable
decision-making - all of which are essential for accurate image retrieval.</li>
<li>Extensive experiments on five datasets demonstrate the
effectiveness of our CoTRR method, which achieves state-of-the-art performance
across three image retrieval tasks, including text-to-image retrieval (TIR),
composed image retrieval (CIR) and chat-based image retrieval (Chat-IR).</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.14746v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.14746v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-09-19 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
