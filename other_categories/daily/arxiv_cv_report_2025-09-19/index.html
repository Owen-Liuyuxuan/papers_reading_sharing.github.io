<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-09-19 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DEâ«¶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Birdâs-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">è®ºæéè¯»</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-Î±
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-09-18/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-09-22/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-09-19">Arxiv Computer Vision Papers - 2025-09-19</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#arxiv-2025-09-17" class="nav-link">Arxiv è®¡ç®æºè§è§æ¯æ¥æ¥åæ§è¡æè¦ (2025-09-17)</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#sail-vl2-technical-report" class="nav-link">SAIL-VL2 Technical Report</a>
                </li>
                <li class="nav-item">
                    <a href="#scalecua-scaling-open-source-computer-use-agents-with-cross-platform-data" class="nav-link">ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data</a>
                </li>
                <li class="nav-item">
                    <a href="#lightweight-and-accurate-multi-view-stereo-with-confidence-aware-diffusion-model" class="nav-link">Lightweight and Accurate Multi-View Stereo with Confidence-Aware Diffusion Model</a>
                </li>
                <li class="nav-item">
                    <a href="#out-of-sight-trajectories-tracking-fusion-and-prediction" class="nav-link">Out-of-Sight Trajectories: Tracking, Fusion, and Prediction</a>
                </li>
                <li class="nav-item">
                    <a href="#unleashing-the-potential-of-multimodal-llms-for-zero-shot-spatio-temporal-video-grounding" class="nav-link">Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding</a>
                </li>
                <li class="nav-item">
                    <a href="#a-race-bias-free-face-aging-model-for-reliable-kinship-verification" class="nav-link">A Race Bias Free Face Aging Model for Reliable Kinship Verification</a>
                </li>
                <li class="nav-item">
                    <a href="#m4diffuser-multi-view-diffusion-policy-with-manipulability-aware-control-for-robust-mobile-manipulation" class="nav-link">M4Diffuser: Multi-View Diffusion Policy with Manipulability-Aware Control for Robust Mobile Manipulation</a>
                </li>
                <li class="nav-item">
                    <a href="#roboeye-enhancing-2d-robotic-object-identification-with-selective-3d-geometric-keypoint-matching" class="nav-link">RoboEye: Enhancing 2D Robotic Object Identification with Selective 3D Geometric Keypoint Matching</a>
                </li>
                <li class="nav-item">
                    <a href="#df-llava-unlocking-mllms-potential-for-synthetic-image-detection-via-prompt-guided-knowledge-injection" class="nav-link">DF-LLaVA: Unlocking MLLM's potential for Synthetic Image Detection via Prompt-Guided Knowledge Injection</a>
                </li>
                <li class="nav-item">
                    <a href="#chain-of-thought-re-ranking-for-image-retrieval-tasks" class="nav-link">Chain-of-Thought Re-ranking for Image Retrieval Tasks</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-09-19">Arxiv Computer Vision Papers - 2025-09-19</h1>
<h2 id="executive-summary">Executive Summary</h2>
<h2 id="arxiv-2025-09-17">Arxiv è®¡ç®æºè§è§æ¯æ¥æ¥åæ§è¡æè¦ (2025-09-17)</h2>
<p><strong>æ¦è¿°ï¼</strong></p>
<p>ä»å¤©ç Arxiv è®¡ç®æºè§è§è®ºæä¸»è¦å´ç»<strong>å¤æ¨¡æå­¦ä¹ ãå·èº«æºè½ï¼ç¹å«æ¯æºå¨äººæä½ï¼ã3D è§è§ä»¥åå¯¹æ¨¡åé²æ£æ§åå¬å¹³æ§çå³æ³¨</strong>ãå¤§åè¯­è¨æ¨¡åï¼LLMsï¼å¨è§è§ä»»å¡ä¸­çåºç¨æç»­æ·±åï¼å°¤å¶æ¯å¨é¶æ ·æ¬å­¦ä¹ åç¥è¯æ³¨å¥æ¹é¢ã</p>
<p><strong>ä¸»è¦ä¸»é¢åè¶å¿ï¼</strong></p>
<ol>
<li><strong>å¤æ¨¡æLLMsçèåä¸åºç¨ï¼</strong> å¤ä¸ªå·¥ä½æ¢ç´¢äºå¤æ¨¡æLLMsï¼MLLMsï¼å¨è§é¢çè§£ï¼é¶æ ·æ¬æ¶ç©ºè§é¢å®ä½ï¼ãåæå¾åæ£æµä»¥åå¾åæ£ç´¢ä¸­çæ½åï¼éè¿æç¤ºå¼å¯¼ç¥è¯æ³¨å¥åæç»´é¾éæåºç­ææ¯æåæ§è½ã</li>
<li><strong>å·èº«æºè½ä¸æºå¨äººæä½ï¼</strong> æºå¨äººæä½åæç¥æ¯æ¾èä¸»é¢ï¼åæ¬å©ç¨å¤è§å¾æ©æ£ç­ç¥è¿è¡é²æ£ç§»å¨æä½ãéè¿3Då ä½å³é®ç¹å¹éå¢å¼º2Då¯¹è±¡è¯å«ï¼ä»¥åæå»ºè·¨å¹³å°æ°æ®ä»¥æ©å±è®¡ç®æºä½¿ç¨ä»£çã</li>
<li><strong>3D è§è§ä¸éå»ºï¼</strong> 3D è§è§é¢åæè¿å±ï¼å¦å©ç¨ç½®ä¿¡åº¦æç¥æ©æ£æ¨¡åå®ç°è½»éçº§ååç¡®çå¤è§å¾ç«ä½ï¼ä»¥åå¨è·è¸ªãèååé¢æµä¸­å¤çâè§çº¿å¤âè½¨è¿¹ã</li>
<li><strong>æ¨¡åé²æ£æ§ä¸å¬å¹³æ§ï¼</strong> æè®ºæå³æ³¨æ¨¡åå¨ç¹å®åºæ¯ä¸çé²æ£æ§ï¼å¦åæå¾åæ£æµï¼åå¬å¹³æ§ï¼å¦æ¶é¤äººè¸èåæ¨¡åä¸­çç§æåè§ï¼ã</li>
<li><strong>å¤§è§æ¨¡æ¨¡åä¸æ°æ®ï¼</strong> SAIL-VL2 å ScaleCUA ç­å·¥ä½è¡¨æäºæå»ºåæ©å±å¤§è§æ¨¡è§è§è¯­è¨æ¨¡ååè·¨å¹³å°æ°æ®çæç»­åªåã</li>
</ol>
<p><strong>ç¹å«éè¦æåæ°çè®ºæï¼</strong></p>
<ul>
<li><strong>"Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding" (Zaiquan Yang et al.)ï¼</strong> è¿ç¯è®ºæå±ç¤ºäºMLLMså¨å¤æè§é¢çè§£ä»»å¡ï¼é¶æ ·æ¬æ¶ç©ºè§é¢å®ä½ï¼ä¸­çå¼ºå¤§è½åï¼é¢ç¤ºäºMLLMså¨æ´é«çº§å«è§é¢æ¨çä¸­çå·¨å¤§æ½åã</li>
<li><strong>"M4Diffuser: Multi-View Diffusion Policy with Manipulability-Aware Control for Robust Mobile Manipulation" (Ju Dong et al.)ï¼</strong> å°å¤è§å¾æ©æ£æ¨¡åä¸å¯æä½æ§æç¥æ§å¶ç¸ç»åï¼ä¸ºæºå¨äººé²æ£ç§»å¨æä½æä¾äºæ°é¢ä¸é«æçè§£å³æ¹æ¡ï¼å¯¹å·èº«æºè½é¢åå·æéè¦æä¹ã</li>
<li><strong>"Lightweight and Accurate Multi-View Stereo with Confidence-Aware Diffusion Model" (Fangjinhua Wang et al.)ï¼</strong> å¼å¥ç½®ä¿¡åº¦æç¥æ©æ£æ¨¡åæ¥æåå¤è§å¾ç«ä½çåç¡®æ§åæçï¼ä¸º3Déå»ºé¢åæä¾äºä¸ä¸ªæåæ¯çè½»éçº§æ¹æ³ã</li>
<li><strong>"DF-LLaVA: Unlocking MLLM's potential for Synthetic Image Detection via Prompt-Guided Knowledge Injection" (Zhuokang Shen et al.)ï¼</strong> å·§å¦å°å©ç¨æç¤ºå¼å¯¼çç¥è¯æ³¨å¥æ¥å¢å¼ºMLLMå¨åæå¾åæ£æµè¿ä¸å³é®å®å¨ä»»å¡ä¸çè½åï¼å±ç¤ºäºMLLMå¨ç¹å®é¢ååºç¨ä¸­ççµæ´»æ§ã</li>
</ul>
<p><strong>æ°å´ç ç©¶æ¹åæææ¯ï¼</strong></p>
<ul>
<li><strong>MLLMså¨é¶æ ·æ¬åå°æ ·æ¬è§é¢çè§£ä¸­çæ·±ååºç¨ï¼</strong> å°¤å¶æ¯æ¶ç©ºå®ä½åå¤æäºä»¶æ¨çã</li>
<li><strong>æ©æ£æ¨¡åå¨3Dè§è§åæºå¨äººæ§å¶ä¸­çå¤åè½æ§ï¼</strong> ä¸ä»ç¨äºçæï¼è¿ç¨äºæé«éå»ºç²¾åº¦åç­ç¥å­¦ä¹ çé²æ£æ§ã</li>
<li><strong>å·èº«æºè½ä¸­è·¨å¹³å°æ°æ®åéç¨ä»£ççæå»ºï¼</strong> æ¨å¨å®ç°æ´å¹¿æ³ãæ´éç¨çæºå¨äººè½åã</li>
<li><strong>éè¿æç¤ºå·¥ç¨åç¥è¯æ³¨å¥æ¥å®å¶åå¢å¼ºMLLMsï¼</strong> ä»¥è§£å³ç¹å®ä¸æ¸¸ä»»å¡ï¼å¦æ£æµèåä¿¡æ¯ã</li>
<li><strong>å¯¹æ¨¡åå¬å¹³æ§ååè§çæç»­å³æ³¨ï¼</strong> å°¤å¶æ¯å¨ææåºç¨å¦äººè¸è¯å«ä¸­ã</li>
</ul>
<p><strong>å»ºè®®éè¯»å¨æçè®ºæï¼</strong></p>
<p>å¯¹äºä¸åå´è¶£çç ç©¶äººåï¼å»ºè®®éè¯»ä»¥ä¸è®ºæï¼</p>
<ul>
<li><strong>å¯¹å¤æ¨¡æLLMsåè§é¢çè§£æå´è¶£ï¼</strong> "Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding" (Zaiquan Yang et al.)</li>
<li><strong>å¯¹æºå¨äººæä½åå·èº«æºè½æå´è¶£ï¼</strong> "M4Diffuser: Multi-View Diffusion Policy with Manipulability-Aware Control for Robust Mobile Manipulation" (Ju Dong et al.)</li>
<li><strong>å¯¹3Déå»ºåæ©æ£æ¨¡åæå´è¶£ï¼</strong> "Lightweight and Accurate Multi-View Stereo with Confidence-Aware Diffusion Model" (Fangjinhua Wang et al.)</li>
<li><strong>å¯¹MLLMså¨å®å¨åæ£æµä»»å¡ä¸­çåºç¨æå´è¶£ï¼</strong> "DF-LLaVA: Unlocking MLLM's potential for Synthetic Image Detection via Prompt-Guided Knowledge Injection" (Zhuokang Shen et al.)</li>
<li><strong>å¯¹å¤§è§æ¨¡æ¨¡ååæ°æ®æ©å±æå´è¶£ï¼</strong> "SAIL-VL2 Technical Report" (Weijie Yin et al.) å "ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data" (Zhaoyang Liu et al.)</li>
</ul>
<p>è¿ä»½æè¦æ¨å¨å¸®å©æ¨å¿«éææ¡ä»æ¥Arxivè®¡ç®æºè§è§é¢åçå³é®è¿å±ï¼ä¸ºæ¨çç ç©¶æä¾æä»·å¼çåèã</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2509.14033v1">SAIL-VL2 Technical Report</a></li>
<li><a href="#2509.15221v1">ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data</a></li>
<li><a href="#2509.15220v1">Lightweight and Accurate Multi-View Stereo with Confidence-Aware Diffusion Model</a></li>
<li><a href="#2509.15219v1">Out-of-Sight Trajectories: Tracking, Fusion, and Prediction</a></li>
<li><a href="#2509.15178v1">Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding</a></li>
<li><a href="#2509.15177v1">A Race Bias Free Face Aging Model for Reliable Kinship Verification</a></li>
<li><a href="#2509.14980v1">M4Diffuser: Multi-View Diffusion Policy with Manipulability-Aware Control for Robust Mobile Manipulation</a></li>
<li><a href="#2509.14966v1">RoboEye: Enhancing 2D Robotic Object Identification with Selective 3D Geometric Keypoint Matching</a></li>
<li><a href="#2509.14957v1">DF-LLaVA: Unlocking MLLM's potential for Synthetic Image Detection via Prompt-Guided Knowledge Injection</a></li>
<li><a href="#2509.14746v1">Chain-of-Thought Re-ranking for Image Retrieval Tasks</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2509.14033v1'></a></p>
<h2 id="sail-vl2-technical-report"><a href="https://arxiv.org/abs/2509.14033v1">SAIL-VL2 Technical Report</a></h2>
<p><strong>Authors:</strong> Weijie Yin, Yongjie Ye, Fangxun Shu, Yue Liao, Zijian Kang, Hongyuan Dong, Haiyang Yu, Dingkang Yang, Jiacong Wang, Han Wang, Wenzhuo Liu, Xiao Liang, Shuicheng Yan, Chao Feng</p>
<p><strong>Published:</strong> 2025-09-17</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM)
for comprehensive multimodal understanding and reasoning. As the successor to
SAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8B
parameter scales across diverse image and video benchmarks, demonstrating
strong capabilities from fine-grained perception to complex reasoning. Three
core innovations drive its effectiveness. First, a large-scale data curation
pipeline with scoring and filtering strategies enhances both quality and
distribution across captioning, OCR, QA, and video data, improving training
efficiency. Second, a progressive training framework begins with a powerful
pre-trained vision encoder (SAIL-ViT), advances through multimodal
pre-training, and culminates in a thinking-fusion SFT-RL hybrid paradigm that
systematically strengthens model capabilities. Third, architectural advances
extend beyond dense LLMs to efficient sparse Mixture-of-Experts (MoE) designs.
With these contributions, SAIL-VL2 demonstrates competitive performance across
106 datasets and achieves state-of-the-art results on challenging reasoning
benchmarks such as MMMU and MathVista. Furthermore, on the OpenCompass
leaderboard, SAIL-VL2-2B ranks first among officially released open-source
models under the 4B parameter scale, while serving as an efficient and
extensible foundation for the open-source multimodal community.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºWeijie Yinç­äººå¨è®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åæ°åçè®ºæâSAIL-VL2 Technical Reportâçå¨é¢æè¦ã</p>
<p><strong>è®ºææè¦ï¼SAIL-VL2ææ¯æ¥å</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³å½åè§è§-è¯­è¨æ¨¡åï¼LVMï¼å¨å®ç°å¨é¢å¤æ¨¡æçè§£åæ¨çæ¹é¢çææãå°½ç®¡ç°æLVMå¨æ§è½ä¸æææåï¼ä½ä»é¢ä¸´è®¡ç®æçãè®­ç»ææ¬ä»¥åå¨ç»ç²åº¦æç¥åå¤ææ¨çä»»å¡ä¸­çå±éæ§ãSAIL-VL2çç®æ æ¯å¼åä¸ä¸ªé«æãå¯æ©å±çå¼æºLVMï¼è½å¤å®ç°æåè¿çæ§è½ï¼å¹¶æ¨å¨å¤æ¨¡æäººå·¥æºè½ï¼AGIï¼çåå±ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
SAIL-VL2çæææ§ä¸»è¦ç±ä»¥ä¸ä¸é¡¹æ ¸å¿åæ°é©±å¨ï¼</p>
<ul>
<li><strong>å¤§è§æ¨¡æ°æ®æ´çæµæ°´çº¿ï¼</strong> è®ºæè®¾è®¡äºä¸ä¸ªå¨é¢çæ°æ®æ´çæµæ°´çº¿ï¼éè¿è¯ååè¿æ»¤ç­ç¥ï¼æåäºå¾åãè§é¢ãOCRåé®ç­æ°æ®çè´¨éååå¸ãè¿æ¾èæé«äºè®­ç»æçï¼å¹¶ç¡®ä¿æ¨¡åè½å¤ä»é«è´¨éãå¤æ ·åçæ°æ®ä¸­å­¦ä¹ ã</li>
<li><strong>æ¸è¿å¼è®­ç»æ¡æ¶ï¼</strong> è®­ç»è¿ç¨åä¸ºä¸ä¸ªé¶æ®µï¼<ul>
<li>é¦åï¼ä½¿ç¨å¼ºå¤§çé¢è®­ç»è§è§ç¼ç å¨ï¼SAIL-ViTï¼è¿è¡ç­èº«éåºï¼å°è§è§è¾åºç²ç²åº¦å°éåºå°LLMåã</li>
<li>å¶æ¬¡ï¼éè¿å¤æ¨¡æé¢è®­ç»è¿è¡ç»ç²åº¦å¯¹é½ï¼è§£éè§è§ç¼ç å¨åééå¨ï¼ä»¥å®ç°æ´æ·±å±æ¬¡çå¯¹é½ã</li>
<li>æåï¼éè¿æç»´èåï¼Thinking-Fusionï¼SFT-RLï¼çç£å¾®è°-å¼ºåå­¦ä¹ ï¼æ··åèå¼ï¼ç³»ç»æ§å°å¼ºåæ¨¡åè½åï¼ä½¿å¶è½å¤è¿è¡å¤ææ¨çã</li>
</ul>
</li>
<li><strong>é«æçç¨çæ··åä¸å®¶ï¼MoEï¼æ¶æï¼</strong> SAIL-VL2è¶è¶äºä¼ ç»çå¯éLLMï¼éç¨äºæ´é«æçç¨çMoEè®¾è®¡ãè¿ä½¿å¾æ¨¡åè½å¤å¨ä¿æè®¡ç®æççåæ¶ï¼å®ç°åæ°è§æ¨¡çæ©å±ï¼å¹¶ç¡®ä¿ä¸å®¶æ¿æ´»çå¹³è¡¡æ§åç¨³å®æ§ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
SAIL-VL2å¨å¤ä¸ªç»´åº¦ä¸å±ç¤ºäºå¼ºå¤§çæ§è½ï¼</p>
<ul>
<li><strong>æåè¿çæ§è½ï¼</strong> SAIL-VL2å¨2Bå8Båæ°è§æ¨¡ä¸ï¼è·¨106ä¸ªæ°æ®éå®ç°äºæåè¿çæ§è½ï¼æ¶µçäºå¾ååè§é¢åºåæµè¯ï¼ä»ç»ç²åº¦æç¥å°å¤ææ¨çé½è¡¨ç°åºå¼ºå¤§çè½åã</li>
<li><strong>åè¶çæ¨çè½åï¼</strong> å¨MMMUåMathVistaç­æææ§æ¨çåºåæµè¯ä¸­ï¼SAIL-VL2åå¾äºæåè¿çç»æãç¹å«æ¯ï¼SAIL-VL2-Thinkingæ¨¡åå¨OpenCompasså¤æ¨¡ææ¨çåºåæµè¯ä¸­åå¾äºé¢åç»æï¼çè³è¶è¶äºGemini-2.0-Flashç­å¼ºå¤§çé­æºæ¨¡åã</li>
<li><strong>é«æä¸å¯æ©å±æ§ï¼</strong> SAIL-VL2-2Bå¨OpenCompassæè¡æ¦ä¸ï¼å¨4Båæ°è§æ¨¡ä»¥ä¸çå¼æºæ¨¡åä¸­æåç¬¬ä¸ï¼è¯æäºå¶ä½ä¸ºé«æä¸å¯æ©å±çå¤æ¨¡æç¤¾åºåºç¡æ¨¡åçæ½åã</li>
<li><strong>ç»ç²åº¦è§è§çè§£ï¼</strong> å¨OCRãé«åè¾¨çææ¡£å¸å±åæåå¤æå¾è¡¨è§£éç­ä»»å¡ä¸­ï¼SAIL-VL2è¡¨ç°åºé«ä¿çåº¦çæç¥è½åï¼å®ç°äºè¶è¶åç­è§æ¨¡æ¨¡åçè¯¦ç»è§è§åºç¡ã</li>
</ul>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸­æ²¡ææç¡®æåSAIL-VL2çæ¾èå±éæ§ãç¶èï¼å¨æ°æ®æ´çé¨åï¼ä½èæå°åææ°æ®å¯è½ä¼å¼å¥è¯­è¨è¡¨è¾¾ä¸çåå¸åå·®ï¼å ä¸ºLLMå¾åäºäº§çåè´¨åçæªè¾ï¼ä½ä»ä»¬åç°æ¨¡åä»è½ä»å¤§è§æ¨¡åææ°æ®ä¸­åçãæ­¤å¤ï¼å¨RLé¶æ®µï¼å¯¹äºæ æ³ç¡®å®æ§éªè¯çå®ç­æ¡çä»»å¡ï¼éè¦LVMä½ä¸ºå¤æ­æ¨¡åæ¥æä¾å¥å±ä¿¡å·ï¼è¿å¯è½å¼å¥ä¸å®ç¨åº¦çä¾èµæ§ææ½å¨åå·®ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºææåºï¼æªæ¥å°ç»§ç»­éè¿ä»¥ä¸æ¹å¼è¿ä¸æ­¥å¢å¼ºSAIL-VLç³»åï¼</p>
<ul>
<li><strong>æ´é«æçæ¶æï¼</strong> æç»­æ¢ç´¢åå¼åæ´é«æçæ¨¡åæ¶æã</li>
<li><strong>å¨é¢çé¢è®­ç»ç­ç¥ï¼</strong> è¿ä¸æ­¥ä¼ååå®åé¢è®­ç»ç­ç¥ã</li>
<li><strong>æ¹è¿çå¼ºåå­¦ä¹ èå¼ï¼</strong> æç»­æ¹è¿å¼ºåå­¦ä¹ æ¹æ³ï¼ä»¥å®ç°æ¨¡åè½åçæç»­æ¼è¿ã</li>
</ul>
<p>æ»èè¨ä¹ï¼SAIL-VL2ä»£è¡¨äºé«æå¤§åè§è§-è¯­è¨æ¨¡åè®¾è®¡ä¸çå¨é¢è¿æ­¥ï¼éè¿å¨æ¶æãè®­ç»ç­ç¥åæ°æ®æ´çæ¹é¢çåæ°ï¼å®ç°äºæçåæ§è½çæç»­æåï¼ä¸ºå¼æºå¤æ¨¡æç¤¾åºå¥ å®äºåå®çåºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM)
for comprehensive multimodal understanding and reasoning.</li>
<li>As the successor to
SAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8B
parameter scales across diverse image and video benchmarks, demonstrating
strong capabilities from fine-grained perception to complex reasoning.</li>
<li>With these contributions, SAIL-VL2 demonstrates competitive performance across
106 datasets and achieves state-of-the-art results on challenging reasoning
benchmarks such as MMMU and MathVista.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.14033v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.14033v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.15221v1'></a></p>
<h2 id="scalecua-scaling-open-source-computer-use-agents-with-cross-platform-data"><a href="https://arxiv.org/abs/2509.15221v1">ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data</a></h2>
<p><strong>Authors:</strong> Zhaoyang Liu, JingJing Xie, Zichen Ding, Zehao Li, Bowen Yang, Zhenyu Wu, Xuehui Wang, Qiushi Sun, Shi Liu, Weiyun Wang, Shenglong Ye, Qingyun Li, Zeyue Tian, Gen Luo, Xiangyu Yue, Biqing Qi, Kai Chen, Bowen Zhou, Yu Qiao, Qifeng Chen, Wenhai Wang</p>
<p><strong>Published:</strong> 2025-09-18</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Vision-Language Models (VLMs) have enabled computer use agents (CUAs) that
operate GUIs autonomously, showing great potential, yet progress is limited by
the lack of large-scale, open-source computer use data and foundation models.
In this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. It
offers a large-scale dataset spanning 6 operating systems and 3 task domains,
built via a closed-loop pipeline uniting automated agents with human experts.
Trained on this scaled-up data, ScaleCUA can operate seamlessly across
platforms. Specifically, it delivers strong gains over baselines (+26.6 on
WebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-art
results (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% on
WebArena-Lite-v2). These findings underscore the power of data-driven scaling
for general-purpose computer use agents. We will release data, models, and code
to advance future research: https://github.com/OpenGVLab/ScaleCUA.</p>
<p><strong>Analysis:</strong></p>
<p>ä»¥ä¸æ¯å³äºZhaoyang Liuç­äººæ°åçè®ºæâScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Dataâçæè¦ï¼</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³å½åè®¡ç®æºä½¿ç¨ä»£çï¼CUAsï¼åå±é¢ä¸´çæ ¸å¿ææï¼ç¼ºä¹å¤§è§æ¨¡ãå¼æºçè®¡ç®æºä½¿ç¨æ°æ®ååºç¡æ¨¡åï¼è¿éå¶äºè§è§è¯­è¨æ¨¡åï¼VLMsï¼å¨èªä¸»æä½å¾å½¢ç¨æ·çé¢ï¼GUIsï¼æ¹é¢çæ½åãç°ææ°æ®æ¶éææ¬é«æãé¾ä»¥æ©å±ä¸éç¨æ§å·®ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>å¤§è§æ¨¡è·¨å¹³å°æ°æ®éï¼</strong> å¼å¥ScaleCUAæ°æ®éï¼æ¶µçWindowsãmacOSãLinuxãAndroidãiOSåWebå­å¤§æä½ç³»ç»ï¼ä»¥åGUIçè§£ãGUIå®ä½åä»»å¡å®æä¸å¤§ä»»å¡é¢åã
*   <strong>é­ç¯æ°æ®æ¶éç®¡éï¼</strong> éç¨èªå¨åä»£çä¸äººç±»ä¸å®¶ç¸ç»åçé­ç¯ç®¡éï¼é«ææ¶éé«è´¨éçåå§è½¨è¿¹æ°æ®ï¼åæ¬å±å¹æªå¾åç»æååæ°æ®ï¼å¹¶è¿è¡æ æ³¨åå¢å¼ºã
*   <strong>ç»ä¸å¨ä½ç©ºé´ï¼</strong> è®¾è®¡äºä¸ä¸ªè·¨å¹³å°çç»ä¸å¨ä½ç©ºé´ï¼ä½¿å¾ä»£çè½å¤ä»¥æ ååæ¹å¼ä¸å¼æç¯å¢äº¤äºï¼ç®åäºæ°æ®æ æ³¨åç­ç¥å­¦ä¹ ã
*   <strong>ScaleCUAåºç¡æ¨¡åå®¶æï¼</strong> åºäºQwen2.5-VLè®­ç»äºä¸ç³»åScaleCUAåºç¡ä»£çæ¨¡åï¼æ¯æä¸ç§æ¨çèå¼ï¼å®ä½æ¨¡å¼ï¼Grounding Modeï¼ãç´æ¥å¨ä½æ¨¡å¼ï¼Direct Action Modeï¼åæ¨çå¨ä½æ¨¡å¼ï¼Reasoned Action Modeï¼ï¼ä»¥å®ç°æç¥ãæ¨çåå¨ä½çç»ä¸ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>åè¶çæ§è½æåï¼</strong> ScaleCUAå¨å¤ä¸ªGUIåºåæµè¯ä¸­åå¾äºæ¾èçæ§è½æåï¼ä¾å¦å¨WebArena-Lite-v2ä¸æå+26.6ï¼å¨ScreenSpot-Proä¸æå+10.7ã
*   <strong>æ°çSOTAç»æï¼</strong> å¨MMBench-GUI L1-Hardä¸è¾¾å°94.4%ï¼å¨OSWorld-Gä¸è¾¾å°60.6%ï¼å¨WebArena-Lite-v2ä¸è¾¾å°47.4%ï¼ååä¸æ°çæåè¿ï¼SOTAï¼è®°å½ã
*   <strong>æ°æ®é©±å¨æ©å±çæææ§ï¼</strong> å®éªç»æå¼ºè°äºæ°æ®é©±å¨æ©å±å¯¹äºéç¨è·¨å¹³å°CUAsçå¼ºå¤§ä½ç¨ï¼è¯æäºå¤æ ·åè®­ç»è¯­æåºè½æ¾èå¢å¼ºè§è§çè§£è½åã
*   <strong>æ¨çæ¨¡å¼çä¼å¿ï¼</strong> æ¨çå¨ä½æ¨¡å¼ï¼RAMï¼å¨ææåºåæµè¯ä¸­åä¼äºç´æ¥å¨ä½æ¨¡å¼ï¼DAMï¼ï¼å°¤å¶å¨å¤æå¤æ­¥éª¤ç¯å¢ä¸­è¡¨ç°çªåºï¼è¡¨ææ¾å¼æ¨çæå©äºç»´æä»»å¡è¿è´¯æ§å¹¶åå°éè¯¯ä¼ æ­ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>ä»£çæ¶éæ°æ®è´¨éï¼</strong> èªå¨åä»£çæ¶éçæ°æ®è´¨éä»è½åäºäººç±»ä¸å®¶æ æ³¨çæ°æ®ï¼è§åé©±å¨çæ¢ç´¢å¯è½äº§çè¯­ä¹è¾å¼±çè½¨è¿¹ã
*   <strong>é«çº§ä»£çæºå¶ï¼</strong> å½åå·¥ä½å°æªæ´ååå°ãåºäºè®°å¿çå³ç­æåå±è§åç­é«çº§ä»£çæºå¶ã
*   <strong>è®°å¿æºå¶çå±éæ§ï¼</strong> å½åè®°å¿æºå¶è¾ä¸ºåçº§ï¼å°è¿å¾æä½è§ä¸ºæå¹³åå²ï¼è¿éå¶äºé¿å¨ææ¨çè½åã
*   <strong>è·¨åºç¨æ³åï¼</strong> å¨é¿å¨ææ¨çåè·¨åºç¨æ³åæ¹é¢ä»å­å¨å±éæ§ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>èªæ¹è¿å­¦ä¹ å¾ªç¯ï¼</strong> ææç»åèªå¨åæ°æ®æ¶éä¸è¿­ä»£æ¨¡åä¼åï¼å½¢æä¸ä¸ªèªæ¹è¿å­¦ä¹ å¾ªç¯ã
*   <strong>é«çº§ä»£çæºå¶éæï¼</strong> æ¢ç´¢å¹¶æ´åå¼ºåå­¦ä¹ ãç­ç¥å¥å±æ¨¡åï¼PRMsï¼ãåå°ãè®°å¿ååå±è§åç­é«çº§ä»£çæºå¶ã
*   <strong>è½»éçº§è®°å¿ç³»ç»ï¼</strong> å¼åè½å¤æè·æ¶é´ä¾èµå³ç³»çè½»éçº§ä¸ææçè®°å¿ç³»ç»ã
*   <strong>æ°æ®æ··åç­ç¥ï¼</strong> éå¯¹éç¨å¤æ¨¡ææ°æ®ä¸GUIç¹å®æ°æ®ä¹é´çå²çªä¼åä¿¡å·ï¼éè¦æ´ç²¾ç»çæ°æ®æ··åç­ç¥ã
*   <strong>åçæ¡é¢ç¯å¢è¯ä¼°ï¼</strong> ä¼åå¨åçæ¡é¢ç¯å¢ä¸­æ§è¡åºäºWebçä»»å¡ï¼å¯è½éè¦å¼åæ°çåºåæµè¯ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. It
offers a large-scale dataset spanning 6 operating systems and 3 task domains,
built via a closed-loop pipeline uniting automated agents with human experts.</li>
<li>Specifically, it delivers strong gains over baselines (+26.6 on
WebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-art
results (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% on
WebArena-Lite-v2).</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.15221v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.15221v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.15220v1'></a></p>
<h2 id="lightweight-and-accurate-multi-view-stereo-with-confidence-aware-diffusion-model"><a href="https://arxiv.org/abs/2509.15220v1">Lightweight and Accurate Multi-View Stereo with Confidence-Aware Diffusion Model</a></h2>
<p><strong>Authors:</strong> Fangjinhua Wang, Qingshan Xu, Yew-Soon Ong, Marc Pollefeys</p>
<p><strong>Published:</strong> 2025-09-18</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>To reconstruct the 3D geometry from calibrated images, learning-based
multi-view stereo (MVS) methods typically perform multi-view depth estimation
and then fuse depth maps into a mesh or point cloud. To improve the
computational efficiency, many methods initialize a coarse depth map and then
gradually refine it in higher resolutions. Recently, diffusion models achieve
great success in generation tasks. Starting from a random noise, diffusion
models gradually recover the sample with an iterative denoising process. In
this paper, we propose a novel MVS framework, which introduces diffusion models
in MVS. Specifically, we formulate depth refinement as a conditional diffusion
process. Considering the discriminative characteristic of depth estimation, we
design a condition encoder to guide the diffusion process. To improve
efficiency, we propose a novel diffusion network combining lightweight 2D U-Net
and convolutional GRU. Moreover, we propose a novel confidence-based sampling
strategy to adaptively sample depth hypotheses based on the confidence
estimated by diffusion model. Based on our novel MVS framework, we propose two
novel MVS methods, DiffMVS and CasDiffMVS. DiffMVS achieves competitive
performance with state-of-the-art efficiency in run-time and GPU memory.
CasDiffMVS achieves state-of-the-art performance on DTU, Tanks &amp; Temples and
ETH3D. Code is available at: https://github.com/cvg/diffmvs.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Fangjinhua Wangç­äººæ°åçè®ºæâLightweight and Accurate Multi-View Stereo with Confidence-Aware Diffusion Modelâçå¨é¢æè¦ã</p>
<hr />
<h3 id="_1">è®ºææè¦ï¼åºäºç½®ä¿¡åº¦æç¥çæ©æ£æ¨¡åå®ç°è½»éçº§é«ç²¾åº¦å¤è§è§ç«ä½å¹é</h3>
<p><strong>1. è§£å³çä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³å¤è§è§ç«ä½å¹éï¼MVSï¼ä¸­3Då ä½éå»ºçæçåç²¾åº¦é®é¢ãä¼ ç»çMVSæ¹æ³éå¸¸éè¿å¤è§è§æ·±åº¦ä¼°è®¡åæ·±åº¦å¾èåæ¥éå»º3Då ä½ãä¸ºäºæé«è®¡ç®æçï¼è®¸å¤æ¹æ³ä¼ååå§åä¸ä¸ªç²ç¥çæ·±åº¦å¾ï¼ç¶åéæ­¥å¨é«åè¾¨çä¸è¿è¡ç»åãç¶èï¼ç°æçå­¦ä¹ åMVSæ¹æ³å¨å¤çåç§ååãéæä¼¯è¡¨é¢åä½çº¹çåºåç­æææ§åºæ¯æ¶ä»é¢ä¸´å°é¾ï¼å¹¶ä¸å¨æçåç²¾åº¦ä¹é´å­å¨æè¡¡ãç¹å«æ¯ï¼å°æ©æ£æ¨¡åå¼å¥MVSé¢ä¸´çå¦ä½ææå©ç¨æ¡ä»¶ä¿¡æ¯ãå¦ä½è¿è¡é«æéæ ·ä»¥åå¦ä½ä¿æè®¡ç®æçç­ææã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥è®ºææåºäºä¸ä¸ªæ°é¢çMVSæ¡æ¶ï¼å°æ©æ£æ¨¡åå¼å¥MVSï¼å¹¶å°å¶åºç¨äºæ·±åº¦ç»åä»»å¡ãä¸»è¦åæ°åæ¬ï¼</p>
<ul>
<li><strong>æ¡ä»¶æ©æ£è¿ç¨çæ·±åº¦ç»åï¼</strong> å°æ·±åº¦ç»åä»»å¡éæ°å®ä¹ä¸ºæ¡ä»¶æ©æ£è¿ç¨ï¼ä»éæºåªå£°å¼å§ï¼éè¿è¿­ä»£å»åªéæ­¥æ¢å¤æ·±åº¦å¾ã</li>
<li><strong>æ¡ä»¶ç¼ç å¨ï¼Condition Encoderï¼ï¼</strong> è®¾è®¡äºä¸ä¸ªæ¡ä»¶ç¼ç å¨ï¼ç¨äºèåå¹éä¿¡æ¯ãå¾åä¸ä¸æåæ·±åº¦ä¸ä¸æç¹å¾ï¼ä»¥æå¯¼æ©æ£è¿ç¨ãè¿ä½¿å¾æ¨¡åè½å¤æç¥å±é¨ç¸ä¼¼æ§åé¿è·ç¦»ä¸ä¸æä¿¡æ¯ï¼ä»èçæåç¡®çæ·±åº¦é¢æµã</li>
<li><strong>åºäºç½®ä¿¡åº¦çéæ ·ç­ç¥ï¼Confidence-based Sampling Strategyï¼ï¼</strong> å¼å¥äºä¸ç§æ°é¢çéæ ·ç­ç¥ï¼æ ¹æ®æ©æ£æ¨¡åä¼°è®¡çç½®ä¿¡åº¦èªéåºå°çæåç´ çº§çå¤ä¸ªæ·±åº¦åè®¾ãè¿åæäºä¼ ç»æ¹æ³åºå®éæ ·èå´çå±éæ§ï¼éè¿è°æ´éæ ·èå´æ¥æè·éå±é¨ä¸é¶ä¼åä¿¡æ¯ï¼ä»èæé«å»åªè¿ç¨çæçååç¡®æ§ã</li>
<li><strong>è½»éçº§æ©æ£ç½ç»ï¼Lightweight Diffusion Networkï¼ï¼</strong> æåºäºä¸ç§ç»åè½»éçº§2D U-Netåå·ç§¯GRUçæ°åæ©æ£ç½ç»ãéè¿å¨åä¸ªæ©æ£æ¶é´æ­¥åè¿è¡å¤è¿­ä»£ç»åï¼å¹¶å©ç¨GRUæè·åå²ä¿¡æ¯ï¼æ¾èæé«äºè®¡ç®æçï¼é¿åäºä½¿ç¨å¤§åæå å U-Netã</li>
<li><strong>ä¸¤ç§æ°åMVSæ¹æ³ï¼</strong> åºäºè¯¥æ¡æ¶ï¼æåºäºDiffMVSåCasDiffMVSãDiffMVSä¸æ³¨äºå®æ¶åºç¨ï¼éè¿åé¶æ®µæ©æ£æ¨¡åè¿è¡æ·±åº¦ç»åï¼CasDiffMVSåéè¿ä¸¤é¶æ®µçº§èæ©æ£ç»åï¼æ¨å¨å®ç°é«ç²¾åº¦éå»ºã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
å®éªç»æè¡¨æï¼è¯¥æ¹æ³å¨å¤ä¸ªåºåæµè¯ä¸åå¾äºæ¾èçæ§è½ï¼</p>
<ul>
<li><strong>DiffMVSï¼</strong> å¨è¿è¡æ¶åGPUåå­æçæ¹é¢ï¼DiffMVSè¾¾å°äºä¸ç°æææ¯ç¸å½çæ§è½ï¼åæ¶å¨Tanks &amp; TemplesåETH3Dæ°æ®éä¸åå¾äºæç«äºåçéå»ºæ§è½ã</li>
<li><strong>CasDiffMVSï¼</strong> å¨DTUãTanks &amp; TemplesåETH3Dæ°æ®éä¸ï¼CasDiffMVSå®ç°äºæåè¿çéå»ºæ§è½ï¼åæ¶ä¿æäºé«æçã</li>
<li><strong>æçä¼å¿ï¼</strong> ç¸æ¯äºæåè¿çIterMVSæ¹æ³ï¼DiffMVSå¨GPUåå­æ¶èä¸åå°äº9.13%ï¼éåº¦æåäº69.49%ãCasDiffMVSçæçä¸PatchmatchNetç¸å½ï¼ä½æ§è½ä¼äºå¶ä»é¡¶å°æ¹æ³ã</li>
<li><strong>æ³åè½åï¼</strong> è¯¥æ¹æ³å¨Tanks &amp; TemplesåETH3Dç­æææ§åºæ¯ä¸­è¡¨ç°åºå¼ºå¤§çé¶æ ·æ¬æ³åè½åï¼è½å¤çææ´å®æ´ãæ´åç¡®çè¡¨é¢ã</li>
<li><strong>æ¶èç ç©¶ï¼</strong> éªè¯äºæ©æ£æºå¶ãæ¡ä»¶ç¼ç å¨ï¼åæ¬ææ¬ä½ãæ·±åº¦ä¸ä¸æåå¾åä¸ä¸æï¼ä»¥ååºäºç½®ä¿¡åº¦çéæ ·ç­ç¥çæææ§ãç»æè¡¨æï¼è¿äºç»ä»¶å¯¹äºæé«éå»ºç²¾åº¦åæ³åè½åè³å³éè¦ã</li>
</ul>
<p>è¿äºç»æçæä¹å¨äºï¼è¯¥è®ºææåå°å°æ©æ£æ¨¡åå¼å¥MVSï¼å¹¶å¨æçåç²¾åº¦ä¹é´åå¾äºæ°çå¹³è¡¡ï¼ä¸ºMVSé¢åæä¾äºä¸ä¸ªå¼ºå¤§ä¸è½»éçº§çæ°åºçº¿ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸­æ²¡ææç¡®æåå·ä½çå±éæ§ï¼ä½ä»æ¹æ³è®¾è®¡åå®éªè®¾ç½®ä¸­å¯ä»¥æ¨æ­åºä¸äºæ½å¨çæ¹é¢ï¼</p>
<ul>
<li><strong>è¶åæ°è°ä¼ï¼</strong> æ©æ£æ¨¡åçåªå£°å°ºåº¦ãéæ ·æ­¥æ°ç­è¶åæ°éè¦ä»ç»è°ä¼ï¼ä»¥å¹³è¡¡æ§è½åæçãè½ç¶è®ºææä¾äºé»è®¤è®¾ç½®ï¼ä½å¨ä¸ååºæ¯ä¸å¯è½éè¦è¿ä¸æ­¥ä¼åã</li>
<li><strong>è®­ç»æ°æ®ä¾èµï¼</strong> å°½ç®¡å¨BlendedMVSä¸è¿è¡äºå¾®è°ä»¥æé«æ³åè½åï¼ä½ä½ä¸ºå­¦ä¹ åæ¹æ³ï¼å¶æ§è½ä»å¯è½åå°è®­ç»æ°æ®åå¸çå½±åã</li>
<li><strong>å¤æåºæ¯çé²æ£æ§ï¼</strong> å°½ç®¡å¨æææ§åºæ¯ä¸­è¡¨ç°è¯å¥½ï¼ä½æç«¯æåµï¼å¦æåº¦ä½çº¹çãå¼ºååç­ï¼ä¸çé²æ£æ§ä»æå¾è¿ä¸æ­¥æ¢ç´¢ã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è¯¥è®ºæä¸ºMVSé¢åçæªæ¥ç ç©¶å¼è¾äºå¤ä¸ªæ¹åï¼</p>
<ul>
<li><strong>æ´é«æçæ©æ£æ¨¡åï¼</strong> è¿ä¸æ­¥æ¢ç´¢æ´è½»éçº§ãæ´å¿«çæ©æ£æ¨¡åæ¶æï¼ä»¥éåºæ´å¤èµæºåéçè®¾å¤ã</li>
<li><strong>èªéåºåªå£°è°åº¦ï¼</strong> ç ç©¶æ´æºè½çåªå£°è°åº¦ç­ç¥ï¼ä½¿å¶è½å¤æ ¹æ®åºæ¯ç¹æ§æä¼°è®¡çæ·±åº¦ä¸ç¡®å®æ§èªéåºè°æ´ï¼è¿ä¸æ­¥æé«ç²¾åº¦åé²æ£æ§ã</li>
<li><strong>å¤æ¨¡æèåï¼</strong> æ¢ç´¢å°æ©æ£æ¨¡åä¸å¶ä»ä¼ æå¨æ°æ®ï¼å¦LiDARãIMUç­ï¼èåï¼ä»¥è¿ä¸æ­¥æåMVSå¨å¤æç¯å¢ä¸çæ§è½ã</li>
<li><strong>å®æ¶MVSåºç¨ï¼</strong> è¿ä¸æ­¥ä¼åDiffMVSï¼ä½¿å¶å¨å®æ¶MVSåºç¨ä¸­åæ¥æ´å¤§æ½åï¼ä¾å¦æºå¨äººå¯¼èªãèªå¨é©¾é©¶ç­ã</li>
<li><strong>æ çç£æåçç£å­¦ä¹ ï¼</strong> æ¢ç´¢å¨MVSä¸­å©ç¨æ©æ£æ¨¡åè¿è¡æ çç£æåçç£å­¦ä¹ ï¼ä»¥åå°å¯¹å¤§éæ æ³¨æ°æ®çä¾èµã</li>
</ul>
<hr />
<p>è¿ç¯è®ºæéè¿å°æ©æ£æ¨¡åä¸MVSä»»å¡ç¸ç»åï¼å¹¶å¼å¥ä¸ç³»ååæ°æ§çè®¾è®¡ï¼ä¸º3Då ä½éå»ºé¢åå¸¦æ¥äºæ°ççªç ´ï¼ç¹å«æ¯å¨æçåç²¾åº¦æ¹é¢åå¾äºä»¤äººå°è±¡æ·±å»çææã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In
this paper, we propose a novel MVS framework, which introduces diffusion models
in MVS.</li>
<li>To improve
efficiency, we propose a novel diffusion network combining lightweight 2D U-Net
and convolutional GRU.</li>
<li>Moreover, we propose a novel confidence-based sampling
strategy to adaptively sample depth hypotheses based on the confidence
estimated by diffusion model.</li>
<li>Based on our novel MVS framework, we propose two
novel MVS methods, DiffMVS and CasDiffMVS.</li>
<li>DiffMVS achieves competitive
performance with state-of-the-art efficiency in run-time and GPU memory.</li>
<li>CasDiffMVS achieves state-of-the-art performance on DTU, Tanks &amp; Temples and
ETH3D.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.15220v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.15220v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.15219v1'></a></p>
<h2 id="out-of-sight-trajectories-tracking-fusion-and-prediction"><a href="https://arxiv.org/abs/2509.15219v1">Out-of-Sight Trajectories: Tracking, Fusion, and Prediction</a></h2>
<p><strong>Authors:</strong> Haichao Zhang, Yi Xu, Yun Fu</p>
<p><strong>Published:</strong> 2025-09-18</p>
<p><strong>Categories:</strong> cs.CV, cs.LG, cs.MA, cs.MM, cs.RO, 68T45, 68U10, 68T07, 68T40, 93C85, 93E11, 62M20, 62M10, 68U05, 94A12, F.2.2; I.2.9; I.2.10; I.4.1; I.4.8; I.4.9; I.5.4; I.3.7</p>
<p><strong>Abstract:</strong></p>
<p>Trajectory prediction is a critical task in computer vision and autonomous
systems, playing a key role in autonomous driving, robotics, surveillance, and
virtual reality. Existing methods often rely on complete and noise-free
observational data, overlooking the challenges associated with out-of-sight
objects and the inherent noise in sensor data caused by limited camera
coverage, obstructions, and the absence of ground truth for denoised
trajectories. These limitations pose safety risks and hinder reliable
prediction in real-world scenarios. In this extended work, we present
advancements in Out-of-Sight Trajectory (OST), a novel task that predicts the
noise-free visual trajectories of out-of-sight objects using noisy sensor data.
Building on our previous research, we broaden the scope of Out-of-Sight
Trajectory Prediction (OOSTraj) to include pedestrians and vehicles, extending
its applicability to autonomous driving, robotics, surveillance, and virtual
reality. Our enhanced Vision-Positioning Denoising Module leverages camera
calibration to establish a vision-positioning mapping, addressing the lack of
visual references, while effectively denoising noisy sensor data in an
unsupervised manner. Through extensive evaluations on the Vi-Fi and JRDB
datasets, our approach achieves state-of-the-art performance in both trajectory
denoising and prediction, significantly surpassing previous baselines.
Additionally, we introduce comparisons with traditional denoising methods, such
as Kalman filtering, and adapt recent trajectory prediction models to our task,
providing a comprehensive benchmark. This work represents the first initiative
to integrate vision-positioning projection for denoising noisy sensor
trajectories of out-of-sight agents, paving the way for future advances. The
code and preprocessed datasets are available at github.com/Hai-chao-Zhang/OST</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºHaichao Zhang, Yi Xu, Yun Fuæ°åçè®ºæâOut-of-Sight Trajectories: Tracking, Fusion, and Predictionâçå¨é¢æè¦ï¼æ¶µçäºæ¨è¦æ±çææè¦ç¹ï¼</p>
<p><strong>è®ºææè¦ï¼Out-of-Sight Trajectories: Tracking, Fusion, and Prediction</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³è®¡ç®æºè§è§åèªä¸»ç³»ç»ä¸­è½¨è¿¹é¢æµé¢ä¸´çå³é®ææï¼ç¹å«æ¯å½ç®æ ç©ä½âä¸å¯è§âï¼out-of-sightï¼æ¶ãç°ææ¹æ³éå¸¸ä¾èµäºå®æ´ãæ åªå£°çè§æµæ°æ®ï¼ä½ç°å®ä¸çä¸­ï¼ç±äºæåå¤´è¦çèå´æéãéç¢ç©åç¼ºä¹å»åªè½¨è¿¹ççå¼ï¼ä¼ æå¨æ°æ®å¾å¾å­å¨åªå£°ï¼ä¸ç©ä½å¯è½å®å¨è¶åºè§éãè¿å¯¼è´äºå®å¨é£é©ï¼å¹¶é»ç¢äºå¨èªå¨é©¾é©¶ãæºå¨äººãçæ§åèæç°å®ç­åºç¨ä¸­è¿è¡å¯é çè½¨è¿¹é¢æµãæ ¸å¿é®é¢æ¯å¦ä½å©ç¨æåªå£°çä¼ æå¨æ°æ®ï¼é¢æµä¸å¯è§ç©ä½çæ åªå£°è§è§è½¨è¿¹ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è´¡ç®ï¼</strong>
è¯¥è®ºææåºäºâOut-of-Sight Trajectory (OST)âè¿ä¸æ°é¢ä»»å¡ï¼å¹¶å¼å¥äºä¸ä¸ªåæ°çâVision-Positioning Denoising Module (VPD)âæ¡æ¶æ¥è§£å³å®ãä¸»è¦è´¡ç®åæ¬ï¼
*   <strong>ä»»å¡æ©å±ï¼</strong> å°OSTä»»å¡çèå´ä»è¡äººæ©å±å°è½¦è¾ï¼ä½¿å¶éç¨äºæ´å¹¿æ³çåºæ¯ï¼å¦èªå¨é©¾é©¶ãæºå¨äººãçæ§åèæç°å®ã
*   <strong>æ çç£å»åªï¼</strong> éå¯¹æåªå£°çä¼ æå¨æ°æ®ï¼æåºäºä¸ç§æ çç£å»åªæ¹æ³ãç±äºç¼ºä¹å»åªè½¨è¿¹ççå¼ï¼ä¼ ç»çç£å­¦ä¹ æ¹æ³ä¸å¯è¡ãè¯¥æ¹æ³éè¿å©ç¨è§è§å®ä½æå½±æ¥æå»ºææçå»åªçç£ã
*   <strong>è§è§å®ä½æå½±ï¼</strong> å¼å¥äºâVision-Positioning Projection Module (VPP)âåâMapping Parameters Estimator (MPE)âãVPPè´è´£å°å»åªåçä¼ æå¨è½¨è¿¹ï¼3Dä¸çåæ ï¼æ å°å°2Dæåå¤´åæ ãMPEéè¿åæå¯è§ç©ä½çè§è§åä¼ æå¨è½¨è¿¹ä¹é´çç¸å³æ§ï¼å¨æé¢æµæåå¤´ç©éµåµå¥ï¼åæ¬åååå¤åï¼ï¼ä»èè§£å³äºç¼ºä¹ç´æ¥è§è§åèçé®é¢ã
*   <strong>Transformeræ¶æï¼</strong> ä¼ æå¨å»åªç¼ç å¨ï¼SDEï¼ãæ å°åæ°ä¼°è®¡å¨ï¼MPEï¼åä¸å¯è§ç©ä½é¢æµè§£ç å¨ï¼OPDï¼åéç¨åºäºTransformerçæ¶æï¼ä»¥ææææè½¨è¿¹æ°æ®ä¸­çæ¶åºåä¸ä¸æä¾èµæ§ã
*   <strong>æ¨¡ååè®¾è®¡ï¼</strong> æ´ä¸ªæ¡æ¶è®¾è®¡ä¸ºæ¨¡ååï¼åæ¬SDEï¼å»åªï¼ãMPEï¼æ å°åæ°ä¼°è®¡ï¼ãVPPï¼è§è§æå½±ï¼åOPDï¼é¢æµï¼ï¼æ¯ä¸ªæ¨¡åååå·¥ä½ï¼ç¡®ä¿å¯¹æåªå£°åä¸å®æ´ä¼ æå¨æ°æ®çå¨é¢å¤çã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æåè¿çæ§è½ï¼</strong> å¨Vi-FiåJRDBæ°æ®éä¸çå¹¿æ³è¯ä¼°è¡¨æï¼è¯¥æ¹æ³å¨è½¨è¿¹å»åªåé¢æµæ¹é¢åè¾¾å°äºæåè¿çæ§è½ï¼æ¾èè¶è¶äºç°æåºçº¿ã
*   <strong>å»åªçéè¦æ§ï¼</strong> å®éªç»æå¼ºè°äºé²æ£å»åªå¨æé«ä¸å¯è§ç©ä½è½¨è¿¹é¢æµåç¡®æ§æ¹é¢çå³é®ä½ç¨ï¼å»åªæ§è½çæåç´æ¥è½¬åä¸ºé¢æµåç¡®æ§çæé«ã
*   <strong>VPDæ¨¡åçæææ§ï¼</strong> æ¶èç ç©¶è¯å®äºVPDæ¨¡åä¸­æ¯ä¸ªç»ä»¶ï¼SDEãMPEãVPPãOPDï¼çå¿è¦æ§ï¼å®ä»¬å±åä¿è¿äºæ¨¡åå®ç°æä¼æ§è½ã
*   <strong>æ³åè½åï¼</strong> è¯¥æ¨¡åå¨å¤çä¸åä¼ æå¨åªå£°åºæ¯åå©ç¨ç²¾ç¡®è§è§çç£æ¹é¢è¡¨ç°åºé²æ£æ§ï¼å¨ä¸¤ä¸ªæ°æ®éä¸åè¡¨ç°åºä¸è´çæ§è½ã
*   <strong>å¼åæ§å·¥ä½ï¼</strong> è¿æ¯é¦æ¬¡å°è¯æ´åè§è§å®ä½æå½±æ¥å»åªä¸å¯è§ç©ä½çæåªå£°ä¼ æå¨è½¨è¿¹ï¼ä¸ºè¯¥é¢åçæªæ¥åå±éºå¹³äºéè·¯ã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
*   <strong>æåå¤´æ ¡åçéå¼çº¦æï¼</strong> è®ºææåºï¼å¨è®¸å¤å®éåºæ¯ä¸­ï¼æ°æ®éä¸æä¾ååç©éµï¼ä¸å®éæåå¤´æä½ï¼å¦åç¦ãèªå¨å¯¹ç¦ï¼å¯è½æ¹åè¿äºåæ°ãè½ç¶æ¨¡åè½å¤ä¼°è®¡ç»ä¸çåµå¥æ¥å¤çè¿äºå¤æåºæ¯ï¼ä½å½ååç©éµå¯ç¨æ¶ï¼ç´æ¥æ´åå®ä»¬ä»éèèã
*   <strong>ä¸å¯è§ç©ä½çæç¥è·ç¦»ï¼</strong> æ¨¡åå¨å¤çè·ç¦»éå¸¸è¿çç©ä½ï¼ä¾å¦å è±éå¤ï¼æ¶ï¼æ§è½å¯è½ä¼ä¸éãç®åæ¨¡åå¨æ°æ®éèå´åçè·ç¦»ä¸è¡¨ç°è¯å¥½ï¼ä½å¯¹äºæ´è¿è·ç¦»çæ§è½ä»éè¯ä¼°ãæç«¯è·ç¦»çç©ä½å¯è½å¯¼è´åå¸å¤ï¼out-of-distributionï¼é®é¢ï¼å½±åæ§è½ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>å¤çæç«¯åªå£°ï¼</strong> è¿ä¸æ­¥ç ç©¶å¦ä½å¤çæç«¯åªå£°æåµï¼ä»¥æé«æ¨¡åå¨æ´å·æææ§ç¯å¢ä¸­çé²æ£æ§ã
*   <strong>å®æ¶å®ç°ï¼</strong> æ¢ç´¢å¦ä½ä¼åæ¨¡åä»¥å®ç°å®æ¶è½¨è¿¹é¢æµï¼è¿å¯¹äºèªå¨é©¾é©¶ç­å¯¹å»¶è¿ææçåºç¨è³å³éè¦ã
*   <strong>æ´è¿çæç¥è·ç¦»ï¼</strong> è¯ä¼°åæ¹è¿æ¨¡åå¨å¤çæ´è¿è·ç¦»ä¸å¯è§ç©ä½æ¶çæ§è½ï¼ä»¥è§£å³åå¸å¤æ°æ®é®é¢ã
*   <strong>æ´å¤æçç¯å¢ï¼</strong> è¿ä¸æ­¥ç ç©¶å¨è§è§è§æµä¸ç¡®å®æè¾å¥ä¿¡å·åºæåªå£°çå¤æç°å®ä¸çåºæ¯ä¸­ï¼å¦ä½æé«è½¨è¿¹é¢æµçåç¡®æ§åå¯é æ§ã
*   <strong>å¤æ¨¡æèåçæ©å±ï¼</strong> æ¢ç´¢å°æ´å¤æ¨¡æï¼å¦é·è¾¾ãæ¿åé·è¾¾ç­ï¼æ´åå°æ¡æ¶ä¸­ï¼ä»¥è¿ä¸æ­¥å¢å¼ºå»åªåé¢æµè½åã</p>
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæéè¿å¼å¥OSTä»»å¡ååæ°çVPDæ¡æ¶ï¼ä¸ºè§£å³ä¸å¯è§ç©ä½çè½¨è¿¹é¢æµé®é¢æä¾äºå¼åæ§çè§£å³æ¹æ¡ãå®éè¿æ çç£å»åªåè§è§å®ä½æå½±ï¼ææå°å¤çäºæåªå£°çä¼ æå¨æ°æ®ï¼å¹¶å¨å¤ä¸ªå³é®åºç¨é¢ååå¾äºæ¾èè¿å±ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this extended work, we present
advancements in Out-of-Sight Trajectory (OST), a novel task that predicts the
noise-free visual trajectories of out-of-sight objects using noisy sensor data.</li>
<li>Through extensive evaluations on the Vi-Fi and JRDB
datasets, our approach achieves state-of-the-art performance in both trajectory
denoising and prediction, significantly surpassing previous baselines.</li>
<li>Additionally, we introduce comparisons with traditional denoising methods, such
as Kalman filtering, and adapt recent trajectory prediction models to our task,
providing a comprehensive benchmark.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.15219v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.15219v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.15178v1'></a></p>
<h2 id="unleashing-the-potential-of-multimodal-llms-for-zero-shot-spatio-temporal-video-grounding"><a href="https://arxiv.org/abs/2509.15178v1">Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding</a></h2>
<p><strong>Authors:</strong> Zaiquan Yang, Yuhao Liu, Gerhard Hancke, Rynson W. H. Lau</p>
<p><strong>Published:</strong> 2025-09-18</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Spatio-temporal video grounding (STVG) aims at localizing the spatio-temporal
tube of a video, as specified by the input text query. In this paper, we
utilize multimodal large language models (MLLMs) to explore a zero-shot
solution in STVG. We reveal two key insights about MLLMs: (1) MLLMs tend to
dynamically assign special tokens, referred to as \textit{grounding tokens},
for grounding the text query; and (2) MLLMs often suffer from suboptimal
grounding due to the inability to fully integrate the cues in the text query
(\textit{e.g.}, attributes, actions) for inference. Based on these insights, we
propose a MLLM-based zero-shot framework for STVG, which includes novel
decomposed spatio-temporal highlighting (DSTH) and temporal-augmented
assembling (TAS) strategies to unleash the reasoning ability of MLLMs. The DSTH
strategy first decouples the original query into attribute and action
sub-queries for inquiring the existence of the target both spatially and
temporally. It then uses a novel logit-guided re-attention (LRA) module to
learn latent variables as spatial and temporal prompts, by regularizing token
predictions for each sub-query. These prompts highlight attribute and action
cues, respectively, directing the model's attention to reliable spatial and
temporal related visual regions. In addition, as the spatial grounding by the
attribute sub-query should be temporally consistent, we introduce the TAS
strategy to assemble the predictions using the original video frames and the
temporal-augmented frames as inputs to help improve temporal consistency. We
evaluate our method on various MLLMs, and show that it outperforms SOTA methods
on three common STVG benchmarks.
  The code will be available at https://github.com/zaiquanyang/LLaVA_Next_STVG.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Zaiquan Yang, Yuhao Liu, Gerhard Hancke, Rynson W. H. Lauæ°åçè®ºæâUnleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Groundingâçå¨é¢æè¦ã</p>
<hr />
<h3 id="unleashing-the-potential-of-multimodal-llms-for-zero-shot-spatio-temporal-video-grounding_1">è®ºææè¦ï¼Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding</h3>
<p><strong>1. è§£å³çä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³<strong>æ¶ç©ºè§é¢å®ä½ï¼Spatio-Temporal Video Grounding, STVGï¼</strong>ä»»å¡ï¼å³æ ¹æ®ç»å®çææ¬æ¥è¯¢ï¼å¨è§é¢ä¸­å®ä½ç®æ å¯¹è±¡çæ¶ç©ºç®¡ï¼spatio-temporal tubeï¼ãä¼ ç»çSTVGæ¹æ³éå¸¸ä¾èµäºæè´µçå¸§çº§æ æ³¨è¿è¡å¨çç£è®­ç»ï¼èæ¬æåæ¢ç´¢å©ç¨å¤æ¨¡æå¤§è¯­è¨æ¨¡åï¼MLLMsï¼å®ç°<strong>é¶æ ·æ¬ï¼zero-shotï¼</strong>STVGè§£å³æ¹æ¡ï¼ä»¥åè½»æ æ³¨è´æå¹¶æé«æ³åè½åãè®ºæç¹å«å³æ³¨MLLMså¨å¤çå¤æè§é¢æ¥è¯¢æ¶ï¼ç±äºæªè½ååæ´åææ¬æ¥è¯¢ä¸­çå±æ§åå¨ä½çº¿ç´¢èå¯¼è´çæ¬¡ä¼å®ä½é®é¢ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è´¡ç®ï¼</strong>
ä½èåºäºå¯¹MLLMsçä¸¤ä¸ªå³é®æ´å¯ï¼å³MLLMsä¼å¨æåéâå®ä½ä»¤çâè¿è¡ææ¬æ¥è¯¢å®ä½ï¼ä½å¸¸å æªè½ååå©ç¨ææ¬çº¿ç´¢èå¯¼è´å®ä½æ¬¡ä¼ï¼ï¼æåºäºä¸ä¸ªæ°é¢çé¶æ ·æ¬STVGæ¡æ¶ï¼åå«ä»¥ä¸æ ¸å¿ç­ç¥ï¼</p>
<ul>
<li><strong>åè§£æ¶ç©ºé«äº®ï¼Decomposed Spatio-Temporal Highlighting, DSTHï¼ç­ç¥ï¼</strong><ul>
<li>å°åå§ææ¬æ¥è¯¢åè§£ä¸º<strong>å±æ§å­æ¥è¯¢</strong>å<strong>å¨ä½å­æ¥è¯¢</strong>ï¼åå«ç¨äºå¨ç©ºé´åæ¶é´ä¸æ¥è¯¢ç®æ çå­å¨ã</li>
<li>å¼å¥<strong>å¯¹æ°å¼å¯¼éæ³¨æåï¼Logit-guided Re-attention, LRAï¼æ¨¡å</strong>ï¼éè¿æ­£ååæ¯ä¸ªå­æ¥è¯¢çä»¤çé¢æµï¼å­¦ä¹ æ½å¨åéä½ä¸ºç©ºé´åæ¶é´æç¤ºãè¿äºæç¤ºè½å¤åå«é«äº®å±æ§åå¨ä½çº¿ç´¢ï¼å¼å¯¼æ¨¡åå³æ³¨å¯é çãä¸æ¶ç©ºç¸å³çè§è§åºåã</li>
</ul>
</li>
<li><strong>æ¶åºå¢å¼ºç»è£ï¼Temporal-Augmented Assembling, TASï¼ç­ç¥ï¼</strong><ul>
<li>ä¸ºäºæé«ç©ºé´å®ä½çæ¶åºä¸è´æ§ï¼ç¹å«æ¯å±æ§å­æ¥è¯¢çå®ä½ï¼ï¼TASç­ç¥å©ç¨åå§è§é¢å¸§åæ¶åºå¢å¼ºå¸§ï¼ä¾å¦ï¼åè½¬å¸§é¡ºåºï¼ä½ä¸ºè¾å¥ï¼ç»è£ä¸åé¢æµä»¥æ¹åæ¶åºä¸è´æ§ã</li>
</ul>
</li>
<li><strong>å®ä½ä»¤çè¯å«ï¼Grounding Token Identificationï¼ï¼</strong> è®ºæåç°MLLMsä¼å¨æåéå·æé«è§è§æ¿æ´»åº¦çç¹æ®ä»¤çï¼ç§°ä¸ºâå®ä½ä»¤çâï¼ï¼è¿äºä»¤çå¨å®ä½ææ¬ç¸å³åºåæ¹é¢è¡¨ç°åºè²ï¼å¹¶å©ç¨è¿ä¸åç°æå»ºäºé¶æ ·æ¬STVGæ¡æ¶ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
è¯¥æ¹æ³å¨å¤ç§MLLMsä¸è¿è¡äºè¯ä¼°ï¼å¹¶å¨ä¸ä¸ªå¸¸è§çSTVGåºåæµè¯ï¼HCSTVG-v1ãHCSTVG-v2åVidSTGï¼ä¸åå¾äºæ¾èä¼äºç°æSOTAæ¹æ³çæ§è½ã
*   ä¾å¦ï¼åºäºLLaVA-Next-Video-7Bæ¨¡åï¼è¯¥æ¹æ³å¨vIoU@0.3åvIoU@0.5ææ ä¸åå«æ¯E3Mé«åº4.2%å1.8%ã
*   ä¸æ´å¼ºå¤§çLLaVA-OneVision-7Bæ¨¡åç»åæ¶ï¼æ§è½æåæ´ä¸ºæ¾èï¼åå«è¾¾å°12.1%å5.7%ã
*   å³ä½¿å¨å¨ä½çº¿ç´¢è¾å°çVidSTGæ°æ®éä¸ï¼è¯¥æ¡æ¶ä¹è¶è¶äºä¹åçSOTAæ¹æ³ï¼å±ç°äºå¼ºå¤§çæ³åè½åã
*   æ¶èå®éªè¯æäºDSTHç­ç¥ï¼åæ¬å­æ¥è¯¢åè§£åLRAæ¨¡åï¼ä»¥åTASç­ç¥çæææ§ï¼å®ä»¬è½å¤å¼å¯¼MLLMsæ´å¥½å°å³æ³¨æ¶ç©ºç¸å³åºåï¼å¹¶æé«ç©ºé´å®ä½çé²æ£æ§ã</p>
<p>è¿äºç»æè¡¨æï¼éè¿ææå©ç¨MLLMsçåå¨è½åå¹¶å¼å¥åæ°çé«äº®åç»è£ç­ç¥ï¼å¯ä»¥æ¾èæåé¶æ ·æ¬STVGçæ§è½ï¼çè³è¶è¶ä¸äºå¼±çç£æ¹æ³ï¼å¹¶æ¥è¿å¨çç£æ¹æ³çæ°´å¹³ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºææåºï¼è¯¥æ¹æ³å­å¨å±éæ§ãç±äºMLLMsçé«è®¡ç®æ¶èï¼å®å¯è½é¾ä»¥å¾å¥½å°å¤çé¿è§é¢ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
æªæ¥çå·¥ä½å¯ä»¥èèå¨æ¨¡åè®¾è®¡ä¸­æ´å<strong>ä»¤çåªæï¼token pruningï¼</strong>å<strong>å³é®å¸§éæ©ï¼key frame selectionï¼</strong>ææ¯ï¼ä»¥è§£å³å¤çé¿è§é¢æ¶çè®¡ç®æçé®é¢ã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>Based on these insights, we
propose a MLLM-based zero-shot framework for STVG, which includes novel
decomposed spatio-temporal highlighting (DSTH) and temporal-augmented
assembling (TAS) strategies to unleash the reasoning ability of MLLMs. The DSTH
strategy first decouples the original query into attribute and action
sub-queries for inquiring the existence of the target both spatially and
temporally.</li>
<li>It then uses a novel logit-guided re-attention (LRA) module to
learn latent variables as spatial and temporal prompts, by regularizing token
predictions for each sub-query.</li>
<li>In addition, as the spatial grounding by the
attribute sub-query should be temporally consistent, we introduce the TAS
strategy to assemble the predictions using the original video frames and the
temporal-augmented frames as inputs to help improve temporal consistency.</li>
<li>We
evaluate our method on various MLLMs, and show that it outperforms SOTA methods
on three common STVG benchmarks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.15178v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.15178v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.15177v1'></a></p>
<h2 id="a-race-bias-free-face-aging-model-for-reliable-kinship-verification"><a href="https://arxiv.org/abs/2509.15177v1">A Race Bias Free Face Aging Model for Reliable Kinship Verification</a></h2>
<p><strong>Authors:</strong> Ali Nazari, Bardiya Kariminia, Mohsen Ebrahimi Moghaddam</p>
<p><strong>Published:</strong> 2025-09-18</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>The age gap in kinship verification addresses the time difference between the
photos of the parent and the child. Moreover, their same-age photos are often
unavailable, and face aging models are racially biased, which impacts the
likeness of photos. Therefore, we propose a face aging GAN model, RA-GAN,
consisting of two new modules, RACEpSp and a feature mixer, to produce racially
unbiased images. The unbiased synthesized photos are used in kinship
verification to investigate the results of verifying same-age parent-child
images. The experiments demonstrate that our RA-GAN outperforms SAM-GAN on an
average of 13.14\% across all age groups, and CUSP-GAN in the 60+ age group by
9.1\% in terms of racial accuracy. Moreover, RA-GAN can preserve subjects'
identities better than SAM-GAN and CUSP-GAN across all age groups.
Additionally, we demonstrate that transforming parent and child images from the
KinFaceW-I and KinFaceW-II datasets to the same age can enhance the
verification accuracy across all age groups. The accuracy increases with our
RA-GAN for the kinship relationships of father-son and father-daughter,
mother-son, and mother-daughter, which are 5.22, 5.12, 1.63, and 0.41,
respectively, on KinFaceW-I. Additionally, the accuracy for the relationships
of father-daughter, father-son, and mother-son is 2.9, 0.39, and 1.6 on
KinFaceW-II, respectively. The code is available
at~\href{https://github.com/bardiya2254kariminia/An-Age-Transformation-whitout-racial-bias-for-Kinship-verification}{Github}</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Ali Nazari, Bardiya Kariminia, Mohsen Ebrahimi Moghaddamæ°åçè®ºæâA Race Bias Free Face Aging Model for Reliable Kinship Verificationâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼A Race Bias Free Face Aging Model for Reliable Kinship Verification</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥ç ç©¶ä¸»è¦è§£å³äº²å±å³ç³»éªè¯ï¼Kinship Verification, KVï¼ä¸­çä¸¤å¤§ææï¼
*   <strong>å¹´é¾å·®è·é®é¢ï¼</strong> ç¶æ¯åå­å¥³çç§çéå¸¸å­å¨æ¾èçå¹´é¾å·®å¼ï¼ä¸é¾ä»¥è·ååé¾ç§çï¼è¿ä¸¥éå½±åäºäº²å±å³ç³»éªè¯çåç¡®æ§ã
*   <strong>ç°æé¢é¨èåæ¨¡åçç§æåè§ï¼</strong> å½åçé¢é¨èåæ¨¡åå¨çæå¾åæ¶å­å¨ç§æåè§ï¼è¿å¯¼è´åæå¾åä¸åå§å¾åçç¸ä¼¼åº¦éä½ï¼è¿èå½±åäº²å±å³ç³»éªè¯çå¯é æ§ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ä¸ºäºè§£å³ä¸è¿°é®é¢ï¼ä½èæåºäºä¸ç§åä¸º <strong>RA-GAN (RaceAgingGAN)</strong> çé¢é¨èåçæå¯¹æç½ç»æ¨¡åï¼å¶æ ¸å¿åæ°åæ¬ï¼
*   <strong>RACEpSp æ¨¡åï¼</strong> è¿æ¯ä¸ä¸ªæ°é¢çæ¨¡åï¼æ¨å¨æ¶é¤é¢é¨èåè¿ç¨ä¸­äº§ççç§æåè§ï¼ç¡®ä¿åæå¾åå¨ç§æä¸æ¯æ åçãå®å©ç¨é¢è®­ç»çResNet34æ¨¡åå¨Fairfaceæ°æ®éä¸è¿è¡è®­ç»ï¼ä»¥æ´å¥½å°å¤çå¤æ ·åçé¢é¨å§¿æåç§æä¿¡æ¯ã
*   <strong>ç¹å¾æ··åå¨ï¼Feature Mixerï¼ï¼</strong> è¯¥æ¨¡åç¨äºèåå¹´é¾ç¹å¾åç§æç¹å®é¢é¨ç¹å¾ï¼ä»¥æ¾å°æä½³ç»åï¼ä»èçæå¨ç§æä¸æ åä¸èº«ä»½ä¿ççå¾åã
*   <strong>æ°æ°æ®éçæå»ºï¼</strong> ä½èä»UTKFaceæ°æ®éä¸­æ¶éå¹¶æå»ºäºä¸ä¸ªæ°çãç§æå¹³è¡¡çæ°æ®éï¼ä»¥è§£å³ç°ææ°æ®éå¨ç§æå±æ§ä¸çä¸å¹³è¡¡é®é¢ï¼ä»èè®­ç»åºæ´å·æ³åè½åçæ¨¡åã
*   <strong>åé¾å¾åçäº²å±å³ç³»éªè¯ï¼</strong> æåºå°ç¶æ¯åå­å¥³çå¾åè½¬æ¢ä¸ºç¸åå¹´é¾ï¼ä»¥æé«äº²å±å³ç³»éªè¯çåç¡®æ§ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
å®éªç»ææ¾èå°è¯æäºRA-GANçæææ§ï¼
*   <strong>ç§æåç¡®æ§æåï¼</strong> RA-GANå¨ææå¹´é¾ç»çç§æåç¡®æ§æ¹é¢å¹³åä¼äºSAM-GAN 13.14%ï¼å¨60å²ä»¥ä¸å¹´é¾ç»ä¸­ä¼äºCUSP-GAN 9.1%ãè¿è¡¨æRA-GANè½å¤çæç§ææ åçåæå¾åã
*   <strong>èº«ä»½ä¿çè½åï¼</strong> RA-GANå¨ææå¹´é¾ç»ä¸­æ¯SAM-GANåCUSP-GANæ´å¥½å°ä¿çäºåè¯èçèº«ä»½ï¼è¿å¯¹äºäº²å±å³ç³»éªè¯è³å³éè¦ã
*   <strong>äº²å±å³ç³»éªè¯åç¡®æ§æé«ï¼</strong> å°KinFaceW-IåKinFaceW-IIæ°æ®éä¸­çç¶æ¯åå­å¥³å¾åè½¬æ¢ä¸ºç¸åå¹´é¾åï¼äº²å±å³ç³»éªè¯çåç¡®æ§æ¾èæé«ãå·ä½èè¨ï¼å¨KinFaceW-Iæ°æ®éä¸ï¼ç¶å­ãç¶å¥³ãæ¯å­åæ¯å¥³å³ç³»çåç¡®æ§åå«æé«äº5.22%ã5.12%ã1.63%å0.41%ãå¨KinFaceW-IIæ°æ®éä¸ï¼ç¶å¥³ãç¶å­åæ¯å­å³ç³»çåç¡®æ§åå«æé«äº2.9%ã0.39%å1.6%ã
*   <strong>å¹´é¾è½¬æ¢è¯¯å·®ï¼MAEï¼éä½ï¼</strong> RA-GANå¨å¹´é¾è½¬æ¢æ¹é¢è¡¨ç°ä¼å¼ï¼é¤äº20å²å¹´é¾ç»å¤ï¼å¨ææå¶ä»å¹´é¾ç»ä¸­ï¼å¶ç®æ å¹´é¾è½¬æ¢è¯¯å·®åä½äºCUSPæ¨¡åã</p>
<p>è¿äºç»æçæä¹å¨äºï¼RA-GANä¸ä»è§£å³äºé¢é¨èåæ¨¡åä¸­çç§æåè§é®é¢ï¼è¿éè¿çæé«è´¨éãèº«ä»½ä¿çä¸ç§ææ åçåé¾é¢é¨å¾åï¼æ¾èæåäºäº²å±å³ç³»éªè¯çåç¡®æ§åå¯é æ§ï¼å°¤å¶æ¯å¨å¤ççå®ä¸çä¸­ä¸åæ¥æºåæææ¶é´ç§ççæææ¶ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸­å¹¶æªæç¡®æåºå½åç ç©¶çå±éæ§ï¼ä½ä»å¶è®¨è®ºåæªæ¥å·¥ä½æ¹åå¯ä»¥æ¨æ­åºä¸äºéå«çæ¹é¢ï¼
*   <strong>æ°æ®éçå¹´é¾åå¸ä¸åï¼</strong> å°½ç®¡ä½èæå»ºäºç§æå¹³è¡¡çæ°æ®éï¼ä½ä»æåºå¦æè¦æ±æ°æ®éå¨ç§æåå¹´é¾ä¸é½ååï¼å¾åæ°éä¼åå°ï¼è¿å¯è½æç¤ºå¨æäºå¹´é¾æ®µçæ°æ®éä»æå¾æé«ã
*   <strong>æ¨¡åæ³åè½åï¼</strong> å°½ç®¡RA-GANå¨ç§æåèº«ä»½ä¿çæ¹é¢è¡¨ç°åºè²ï¼ä½é¢é¨èåæ¨¡åéå¸¸é¾ä»¥æ³åå°æªååä»£è¡¨çå¹´é¾ç»ãç§ææé¢é¨ç»æï¼è¿å¯è½æ¯ä¸ä¸ªæç»­çææã
*   <strong>å¯¹å¨è¸å¾åçä¾èµï¼</strong> GANæ¨¡åéå¸¸éè¦å¨è¸å¾åè¿è¡è®­ç»ï¼èKinFaceW-Iç­æ°æ®éä¸­çå¾åæ¯è£åªè¿çï¼éè¦é¢å¤çéåå¢å¼ºåèªç¼ç å¨è½¬æ¢æ­¥éª¤æ¥çæå¨è¸å¾åï¼è¿å¢å äºå¤ççå¤ææ§ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºæçç»è®ºåæªæ¥å·¥ä½é¨åæåºäºä»¥ä¸æ½å¨ç ç©¶æ¹åï¼
*   <strong>è¿ä¸æ­¥æ¶é¤ç§æåè§ï¼</strong> ä½èå¼ºè°å¶æ¹æ³å¨èåç§æç¹å¾ä»¥æ¶é¤ç§æåè§æ¹é¢å·æé«æ½åï¼è¿æç¤ºæªæ¥å¯ä»¥ç»§ç»­æ¢ç´¢æ´åè¿çææ¯æ¥å½»åºæ ¹é¤ç§æåè§ï¼å¹¶çæä¸åå§å¾åç§æç±»å«é«åº¦ç¸ä¼¼çå¾åã
*   <strong>æ´å¹¿æ³çå¹´é¾èå´åå¤æ ·æ§ï¼</strong> å°½ç®¡RA-GANè¦çäº20-80å²çå¹´é¾èå´ï¼ä½æªæ¥å¯ä»¥æ¢ç´¢æ´å¹¿æ³çå¹´é¾èå´ï¼å¹¶è¿ä¸æ­¥æé«æ¨¡åå¨æç«¯å¹´é¾ï¼å¦å¿ç«¥æèå¹´ï¼çæ§è½ã
*   <strong>æ´å¤æçäº²å±å³ç³»éªè¯åºæ¯ï¼</strong> è®ºæä¸»è¦å³æ³¨ç¶æ¯-å­å¥³å³ç³»ï¼æªæ¥å¯ä»¥æ©å±å°å¶ä»æ´å¤æçäº²å±å³ç³»ï¼å¦ç¥ç¶æ¯-å­å­å¥³ãåå¼å§å¦¹ç­ï¼ã
*   <strong>å®æ¶åºç¨åæçï¼</strong> å°½ç®¡æ·±åº¦å­¦ä¹ æ¨¡åå¨åç¡®æ§ä¸è¡¨ç°åºè²ï¼ä½å¶è®¡ç®èµæºéæ±åæ¨çéåº¦å¯è½æä¸ºå®æ¶åºç¨çç¶é¢ï¼æªæ¥å¯ä»¥ç ç©¶æ´é«æçæ¨¡åæ¶æåä¼åæ¹æ³ã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>Therefore, we propose a face aging GAN model, RA-GAN,
consisting of two new modules, RACEpSp and a feature mixer, to produce racially
unbiased images.</li>
<li>The experiments demonstrate that our RA-GAN outperforms SAM-GAN on an
average of 13.14\% across all age groups, and CUSP-GAN in the 60+ age group by
9.1\% in terms of racial accuracy.</li>
<li>Additionally, we demonstrate that transforming parent and child images from the
KinFaceW-I and KinFaceW-II datasets to the same age can enhance the
verification accuracy across all age groups.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.15177v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.15177v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.14980v1'></a></p>
<h2 id="m4diffuser-multi-view-diffusion-policy-with-manipulability-aware-control-for-robust-mobile-manipulation"><a href="https://arxiv.org/abs/2509.14980v1">M4Diffuser: Multi-View Diffusion Policy with Manipulability-Aware Control for Robust Mobile Manipulation</a></h2>
<p><strong>Authors:</strong> Ju Dong, Lei Zhang, Liding Zhang, Yao Ling, Yu Fu, Kaixin Bai, ZoltÃ¡n-Csaba MÃ¡rton, Zhenshan Bing, Zhaopeng Chen, Alois Christian Knoll, Jianwei Zhang</p>
<p><strong>Published:</strong> 2025-09-18</p>
<p><strong>Categories:</strong> cs.RO, cs.AI, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Mobile manipulation requires the coordinated control of a mobile base and a
robotic arm while simultaneously perceiving both global scene context and
fine-grained object details. Existing single-view approaches often fail in
unstructured environments due to limited fields of view, exploration, and
generalization abilities. Moreover, classical controllers, although stable,
struggle with efficiency and manipulability near singularities. To address
these challenges, we propose M4Diffuser, a hybrid framework that integrates a
Multi-View Diffusion Policy with a novel Reduced and Manipulability-aware QP
(ReM-QP) controller for mobile manipulation. The diffusion policy leverages
proprioceptive states and complementary camera perspectives with both
close-range object details and global scene context to generate task-relevant
end-effector goals in the world frame. These high-level goals are then executed
by the ReM-QP controller, which eliminates slack variables for computational
efficiency and incorporates manipulability-aware preferences for robustness
near singularities. Comprehensive experiments in simulation and real-world
environments show that M4Diffuser achieves 7 to 56 percent higher success rates
and reduces collisions by 3 to 31 percent over baselines. Our approach
demonstrates robust performance for smooth whole-body coordination, and strong
generalization to unseen tasks, paving the way for reliable mobile manipulation
in unstructured environments. Details of the demo and supplemental material are
available on our project website https://sites.google.com/view/m4diffuser.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾å¯¹Ju Dongç­äººæ°åçè®ºæâM4Diffuser: Multi-View Diffusion Policy with Manipulability-Aware Control for Robust Mobile Manipulationâçå¨é¢æè¦ã</p>
<hr />
<h3 id="m4diffuser">è®ºææè¦ï¼M4Diffuser: å·æå¯æä½æ§æç¥æ§å¶çå¤è§è§æ©æ£ç­ç¥ï¼å®ç°é²æ£çç§»å¨æä½</h3>
<p><strong>1. è§£å³çä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³ç§»å¨æä½ä¸­çæ ¸å¿ææï¼å³å¦ä½å¨éç»æåç¯å¢ä¸­å®ç°ç§»å¨åºåº§åæºæ¢°èçåè°æ§å¶ï¼åæ¶æç¥å¨å±åºæ¯ä¸ä¸æåç²¾ç»ç©ä½ç»èãç°ææ¹æ³å­å¨ä»¥ä¸å±éæ§ï¼
*   <strong>åè§è§æ¹æ³</strong>ï¼ç±äºè§éæéãæ¢ç´¢è½ååæ³åè½åä¸è¶³ï¼å¨éç»æåç¯å¢ä¸­è¡¨ç°ä¸ä½³ã
*   <strong>ç»å¸æ§å¶å¨</strong>ï¼è½ç¶ç¨³å®ï¼ä½å¨æçåæ¥è¿å¥å¼ç¹æ¶çå¯æä½æ§æ¹é¢å­å¨å°é¾ï¼éå¸¸ä¾èµäºæ¾å¼åéï¼å¢å äºè®¡ç®å¼éå¹¶éä½äºè½¨è¿¹å¹³æ»åº¦ã
*   <strong>å­¦ä¹ é©±å¨æ¹æ³</strong>ï¼è½ç¶éåºæ§åæ³åè½åå¼ºï¼ä½å®éé¨ç½²æ¶ç¨³å®æ§ä¸è¶³ï¼å®¹æå è§è§è¾å¥é®æ¡æè¶åºè§éèå¤±è´¥ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
M4Diffuser æåºäºä¸ç§æ··åæ¡æ¶ï¼ç»åäºå¤è§è§æ©æ£ç­ç¥åæ°åçâReduced and Manipulability-aware QP (ReM-QP)âæ§å¶å¨ï¼å¶ä¸»è¦åæ°ç¹åæ¬ï¼</p>
<ul>
<li>
<p><strong>å¤è§è§æ©æ£Transformerç­ç¥ (Multi-View Diffusion Transformer Policy)</strong>ï¼</p>
<ul>
<li>è¯¥ç­ç¥éè¿ç»åäºè¡¥çè§è§åæ¬ä½ç¶æï¼åæ¶ææå±é¨ç©ä½ç»èåå¨å±åºæ¯ä¸ä¸æã</li>
<li>å©ç¨Transformerç¼ç å¨åæ¡ä»¶å»åªæ©æ£è¿ç¨ï¼çæä¸çåæ ç³»ä¸­ä»»å¡ç¸å³çæ«ç«¯æ§è¡å¨ç®æ ã</li>
<li>å¤è§è§è¾å¥æ¾èæé«äºé²æ£æ§åæ³åè½åï¼è§£å³äºåè§è§æç¥ä¸è¶³çé®é¢ã</li>
</ul>
</li>
<li>
<p><strong>Reduced and Manipulability-aware QP (ReM-QP) æ§å¶å¨</strong>ï¼</p>
<ul>
<li>å¨ä½å±æ§å¶å±é¢ï¼ReM-QP æ¶é¤äºä¼ ç»QPå¬å¼ä¸­çæ¾å¼åéï¼ä»èæé«äºè®¡ç®æçã</li>
<li>å¼å¥äºåºäºéæ¡ä»¶æ°ï¼ICNï¼çå¯æä½æ§åå¥½ï¼ä»¥ç¡®ä¿å¨æ¥è¿å¥å¼ç¹æ¶çç¨³å®æ§åå¹³æ»æ§ï¼æé«äºé²æ£æ§ã</li>
<li>éè¿æ åä¸ç­å¼çº¦æç¡®ä¿äºå®å¨æ§ã</li>
</ul>
</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶éè¦æ§ï¼</strong>
M4Diffuser å¨ä»¿çåçå®ä¸çç¯å¢ä¸­è¿è¡äºå¹¿æ³å®éªï¼ç»æè¡¨æï¼</p>
<ul>
<li><strong>æ§è½æå</strong>ï¼ä¸åºçº¿æ¹æ³ç¸æ¯ï¼M4Diffuser çæåçæé«äº7%å°56%ï¼ç¢°æçéä½äº3%å°31%ã</li>
<li><strong>é²æ£æ§ä¸æ³åè½å</strong>ï¼è¯¥æ¹æ³å¨éç»æåç¯å¢ä¸­å®ç°äºå¹³æ»å¨èº«åè°çé²æ£æ§è½ï¼å¹¶å¯¹æªè§è¿çä»»å¡è¡¨ç°åºå¼ºå¤§çæ³åè½åã</li>
<li><strong>ä¸SOTAæ¹æ³çæ¯è¾</strong>ï¼M4Diffuser æ¾èä¼äºä¼ ç»è§åæ¹æ³åçº¯å­¦ä¹ æ¹æ³ï¼å¹³åæåçæé«äº28.4%ï¼ç¢°æçéä½äº69%ãä¸æåè¿çæ¹æ³ï¼å¦HoMeRï¼ç¸æ¯ï¼æåçæé«äº10.0%ï¼ç¢°æçåå°äº5.2%ã</li>
<li><strong>ReM-QPçæçä¸å¹³æ»åº¦</strong>ï¼ReM-QP å°ä»»å¡æ§è¡æ¶é´ç¼©ç­äº28%ï¼æ«ç«¯æ§è¡å¨å å éåº¦ï¼jerkï¼éä½äº35%ï¼å¨æçåè½¨è¿¹å¹³æ»åº¦ä¹é´åå¾äºè¯å¥½å¹³è¡¡ã</li>
</ul>
<p>è¿äºç»æçéè¦æ§å¨äºï¼M4Diffuser ä¸ºå¨éç»æåç¯å¢ä¸­å®ç°å¯é çç§»å¨æä½éºå¹³äºéè·¯ï¼è§£å³äºç°ææ¹æ³å¨é²æ£æ§ãæçåæ³åè½åæ¹é¢çå³é®éå¶ã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
è®ºæä¸­å¹¶æªæç¡®ååºM4Diffuserèªèº«çå±éæ§ï¼ä½éè¿å¯¹ç°ææ¹æ³çè®¨è®ºï¼å¯ä»¥æ¨æ­åºM4Diffuseræ¨å¨åæçææï¼è¿äºææå¨ä¸å®ç¨åº¦ä¸ä¹å¯è½ææå¶æªæ¥æ¹è¿çæ¹åï¼</p>
<ul>
<li><strong>æ°æ®ä¾èµæ§</strong>ï¼å°½ç®¡æ¨¡ä»¿å­¦ä¹ æ°æ®æçé«ï¼ä½è®­ç»ä»ç¶éè¦ä¸å®¶æ¼ç¤ºæ°æ®ã</li>
<li><strong>å¤æç¯å¢ä¸­çæ¢ç´¢è½å</strong>ï¼è½ç¶å¤è§è§ç­ç¥å¢å¼ºäºæç¥ï¼ä½æºå¨äººåºå¯¹é«åº¦å¤æãå®å¨æªç¥çç¯å¢æ¶çæ¢ç´¢è½åä»ææåç©ºé´ã</li>
<li><strong>è®¡ç®èµæº</strong>ï¼è½ç¶ReM-QPæé«äºè®¡ç®æçï¼ä½æ´ä¸ªæ··åæ¡æ¶å¨å®æ¶é¨ç½²æ¶ä»å¯è½é¢ä¸´è®¡ç®èµæºçéæ±ã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºææåºï¼æªæ¥çå·¥ä½å°éä¸­äºï¼</p>
<ul>
<li><strong>è¯­è¨åå¤æ¨¡æå¼å¯¼çç§»å¨æä½ (Language- and Multimodal-guided Mobile Manipulation)</strong>ï¼å°è¯­è¨æä»¤åæ´å¤æ¨¡æä¿¡æ¯æ´åå°ç§»å¨æä½ä¸­ï¼ä»¥å®ç°æ´é«çº§å«çä»»å¡çè§£åæ§è¡ã</li>
</ul>
<hr />
<p>è¿ä»½æè¦æ¸æ°å°æ¦è¿°äºè®ºæçæ ¸å¿åå®¹ï¼çªåºäºå¶å¨ç§»å¨æä½é¢åçææ¯è´¡ç®åæ½å¨å½±åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address
these challenges, we propose M4Diffuser, a hybrid framework that integrates a
Multi-View Diffusion Policy with a novel Reduced and Manipulability-aware QP
(ReM-QP) controller for mobile manipulation.</li>
<li>Our approach
demonstrates robust performance for smooth whole-body coordination, and strong
generalization to unseen tasks, paving the way for reliable mobile manipulation
in unstructured environments.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.14980v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.14980v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.14966v1'></a></p>
<h2 id="roboeye-enhancing-2d-robotic-object-identification-with-selective-3d-geometric-keypoint-matching"><a href="https://arxiv.org/abs/2509.14966v1">RoboEye: Enhancing 2D Robotic Object Identification with Selective 3D Geometric Keypoint Matching</a></h2>
<p><strong>Authors:</strong> Xingwu Zhang, Guanxuan Li, Zhuocheng Zhang, Zijun Long</p>
<p><strong>Published:</strong> 2025-09-18</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>The rapidly growing number of product categories in large-scale e-commerce
makes accurate object identification for automated packing in warehouses
substantially more difficult. As the catalog grows, intra-class variability and
a long tail of rare or visually similar items increase, and when combined with
diverse packaging, cluttered containers, frequent occlusion, and large
viewpoint changes-these factors amplify discrepancies between query and
reference images, causing sharp performance drops for methods that rely solely
on 2D appearance features. Thus, we propose RoboEye, a two-stage identification
framework that dynamically augments 2D semantic features with domain-adapted 3D
reasoning and lightweight adapters to bridge training deployment gaps. In the
first stage, we train a large vision model to extract 2D features for
generating candidate rankings. A lightweight 3D-feature-awareness module then
estimates 3D feature quality and predicts whether 3D re-ranking is necessary,
preventing performance degradation and avoiding unnecessary computation. When
invoked, the second stage uses our robot 3D retrieval transformer, comprising a
3D feature extractor that produces geometry-aware dense features and a
keypoint-based matcher that computes keypoint-correspondence confidences
between query and reference images instead of conventional cosine-similarity
scoring. Experiments show that RoboEye improves Recall@1 by 7.1% over the prior
state of the art (RoboLLM). Moreover, RoboEye operates using only RGB images,
avoiding reliance on explicit 3D inputs and reducing deployment costs. The code
used in this paper is publicly available at:
https://github.com/longkukuhi/RoboEye.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Xingwu Zhangç­äººæ°åçè®ºæâRoboEye: Enhancing 2D Robotic Object Identification with Selective 3D Geometric Keypoint Matchingâçå¨é¢æè¦ã</p>
<hr />
<h3 id="roboeye-3d2d">RoboEye: éè¿éæ©æ§3Då ä½å³é®ç¹å¹éå¢å¼º2Dæºå¨äººç©ä½è¯å«çæè¦</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
éçå¤§è§æ¨¡çµåäº§åç®å½çå¿«éå¢é¿ï¼ä»åºèªå¨ååè£ä¸­çç©ä½è¯å«åå¾è¶æ¥è¶å°é¾ãç°ææ¹æ³ä¸»è¦ä¾èµ2Då¤è§ç¹å¾ï¼ä½å¨é¢å¯¹ç±»ååå¼æ§ãé¿å°¾åå¸ãå¤æ ·ååè£ãæä¹±å®¹å¨ãé¢ç¹é®æ¡åå¤§å¹åº¦è§è§ååç­æææ§åºæ¯æ¶ï¼å¶æ§è½ä¼æ¥å§ä¸éãè¿äºå ç´ æ¾å¤§äºæ¥è¯¢å¾åååèå¾åä¹é´çå·®å¼ï¼ä½¿å¾ä»åºäº2Dç¹å¾çæ¹æ³é¾ä»¥æ³åãå æ­¤ï¼æ ¸å¿ç ç©¶é®é¢æ¯å¦ä½å¨ä¸ä¾èµæ¾å¼3Dè¾å¥ï¼å¦ç¹äºææ·±åº¦å¾ï¼çæåµä¸ï¼å©ç¨3Då ä½çº¿ç´¢æ¥æé«å¨å¤æä»åºæ¡ä»¶ä¸çç©ä½è¯å«é²æ£æ§ï¼åæ¶éä½é¨ç½²ææ¬ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
RoboEyeæåºäºä¸ä¸ªä¸¤é¶æ®µè¯å«æ¡æ¶ï¼å¨æå°å°2Dè¯­ä¹ç¹å¾ä¸é¢åéåºç3Dæ¨çåè½»éçº§ééå¨ç¸ç»åï¼ä»¥å¼¥åè®­ç»ä¸é¨ç½²ä¹é´çå·®è·ã
*   <strong>ä¸¤é¶æ®µè¯å«æ¡æ¶ï¼</strong>
    *   <strong>ç¬¬ä¸é¶æ®µï¼2Dæ£ç´¢ï¼ï¼</strong> ä½¿ç¨å¤§åè§è§æ¨¡åæå2Dç¹å¾ï¼çæåæ­¥çåéæåã
    *   <strong>3Dç¹å¾æç¥æ¨¡åï¼3D-FAMï¼ï¼</strong> ä¸ä¸ªè½»éçº§çæ¨¡åï¼ç¨äºè¯ä¼°3Dç¹å¾çè´¨éï¼å¹¶é¢æµæ¯å¦éè¦è¿è¡3Déæåºãè¿é¿åäºä¸å¿è¦çè®¡ç®ï¼å¹¶å¨3Dçº¿ç´¢åææ¶é²æ­¢æ§è½ä¸éãè¯¥æ¨¡åéè¿MRRé©±å¨ç3Dæç¥è®­ç»ï¼M3ATï¼æ¹æ¡è¿è¡è®­ç»ï¼è¯¥æ¹æ¡è¯å«ä½æ¶3Déæåºè½å¸¦æ¥å®éæ¶çã
    *   <strong>ç¬¬äºé¶æ®µï¼3Déæåºï¼ï¼</strong> å½3D-FAMæ¨¡åè¢«æ¿æ´»æ¶ï¼ä½¿ç¨æºå¨äºº3Dæ£ç´¢Transformerã
*   <strong>æºå¨äºº3Dæ£ç´¢Transformerï¼</strong>
    *   <strong>3Dç¹å¾æåå¨ï¼</strong> éç¨VGGTï¼Visual Geometry Grounded Transformerï¼çèåå¨ç»ä»¶ï¼ä»å¤è§å¾2Då¾åä¸­æ¨æ­3Då ä½ä¿¡æ¯ï¼çæå ä½æç¥çå¯éç¹å¾ã
    *   <strong>å³é®ç¹å¹éå¨ï¼</strong> æ¿æ¢äºä¼ ç»çä½å¼¦ç¸ä¼¼åº¦è¯åï¼éè¿è®¡ç®æ¥è¯¢å¾åååèå¾åä¹é´çå³é®ç¹å¯¹åºç½®ä¿¡åº¦æ¥æä¾æ´é²æ£çç¸ä¼¼æ§åº¦éãè¯¥å¹éå¨åºäºç¨çå³é®ç¹å¹éï¼å¹¶ç»è¿éæ°è®¾è®¡ä»¥çæç½®ä¿¡åº¦åæ°ä½ä¸ºç¸ä¼¼æ§ä¼°è®¡ã
*   <strong>åºäºééå¨çé¢åéåºç­ç¥ï¼</strong> ä¸ºäºå¼¥åVGGTé¢è®­ç»æ°æ®ä¸ä»åºç¹å®æ°æ®éä¹é´çé¢åå·®è·ï¼RoboEyeéç¨äºä¸ç§åºäºééå¨çè®­ç»ç­ç¥ï¼å»ç»3Dç¹å¾æåå¨ï¼ä»å¯¹å¹éå¨è¿è¡è®­ç»ï¼å¹¶ä½¿ç¨è½»éçº§ç¥è¯ééå¨è¿è¡å¢å¼ºï¼ä»¥å®ç°é«æçé¢åéåºã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æ§è½æåï¼</strong> RoboEyeå¨Amazon ARMBenchæ°æ®éä¸ï¼Recall@1ææ æ¯ç°ææåè¿æ¹æ³ï¼RoboLLMï¼æé«äº7.1%ãå¨å¤è§å¾è®¾ç½®ä¸ï¼å¶æ§è½æåæ´ä¸ºæ¾èï¼ä¾å¦å¨å®¹å¨å¾åºæ£ç´¢ä¸­ï¼Recall@1ä»98.0%æé«å°99.4%ï¼+1.4%ï¼ï¼å¨å¨å±å¾åºæ£ç´¢ä¸­ï¼Recall@1æé«äº7.1%ã
*   <strong>é²æ£æ§ï¼</strong> RoboEyeå¨é¢å¯¹å¤§è§æ¨¡ç®å½ãè§è§åå§¿æååãé®æ¡ååè£ååç­æææ¶ï¼è¡¨ç°åºåè¶çé²æ£æ§ã
*   <strong>æçï¼</strong> 3Dç¹å¾æç¥æ¨¡åå¨å¹³è¡¡æçåé²æ£æ§æ¹é¢åæ¥äºå³é®ä½ç¨ãå®ä»å¨å¿è¦æ¶æ¿æ´»3Dæ¨çï¼ä¿æäºæ¥è¿2Dè¿è¡æ¶çéåº¦ï¼åæ¶ä¿çäºå ä½éªè¯çä¼å¿ã
*   <strong>æ éæ¾å¼3Dè¾å¥ï¼</strong> RoboEyeä»ä½¿ç¨RGBå¾åè¿è¡æä½ï¼é¿åäºå¯¹æ¾å¼3Dè¾å¥ï¼å¦ç¹äºææ·±åº¦å¾ï¼çä¾èµï¼ä»èéä½äºé¨ç½²ææ¬ã
*   <strong>æ¨¡åè§æ¨¡ä¸æ§è½ï¼</strong> å®éªè¡¨æï¼ç®åå°å¢å 2Dç¹å¾æåå¨çæ¨¡åè§æ¨¡ä¸è¶³ä»¥è§£å³ä»åºç¯å¢ä¸­çææãRoboEyeéè¿ç»å3Dæç¥ç»ä»¶ï¼ä»¥å¯æ¯çåæ°è§æ¨¡å®ç°äºæ¾èçæ§è½æåã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
*   <strong>è®¡ç®å¼éï¼</strong> å°½ç®¡RoboEyeéè¿3D-FAMæ¨¡ååå°äºä¸å¿è¦ç3Dè®¡ç®ï¼ä½3Déæåºæ¬èº«ä»ç¶ä¼å¸¦æ¥ä¸å®çè®¡ç®å¼éãå¨æ²¡ææç¥æ¨¡åçæåµä¸ï¼æ æ¡ä»¶3Déæåºä¼æ¾èå¢å å»¶è¿ã
*   <strong>åªå£°æææ§ï¼</strong> è®ºææåºï¼å¨2Déæåºé¶æ®µï¼å¦æåå«è¿å¤ä½è´¨éçåéå¯¹è±¡ï¼å¯è½ä¼å¼å¥åªå£°ï¼ç¨éå¤å«ä¿¡å·ï¼å¹¶å¯è½å¯¼è´æ§è½ç¥å¾®ä¸éã
*   <strong>ç¡¬ä»¶éå¶ï¼</strong> è®ºææå°ï¼å¨ä¸RoboLLMçå¯¹æ¯ä¸­ï¼ç±äºç¡¬ä»¶éå¶ï¼è¾å°çæ¹æ¬¡å¤§å°ï¼ï¼å¶2Dæ¨¡ååå§æ§è½ç¥ä½äºRoboLLMï¼è¿è¡¨æå¯¹æ¯å­¦ä¹ éå¸¸åçäºæ´å¤§çæ¹æ¬¡ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºæä¸­æ²¡ææç¡®æåºæªæ¥çç ç©¶æ¹åï¼ä½ä»å¶è´¡ç®åå±éæ§æ¥çï¼å¯ä»¥æ¨æ­åºä»¥ä¸æ½å¨æ¹åï¼
*   <strong>æ´è½»éçº§ç3Dæ¨çï¼</strong> è¿ä¸æ­¥ä¼å3Dç¹å¾æååå¹éæºå¶ï¼ä»¥å¨ä¿ææ§è½çåæ¶è¿ä¸æ­¥éä½è®¡ç®ææ¬åæ¨çå»¶è¿ï¼ä½¿å¶æ´éç¨äºèµæºåéçè¾¹ç¼è®¾å¤ã
*   <strong>èªéåºå³é®ç¹éæ©ï¼</strong> æ¢ç´¢æ´æºè½çå³é®ç¹éæ©ç­ç¥ï¼ä»¥é¿åå¼å¥ä½è´¨éçå³é®ç¹ï¼ä»èæé«å ä½éªè¯çåç¡®æ§åç¨³å®æ§ã
*   <strong>å¤æ¨¡æèåçæ³åï¼</strong> è½ç¶RoboEyeé¿åäºæ¾å¼3Dè¾å¥ï¼ä½å¯ä»¥ç ç©¶å¦ä½å¨ä¸å¢å é¨ç½²å¤ææ§çåæä¸ï¼æ´ææå°èåå¶ä»éå¼æ¨¡æä¿¡æ¯ï¼ä¾å¦ï¼éè¿ç©çæ¨¡ææè¯­ä¹ç¥è¯ï¼ï¼ä»¥è¿ä¸æ­¥å¢å¼ºè¯å«é²æ£æ§ã
*   <strong>é¶æ ·æ¬/å°æ ·æ¬å­¦ä¹ ï¼</strong> é´äºçµåäº§åç®å½çå¿«éå¢é¿åé¿å°¾åå¸ï¼æ¢ç´¢å¦ä½å°RoboEyeæ¡æ¶æ©å±å°é¶æ ·æ¬æå°æ ·æ¬è¯å«åºæ¯ï¼ä»¥å¤çæ°äº§åæç¨æäº§åã</p>
<hr />
<p>è¿ç¯è®ºæéè¿å¶åæ°çä¸¤é¶æ®µæ¡æ¶åéæ©æ§3Då ä½å³é®ç¹å¹éæ¹æ³ï¼ä¸ºæºå¨äººç©ä½è¯å«é¢åå¸¦æ¥äºæ¾èçè¿æ­¥ï¼ç¹å«æ¯å¨å·ææææ§çä»åºç¯å¢ä¸­ãå®ææå°è§£å³äºä¼ ç»2Dæ¹æ³å¨å¤æåºæ¯ä¸çå±éæ§ï¼åæ¶é¿åäºå¯¹æè´µ3Dä¼ æå¨çä¾èµï¼ä¸ºå®éé¨ç½²æä¾äºé«æä¸é²æ£çè§£å³æ¹æ¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Thus, we propose RoboEye, a two-stage identification
framework that dynamically augments 2D semantic features with domain-adapted 3D
reasoning and lightweight adapters to bridge training deployment gaps.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.14966v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.14966v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.14957v1'></a></p>
<h2 id="df-llava-unlocking-mllms-potential-for-synthetic-image-detection-via-prompt-guided-knowledge-injection"><a href="https://arxiv.org/abs/2509.14957v1">DF-LLaVA: Unlocking MLLM's potential for Synthetic Image Detection via Prompt-Guided Knowledge Injection</a></h2>
<p><strong>Authors:</strong> Zhuokang Shen, Kaisen Zhang, Bohan Jia, Yuan Fang, Zhou Yu, Shaohui Lin</p>
<p><strong>Published:</strong> 2025-09-18</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>With the increasing prevalence of synthetic images, evaluating image
authenticity and locating forgeries accurately while maintaining human
interpretability remains a challenging task. Existing detection models
primarily focus on simple authenticity classification, ultimately providing
only a forgery probability or binary judgment, which offers limited explanatory
insights into image authenticity. Moreover, while MLLM-based detection methods
can provide more interpretable results, they still lag behind expert models in
terms of pure authenticity classification accuracy. To address this, we propose
DF-LLaVA, a simple yet effective framework that unlocks the intrinsic
discrimination potential of MLLMs. Our approach first extracts latent knowledge
from MLLMs and then injects it into training via prompts. This framework allows
LLaVA to achieve outstanding detection accuracy exceeding expert models while
still maintaining the interpretability offered by MLLMs. Extensive experiments
confirm the superiority of our DF-LLaVA, achieving both high accuracy and
explainability in synthetic image detection. Code is available online at:
https://github.com/Eliot-Shen/DF-LLaVA.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Zhuokang Shenç­äººæ°åçè®ºæâDF-LLaVA: Unlocking MLLM's potential for Synthetic Image Detection via Prompt-Guided Knowledge Injectionâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼DF-LLaVA: éè¿æç¤ºå¼å¯¼çç¥è¯æ³¨å¥è§£éMLLMå¨åæå¾åæ£æµä¸­çæ½å</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
éççææ¨¡åï¼å¦æ©æ£æ¨¡åï¼çå¿«éåå±ï¼åæå¾åæ¥çæ®åï¼å¯¹å¾åçå®æ§è¯ä¼°åä¼ªé å®ä½æåºäºä¸¥å³»ææãç°ææ£æµæ¨¡åä¸»è¦å³æ³¨ç®åççå®æ§äºåç±»ï¼æä¾æéçè§£éæ§æ´å¯ãè½ç¶åºäºå¤æ¨¡æå¤§è¯­è¨æ¨¡åï¼MLLMï¼çæ£æµæ¹æ³è½æä¾æ´å·è§£éæ§çç»æï¼ä½å¨çº¯ç²¹ççå®æ§åç±»åç¡®æ§æ¹é¢ä»è½åäºä¸å®¶æ¨¡åãå æ­¤ï¼æ ¸å¿ç ç©¶é®é¢æ¯å¦ä½å¼åä¸ç§æ¢è½å®ç°é«æ£æµåç¡®æ§ï¼åè½æä¾äººç±»å¯è§£éçä¼ªé è¯æ®çä¸å®¶æ¨¡åï¼ä»¥åºå¯¹åæå¾åæ£æµçææã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
æ¬ææåºäºDF-LLaVAï¼ä¸ä¸ªç®åèææçæ¡æ¶ï¼æ¨å¨è§£éMLLMå¨åæå¾åæ£æµä¸­çåå¨å¤å«æ½åãå¶å³é®åæ°åæ¹æ³è®ºè´¡ç®åæ¬ï¼
*   <strong>ç¥è¯æåä¸æ³¨å¥æ¡æ¶ï¼</strong> DF-LLaVAé¦åä»MLLMçè§è§ç¼ç å¨ä¸­æåæ½å¨çå¤å«ç¥è¯ï¼éè¿è®­ç»ä¸ä¸ªäºåç±»å¨ï¼ï¼ç¶åéè¿æç¤ºï¼promptsï¼å°å¶æ³¨å¥å°è®­ç»è¿ç¨ä¸­ãè¿ç§æ¹æ³åè®¸LLaVAå¨ä¿æMLLMè§£éæ§çåæ¶ï¼æ¾èæé«æ£æµåç¡®æ§ã
*   <strong>è§è§ç¼ç å¨å¤å«æ½åæ­ç¤ºï¼</strong> è®ºææ­ç¤ºäºMLLMçå¤å«æ½åä¸»è¦å­å¨äºå¶è§è§ç¼ç å¨ä¸­ãéè¿å¨CLIP-ViTç[CLS] tokenä¸è®­ç»äºåç±»å¨ï¼å¹¶å°å¶æ¦çè¾åºä½ä¸ºåµå¥ç¥è¯æ³¨å¥æç¤ºï¼DF-LLaVAè½å¤ææå©ç¨è¿ä¸æ½åã
*   <strong>å¤è§è§ä¼ªå½±è§£éï¼</strong> DF-LLaVAè½å¤ä»å¤ä¸ªè§è§ï¼å¦ç»æãå¤±çåç©çç¹å¾ï¼è¯å«åæå¾åæ¨¡åäº§ççä¼ªå½±ï¼ä»èå¢å¼ºäºå¯¹äººç±»çè§£éæ§ã
*   <strong>åºäºLLaVA-v1.5çæ¶æï¼</strong> æ¡æ¶æå»ºå¨LLaVA-v1.5æ¶æä¹ä¸ï¼åå«è§è§ç¼ç å¨ï¼CLIP-ViT(L-14)ï¼ãè§è§/è¯­è¨æå½±å¨ãçº¿æ§å¤´é¨åå¤§åè¯­è¨æ¨¡åï¼Vicuna-v1.5-7Bï¼ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>åè¶çæ£æµåç¡®æ§ï¼</strong> DF-LLaVAå¨FakeClueåLOKIæ°æ®éä¸å®ç°äºè¶è¶ç°æä¸å®¶æ¨¡åçæ£æµåç¡®æ§ãä¸å¼ºå¤§çå¼æºæ¨¡åQwen2-VL-72Bç¸æ¯ï¼DF-LLaVAå¨AccåF1ä¸å¹³åæåäº29.5%å40.1%ãç¸å¯¹äºååçMLLMæ¹æ³FakeVLMï¼DF-LLaVAå¨AccåF1ä¸å¹³åæåäº4.2%å3.9%ã
*   <strong>å¼ºå¤§çè§£éæ§ï¼</strong> DF-LLaVAå¨CSSåROUGE_Lææ ä¸è¡¨ç°åºè²ï¼æä¾äºæ¯FakeVLMåéç¨MLLMæ´åç¡®ãå¯é çä¼ªå½±è§£éã
*   <strong>æ³åè½åï¼</strong> å¨DMimageæ°æ®éä¸çå®éªç»æè¡¨æï¼DF-LLaVAçæ§è½ä¸ä¸å®¶æ¨¡åç¸å½çè³è¶è¶ï¼å°¤å¶å¨ä¼ªé å¾åæ£æµæ¹é¢è¡¨ç°æä½³ã
*   <strong>æ¶èå®éªéªè¯ï¼</strong> æ¶èç ç©¶è¯å®äºæç¤ºå¼å¯¼ç¥è¯æ³¨å¥ï¼PGKIï¼æ¡æ¶çæææ§ï¼å³ä½¿å¨LLaVA-FullFTè®¾ç½®ä¸ï¼PGKIä¹è½å¸¦æ¥è¿ä¸æ­¥çæ§è½æåï¼å°¤å¶å¨æéè®­ç»èµæºä¸ï¼å¯¹æåLLaVAçå¤å«è½åè³å³éè¦ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>è®­ç»æ°æ®ç±»å«ä¸å¹³è¡¡ï¼</strong> è®ºææå°ï¼DF-LLaVAå¨çå®ç±»å«ä¸çè¡¨ç°å¯è½ç¥å¼±äºä¸å®¶æ¨¡åï¼è¿å¯è½æ¯ç±äºè®­ç»æ°æ®ä¸­ç±»å«ä¸å¹³è¡¡é æçã
*   <strong>ä½ç§©ééå¨ï¼LoRAï¼çå±éæ§ï¼</strong> å®éªç»æè¡¨æï¼LLaVA-LoRAå¨å¤å«ä»»å¡ä¸çæ§è½æ¾èä½äºLLaVA-FullFTï¼è¿æç¤ºä½ç§©ééå¨å¯è½ä¸éåæ­¤ç±»ä»»å¡ã
*   <strong>FakeVLMçè§è§ç¼ç å¨å¾®è°ï¼</strong> è®ºææåºï¼FakeVLMéè¿å¾®è°è§è§ç¼ç å¨å¯è½æ°ä¹±äºå¶è§è§è¡¨ç¤ºï¼å¯¼è´å¶æ§è½ç¥ä½äºLLaVA-FullFTã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ¢ç´¢å¶ä»MLLMæ¶æçå¤å«æ½åï¼</strong> æªæ¥ç ç©¶å¯ä»¥è¿ä¸æ­¥æ¢ç´¢å¶ä»MLLMæ¶æå¨åæå¾åæ£æµä¸­çå¤å«æ½åã
*   <strong>å¢å¼ºå¤å«è½åçç­ç¥ï¼</strong> è¿ä¸æ­¥ç ç©¶å¢å¼ºMLLMå¤å«è½åçç­ç¥ï¼ä»¥åºå¯¹ä¸æ­æ¼åçåæå¾åçæææ¯ã</p>
<hr />
<p>æ»èè¨ä¹ï¼DF-LLaVAéè¿åæ°çæç¤ºå¼å¯¼ç¥è¯æ³¨å¥æ¡æ¶ï¼æåå°å°MLLMçåå¨å¤å«è½åè½¬åä¸ºåè¶çåæå¾åæ£æµæ§è½ï¼åæ¶ä¿æäºé«åº¦çäººç±»å¯è§£éæ§ãè¿ä¸ºè®¡ç®æºè§è§é¢åå¨åºå¯¹æ¥çå¢é¿çåæåå®¹æææ¹é¢æä¾äºä¸ä¸ªæåæ¯çæ¹åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we propose
DF-LLaVA, a simple yet effective framework that unlocks the intrinsic
discrimination potential of MLLMs. Our approach first extracts latent knowledge
from MLLMs and then injects it into training via prompts.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.14957v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.14957v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.14746v1'></a></p>
<h2 id="chain-of-thought-re-ranking-for-image-retrieval-tasks"><a href="https://arxiv.org/abs/2509.14746v1">Chain-of-Thought Re-ranking for Image Retrieval Tasks</a></h2>
<p><strong>Authors:</strong> Shangrong Wu, Yanghong Zhou, Yang Chen, Feng Zhang, P. Y. Mok</p>
<p><strong>Published:</strong> 2025-09-18</p>
<p><strong>Categories:</strong> cs.CV, cs.IR</p>
<p><strong>Abstract:</strong></p>
<p>Image retrieval remains a fundamental yet challenging problem in computer
vision. While recent advances in Multimodal Large Language Models (MLLMs) have
demonstrated strong reasoning capabilities, existing methods typically employ
them only for evaluation, without involving them directly in the ranking
process. As a result, their rich multimodal reasoning abilities remain
underutilized, leading to suboptimal performance. In this paper, we propose a
novel Chain-of-Thought Re-Ranking (CoTRR) method to address this issue.
Specifically, we design a listwise ranking prompt that enables MLLM to directly
participate in re-ranking candidate images. This ranking process is grounded in
an image evaluation prompt, which assesses how well each candidate aligns with
users query. By allowing MLLM to perform listwise reasoning, our method
supports global comparison, consistent reasoning, and interpretable
decision-making - all of which are essential for accurate image retrieval. To
enable structured and fine-grained analysis, we further introduce a query
deconstruction prompt, which breaks down the original query into multiple
semantic components. Extensive experiments on five datasets demonstrate the
effectiveness of our CoTRR method, which achieves state-of-the-art performance
across three image retrieval tasks, including text-to-image retrieval (TIR),
composed image retrieval (CIR) and chat-based image retrieval (Chat-IR). Our
code is available at https://github.com/freshfish15/CoTRR .</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Shangrong Wuç­äººæ°åçè®ºæâChain-of-Thought Re-ranking for Image Retrieval Tasksâçå¨é¢æè¦ã</p>
<hr />
<h3 id="chain-of-thought-re-ranking-for-image-retrieval-tasks_1">è®ºæãChain-of-Thought Re-ranking for Image Retrieval Tasksãæè¦</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
å¾åæ£ç´¢æ¯è®¡ç®æºè§è§é¢åçä¸ä¸ªåºç¡ä¸å·ææææ§çé®é¢ãå°½ç®¡å¤æ¨¡æå¤§åè¯­è¨æ¨¡åï¼MLLMsï¼å¨æ¨çè½åæ¹é¢åå¾äºæ¾èè¿å±ï¼ä½ç°ææ¹æ³éå¸¸ä»å°MLLMsç¨äºè¯ä¼°ï¼èæªç´æ¥åä¸å¾åçæåºè¿ç¨ãè¿å¯¼è´MLLMsä¸°å¯çå¤æ¨¡ææ¨çè½åæªè¢«ååå©ç¨ï¼ä»èå½±åäºå¾åæ£ç´¢çæ§è½ãæ¬ææ¨å¨è§£å³è¿ä¸é®é¢ï¼å³å¦ä½ææå©ç¨MLLMsçæ¨çè½åï¼ä½¿å¶ç´æ¥åä¸å¾åæ£ç´¢çéæåºè¿ç¨ï¼ä»¥æåæ£ç´¢åç¡®æ§ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
æ¬ææåºäºä¸ç§æ°é¢ç<strong>æç»´é¾éæåºï¼Chain-of-Thought Re-Ranking, CoTRRï¼</strong>æ¹æ³ï¼ä»¥è§£å³ä¸è¿°é®é¢ãCoTRRçæ ¸å¿åæ°åæ¹æ³è®ºè´¡ç®åæ¬ï¼</p>
<ul>
<li><strong>MLLMç´æ¥åä¸éæåºï¼</strong> CoTRRè®¾è®¡äºä¸ä¸ª<strong>åè¡¨å¼æåºæç¤ºï¼listwise ranking promptï¼</strong>ï¼ä½¿MLLMè½å¤ç´æ¥åä¸åéå¾åçéæåºï¼èéä»ä»è¿è¡è¯ä¼°ãè¿ä½¿å¾MLLMè½å¤è¿è¡å¨å±æ¯è¾åä¸è´æ§æ¨çã</li>
<li><strong>å¾åè¯ä¼°æç¤ºï¼</strong> æåºè¿ç¨åºäºä¸ä¸ª<strong>å¾åè¯ä¼°æç¤ºï¼image evaluation promptï¼</strong>ï¼è¯¥æç¤ºè¯ä¼°æ¯ä¸ªåéå¾åä¸ç¨æ·æ¥è¯¢çå¹éç¨åº¦ãè¿ç§è¯ä¼°æ¹å¼æ¯æå¯è§£éçå³ç­å¶å®ã</li>
<li><strong>æ¥è¯¢è§£ææç¤ºï¼</strong> ä¸ºäºå®ç°ç»æååç»ç²åº¦çåæï¼CoTRRå¼å¥äºä¸ä¸ª<strong>æ¥è¯¢è§£ææç¤ºï¼query deconstruction promptï¼</strong>ãè¯¥æç¤ºå°åå§æ¥è¯¢åè§£ä¸ºå¤ä¸ªè¯­ä¹ç»ä»¶ï¼ä¾å¦ï¼ä¸»è¦å¯¹è±¡ãæ´»å¨ãå³é®ç»èãç¯å¢ãæ°å´ï¼ï¼ä»èå®ç°æ´ææååç¡®çå¹éæ¯è¾ã</li>
<li><strong>ç»ä¸æ¡æ¶ï¼</strong> CoTRRä¸ä¾èµäºç¹å®çåå§æ£ç´¢æ¹æ³ï¼ä½¿å¶è½å¤è½»æ¾ãæ ç¼å°åºç¨äºå¤ç§å¾åæ£ç´¢ä»»å¡ï¼åæ¬ææ¬å°å¾åæ£ç´¢ï¼TIRï¼ãç»åå¾åæ£ç´¢ï¼CIRï¼ååºäºèå¤©çå¾åæ£ç´¢ï¼Chat-IRï¼ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
CoTRRå¨äºä¸ªæ°æ®éä¸è¿è¡äºå¹¿æ³å®éªï¼å¹¶å¨ä¸ç§å¾åæ£ç´¢ä»»å¡ï¼TIRãCIRåChat-IRï¼ä¸åå¾äº<strong>æåè¿çæ§è½</strong>ã</p>
<ul>
<li><strong>CIRä»»å¡ï¼</strong> å¨CIRRåCIRCOæ°æ®éä¸ï¼CoTRRå¨R@1åmAP@kç­ææ ä¸æ¾èä¼äºåºçº¿æ¹æ³ï¼å¦OSrCIRåImageScopeï¼ï¼å°¤å¶æ¯å¨R@1ææ ä¸åå¾äºæ¾èæåãä¾å¦ï¼ä½¿ç¨ViT-B/32ä½ä¸ºéª¨å¹²ç½ç»æ¶ï¼CoTRRå¨CIRRæ°æ®éä¸çR@1åR@5åå«æ¯ImageScopeæåäº12.41%å6.53%ã</li>
<li><strong>TIRä»»å¡ï¼</strong> å¨Flickr30KåMSCOCOæ°æ®éä¸ï¼CoTRRåæ ·è¶è¶äºåå§CLIPåImageScopeï¼éªè¯äºå¶å¨ææ¬å°å¾åæ£ç´¢ä¸­çæææ§ã</li>
<li><strong>Chat-IRä»»å¡ï¼</strong> å¨VisDialæ°æ®éä¸ï¼CoTRRå¨å¤ä¸ªå¯¹è¯è½®æ¬¡ä¸­æç»­ä¼äºOpenCLIPãPlugIRåImageScopeï¼è¡¨æå¶å¨äº¤äºå¼æ£ç´¢åºæ¯ä¸­çå¼ºå¤§è½åã</li>
<li><strong>æ¶èç ç©¶ï¼</strong> æ¶èå®éªè¯å®äºCoTRRä¸­åè¡¨å¼æåºãæ¥è¯¢è§£æåå¾åè¯ä¼°æ¨¡åçäºè¡¥æ§åæææ§ï¼æ¯ä¸ªç»ä»¶é½å¯¹æ§è½æåæè´¡ç®ãæ­¤å¤ï¼CoTRRå¨ä¸åMLLMï¼å¦Gemini 2.5 ProãQwen-VL-Maxï¼ä¸çè¡¨ç°ç¨³å¥ï¼å¶ä¸­Gemini 2.5 Proè¡¨ç°æä½³ï¼è¿è¡¨æå¤§åææä»¤å¯¹é½æ´å¥½çMLLMè½æä¾æ´å¼ºçåºç¡åè¯ä¼°è½åã</li>
</ul>
<p>è¿äºç»æè¡¨æï¼CoTRRéè¿å°MLLMç´æ¥æ´åå°éæåºæµç¨ä¸­ï¼å¹¶å©ç¨å¶å¼ºå¤§çæ¨çè½åè¿è¡ç»è´çå¾åè¯ä¼°ååè¡¨å¼æ¯è¾ï¼æ¾èæåäºå¾åæ£ç´¢çåç¡®æ§åé²æ£æ§ã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
è®ºæä¸­æªæç¡®æåCoTRRæ¹æ³çå·ä½å±éæ§ãç¶èï¼ä»å¶æ¹æ³è®ºåå®éªè®¾ç½®æ¥çï¼å¯è½å­å¨çéæ§å±éåæ¬ï¼</p>
<ul>
<li><strong>è®¡ç®ææ¬ï¼</strong> ä¾èµå¤§åMLLMè¿è¡æç»´é¾æ¨çååè¡¨å¼éæåºå¯è½å¸¦æ¥è¾é«çè®¡ç®ææ¬åå»¶è¿ï¼å°¤å¶æ¯å¨å¤çå¤§è§æ¨¡åééæ¶ã</li>
<li><strong>æç¤ºå·¥ç¨çæææ§ï¼</strong> CoTRRçæ§è½å¯è½å¯¹æç¤ºçè®¾è®¡ï¼å¦æ¥è¯¢è§£æåå¾åè¯ä¼°æç¤ºï¼ææï¼éè¦ç²¾ç»çæç¤ºå·¥ç¨æ¥ä¼åã</li>
<li><strong>MLLMçéç¨æ§ï¼</strong> å°½ç®¡CoTRRå¨ä¸åMLLMä¸è¡¨ç°ç¨³å¥ï¼ä½å¶æ§è½ä»ååºå±MLLMè½åçéå¶ãå¦æMLLMå¨ç¹å®é¢åæå¤ææ¨çä»»å¡ä¸è¡¨ç°ä¸ä½³ï¼CoTRRçæ§è½ä¹å¯è½åå½±åã</li>
<li><strong>å¯æ©å±æ§ï¼</strong> å°½ç®¡è®ºææå°äºå¤çtop-Kåééï¼ä½å¯¹äºéå¸¸å¤§çæ£ç´¢ç»æéï¼å°ææåéå¾åè¾å¥MLLMè¿è¡åè¡¨å¼æåºå¯è½é¢ä¸´å¯æ©å±æ§ææã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
åºäºæ¬è®ºæçå·¥ä½ï¼æªæ¥ç ç©¶å¯ä»¥æ¢ç´¢ä»¥ä¸æ¹åï¼</p>
<ul>
<li><strong>æçä¼åï¼</strong> æ¢ç´¢æ´é«æçMLLMæ¨çç­ç¥æè¿ä¼¼æ¹æ³ï¼ä»¥éä½CoTRRçè®¡ç®ææ¬åå»¶è¿ï¼ä½¿å¶éç¨äºå®æ¶æå¤§è§æ¨¡æ£ç´¢ç³»ç»ã</li>
<li><strong>èªéåºæç¤ºï¼</strong> ç ç©¶å¦ä½å¨æçææèªéåºè°æ´æ¥è¯¢è§£æåå¾åè¯ä¼°æç¤ºï¼ä»¥æ´å¥½å°éåºä¸åæ¥è¯¢ç±»åãç¨æ·æå¾åå¾ååå®¹ã</li>
<li><strong>å¤æ¨¡æåé¦ï¼</strong> æ¢ç´¢é¤äºææ¬è¯ä¼°ä¹å¤ï¼å¦ä½å°å¶ä»æ¨¡æï¼å¦è§è§æ³¨æåå¾ãç¨æ·äº¤äºè¡ä¸ºï¼çåé¦æ´åå°MLLMçéæåºè¿ç¨ä¸­ã</li>
<li><strong>æ´å¤æçæ¨çï¼</strong> å°CoTRRæ©å±å°æ´å¤æçå¾åæ£ç´¢åºæ¯ï¼ä¾å¦å¤è½®å¯¹è¯ä¸­æ¶åæ´æ·±å±æ¬¡è¯­ä¹çè§£åä¸ä¸ææ¨ççä»»å¡ã</li>
<li><strong>å°æ ·æ¬/é¶æ ·æ¬å­¦ä¹ ï¼</strong> è¿ä¸æ­¥æ¢ç´¢CoTRRå¨å°æ ·æ¬æé¶æ ·æ¬åºæ¯ä¸çæ§è½ï¼ä»¥åå¦ä½éè¿å°éç¤ºä¾ææ éé¢å¤è®­ç»æ¥æåå¶æ³åè½åã</li>
<li><strong>ç¨æ·åå¥½å­¦ä¹ ï¼</strong> ç»åç¨æ·åå¥½å­¦ä¹ æºå¶ï¼ä½¿CoTRRè½å¤æ ¹æ®ä¸ªä½ç¨æ·çåå²è¡ä¸ºååé¦è¿è¡ä¸ªæ§åéæåºã</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we propose a
novel Chain-of-Thought Re-Ranking (CoTRR) method to address this issue.</li>
<li>By allowing MLLM to perform listwise reasoning, our method
supports global comparison, consistent reasoning, and interpretable
decision-making - all of which are essential for accurate image retrieval.</li>
<li>Extensive experiments on five datasets demonstrate the
effectiveness of our CoTRR method, which achieves state-of-the-art performance
across three image retrieval tasks, including text-to-image retrieval (TIR),
composed image retrieval (CIR) and chat-based image retrieval (Chat-IR).</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.14746v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.14746v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-09-19 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
