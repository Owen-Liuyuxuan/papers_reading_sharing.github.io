<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-12-11 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-12-10/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-12-12/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-12-11">Arxiv Computer Vision Papers - 2025-12-11</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#hif-vla-hindsight-insight-and-foresight-through-motion-representation-for-vision-language-action-models" class="nav-link">HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models</a>
                </li>
                <li class="nav-item">
                    <a href="#token-expand-merge-training-free-token-compression-for-vision-language-action-models" class="nav-link">Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models</a>
                </li>
                <li class="nav-item">
                    <a href="#gains-gaussian-based-inverse-rendering-from-sparse-multi-view-captures" class="nav-link">GAINS: Gaussian-based Inverse Rendering from Sparse Multi-View Captures</a>
                </li>
                <li class="nav-item">
                    <a href="#splatent-splatting-diffusion-latents-for-novel-view-synthesis" class="nav-link">Splatent: Splatting Diffusion Latents for Novel View Synthesis</a>
                </li>
                <li class="nav-item">
                    <a href="#nordfkb-a-fine-grained-benchmark-dataset-for-geospatial-ai-in-norway" class="nav-link">NordFKB: a fine-grained benchmark dataset for geospatial AI in Norway</a>
                </li>
                <li class="nav-item">
                    <a href="#visualactbench-can-vlms-see-and-act-like-a-human" class="nav-link">VisualActBench: Can VLMs See and Act like a Human?</a>
                </li>
                <li class="nav-item">
                    <a href="#yopo-nav-visual-navigation-using-3dgs-graphs-from-one-pass-videos" class="nav-link">YOPO-Nav: Visual Navigation using 3DGS Graphs from One-Pass Videos</a>
                </li>
                <li class="nav-item">
                    <a href="#visual-heading-prediction-for-autonomous-aerial-vehicles" class="nav-link">Visual Heading Prediction for Autonomous Aerial Vehicles</a>
                </li>
                <li class="nav-item">
                    <a href="#uniugp-unifying-understanding-generation-and-planing-for-end-to-end-autonomous-driving" class="nav-link">UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving</a>
                </li>
                <li class="nav-item">
                    <a href="#simultaneous-tactile-visual-perception-for-learning-multimodal-robot-manipulation" class="nav-link">Simultaneous Tactile-Visual Perception for Learning Multimodal Robot Manipulation</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-12-11">Arxiv Computer Vision Papers - 2025-12-11</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，作为一名专注于计算机视觉和机器学习的研究助理，我将为您提供一份关于2025年12月10日 arXiv 计算机视觉领域论文的简明执行摘要。这份摘要旨在帮助忙碌的研究人员快速了解该领域的最新进展。</p>
<hr />
<p><strong>执行摘要：2025年12月10日 arXiv 计算机视觉论文精选</strong></p>
<p><strong>日期：</strong> 2025年12月10日
<strong>领域：</strong> 计算机视觉</p>
<p><strong>1. 主要主题与趋势：</strong></p>
<p>本期 arXiv 论文集呈现出几个显著的主题：</p>
<ul>
<li><strong>多模态融合与理解：</strong> 多个研究（如 HiF-VLA, VisualActBench, UniUGP, Simultaneous Tactile-Visual Perception）强调了将视觉信息与语言、动作甚至触觉相结合，以实现更全面、更智能的系统。</li>
<li><strong>高效模型训练与推理：</strong> Token Expand-Merge 等工作表明，在不牺牲性能的前提下，对模型进行训练无关的压缩和优化是当前的研究热点。</li>
<li><strong>新颖的渲染与三维重建技术：</strong> Splatent 和 GAINS 展示了利用扩散模型和高斯溅射等技术，在稀疏视图下实现高质量新视角合成和逆向渲染的进步。</li>
<li><strong>机器人与自主系统：</strong> YOPO-Nav 和 Visual Heading Prediction 等论文聚焦于提升机器人在导航、感知和执行任务的能力，尤其是在视觉导航和自主飞行领域。</li>
<li><strong>地理空间 AI 的发展：</strong> NordFKB 的发布标志着针对特定地理区域（挪威）的精细化地理空间 AI 基准数据集的出现，预示着该领域将更加注重实际应用和区域性研究。</li>
</ul>
<p><strong>2. 亮点与创新性论文：</strong></p>
<ul>
<li><strong>HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models</strong> 提出了一种新颖的运动表示方法，旨在提升 Vision-Language-Action (VLA) 模型在理解和预测动作方面的能力，这对于更高级别的机器人控制和人机交互至关重要。</li>
<li><strong>Splatent: Splatting Diffusion Latents for Novel View Synthesis</strong> 将扩散模型的强大生成能力与高斯溅射技术相结合，为新视角合成带来了新的可能性，尤其是在处理复杂场景和生成逼真细节方面。</li>
<li><strong>UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving</strong> 尝试端到端地统一自动驾驶中的理解、生成和规划，这可能为实现更流畅、更安全的自动驾驶系统提供一条新的路径。</li>
</ul>
<p><strong>3. 新兴研究方向与技术：</strong></p>
<ul>
<li><strong>运动表示在多模态模型中的应用：</strong> HiF-VLA 的工作表明，深入理解和利用运动信息将是提升 VLA 模型能力的关键。</li>
<li><strong>扩散模型与三维视觉的结合：</strong> Splatent 展示了扩散模型在三维重建和新视角合成领域的潜力，预示着未来会有更多此类融合研究。</li>
<li><strong>训练无关的模型压缩：</strong> Token Expand-Merge 的方法为在不进行额外训练的情况下提高模型效率提供了实用的解决方案。</li>
<li><strong>触觉与视觉的融合感知：</strong> Simultaneous Tactile-Visual Perception 的研究表明，多模态融合（特别是触觉与视觉）在机器人操作任务中具有巨大潜力。</li>
<li><strong>面向特定领域的精细化基准数据集：</strong> NordFKB 的出现强调了构建针对特定应用场景和地理区域的高质量数据集的重要性。</li>
</ul>
<p><strong>4. 建议阅读全文的论文：</strong></p>
<p>考虑到其潜在的影响力和创新性，以下论文值得深入阅读：</p>
<ul>
<li><strong>HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models:</strong> 对于关注 VLA 模型、机器人控制和人机交互的研究者。</li>
<li><strong>Splatent: Splatting Diffusion Latents for Novel View Synthesis:</strong> 对于在三维视觉、新视角合成和生成模型领域的研究者。</li>
<li><strong>UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving:</strong> 对于自动驾驶、端到端学习和规划领域的研究者。</li>
<li><strong>Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models:</strong> 对于关注模型效率和部署的研究者。</li>
</ul>
<hr />
<p>希望这份摘要能帮助您快速把握近期计算机视觉领域的重要动态。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2512.09928v1">HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models</a></li>
<li><a href="#2512.09927v1">Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models</a></li>
<li><a href="#2512.09925v1">GAINS: Gaussian-based Inverse Rendering from Sparse Multi-View Captures</a></li>
<li><a href="#2512.09923v1">Splatent: Splatting Diffusion Latents for Novel View Synthesis</a></li>
<li><a href="#2512.09913v1">NordFKB: a fine-grained benchmark dataset for geospatial AI in Norway</a></li>
<li><a href="#2512.09907v1">VisualActBench: Can VLMs See and Act like a Human?</a></li>
<li><a href="#2512.09903v1">YOPO-Nav: Visual Navigation using 3DGS Graphs from One-Pass Videos</a></li>
<li><a href="#2512.09898v1">Visual Heading Prediction for Autonomous Aerial Vehicles</a></li>
<li><a href="#2512.09864v1">UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving</a></li>
<li><a href="#2512.09851v1">Simultaneous Tactile-Visual Perception for Learning Multimodal Robot Manipulation</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2512.09928v1'></a></p>
<h2 id="hif-vla-hindsight-insight-and-foresight-through-motion-representation-for-vision-language-action-models"><a href="https://arxiv.org/abs/2512.09928v1">HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models</a></h2>
<p><strong>Authors:</strong> Minghui Lin, Pengxiang Ding, Shu Wang, Zifeng Zhuang, Yang Liu, Xinyang Tong, Wenxuan Song, Shangke Lyu, Siteng Huang, Donglin Wang</p>
<p><strong>Published:</strong> 2025-12-10</p>
<p><strong>Categories:</strong> cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Vision-Language-Action (VLA) models have recently enabled robotic manipulation by grounding visual and linguistic cues into actions. However, most VLAs assume the Markov property, relying only on the current observation and thus suffering from temporal myopia that degrades long-horizon coherence. In this work, we view motion as a more compact and informative representation of temporal context and world dynamics, capturing inter-state changes while filtering static pixel-level noise. Building on this idea, we propose HiF-VLA (Hindsight, Insight, and Foresight for VLAs), a unified framework that leverages motion for bidirectional temporal reasoning. HiF-VLA encodes past dynamics through hindsight priors, anticipates future motion via foresight reasoning, and integrates both through a hindsight-modulated joint expert to enable a ''think-while-acting'' paradigm for long-horizon manipulation. As a result, HiF-VLA surpasses strong baselines on LIBERO-Long and CALVIN ABC-D benchmarks, while incurring negligible additional inference latency. Furthermore, HiF-VLA achieves substantial improvements in real-world long-horizon manipulation tasks, demonstrating its broad effectiveness in practical robotic settings.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models”的全面摘要，重点关注其在计算机视觉和机器学习领域的贡献：</p>
<p><strong>论文题目：</strong> HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models</p>
<p><strong>作者：</strong> Minghui Lin, Pengxiang Ding, Shu Wang, Zifeng Zhuang, Yang Liu, Xinyang Tong, Wenxuan Song, Shangke Lyu, Siteng Huang, Donglin Wang</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>该论文主要解决了当前视觉-语言-动作（VLA）模型在处理<strong>长时序机器人操作任务</strong>时存在的“<strong>时间近视</strong>”（temporal myopia）问题。现有VLA模型大多假设马尔可夫性质，仅依赖当前观测来预测动作，导致在长序列操作中难以维持连贯性和因果关系，从而影响任务的整体表现。此外，传统的通过堆叠历史帧来引入时间信息的方法存在计算开销大、冗余信息多、难以聚焦关键动态等缺点。</p>
<p><strong>2. 关键创新与方法论贡献：</strong></p>
<p>作者提出了一个名为 <strong>HiF-VLA (Hindsight, Insight, and Foresight for VLAs)</strong> 的统一框架，其核心创新在于：</p>
<ul>
<li><strong>将运动（Motion）作为核心表示：</strong> 论文认为运动比原始像素帧更能紧凑、高效地捕捉时序上下文和世界动力学，能够过滤掉静态噪声，突出关键的交互变化。</li>
<li><strong>引入双向时序推理：</strong> HiF-VLA整合了三个关键能力：<ul>
<li><strong>Hindsight (回溯)：</strong> 通过编码历史运动信息（Motion Vectors, MVs）来获取过去动力学的结构化先验，以理解当前状态的成因。</li>
<li><strong>Insight (洞察)：</strong> 利用VLM（视觉语言模型）解析任务指令和当前观测，理解当前状态。</li>
<li><strong>Foresight (预见)：</strong> 预测未来的运动轨迹（以MVs形式）和潜在动作，以规划未来的行为。</li>
</ul>
</li>
<li><strong>Hindsight-Modulated Joint Expert (回溯调制的联合专家)：</strong> 这是一个创新的融合模块，它将回溯（Hindsight）信息作为一种条件先验，通过自适应层归一化（AdaLN）来调制联合推理过程。该专家模块能够融合回溯、洞察和预见信息，实现“<strong>边思考边行动</strong>”（think-while-acting）的范式，生成时序连贯且因果一致的动作。</li>
<li><strong>高效的运动表示（Motion Vectors, MVs）：</strong> 借鉴视频编码技术，使用MVs来表示历史帧之间的运动，显著减少了冗余，提高了效率，同时保留了关键的动态信息。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>性能提升：</strong> HiF-VLA在LIBERO-Long和CALVIN ABC-D等长时序机器人操作基准测试中取得了<strong>显著的性能提升</strong>，超越了现有强有力的基线方法。</li>
<li><strong>效率与可扩展性：</strong> 尽管引入了更复杂的时序推理，HiF-VLA的<strong>推理延迟几乎没有增加</strong>，并且在增加历史信息长度时表现出<strong>优异的可扩展性</strong>，有效解决了传统方法中的效率和冗余问题。</li>
<li><strong>真实世界表现：</strong> 在真实的机器人操作任务中，HiF-VLA也展现了<strong>强大的鲁棒性和泛化能力</strong>，能够成功执行复杂的多步操作，验证了其在实际应用中的有效性。</li>
<li><strong>意义：</strong> 该研究为解决VLA模型在长时序任务中的挑战提供了一种新颖且高效的解决方案，通过将运动作为核心时序表示，并引入双向时序推理，显著提升了机器人的规划和执行能力。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong></p>
<ul>
<li><strong>运动表示的准确性：</strong> 论文提到，当前的运动表示依赖于估计的准确性，并且可能对高度动态场景中的噪声敏感。</li>
<li><strong>对3D感知的需求：</strong> 在分析失败案例时，作者指出，未来工作可能需要整合更丰富的3D感知信息来进一步增强模型在真实世界中的鲁棒性。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>提升运动表示的鲁棒性：</strong> 探索更鲁棒的运动估计方法，以应对噪声和动态场景。</li>
<li><strong>融合3D感知信息：</strong> 将更丰富的3D几何信息整合到框架中，以提高对空间关系的理解和操作的准确性。</li>
<li><strong>大规模预训练：</strong> 探索利用大规模互联网视频数据来增强运动理解和生成能力。</li>
<li><strong>探索更复杂的场景：</strong> 将HiF-VLA应用于更具挑战性的、需要更精细操作和更长时序依赖的任务中。</li>
</ul>
<p>总而言之，HiF-VLA通过创新的运动表示和双向时序推理机制，有效地克服了现有VLA模型在长时序机器人操作中的局限性，在性能、效率和真实世界应用方面都取得了显著的进展，为该领域的研究提供了重要的贡献。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Building on this idea, we propose HiF-VLA (Hindsight, Insight, and Foresight for VLAs), a unified framework that leverages motion for bidirectional temporal reasoning.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.09928v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.09928v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.09927v1'></a></p>
<h2 id="token-expand-merge-training-free-token-compression-for-vision-language-action-models"><a href="https://arxiv.org/abs/2512.09927v1">Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models</a></h2>
<p><strong>Authors:</strong> Yifan Ye, Jiaqi Ma, Jun Cen, Zhihe Lu</p>
<p><strong>Published:</strong> 2025-12-10</p>
<p><strong>Categories:</strong> cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Vision-Language-Action (VLA) models pretrained on large-scale multimodal datasets have emerged as powerful foundations for robotic perception and control. However, their massive scale, often billions of parameters, poses significant challenges for real-time deployment, as inference becomes computationally expensive and latency-sensitive in dynamic environments. To address this, we propose Token Expand-and-Merge-VLA (TEAM-VLA), a training-free token compression framework that accelerates VLA inference while preserving task performance. TEAM-VLA introduces a dynamic token expansion mechanism that identifies and samples additional informative tokens in the spatial vicinity of attention-highlighted regions, enhancing contextual completeness. These expanded tokens are then selectively merged in deeper layers under action-aware guidance, effectively reducing redundancy while maintaining semantic coherence. By coupling expansion and merging within a single feed-forward pass, TEAM-VLA achieves a balanced trade-off between efficiency and effectiveness, without any retraining or parameter updates. Extensive experiments on LIBERO benchmark demonstrate that TEAM-VLA consistently improves inference speed while maintaining or even surpassing the task success rate of full VLA models. The code is public available on \href{https://github.com/Jasper-aaa/TEAM-VLA}{https://github.com/Jasper-aaa/TEAM-VLA}</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models</p>
<p><strong>作者：</strong> Yifan Ye, Jiaqi Ma, Jun Cen, Zhihe Lu</p>
<hr />
<p><strong>摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>本文旨在解决当前大型视觉-语言-动作（VLA）模型在实际部署中面临的计算效率和推理延迟问题。这些模型虽然在机器人感知和控制方面表现出色，但其庞大的规模（数十亿参数）导致推理过程计算量大、延迟高，难以满足动态、高频的实时控制场景需求。现有方法如 token 剪枝虽然有效，但通常需要额外的训练、可学习模块或依赖历史帧信息，增加了系统复杂性、内存开销，并可能降低鲁棒性。因此，迫切需要一种<strong>无需训练、仅依赖当前观测、且能独立于时间序列</strong>的 token 压缩策略。</p>
<p><strong>2. 主要创新点/方法贡献：</strong></p>
<p>作者提出了 <strong>TEAM-VLA (Token Expand-and-Merge-VLA)</strong> 框架，一个<strong>完全训练免费、仅依赖当前观测</strong>的 token 压缩方法，旨在加速 VLA 模型推理同时保持甚至提升任务性能。其核心创新在于两个阶段的协同工作：</p>
<ul>
<li>
<p><strong>Token 扩展 (Token Expanding)：</strong></p>
<ul>
<li><strong>目标：</strong> 解决基于相似度采样得到的 token 响应稀疏问题，重建完整的物体前景区域。</li>
<li><strong>方法：</strong><ul>
<li><strong>相似度采样 (Similarity Sampling)：</strong> 计算图像 token 与语言 embedding 的余弦相似度，识别与语言指令最相关的稀疏图像 token 作为“前景锚点”。</li>
<li><strong>密度计算与卷积扫描 (Density Computation &amp; Convolutional Scan)：</strong> 应用卷积操作计算局部密度，识别高密度区域。</li>
<li><strong>区域扩展 (Regional Expansion)：</strong><ul>
<li><strong>密集区域扩展 (Dense Area Expanding)：</strong> 对高密度区域进行确定性扩张，覆盖其空间邻域。</li>
<li><strong>稀疏区域扩展 (Sparse Area Expanding)：</strong> 对稀疏区域进行受控的随机扩张，以保留潜在的前景候选。</li>
</ul>
</li>
<li><strong>上下文采样 (Context Sampling)：</strong> 随机采样少量背景 token 作为补充上下文，以维持空间感知能力。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Token 合并 (Token Merging)：</strong></p>
<ul>
<li><strong>目标：</strong> 在 VLA 模型的中层，进一步压缩 token 数量，同时保留关键的语义和动作相关信息。</li>
<li><strong>方法：</strong><ul>
<li><strong>动作引导的软二分匹配 (Action-Guided Soft Bipartite Matching)：</strong> 利用文本和动作 token 来识别最重要的 M 个图像 token 作为源集 (S)，其余作为目标集 (T)。通过计算 S 和 T 之间的相似度矩阵，并应用软二分匹配，将目标 token 加权平均到最相似的源 token 中，实现 token 的压缩和信息的保留。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>TEAM-VLA 将扩展和合并机制集成在<strong>单个前向传播</strong>过程中，实现了效率和有效性之间的平衡，无需任何重训练或参数更新。</p>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>性能提升：</strong> 在 LIBERO benchmark 上，TEAM-VLA 显著提高了推理速度，将 OpenVLA-OFT 的推理时间从 109 ms 降低到 72.1 ms，实现了 <strong>1.5 倍以上的加速</strong>，同时保持或<strong>超越了完整 VLA 模型的任务成功率</strong>。</li>
<li><strong>效率与性能的权衡：</strong> 与其他方法相比，TEAM-VLA 在显著减少 token 数量的同时，保持了高成功率。例如，与 EfficientVLA 相比，TEAM-VLA 仅增加了 1.5 ms 的推理时间，但成功率提高了 7.7%。</li>
<li><strong>训练免费与独立性：</strong> 该方法无需额外训练，不依赖历史帧信息（buffer-free），这使其在实际部署中更加灵活和鲁棒，尤其是在环境变化快或时间连续性不可靠的情况下。</li>
<li><strong>可视化验证：</strong> 论文通过可视化展示了扩展机制如何从稀疏的相似度区域重建出更连贯、更具空间信息的前景区域，证明了其有效性。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>阈值 τ 的选择：</strong> 论文的消融研究表明，密度阈值 τ 的选择需要在扩展区域的完整性和计算效率之间进行权衡。过大的 τ 会导致成功率下降，而过小的 τ 则会增加计算量。论文最终选择 τ=1 作为默认设置，以在准确性和效率之间取得良好平衡。</li>
<li><strong>合并层选择：</strong> 合并操作的最佳层选择也需要通过消融实验确定，以在不同层级上找到成功率和效率的最佳点。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>更精细的 token 扩展策略：</strong> 探索更复杂的空间扩展和上下文采样策略，以进一步优化前景区域的重建。</li>
<li><strong>动态调整合并参数：</strong> 研究如何根据任务的复杂性或实时反馈动态调整合并的 token 数量（top-M），以实现更精细的加速。</li>
<li><strong>跨模态信息融合的进一步优化：</strong> 探索如何更有效地融合视觉、语言和动作信息，以提升模型在更广泛、更具挑战性的任务上的表现。</li>
<li><strong>在更广泛的机器人任务和硬件平台上的验证：</strong> 将 TEAM-VLA 应用于更多样化的机器人任务和不同的硬件平台，以评估其通用性和实际部署潜力。</li>
</ul>
<p><strong>总结：</strong></p>
<p>TEAM-VLA 提出了一种新颖的、训练免费的 VLA 模型 token 压缩框架，通过“扩展”和“合并”两个核心机制，在不依赖历史信息的情况下，显著提高了推理效率，同时保持了出色的任务性能。该方法为解决大型 VLA 模型在实时机器人应用中的部署挑战提供了一个有前景的解决方案。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we propose Token Expand-and-Merge-VLA (TEAM-VLA), a training-free token compression framework that accelerates VLA inference while preserving task performance.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.09927v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.09927v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.09925v1'></a></p>
<h2 id="gains-gaussian-based-inverse-rendering-from-sparse-multi-view-captures"><a href="https://arxiv.org/abs/2512.09925v1">GAINS: Gaussian-based Inverse Rendering from Sparse Multi-View Captures</a></h2>
<p><strong>Authors:</strong> Patrick Noras, Jun Myeong Choi, Didier Stricker, Pieter Peers, Roni Sengupta</p>
<p><strong>Published:</strong> 2025-12-10</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent advances in Gaussian Splatting-based inverse rendering extend Gaussian primitives with shading parameters and physically grounded light transport, enabling high-quality material recovery from dense multi-view captures. However, these methods degrade sharply under sparse-view settings, where limited observations lead to severe ambiguity between geometry, reflectance, and lighting. We introduce GAINS (Gaussian-based Inverse rendering from Sparse multi-view captures), a two-stage inverse rendering framework that leverages learning-based priors to stabilize geometry and material estimation. GAINS first refines geometry using monocular depth/normal and diffusion priors, then employs segmentation, intrinsic image decomposition (IID), and diffusion priors to regularize material recovery. Extensive experiments on synthetic and real-world datasets show that GAINS significantly improves material parameter accuracy, relighting quality, and novel-view synthesis compared to state-of-the-art Gaussian-based inverse rendering methods, especially under sparse-view settings. Project page: https://patrickbail.github.io/gains/</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：GAINS: Gaussian-based Inverse Rendering from Sparse Multi-View Captures</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话）：</strong></p>
<p>该论文提出了一种名为GAINS的两阶段逆渲染框架，旨在解决现有基于高斯溅射（Gaussian Splatting）的逆渲染方法在稀疏多视图场景下的性能下降问题。GAINS通过引入学习型先验（包括单目深度/法线、扩散模型和分割）来稳定几何和材质的估计，从而在稀疏视图下实现更准确的材质恢复、更优的重光照效果和新视角合成。</p>
<p><strong>2. 关键创新或方法论：</strong></p>
<p>GAINS的核心创新在于其<strong>两阶段的、结合学习型先验的逆渲染策略</strong>，特别针对稀疏多视图设置进行了优化。具体来说：</p>
<ul>
<li><strong>阶段一：几何精炼。</strong> 利用单目深度和法线估计，以及强大的扩散模型先验，来稳定和提升几何形状的准确性。这有助于在有限的视图信息下，克服几何不确定性。</li>
<li><strong>阶段二：材质恢复正则化。</strong> 引入图像分割信息，结合内在图像分解（Intrinsic Image Decomposition, IID）和扩散模型先验，来约束材质参数的恢复过程。通过将材质分解为反射率和光照成分，并利用先验知识进行正则化，可以有效缓解稀疏视图下材质与光照之间的混淆。</li>
</ul>
<p>这种方法将传统的基于物理的渲染（如高斯溅射）与现代的深度学习先验（特别是扩散模型）相结合，形成了一个更鲁棒的逆渲染流程。</p>
<p><strong>3. 对该领域的潜在影响：</strong></p>
<p>GAINS的提出对计算机视觉领域的逆渲染研究具有重要意义，尤其是在以下几个方面：</p>
<ul>
<li><strong>扩展了高斯溅射的应用范围：</strong> 解决了高斯溅射方法在实际应用中常见的稀疏视图限制，使其在更多受限采集条件下的场景中变得可行。</li>
<li><strong>提升了材质恢复的鲁棒性：</strong> 通过引入多样的学习型先验，显著提高了在信息不足情况下的材质参数估计精度，这是许多下游应用的关键。</li>
<li><strong>推动了新一代3D内容生成：</strong> 更准确的几何和材质恢复能力，将直接促进高质量的3D场景重建、虚拟现实/增强现实内容创作以及数字人等领域的发展。</li>
<li><strong>为稀疏数据下的3D重建提供了新思路：</strong> 证明了结合多模态先验（如单目深度、分割）和生成模型（如扩散模型）是解决稀疏数据挑战的有效途径。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用：</strong></p>
<ul>
<li><strong>3D重建与场景理解：</strong> 在摄影测量、激光扫描等领域，当采集数据稀疏时，GAINS的方法可以提供更可靠的几何和材质信息。</li>
<li><strong>虚拟现实（VR）与增强现实（AR）：</strong> 为创建更逼真、交互性更强的虚拟环境提供基础，尤其是在用户仅能提供少量照片的情况下。</li>
<li><strong>数字内容创作（DCC）：</strong> 艺术家和设计师可以利用更少的输入数据来创建高质量的3D资产和场景。</li>
<li><strong>电影与游戏制作：</strong> 简化3D资产的建模和材质制作流程，提高效率和真实感。</li>
<li><strong>机器人感知：</strong> 在机器人导航和场景理解中，从有限的传感器数据中恢复场景的物理属性。</li>
<li><strong>医学影像：</strong> 在某些医学扫描数据稀疏的情况下，可能有助于更精确地重建和分析解剖结构。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性：</strong></p>
<p>尽管摘要强调了GAINS在稀疏视图下的优势，但仍可以推断出一些潜在的局限性：</p>
<ul>
<li><strong>对先验的依赖性：</strong> 框架的性能在很大程度上依赖于所使用的学习型先验（单目深度/法线、扩散模型、分割）的质量和泛化能力。如果这些先验在特定场景下表现不佳，可能会影响GAINS的整体效果。</li>
<li><strong>计算成本：</strong> 引入扩散模型等深度学习组件可能会增加训练和推理的计算成本，尤其是在大规模场景下。</li>
<li><strong>对初始几何的敏感性（可能）：</strong> 虽然论文声称“稳定几何”，但如果初始的几何估计非常差，即使有先验的辅助，也可能难以完全纠正。</li>
<li><strong>“稀疏”的定义：</strong> 摘要中“稀疏”的具体程度并未量化。在极端稀疏的情况下（例如只有一两张图像），其性能仍可能受到限制。</li>
<li><strong>材质模型的复杂度：</strong> 摘要提到“材质参数”，但并未具体说明材质模型的复杂度（例如，是否支持各向异性、次表面散射等）。如果需要恢复非常复杂的材质属性，可能仍有挑战。</li>
</ul>
<p>总而言之，GAINS是一项非常有前景的研究，它巧妙地结合了基于物理的渲染技术和强大的深度学习先验，有效地解决了稀疏多视图逆渲染中的关键挑战，为该领域带来了新的突破。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce GAINS (Gaussian-based Inverse rendering from Sparse multi-view captures), a two-stage inverse rendering framework that leverages learning-based priors to stabilize geometry and material estimation.</li>
<li>Extensive experiments on synthetic and real-world datasets show that GAINS significantly improves material parameter accuracy, relighting quality, and novel-view synthesis compared to state-of-the-art Gaussian-based inverse rendering methods, especially under sparse-view settings.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.09925v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.09925v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.09923v1'></a></p>
<h2 id="splatent-splatting-diffusion-latents-for-novel-view-synthesis"><a href="https://arxiv.org/abs/2512.09923v1">Splatent: Splatting Diffusion Latents for Novel View Synthesis</a></h2>
<p><strong>Authors:</strong> Or Hirschorn, Omer Sela, Inbar Huberman-Spiegelglas, Netalee Efrat, Eli Alshan, Ianir Ideses, Frederic Devernay, Yochai Zvik, Lior Fritz</p>
<p><strong>Published:</strong> 2025-12-10</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Radiance field representations have recently been explored in the latent space of VAEs that are commonly used by diffusion models. This direction offers efficient rendering and seamless integration with diffusion-based pipelines. However, these methods face a fundamental limitation: The VAE latent space lacks multi-view consistency, leading to blurred textures and missing details during 3D reconstruction. Existing approaches attempt to address this by fine-tuning the VAE, at the cost of reconstruction quality, or by relying on pre-trained diffusion models to recover fine-grained details, at the risk of some hallucinations. We present Splatent, a diffusion-based enhancement framework designed to operate on top of 3D Gaussian Splatting (3DGS) in the latent space of VAEs. Our key insight departs from the conventional 3D-centric view: rather than reconstructing fine-grained details in 3D space, we recover them in 2D from input views through multi-view attention mechanisms. This approach preserves the reconstruction quality of pretrained VAEs while achieving faithful detail recovery. Evaluated across multiple benchmarks, Splatent establishes a new state-of-the-art for VAE latent radiance field reconstruction. We further demonstrate that integrating our method with existing feed-forward frameworks, consistently improves detail preservation, opening new possibilities for high-quality sparse-view 3D reconstruction.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Splatent: Splatting Diffusion Latents for Novel View Synthesis”的全面摘要，重点关注其在计算机视觉和机器学习领域的贡献：</p>
<p><strong>论文摘要：Splatent: Splatting Diffusion Latents for Novel View Synthesis</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>该论文主要解决了在变分自编码器（VAE）的潜在空间中进行新视角合成（Novel View Synthesis）时遇到的一个关键问题：<strong>VAE 潜在空间缺乏多视图一致性</strong>。这导致基于潜在空间辐射场（Latent Radiance Field）的方法在进行三维重建时，会产生模糊的纹理和丢失的细节。现有方法要么通过微调 VAE 来牺牲重建质量，要么依赖预训练的扩散模型来恢复细节，但可能引入幻觉。</p>
<p><strong>2. 关键创新/方法贡献：</strong></p>
<p>Splatent 提出了一种新颖的、基于扩散模型的增强框架，用于处理 VAE 潜在空间中的三维高斯辐射场（3D Gaussian Splatting, 3DGS）。其核心创新在于：</p>
<ul>
<li><strong>2D 视角下的细节恢复：</strong> 与传统的 3D 中心视角不同，Splatent 不直接在 3D 空间中重建细节，而是通过<strong>多视图注意力机制</strong>从输入视图中在 <strong>2D 空间</strong>恢复高频细节。</li>
<li><strong>冻结 VAE 的使用：</strong> 该方法在 VAE 冻结的情况下进行工作，这保留了预训练 VAE 的重建质量和泛化能力，避免了微调 VAE 可能带来的负面影响。</li>
<li><strong>基于扩散模型的潜在空间精炼：</strong> Splatent 利用单步扩散模型，通过将渲染的潜在特征与参考视图拼接成网格，并利用自注意力机制来融合信息，从而有效地恢复细节并减少伪影。</li>
<li><strong>与前馈模型的集成：</strong> 该框架能够无缝集成到现有的前馈潜在 3DGS 模型（如 MVSplat360）中，显著提升其细节保持能力。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>状态艺术的性能：</strong> Splatent 在 VAE 潜在空间辐射场重建方面取得了新的<strong>状态艺术（state-of-the-art）</strong>性能，在多个基准测试中均超越了现有方法。</li>
<li><strong>高质量细节恢复：</strong> 实验证明，Splatent 能够生成更清晰、更逼真的新视角图像，有效恢复了传统方法中丢失的细节。</li>
<li><strong>提升前馈模型：</strong> 将 Splatent 集成到 MVSplat360 等前馈模型中，显著提高了稀疏视图下的三维重建质量，为高分辨率、稀疏视图三维重建开辟了新的可能性。</li>
<li><strong>多视图一致性分析：</strong> 论文深入分析了 VAE 潜在空间在多视图一致性方面的不足，为后续研究提供了理论基础。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>潜在空间的固有挑战：</strong> 由于 VAE 潜在空间是信息有损的，并且其表示可能不完全符合 3D 一致性，因此恢复细节比在 RGB 空间中进行精炼更为困难。</li>
<li><strong>对预训练 VAE 的依赖：</strong> 该方法的性能受到预训练 VAE 质量的限制。</li>
<li><strong>计算成本：</strong> 虽然使用了单步扩散模型以提高推理效率，但训练过程仍然需要大量的计算资源。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>更高效的潜在空间表示：</strong> 探索能够更好地保持多视图一致性的 VAE 潜在空间表示，以进一步简化细节恢复过程。</li>
<li><strong>更精细的注意力机制：</strong> 研究更先进的注意力机制，以更有效地融合来自不同视图的信息。</li>
<li><strong>跨领域应用：</strong> 将 Splatent 的思想应用于其他需要处理潜在空间表示的生成模型和三维任务中。</li>
<li><strong>实时性提升：</strong> 进一步优化模型以实现更快的实时渲染和重建。</li>
</ul>
<p><strong>总结：</strong></p>
<p>Splatent 论文在 VAE 潜在空间三维重建领域做出了重要贡献。它通过创新的 2D 视角细节恢复策略和与扩散模型的有效结合，成功解决了现有方法在多视图一致性方面的核心痛点，显著提升了新视角合成的质量和细节保真度。该方法不仅在独立任务上表现出色，还能有效增强现有前馈模型，为未来在内存效率和与生成模型集成方面的高质量三维重建开辟了新的道路。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present Splatent, a diffusion-based enhancement framework designed to operate on top of 3D Gaussian Splatting (3DGS) in the latent space of VAEs. Our key insight departs from the conventional 3D-centric view: rather than reconstructing fine-grained details in 3D space, we recover them in 2D from input views through multi-view attention mechanisms.</li>
<li>Evaluated across multiple benchmarks, Splatent establishes a new state-of-the-art for VAE latent radiance field reconstruction.</li>
<li>We further demonstrate that integrating our method with existing feed-forward frameworks, consistently improves detail preservation, opening new possibilities for high-quality sparse-view 3D reconstruction.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.09923v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.09923v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.09913v1'></a></p>
<h2 id="nordfkb-a-fine-grained-benchmark-dataset-for-geospatial-ai-in-norway"><a href="https://arxiv.org/abs/2512.09913v1">NordFKB: a fine-grained benchmark dataset for geospatial AI in Norway</a></h2>
<p><strong>Authors:</strong> Sander Riisøen Jyhne, Aditya Gupta, Ben Worsley, Marianne Andersen, Ivar Oveland, Alexander Salveson Nossum</p>
<p><strong>Published:</strong> 2025-12-10</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We present NordFKB, a fine-grained benchmark dataset for geospatial AI in Norway, derived from the authoritative, highly accurate, national Felles KartdataBase (FKB). The dataset contains high-resolution orthophotos paired with detailed annotations for 36 semantic classes, including both per-class binary segmentation masks in GeoTIFF format and COCO-style bounding box annotations. Data is collected from seven geographically diverse areas, ensuring variation in climate, topography, and urbanization. Only tiles containing at least one annotated object are included, and training/validation splits are created through random sampling across areas to ensure representative class and context distributions. Human expert review and quality control ensures high annotation accuracy. Alongside the dataset, we release a benchmarking repository with standardized evaluation protocols and tools for semantic segmentation and object detection, enabling reproducible and comparable research. NordFKB provides a robust foundation for advancing AI methods in mapping, land administration, and spatial planning, and paves the way for future expansions in coverage, temporal scope, and data modalities.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供这篇论文的全面中文摘要。</p>
<p><strong>论文题目：</strong> NordFKB: A Fine-Grained Benchmark Dataset for Geospatial AI in Norway</p>
<p><strong>作者：</strong> Sander Riisøen Jyhne, Aditya Gupta, Ben Worsley, Marianne Andersen, Ivar Oveland, Alexander Salveson Nossum</p>
<hr />
<p><strong>全面摘要</strong></p>
<p><strong>1. 主要问题或研究问题：</strong></p>
<p>该论文旨在解决地理空间人工智能（Geospatial AI）领域中，尤其是在挪威地区，缺乏高质量、精细化、权威性公开基准数据集的问题。现有的公开数据集往往在地理范围、语义粒度或数据来源的权威性上存在不足，难以满足挪威本地化、精细化地图绘制、土地管理和空间规划等应用的需求。因此，研究的核心问题是如何构建一个能够支持挪威地理空间AI研究和应用发展的、具有高精度和广泛语义覆盖的基准数据集。</p>
<p><strong>2. 关键创新或方法论贡献：</strong></p>
<ul>
<li><strong>权威数据来源与精细化标注：</strong> NordFKB数据集的核心创新在于其数据来源于挪威权威、高精度、标准化的国家地理信息数据库——Felles KartdataBase (FKB)。该数据集提供了36个精细的语义类别，覆盖了广泛的地理特征。</li>
<li><strong>多模态标注格式：</strong> 数据集包含高分辨率正射影像，并为每个类别提供了两种标注格式：<ul>
<li><strong>逐类二值分割掩码（GeoTIFF格式）：</strong> 提供了像素级的精确边界信息，支持精细的语义分割任务，并保留了地理空间元数据。</li>
<li><strong>COCO格式的边界框标注（JSON格式）：</strong> 支持目标检测任务，具有良好的兼容性。</li>
</ul>
</li>
<li><strong>地理多样性与代表性：</strong> 数据集从挪威七个地理位置多样化的区域收集，涵盖了不同的气候、地形和城市化程度，以确保数据集的代表性和模型在不同环境下的鲁棒性。</li>
<li><strong>标准化评估协议与基准库：</strong> 除了数据集本身，论文还发布了一个配套的基准库，包含标准化的评估协议和工具，用于语义分割和目标检测任务，旨在促进研究的可复现性和模型间的公平比较。</li>
<li><strong>人工专家审核与质量控制：</strong> 所有标注都经过人工专家审核和质量控制，以确保高精度的标注质量。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<ul>
<li><strong>构建了首个挪威精细化地理空间AI基准数据集：</strong> NordFKB数据集的发布填补了挪威地区在权威、精细化地理空间AI基准数据集方面的空白。</li>
<li><strong>促进了地理空间AI在挪威的应用：</strong> 该数据集为挪威的地图绘制、土地管理和空间规划等领域提供了强大的AI研究和应用基础，能够训练出更符合国家标准和实际需求的AI模型。</li>
<li><strong>提升了研究的可比性和可复现性：</strong> 标准化的评估协议和基准库使得不同研究组和方法之间的结果能够进行公平、可比的比较，加速了该领域的研究进展。</li>
<li><strong>为模型鲁棒性测试提供了平台：</strong> 数据集的多样化地理区域和自然存在的类别不平衡，为测试模型在不同条件下的鲁棒性和处理稀有类别的能力提供了机会。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>地理覆盖范围有限：</strong> 尽管选择了七个多样化的区域，但数据集的覆盖范围仍不能代表挪威所有类型的地理环境。</li>
<li><strong>类别不平衡：</strong> 数据集中存在自然类别不平衡现象，一些常见类别（如建筑、道路）的数量远多于稀有类别（如砾石坑、公园细节），这可能影响模型在稀有类别上的性能。</li>
<li><strong>径向位移影响：</strong> 由于正射校正过程中使用了数字高程模型，对于高于地面的物体（如建筑物），其在影像中的位置可能与地面上的精确位置存在微小偏移（径向位移），这可能会影响像素级分割指标的准确性。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>扩大地理覆盖范围：</strong> 纳入更多挪威地区的地理环境，以提高数据集的代表性和鲁棒性。</li>
<li><strong>引入时间维度数据：</strong> 整合时间序列数据，以支持变化检测和长期监测任务。</li>
<li><strong>集成多模态数据：</strong> 结合激光雷达（LiDAR）或多光谱影像等其他数据源，以支持多模态分析，并提高对齐鲁棒性。</li>
<li><strong>增加新的基准任务：</strong> 引入更多不同类型的基准任务，以鼓励更广泛的研究参与和方法创新。</li>
<li><strong>支持三维重建研究：</strong> 包含原始单张图像和高精度三维矢量数据标注，以支持从航空传感器数据进行三维重建的研究。</li>
<li><strong>隐私和伦理考量：</strong> 在与其他数据集结合使用时，需注意避免潜在的隐私泄露问题，尤其是在涉及居民区或敏感基础设施的场景。</li>
</ul>
<p><strong>总结：</strong></p>
<p>NordFKB数据集的发布是挪威地理空间AI领域的一项重要贡献。它通过利用权威的FKB数据，提供了高质量、精细化、多格式的标注，并辅以标准化的评估工具，极大地降低了该领域研究的门槛，促进了模型在挪威本地化应用中的发展。尽管存在一些局限性，但该数据集为未来的研究提供了坚实的基础，并指明了进一步扩展和深化的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present NordFKB, a fine-grained benchmark dataset for geospatial AI in Norway, derived from the authoritative, highly accurate, national Felles KartdataBase (FKB).</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.09913v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.09913v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.09907v1'></a></p>
<h2 id="visualactbench-can-vlms-see-and-act-like-a-human"><a href="https://arxiv.org/abs/2512.09907v1">VisualActBench: Can VLMs See and Act like a Human?</a></h2>
<p><strong>Authors:</strong> Daoan Zhang, Pai Liu, Xiaofei Zhou, Yuan Ge, Guangchen Lan, Jing Bi, Christopher Brinton, Ehsan Hoque, Jiebo Luo</p>
<p><strong>Published:</strong> 2025-12-10</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Vision-Language Models (VLMs) have achieved impressive progress in perceiving and describing visual environments. However, their ability to proactively reason and act based solely on visual inputs, without explicit textual prompts, remains underexplored. We introduce a new task, Visual Action Reasoning, and propose VisualActBench, a large-scale benchmark comprising 1,074 videos and 3,733 human-annotated actions across four real-world scenarios. Each action is labeled with an Action Prioritization Level (APL) and a proactive-reactive type to assess models' human-aligned reasoning and value sensitivity. We evaluate 29 VLMs on VisualActBench and find that while frontier models like GPT4o demonstrate relatively strong performance, a significant gap remains compared to human-level reasoning, particularly in generating proactive, high-priority actions. Our results highlight limitations in current VLMs' ability to interpret complex context, anticipate outcomes, and align with human decision-making frameworks. VisualActBench establishes a comprehensive foundation for assessing and improving the real-world readiness of proactive, vision-centric AI agents.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“VisualActBench: Can VLMs See and Act like a Human?”的全面摘要，重点关注其在计算机视觉和机器学习领域的新颖性和重要性：</p>
<p><strong>论文摘要：VisualActBench: Can VLMs See and Act like a Human?</strong></p>
<p><strong>1. 主要问题/研究问题：</strong></p>
<p>该论文的核心研究问题在于：<strong>当前的视觉语言模型（VLMs）能否仅凭视觉输入，在没有明确文本指令的情况下，主动地进行推理并采取行动，从而展现出类似人类的、具有价值敏感性的决策能力？</strong> 尽管VLMs在理解和描述视觉环境方面取得了显著进展，但它们在主动推理和行动方面的能力，尤其是在纯视觉输入下的表现，仍然是一个未被充分探索的领域。</p>
<p><strong>2. 关键创新/方法论贡献：</strong></p>
<ul>
<li><strong>提出新的任务：视觉动作推理 (Visual Action Reasoning)</strong>：该任务旨在挑战VLMs，使其能够直接从视觉输入中生成上下文感知的、主动的动作，而无需任何文本提示，从而模拟人类在动态环境中的决策过程。</li>
<li><strong>构建大规模基准：VisualActBench</strong>：这是一个包含1,074个视频和3,733个由人类标注的动作的大型基准。这些视频涵盖了四个现实世界的场景：动态导航、家庭服务、安全监控和人机交互。</li>
<li><strong>引入动作优先级级别 (Action Prioritization Level, APL)</strong>：每个动作都被标注了APL，这是一个衡量人类偏好的指标，反映了在特定情境下，动作的紧迫性和重要性。</li>
<li><strong>引入主动/被动动作分类</strong>：动作被进一步分为主动（proactive）和被动（reactive）类型，以评估模型主动性行为的倾向。</li>
<li><strong>开发新的评估指标</strong>：除了标准的精确率、召回率和F1分数外，论文还提出了<strong>加权匹配度量 (Weighted Matching Metrics)</strong> 和 <strong>平均尺度分数 (Average Scale Score)</strong>，以更全面地评估模型在人类价值对齐和优先级判断方面的能力。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<ul>
<li><strong>性能差距显著</strong>：通过在VisualActBench上评估29个VLMs，研究发现，即使是像GPT-4o这样的前沿模型，在主动、高优先级动作的生成方面，与人类水平相比仍存在显著差距。</li>
<li><strong>模型局限性</strong>：结果揭示了当前VLMs在理解复杂上下文、预测结果以及与人类决策框架对齐方面存在局限性。许多模型倾向于生成低APL或被动响应，未能体现人类对预防性干预和高影响力行动的偏好。</li>
<li><strong>场景依赖性</strong>：模型在动态导航和安全监控等视觉线索明确的场景中表现更好，而在家庭服务和人机交互等需要理解抽象意图、用户需求或长期结果的场景中表现较弱。</li>
<li><strong>模型规模与RL的影响</strong>：研究表明，模型规模的增大和强化学习（如MPO）的应用可以提升动作质量和优先级判断能力，尤其是在大型模型上，但存在边际效益递减的现象。</li>
<li><strong>意义</strong>：VisualActBench为评估和改进主动式、以视觉为中心的AI代理的真实世界就绪度提供了一个全面的基础。它强调了从被动观察者转变为主动决策者的重要性，这对于机器人、自动驾驶等领域至关重要。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>人类水平差距</strong>：即使是最先进的模型，在主动、高优先级动作的生成方面，与人类的决策能力仍有较大差距。</li>
<li><strong>场景理解深度</strong>：在需要更深层次的常识推理、用户意图理解和长期规划的场景中，VLMs的表现明显不足。</li>
<li><strong>泛化能力</strong>：虽然基准涵盖了多种场景，但仍可能无法完全捕捉所有现实世界中的复杂性。</li>
<li><strong>模型规模的边际效应</strong>：虽然模型规模增大通常能提升性能，但在极端规模下，收益可能递减，表明架构设计和数据质量的重要性日益凸显。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>提升主动性与价值对齐</strong>：未来的研究应着重于开发能够更好地识别需要主动干预的情境，并生成符合人类价值体系的、高优先级动作的模型。</li>
<li><strong>增强上下文理解与预测能力</strong>：需要改进模型对复杂场景的理解能力，以及预测未来事件和用户需求的能力。</li>
<li><strong>更精细的场景建模</strong>：探索更精细的场景建模方法，以更好地处理需要长期规划和抽象推理的任务。</li>
<li><strong>自适应时间编码与选择性帧推理</strong>：未来的模型设计应关注自适应的时间编码和选择性帧推理，以从视觉流中提取因果相关的线索，避免冗余。</li>
<li><strong>探索更有效的训练策略</strong>：研究如何更有效地利用强化学习等技术，以及如何通过改进数据质量和训练策略来克服模型规模的边际效应。</li>
</ul>
<p>总而言之，这篇论文通过引入Visual Action Reasoning任务和VisualActBench基准，为评估和推动Vision-Language Models在主动决策和人类价值对齐方面的能力迈出了重要一步。它清晰地指出了当前模型的不足，并为未来开发更智能、更可靠的AI代理提供了宝贵的见解和方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce a new task, Visual Action Reasoning, and propose VisualActBench, a large-scale benchmark comprising 1,074 videos and 3,733 human-annotated actions across four real-world scenarios.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.09907v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.09907v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.09903v1'></a></p>
<h2 id="yopo-nav-visual-navigation-using-3dgs-graphs-from-one-pass-videos"><a href="https://arxiv.org/abs/2512.09903v1">YOPO-Nav: Visual Navigation using 3DGS Graphs from One-Pass Videos</a></h2>
<p><strong>Authors:</strong> Ryan Meegan, Adam D'Souza, Bryan Bo Cao, Shubham Jain, Kristin Dana</p>
<p><strong>Published:</strong> 2025-12-10</p>
<p><strong>Categories:</strong> cs.RO, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Visual navigation has emerged as a practical alternative to traditional robotic navigation pipelines that rely on detailed mapping and path planning. However, constructing and maintaining 3D maps is often computationally expensive and memory-intensive. We address the problem of visual navigation when exploration videos of a large environment are available. The videos serve as a visual reference, allowing a robot to retrace the explored trajectories without relying on metric maps. Our proposed method, YOPO-Nav (You Only Pass Once), encodes an environment into a compact spatial representation composed of interconnected local 3D Gaussian Splatting (3DGS) models. During navigation, the framework aligns the robot's current visual observation with this representation and predicts actions that guide it back toward the demonstrated trajectory. YOPO-Nav employs a hierarchical design: a visual place recognition (VPR) module provides coarse localization, while the local 3DGS models refine the goal and intermediate poses to generate control actions. To evaluate our approach, we introduce the YOPO-Campus dataset, comprising 4 hours of egocentric video and robot controller inputs from over 6 km of human-teleoperated robot trajectories. We benchmark recent visual navigation methods on trajectories from YOPO-Campus using a Clearpath Jackal robot. Experimental results show YOPO-Nav provides excellent performance in image-goal navigation for real-world scenes on a physical robot. The dataset and code will be made publicly available for visual navigation and scene representation research.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“YOPO-Nav: Visual Navigation using 3DGS Graphs from One-Pass Videos”的全面中文摘要：</p>
<p><strong>论文题目：</strong> YOPO-Nav: Visual Navigation using 3DGS Graphs from One-Pass Videos</p>
<p><strong>作者：</strong> Ryan Meegan, Adam D'Souza, Bryan Bo Cao, Shubham Jain, Kristin Dana</p>
<hr />
<p><strong>摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
传统机器人导航依赖于详细的地图构建和路径规划，这通常计算成本高昂且内存占用大。本文旨在解决在大型环境中进行视觉导航的问题，尤其是在只有一次性探索视频可用时。核心挑战在于如何在不依赖精确度量地图的情况下，让机器人能够利用这些视频作为视觉参考，重新追踪已探索的轨迹。</p>
<p><strong>2. 主要创新/方法贡献：</strong>
*   <strong>YOPO-Nav框架：</strong> 提出了一种名为YOPO-Nav（You Only Pass Once）的新颖方法，它将环境编码为一个紧凑的空间表示，该表示由相互连接的局部 3D 高斯溅射（3DGS）模型构成。这种表示方式轻量级、可解释且可扩展。
*   <strong>分层导航设计：</strong> YOPO-Nav采用分层设计。首先，一个视觉地点识别（VPR）模块提供粗略的全局定位，将当前观察与3DGS图中的节点关联起来。然后，局部3DGS模型用于精炼目标和中间位姿，以生成控制动作。
*   <strong>3DGS图作为场景表示：</strong> 将拓扑图的概念与3DGS方法相结合，构建了一个由局部3DGS模型组成的图。每个节点代表一个局部区域，通过帧的连续性或视觉相似性连接。这种表示方式能够从单次通过的视频中学习场景，并支持在物理机器人上的导航。
*   <strong>YOPO-Campus数据集：</strong> 引入了一个新的数据集，包含约4小时的机器人视角视频和控制器输入，覆盖了超过6公里的由人类遥操作的机器人轨迹。该数据集在罗格斯大学的校园环境中收集，为评估视觉导航方法提供了真实世界的场景。
*   <strong>姿态估计与动作生成：</strong> 通过将当前相机观测与3DGS模型中的目标帧进行特征匹配，并利用PnP-RANSAC算法估计机器人姿态，然后计算出纠正动作以对齐目标位姿。</p>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>卓越的导航性能：</strong> 在YOPO-Campus数据集上进行的实验表明，YOPO-Nav在图像目标导航任务上表现出色，尤其是在真实世界的复杂场景中，并且在物理机器人（Clearpath Jackal）上得到了验证。
*   <strong>优于SOTA方法：</strong> 与现有的先进方法（如ViNT和NoMad）相比，YOPO-Nav在不同距离阈值下都取得了更高的成功率，尤其是在长轨迹上表现显著。
*   <strong>迁移学习能力：</strong> VPR模型（YOPO-Loc）展示了良好的迁移学习能力，即使在地理上不同的数据集（GND）上进行预训练，也能在YOPO-Campus数据集上取得良好的验证结果。
*   <strong>高效性：</strong> YOPO-Nav的表示方法比传统的3D重建方法更轻量级，同时保留了细节，并且比纯拓扑图方法更具可解释性。
*   <strong>数据集和代码公开：</strong> 作者承诺将公开YOPO-Campus数据集和代码，这将为未来的视觉导航和场景表示研究提供宝贵的资源。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>对场景变化的适应性有限：</strong> YOPO-Nav的场景表示是基于一次性探索视频的快照，因此在面对与探索视频中场景变化较大的情况时（例如，在“建筑区域”案例中），其适应能力会受到限制。
*   <strong>依赖于地面分割：</strong> 姿态估计和动作生成部分依赖于准确的地面分割，尽管使用了SAM-2.1等先进模型，但在极端情况下仍可能存在挑战。
*   <strong>人类干预的必要性：</strong> 在某些复杂或动态变化较大的场景下，仍需要人类的干预来纠正机器人的路径偏差，尽管干预次数被控制在较低水平。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>增强对场景变化的鲁棒性：</strong> 进一步研究如何使YOPO-Nav能够更好地适应场景的动态变化，例如通过动态更新3DGS模型或引入更强的场景理解能力。
*   <strong>完全自主导航：</strong> 探索减少对人类干预的需求，实现更完全自主的导航，尤其是在更具挑战性的环境中。
*   <strong>多模态融合：</strong> 结合其他传感器数据（如激光雷达、IMU等）来进一步提升导航的鲁棒性和精度。
*   <strong>更广泛的应用场景：</strong> 将YOPO-Nav扩展到更广泛的应用场景，例如在动态环境中进行导航，或者与其他机器人任务（如抓取、服务）相结合。
*   <strong>更精细的场景表示：</strong> 探索如何构建更精细、更具语义信息的3DGS图表示，以支持更复杂的导航任务。</p>
<p><strong>总结：</strong>
“YOPO-Nav: Visual Navigation using 3DGS Graphs from One-Pass Videos”论文提出了一种创新的视觉导航方法，它利用一次性探索视频构建了一个轻量级的3D高斯溅射图表示，并结合视觉地点识别实现了高效的图像目标导航。该方法在真实世界场景中表现出色，并引入了有价值的YOPO-Campus数据集。论文的贡献在于提供了一种在计算和内存资源受限的情况下进行视觉导航的有效途径，为未来的研究开辟了新的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To evaluate our approach, we introduce the YOPO-Campus dataset, comprising 4 hours of egocentric video and robot controller inputs from over 6 km of human-teleoperated robot trajectories.</li>
<li>Experimental results show YOPO-Nav provides excellent performance in image-goal navigation for real-world scenes on a physical robot.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.09903v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.09903v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.09898v1'></a></p>
<h2 id="visual-heading-prediction-for-autonomous-aerial-vehicles"><a href="https://arxiv.org/abs/2512.09898v1">Visual Heading Prediction for Autonomous Aerial Vehicles</a></h2>
<p><strong>Authors:</strong> Reza Ahmari, Ahmad Mohammadi, Vahid Hemmati, Mohammed Mynuddin, Parham Kebria, Mahmoud Nabil Mahmoud, Xiaohong Yuan, Abdollah Homaifar</p>
<p><strong>Published:</strong> 2025-12-10</p>
<p><strong>Categories:</strong> cs.RO, cs.AI, cs.CV, cs.MA, eess.SY</p>
<p><strong>Abstract:</strong></p>
<p>The integration of Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) is increasingly central to the development of intelligent autonomous systems for applications such as search and rescue, environmental monitoring, and logistics. However, precise coordination between these platforms in real-time scenarios presents major challenges, particularly when external localization infrastructure such as GPS or GNSS is unavailable or degraded [1]. This paper proposes a vision-based, data-driven framework for real-time UAV-UGV integration, with a focus on robust UGV detection and heading angle prediction for navigation and coordination. The system employs a fine-tuned YOLOv5 model to detect UGVs and extract bounding box features, which are then used by a lightweight artificial neural network (ANN) to estimate the UAV's required heading angle. A VICON motion capture system was used to generate ground-truth data during training, resulting in a dataset of over 13,000 annotated images collected in a controlled lab environment. The trained ANN achieves a mean absolute error of 0.1506° and a root mean squared error of 0.1957°, offering accurate heading angle predictions using only monocular camera inputs. Experimental evaluations achieve 95% accuracy in UGV detection. This work contributes a vision-based, infrastructure- independent solution that demonstrates strong potential for deployment in GPS/GNSS-denied environments, supporting reliable multi-agent coordination under realistic dynamic conditions. A demonstration video showcasing the system's real-time performance, including UGV detection, heading angle prediction, and UAV alignment under dynamic conditions, is available at: https://github.com/Kooroshraf/UAV-UGV-Integration</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Visual Heading Prediction for Autonomous Aerial Vehicles”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Visual Heading Prediction for Autonomous Aerial Vehicles (面向自主航空器的视觉航向预测)</p>
<p><strong>作者：</strong> Reza Ahmari, Ahmad Mohammadi, Vahid Hemmati, Mohammed Mynuddin, Parham Kebria, Mahmoud Nabil Mahmoud, Xiaohong Yuan, Abdollah Homaifar</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
本文旨在解决无人机（UAV）与无人地面车辆（UGV）在实时场景下精确协调的挑战，特别是在GPS/GNSS信号不可用或受损的“GPS/GNSS拒止”环境中。核心问题是如何在不依赖外部定位基础设施的情况下，实现UAV对UGV的鲁棒检测和精确的航向角预测，以支持自主导航和协同作业。</p>
<p><strong>2. 主要创新点/方法贡献：</strong>
*   <strong>视觉为主的端到端框架：</strong> 提出了一种完全基于视觉的、数据驱动的框架，用于UAV与UGV的实时集成。该框架无需外部定位系统（如GPS、GNSS或运动捕捉系统）即可在部署时运行。
*   <strong>YOLOv5用于UGV检测：</strong> 采用微调后的YOLOv5模型，在单目摄像头输入下实现对UGV的实时、高精度检测，并提取其边界框特征。
*   <strong>轻量级ANN用于航向预测：</strong> 利用YOLOv5提取的边界框特征（如中心坐标、面积和长宽比），训练一个轻量级人工神经网络（ANN）来直接预测UAV所需的航向角，以与UGV对齐。这种方法取代了复杂的几何计算。
*   <strong>高质量数据集：</strong> 构建了一个包含超过13,000张标注图像的定制数据集，这些图像在受控实验室环境中，利用VICON运动捕捉系统生成了高精度的地面真实数据，覆盖了多样的UGV轨迹。
*   <strong>模块化设计：</strong> 框架采用模块化设计，支持即插即用，便于替换单个模型或传感器，使其适用于资源受限和基础设施稀疏的环境。</p>
<p><strong>3. 主要研究成果及其意义：</strong>
*   <strong>高精度航向预测：</strong> 训练后的ANN在测试集上实现了0.1506°的平均绝对误差（MAE）和0.1957°的均方根误差（RMSE），表明其能够提供亚度级的精确航向预测。
*   <strong>高精度UGV检测：</strong> YOLOv5模型在UGV检测方面达到了95%的准确率，确保了可靠的目标识别。
*   <strong>计算效率和实时性：</strong> 整个集成管道（YOLOv5检测器+ANN预测器）的平均推理延迟为31毫秒/帧，支持33-35 FPS的实时运行，适合嵌入式平台部署。
*   <strong>GPS/GNSS拒止环境下的可行性：</strong> 该研究证明了仅凭单目视觉输入，即可在无外部定位辅助的情况下实现可靠的UAV-UGV协同导航，为在复杂或受限环境中部署自主系统提供了有力支持。
*   <strong>与现有方法的对比优势：</strong> 与依赖ArUco标记、传感器融合（LiDAR、IMU、GPS）和外部校准的Husky Detection &amp; Tracking等方法相比，本文提出的方法在部署简便性、成本效益和对基础设施的依赖性方面具有显著优势。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>数据集的局限性：</strong> 虽然构建了高质量数据集，但训练和评估主要在受控的室内实验室环境中进行，这可能与真实的室外或半结构化环境存在差异（如光照变化、部分遮挡等）。
*   <strong>对视觉特征的依赖：</strong> 系统高度依赖视觉特征，在极端视觉退化（如强光、低光、大范围遮挡）的情况下，性能可能会受到影响。
*   <strong>未直接处理对抗性威胁：</strong> 该研究并未直接解决对抗性攻击或复杂空域威胁等问题。</p>
<p><strong>5. 未来研究方向：</strong>
*   <strong>增强鲁棒性：</strong> 整合深度估计、光照不变性感知和时间滤波技术，以提高在遮挡、室外多变光照和低光等挑战性条件下的性能。
*   <strong>自监督学习：</strong> 探索自监督学习技术，实现对未知地形的实时适应。
*   <strong>对抗性鲁棒性与可解释性：</strong> 研究对抗性训练和异常检测策略，以增强模型对视觉攻击和数据操纵的鲁棒性，并提高模型的可解释性。
*   <strong>室外和更复杂环境的部署：</strong> 将系统扩展到室外环境，并进一步提升其在更复杂、非结构化地形中的感知和导航能力。
*   <strong>安全性保障：</strong> 进一步研究在关键任务场景下的安全性保障措施。</p>
<p><strong>总结：</strong></p>
<p>这篇论文成功地提出了一种新颖的、仅基于视觉的UAV-UGV协同导航框架。通过结合YOLOv5的高效目标检测能力和轻量级ANN的精确航向预测能力，该系统在不依赖外部定位基础设施的情况下，实现了在GPS/GNSS拒止环境下的可靠UAV-UGV对齐。其计算效率高、部署简便、精度高的特点，使其在搜索救援、环境监测和自主物流等领域具有巨大的应用潜力，为未来自主机器人系统的发展开辟了新的途径。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This work contributes a vision-based, infrastructure- independent solution that demonstrates strong potential for deployment in GPS/GNSS-denied environments, supporting reliable multi-agent coordination under realistic dynamic conditions.</li>
<li>A demonstration video showcasing the system's real-time performance, including UGV detection, heading angle prediction, and UAV alignment under dynamic conditions, is available at: https://github.com/Kooroshraf/UAV-UGV-Integration</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.09898v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.09898v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.09864v1'></a></p>
<h2 id="uniugp-unifying-understanding-generation-and-planing-for-end-to-end-autonomous-driving"><a href="https://arxiv.org/abs/2512.09864v1">UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving</a></h2>
<p><strong>Authors:</strong> Hao Lu, Ziyang Liu, Guangfeng Jiang, Yuanfei Luo, Sheng Chen, Yangang Zhang, Ying-Cong Chen</p>
<p><strong>Published:</strong> 2025-12-10</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Autonomous driving (AD) systems struggle in long-tail scenarios due to limited world knowledge and weak visual dynamic modeling. Existing vision-language-action (VLA)-based methods cannot leverage unlabeled videos for visual causal learning, while world model-based methods lack reasoning capabilities from large language models. In this paper, we construct multiple specialized datasets providing reasoning and planning annotations for complex scenarios. Then, a unified Understanding-Generation-Planning framework, named UniUGP, is proposed to synergize scene reasoning, future video generation, and trajectory planning through a hybrid expert architecture. By integrating pre-trained VLMs and video generation models, UniUGP leverages visual dynamics and semantic reasoning to enhance planning performance. Taking multi-frame observations and language instructions as input, it produces interpretable chain-of-thought reasoning, physically consistent trajectories, and coherent future videos. We introduce a four-stage training strategy that progressively builds these capabilities across multiple existing AD datasets, along with the proposed specialized datasets. Experiments demonstrate state-of-the-art performance in perception, reasoning, and decision-making, with superior generalization to challenging long-tail situations.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving”的全面中文摘要：</p>
<p><strong>论文摘要：UniUGP: 统一理解、生成与规划，实现端到端自动驾驶</strong></p>
<p><strong>1. 研究问题与背景</strong></p>
<p>自动驾驶（AD）系统在处理长尾场景时面临严峻挑战，主要源于有限的世界知识和薄弱的视觉动态建模能力。现有的视觉-语言-动作（VLA）方法难以利用无标签视频进行视觉因果学习，而基于世界模型的方法则缺乏大型语言模型（LLM）的推理能力。这导致现有方法在复杂、罕见场景下的泛化能力和安全性受到限制。</p>
<p><strong>2. 关键创新与方法论贡献</strong></p>
<p>为了解决上述问题，本文提出了 <strong>UniUGP</strong>，一个统一的 <strong>理解-生成-规划（Understanding-Generation-Planning）</strong> 框架，旨在融合多模态信息，实现端到端的自动驾驶。其核心创新点包括：</p>
<ul>
<li><strong>构建多模态专业数据集：</strong> 论文构建了多个专门的、包含复杂场景的推理和规划标注数据集，以弥补现有数据集在长尾场景下的不足。</li>
<li><strong>混合专家架构：</strong> UniUGP采用混合专家架构，协同工作以实现场景理解、未来视频生成和轨迹规划。<ul>
<li><strong>理解专家：</strong> 利用预训练的视觉-语言模型（VLM）提取场景特征和进行因果推理。</li>
<li><strong>生成专家：</strong> 作为世界模型，负责生成未来视频，以进行视觉因果验证。</li>
<li><strong>规划专家：</strong> 基于理解和生成专家的输出，生成物理上一致的驾驶轨迹。</li>
</ul>
</li>
<li><strong>多模态因果对齐：</strong> 通过大规模多模态数据训练，UniUGP能够有效利用预训练VLM和视频生成模型的优势，增强跨模态因果对齐能力。</li>
<li><strong>可解释的链式思考（CoT）推理：</strong> UniUGP能够输出可解释的链式思考推理过程，增强决策的透明度。</li>
<li><strong>四阶段训练策略：</strong> 论文设计了一种创新的四阶段训练策略，逐步构建模型在场景理解、视觉动态建模、文本推理和多能力融合方面的能力，并利用了多个现有AD数据集和新构建的专业数据集。</li>
<li><strong>多任务损失函数：</strong> 设计了多项损失函数，以确保CoT逻辑一致性、轨迹时间平滑性和视频视觉连贯性。</li>
</ul>
<p><strong>3. 主要结果与意义</strong></p>
<p>通过在多个数据集上的广泛实验，UniUGP在以下方面取得了显著成果：</p>
<ul>
<li><strong>状态最优的性能：</strong> 在感知、推理和决策制定方面，UniUGP均达到了当前最先进（state-of-the-art）的性能。</li>
<li><strong>优越的长尾泛化能力：</strong> 实验证明，UniUGP在应对具有挑战性的长尾场景时表现出卓越的泛化能力，显著优于现有方法。</li>
<li><strong>可解释性与安全性提升：</strong> 可解释的CoT推理和物理上一致的轨迹生成，使得模型的决策过程更加透明，并有望提高自动驾驶的安全性。</li>
<li><strong>多模态融合的有效性：</strong> UniUGP成功地将VLM的语义推理能力与世界模型的视觉动态建模能力相结合，实现了端到端的自主驾驶。</li>
</ul>
<p><strong>4. 论文提及的局限性</strong></p>
<p>尽管取得了显著进展，UniUGP仍存在一些局限性：</p>
<ul>
<li><strong>长尾场景的泛化限制：</strong> 对于极端罕见的事件（如前所未有的天气、新颖的障碍物），模型的泛化能力仍受限于训练数据的覆盖范围。</li>
<li><strong>计算效率问题：</strong> 混合专家架构，特别是生成专家，对计算资源需求较高，在资源受限的移动平台上可能需要禁用以保证实时性。</li>
<li><strong>多模态对齐的优化空间：</strong> 尽管通过多项损失函数进行了优化，但语言推理与物理动力学的对齐仍有提升空间，尤其是在复杂的交互场景中，CoT推理可能与物理上一致的轨迹生成存在细微的不一致。</li>
<li><strong>固定训练比例的不足：</strong> 四阶段训练策略在最终融合阶段依赖于固定的数据集比例，未能动态适应不同数据集的互补优势，可能限制了任务协同效应。</li>
</ul>
<p><strong>5. 未来研究方向</strong></p>
<p>基于上述局限性，论文提出了以下未来研究方向：</p>
<ul>
<li><strong>增强长尾泛化能力：</strong> 利用高保真度合成数据生成（如结合世界模型和生成式AI）以及少样本/零样本学习，以提升模型对极端长尾场景的泛化能力。</li>
<li><strong>优化模型效率：</strong> 设计更轻量级的生成专家（如通过知识蒸馏、稀疏激活）并减少多专家之间的冗余计算，以提高模型效率。</li>
<li><strong>深化多模态对齐：</strong> 采用跨模态对比学习和分层融合机制，根据场景复杂度动态调整专家权重，以实现更深度的多模态对齐。</li>
<li><strong>减少数据依赖：</strong> 通过自监督信号（如无监督视频因果推理）减少对标注数据的依赖，并结合持续学习以避免灾难性遗忘。</li>
<li><strong>扩展交互能力：</strong> 支持动态实时反馈（如任务中的语音指令）和多智能体协同，以处理复杂的交通交互场景。</li>
<li><strong>闭环测试与部署：</strong> 将UniUGP集成到闭环系统中进行真实世界测试，建立性能-改进反馈循环，以提升安全性和鲁棒性，最终实现更实用的自动驾驶框架。</li>
</ul>
<p>总而言之，UniUGP在统一理解、生成和规划方面取得了重要突破，为端到端自动驾驶在复杂和长尾场景下的发展奠定了坚实基础，并指明了未来研究的潜在方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce a four-stage training strategy that progressively builds these capabilities across multiple existing AD datasets, along with the proposed specialized datasets.</li>
<li>Experiments demonstrate state-of-the-art performance in perception, reasoning, and decision-making, with superior generalization to challenging long-tail situations.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.09864v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.09864v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.09851v1'></a></p>
<h2 id="simultaneous-tactile-visual-perception-for-learning-multimodal-robot-manipulation"><a href="https://arxiv.org/abs/2512.09851v1">Simultaneous Tactile-Visual Perception for Learning Multimodal Robot Manipulation</a></h2>
<p><strong>Authors:</strong> Yuyang Li, Yinghan Chen, Zihang Zhao, Puhao Li, Tengyu Liu, Siyuan Huang, Yixin Zhu</p>
<p><strong>Published:</strong> 2025-12-10</p>
<p><strong>Categories:</strong> cs.RO, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Robotic manipulation requires both rich multimodal perception and effective learning frameworks to handle complex real-world tasks. See-through-skin (STS) sensors, which combine tactile and visual perception, offer promising sensing capabilities, while modern imitation learning provides powerful tools for policy acquisition. However, existing STS designs lack simultaneous multimodal perception and suffer from unreliable tactile tracking. Furthermore, integrating these rich multimodal signals into learning-based manipulation pipelines remains an open challenge. We introduce TacThru, an STS sensor enabling simultaneous visual perception and robust tactile signal extraction, and TacThru-UMI, an imitation learning framework that leverages these multimodal signals for manipulation. Our sensor features a fully transparent elastomer, persistent illumination, novel keyline markers, and efficient tracking, while our learning system integrates these signals through a Transformer-based Diffusion Policy. Experiments on five challenging real-world tasks show that TacThru-UMI achieves an average success rate of 85.5%, significantly outperforming the baselines of alternating tactile-visual (66.3%) and vision-only (55.4%). The system excels in critical scenarios, including contact detection with thin and soft objects and precision manipulation requiring multimodal coordination. This work demonstrates that combining simultaneous multimodal perception with modern learning frameworks enables more precise, adaptable robotic manipulation.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Simultaneous Tactile-Visual Perception for Learning Multimodal Robot Manipulation”的全面中文摘要，重点关注其在计算机视觉领域的创新性和重要性：</p>
<p><strong>论文题目：</strong> Simultaneous Tactile-Visual Perception for Learning Multimodal Robot Manipulation (用于学习多模态机器人操作的同步触觉-视觉感知)</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
机器人操作任务需要丰富的多模态感知能力和高效的学习框架来处理复杂的现实世界任务。现有的“透肤”（See-Through-Skin, STS）传感器虽然结合了触觉和视觉感知，但普遍存在以下问题：
*   <strong>缺乏同步多模态感知：</strong> 许多设计需要通过控制照明或移动部件来切换触觉和视觉模式，无法实现真正的同步感知。
*   <strong>触觉追踪不可靠：</strong> 尤其是在复杂背景下，触觉标记的跟踪容易受到干扰，导致信息丢失。
*   <strong>多模态信号集成困难：</strong> 将这些丰富的多模态信号有效地整合到基于学习的机器人操作管线中仍然是一个开放的挑战。</p>
<p><strong>2. 主要创新与方法贡献：</strong></p>
<ul>
<li>
<p><strong>TacThru 传感器：</strong></p>
<ul>
<li><strong>同步触觉-视觉感知：</strong> 采用<strong>全透明弹性体</strong>，实现清晰的视觉透视，并结合<strong>持续照明</strong>，无需模式切换，从而实现真正的同步感知。</li>
<li><strong>鲁棒的触觉信号提取：</strong> 引入了<strong>新型“关键线”（keyline）标记</strong>，由内外两层不同颜色的同心圆组成，确保在各种背景下都能保持可见性，即使在物体变形时也能提供可靠的触觉信息。</li>
<li><strong>高效的标记追踪：</strong> 结合<strong>卡尔曼滤波</strong>，实现了对64个标记的高效、鲁棒追踪，处理速度可达6.08毫秒/帧，满足高频感知需求。</li>
<li><strong>兼容性：</strong> 传感器设计兼容标准的视觉基触觉传感器（VBTS）制造流程，便于集成。</li>
</ul>
</li>
<li>
<p><strong>TacThru-UMI 框架：</strong></p>
<ul>
<li><strong>多模态模仿学习：</strong> 将 TacThru 传感器集成到一个基于<strong>Transformer 的扩散策略（Diffusion Policy）</strong>的模仿学习框架中。</li>
<li><strong>统一的信号处理：</strong> 该框架能够将视觉图像（来自腕部摄像头和传感器内部摄像头）、提取的触觉标记偏差以及本体感受信息编码成统一的 token，并利用 Transformer 的注意力机制进行多模态融合，以指导机器人动作。</li>
<li><strong>数据收集与处理：</strong> 框架支持 UMI（Universal Manipulation Interface）的数据收集和处理流程，并集成了 HTC Vive Tracker 以提高在视觉遮挡情况下的追踪稳定性。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>显著的性能提升：</strong> 在五项具有挑战性的现实世界操作任务（包括抓取、放置、分拣和插入）上，TacThru-UMI 实现了 <strong>85.5% 的平均成功率</strong>，显著优于交替触觉-视觉基线（66.3%）和纯视觉基线（55.4%）。</li>
<li><strong>克服传统局限性：</strong><ul>
<li><strong>薄软物体感知：</strong> 在处理如纸巾等薄软物体时，TacThru 的近距离视觉和触觉信息能够有效检测到微小的位移和形变，这是传统触觉传感器难以做到的。</li>
<li><strong>精细视觉辨别：</strong> 在分拣小尺寸螺栓的任务中，TacThru 的近距离视觉能力能够区分具有相似形状但不同颜色的螺栓，而远距离的腕部摄像头则无法做到。</li>
<li><strong>触觉辨别：</strong> 在悬挂剪刀的任务中，TacThru 的触觉反馈能够精确判断剪刀是否成功挂钩，这是纯视觉方法难以实现的。</li>
<li><strong>多模态融合的自适应策略：</strong> 在插入瓶盖任务中，TacThru-UMI 能够根据环境条件（如视野是否被遮挡）<strong>自适应地选择视觉伺服或触觉反馈</strong>进行操作，展现了强大的鲁棒性和灵活性。</li>
</ul>
</li>
<li><strong>重要性：</strong> 该研究表明，<strong>同步多模态感知与先进的学习框架相结合</strong>，能够实现更精确、更适应性强的机器人操作。TacThru 传感器及其配套的 TacThru-UMI 框架为机器人操作领域提供了一个实用的增强方案，降低了多模态集成的门槛。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>视觉编码器的泛化能力：</strong> 虽然研究表明预训练的视觉编码器（如 DINOv2）在 TacThru 的数据上表现良好，但其在处理传感器特有的标记和弹性体变形时，仍然存在一定的“领域偏移”（domain shift）。</li>
<li><strong>数据收集成本：</strong> 尽管框架兼容 UMI，但收集高质量的多模态演示数据仍然需要一定的时间和精力。</li>
</ul>
<p><strong>5. 未来研究方向：</strong></p>
<ul>
<li><strong>大规模数据收集与合成数据：</strong> 结合大规模数据集和合成触觉数据，以支持预训练更专业的编码器。</li>
<li><strong>更复杂的灵巧任务：</strong> 探索利用 TacThru 的同步感知能力来解决更复杂的双手机操作或需要精细力控的任务。</li>
<li><strong>端到端学习的进一步优化：</strong> 探索更高效的 Transformer 架构或多模态融合机制，以进一步提升学习效率和性能。</li>
<li><strong>部署到更广泛的机器人平台：</strong> 将 TacThru 传感器和 TacThru-UMI 框架部署到更多不同类型的机器人平台上，以验证其通用性。</li>
</ul>
<p>总而言之，这篇论文通过提出创新的 TacThru 传感器和 TacThru-UMI 学习框架，成功解决了机器人操作中同步多模态感知和信号集成的一系列关键挑战。其在多种复杂任务中展现出的优越性能，尤其是在处理薄软物体、精细视觉辨别和自适应策略选择方面的能力，为未来更智能、更鲁棒的机器人操作提供了重要的技术基础和方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce TacThru, an STS sensor enabling simultaneous visual perception and robust tactile signal extraction, and TacThru-UMI, an imitation learning framework that leverages these multimodal signals for manipulation.</li>
<li>Our sensor features a fully transparent elastomer, persistent illumination, novel keyline markers, and efficient tracking, while our learning system integrates these signals through a Transformer-based Diffusion Policy.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.09851v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.09851v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-12-11 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
