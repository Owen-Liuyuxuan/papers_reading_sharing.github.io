<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-11-28 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-04
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-11-27/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-12-01/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-11-28">Arxiv Computer Vision Papers - 2025-11-28</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#qwen3-vl-technical-report" class="nav-link">Qwen3-VL Technical Report</a>
                </li>
                <li class="nav-item">
                    <a href="#canvas-to-image-compositional-image-generation-with-multimodal-controls" class="nav-link">Canvas-to-Image: Compositional Image Generation with Multimodal Controls</a>
                </li>
                <li class="nav-item">
                    <a href="#tracegen-world-modeling-in-3d-trace-space-enables-learning-from-cross-embodiment-videos" class="nav-link">TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos</a>
                </li>
                <li class="nav-item">
                    <a href="#g2vlm-geometry-grounded-vision-language-model-with-unified-3d-reconstruction-and-spatial-reasoning" class="nav-link">G^2VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning</a>
                </li>
                <li class="nav-item">
                    <a href="#seeing-without-pixels-perception-from-camera-trajectories" class="nav-link">Seeing without Pixels: Perception from Camera Trajectories</a>
                </li>
                <li class="nav-item">
                    <a href="#uncertainty-quantification-for-visual-object-pose-estimation" class="nav-link">Uncertainty Quantification for Visual Object Pose Estimation</a>
                </li>
                <li class="nav-item">
                    <a href="#multi-crit-benchmarking-multimodal-judges-on-pluralistic-criteria-following" class="nav-link">Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following</a>
                </li>
                <li class="nav-item">
                    <a href="#continual-error-correction-on-low-resource-devices" class="nav-link">Continual Error Correction on Low-Resource Devices</a>
                </li>
                <li class="nav-item">
                    <a href="#mechanisms-of-non-monotonic-scaling-in-vision-transformers" class="nav-link">Mechanisms of Non-Monotonic Scaling in Vision Transformers</a>
                </li>
                <li class="nav-item">
                    <a href="#mogan-improving-motion-quality-in-video-diffusion-via-few-step-motion-adversarial-post-training" class="nav-link">MoGAN: Improving Motion Quality in Video Diffusion via Few-Step Motion Adversarial Post-Training</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-11-28">Arxiv Computer Vision Papers - 2025-11-28</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，作为一名专注于计算机视觉和机器学习的研究助理，我将为您提供一份简明的 Arxiv 计算机视觉领域论文的每日报告执行摘要。</p>
<hr />
<p><strong>Arxiv 计算机视觉领域论文每日报告 - 执行摘要 (2025-11-26)</strong></p>
<p><strong>报告日期:</strong> 2025-11-26
<strong>涵盖论文数量:</strong> 10</p>
<p><strong>1. 主要主题与趋势观察：</strong></p>
<p>本期 Arxiv 论文集中体现了计算机视觉领域在以下几个关键方向的深入探索：</p>
<ul>
<li><strong>多模态理解与生成 (Multimodal Understanding &amp; Generation):</strong> 多个工作致力于融合视觉信息与语言或其他模态，以实现更强大的理解能力和更具创造性的生成。这包括从文本到图像的生成，以及利用多模态信息进行更精细的场景理解。</li>
<li><strong>3D 理解与重建 (3D Understanding &amp; Reconstruction):</strong> 对三维世界的建模和理解是另一个显著趋势。论文探讨了如何从不同视角、不同载体（如轨迹）的视频中学习世界模型，以及如何将几何信息融入视觉语言模型，实现统一的3D重建和空间推理。</li>
<li><strong>模型鲁棒性与效率 (Model Robustness &amp; Efficiency):</strong> 关注模型的可靠性和在资源受限环境下的应用。这包括不确定性量化以提高位姿估计的准确性，以及在低资源设备上进行持续错误修正的技术。</li>
<li><strong>Transformer 架构的深入研究 (In-depth Transformer Research):</strong> 对 Vision Transformer (ViT) 及其变体的机制进行更深入的分析，例如理解其非单调缩放的机制。</li>
</ul>
<p><strong>2. 亮点与创新性论文：</strong></p>
<ul>
<li><strong>"Qwen3-VL Technical Report" (Shuai Bai et al.):</strong> 作为大型多模态模型 (LMM) 的技术报告，它很可能代表了当前 LMM 领域最前沿的进展，尤其是在通用视觉语言理解和生成能力方面，值得关注其技术细节和性能评估。</li>
<li><strong>"TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos" (Seungjae Lee et al.):</strong> 这篇论文提出了一个新颖的框架，能够从不同载体（如机器人、无人机等）的视频中学习世界模型，这对于实现跨载体泛化和更通用的机器人学习具有重要意义。</li>
<li><strong>"G<script type="math/tex">^2</script>VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning" (Wenbo Hu et al.):</strong> 该工作将几何信息深度整合到视觉语言模型中，实现了统一的3D重建和空间推理，这对于需要精确空间理解的应用（如机器人导航、AR/VR）至关重要。</li>
</ul>
<p><strong>3. 新兴研究方向与技术：</strong></p>
<ul>
<li><strong>基于轨迹的感知 (Perception from Camera Trajectories):</strong> "Seeing without Pixels: Perception from Camera Trajectories" 提出了一种不依赖像素信息，而是从相机运动轨迹中进行感知的全新视角，这可能为在极端低光照或无纹理环境下的感知提供新的解决方案。</li>
<li><strong>多模态评估的细粒度化 (Fine-grained Multimodal Evaluation):</strong> "Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following" 关注如何更全面、更细致地评估多模态模型，特别是其遵循复杂指令和多重标准的能力，这预示着未来多模态评估将更加精细化。</li>
<li><strong>视频生成中的运动质量提升 (Improving Motion Quality in Video Generation):</strong> "MoGAN: Improving Motion Quality in Video Diffusion via Few-Step Motion Adversarial Post-Training" 专注于提升视频生成中运动的真实感和流畅度，这是视频生成领域一个持续存在的挑战。</li>
</ul>
<p><strong>4. 建议阅读全文的论文：</strong></p>
<p>基于其潜在的影响力和创新性，以下论文建议优先阅读全文：</p>
<ul>
<li><strong>"Qwen3-VL Technical Report" (Shuai Bai et al.):</strong> 了解当前 LMM 的最新技术和能力边界。</li>
<li><strong>"TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos" (Seungjae Lee et al.):</strong> 探索跨载体学习和3D世界建模的新方法。</li>
<li><strong>"G<script type="math/tex">^2</script>VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning" (Wenbo Hu et al.):</strong> 深入理解几何信息如何增强视觉语言模型。</li>
<li><strong>"Seeing without Pixels: Perception from Camera Trajectories" (Zihui Xue et al.):</strong> 了解一种全新的、非像素依赖的感知范式。</li>
</ul>
<hr />
<p>这份摘要旨在帮助您快速掌握本期 Arxiv 论文的核心内容和趋势。希望对您的研究工作有所助益！</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2511.21631v1">Qwen3-VL Technical Report</a></li>
<li><a href="#2511.21691v1">Canvas-to-Image: Compositional Image Generation with Multimodal Controls</a></li>
<li><a href="#2511.21690v1">TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos</a></li>
<li><a href="#2511.21688v1">G<script type="math/tex">^2</script>VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning</a></li>
<li><a href="#2511.21681v1">Seeing without Pixels: Perception from Camera Trajectories</a></li>
<li><a href="#2511.21666v1">Uncertainty Quantification for Visual Object Pose Estimation</a></li>
<li><a href="#2511.21662v1">Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following</a></li>
<li><a href="#2511.21652v1">Continual Error Correction on Low-Resource Devices</a></li>
<li><a href="#2511.21635v1">Mechanisms of Non-Monotonic Scaling in Vision Transformers</a></li>
<li><a href="#2511.21592v1">MoGAN: Improving Motion Quality in Video Diffusion via Few-Step Motion Adversarial Post-Training</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2511.21631v1'></a></p>
<h2 id="qwen3-vl-technical-report"><a href="https://arxiv.org/abs/2511.21631v1">Qwen3-VL Technical Report</a></h2>
<p><strong>Authors:</strong> Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, Wenbin Ge, Zhifang Guo, Qidong Huang, Jie Huang, Fei Huang, Binyuan Hui, Shutong Jiang, Zhaohai Li, Mingsheng Li, Mei Li, Kaixin Li, Zicheng Lin, Junyang Lin, Xuejing Liu, Jiawei Liu, Chenglong Liu, Yang Liu, Dayiheng Liu, Shixuan Liu, Dunjie Lu, Ruilin Luo, Chenxu Lv, Rui Men, Lingchen Meng, Xuancheng Ren, Xingzhang Ren, Sibo Song, Yuchong Sun, Jun Tang, Jianhong Tu, Jianqiang Wan, Peng Wang, Pengfei Wang, Qiuyue Wang, Yuxuan Wang, Tianbao Xie, Yiheng Xu, Haiyang Xu, Jin Xu, Zhibo Yang, Mingkun Yang, Jianxin Yang, An Yang, Bowen Yu, Fei Zhang, Hang Zhang, Xi Zhang, Bo Zheng, Humen Zhong, Jingren Zhou, Fan Zhou, Jing Zhou, Yuanzhi Zhu, Ke Zhu</p>
<p><strong>Published:</strong> 2025-11-26</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-text understanding, surpassing comparable text-only backbones in several cases; (ii) robust long-context comprehension with a native 256K-token window for both text and interleaved multimodal inputs, enabling faithful retention, retrieval, and cross-referencing across long documents and videos; and (iii) advanced multimodal reasoning across single-image, multi-image, and video tasks, demonstrating leading performance on comprehensive evaluations such as MMMU and visual-math benchmarks (e.g., MathVista and MathVision). Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding. Under comparable token budgets and latency constraints, Qwen3-VL achieves superior performance in both dense and Mixture-of-Experts (MoE) architectures. We envision Qwen3-VL serving as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world workflows.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对“Qwen3-VL Technical Report”论文的全面中文摘要，重点关注其在计算机视觉领域的创新性和重要性：</p>
<p><strong>论文题目：</strong> Qwen3-VL Technical Report</p>
<p><strong>作者：</strong> Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, Wenbin Ge, Zhifang Guo, Qidong Huang, Jie Huang, Fei Huang, Binyuan Hui, Shutong Jiang, Zhaohai Li, Mingsheng Li, Mei Li, Kaixin Li, Zicheng Lin, Junyang Lin, Xuejing Liu, Jiawei Liu, Chenglong Liu, Yang Liu, Dayiheng Liu, Shixuan Liu, Dunjie Lu, Ruilin Luo, Chenxu Lv, Rui Men, Lingchen Meng, Xuancheng Ren, Xingzhang Ren, Sibo Song, Yuchong Sun, Jun Tang, Jianhong Tu, Jianqiang Wan, Peng Wang, Pengfei Wang, Qiuyue Wang, Yuxuan Wang, Tianbao Xie, Yiheng Xu, Haiyang Xu, Jin Xu, Zhibo Yang, Mingkun Yang, Jianxin Yang, An Yang, Bowen Yu, Fei Zhang, Hang Zhang, Xi Zhang, Bo Zheng, Humen Zhong, Jingren Zhou, Fan Zhou, Jing Zhou, Yuanzhi Zhu, Ke Zhu</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题与研究目标：</strong></p>
<p>本论文旨在解决当前多模态大模型在理解和推理长上下文、跨模态信息融合以及复杂多模态任务（如视频理解、空间推理、代理决策等）方面存在的挑战。研究目标是开发一个更强大、更通用的视觉-语言模型（VLM），使其能够在广泛的多模态基准测试中取得优异的性能，并能够无缝集成文本、图像和视频，支持长达 256K token 的上下文。</p>
<p><strong>2. 关键创新与方法论贡献：</strong></p>
<p>Qwen3-VL 的核心创新体现在其架构、训练策略和数据构建上，主要包括：</p>
<ul>
<li><strong>架构升级：</strong><ul>
<li><strong>增强型交错 MROPE (Interleaved MROPE)：</strong> 改进了位置编码机制，通过交错时间 (t)、水平 (h) 和垂直 (w) 维度来平衡频率频谱，从而实现更强的空间-时间建模能力，尤其是在长视频理解方面。</li>
<li><strong>DeepStack 集成：</strong> 引入了 DeepStack 机制，将视觉编码器多层级的特征注入到 LLM 的对应层，以增强视觉-语言的对齐，实现更深层次的多模态融合。</li>
<li><strong>文本时间对齐 (Text-based Time Alignment)：</strong> 针对视频，从绝对时间对齐演进到显式的文本时间戳对齐，为视频提供更精确的时间定位。</li>
</ul>
</li>
<li><strong>训练策略优化：</strong><ul>
<li><strong>长上下文预训练：</strong> 将预训练的上下文长度扩展至 256K token，并采用分阶段的训练策略（S0-S3），逐步适应更长的序列。</li>
<li><strong>平方根重加权 (Square-root Reweighting)：</strong> 用于平衡文本和多模态学习目标，提升多模态性能而不牺牲文本能力。</li>
<li><strong>后训练阶段：</strong> 采用监督微调 (SFT)、强弱蒸馏 (Strong-to-Weak Distillation) 和强化学习 (RL) 等多阶段方法，进一步提升模型的指令遵循、推理和对齐能力。</li>
</ul>
</li>
<li><strong>大规模、高质量的多模态数据构建：</strong><ul>
<li><strong>数据多样性与质量提升：</strong> 涵盖了图像字幕、交错文本-图像、OCR、文档解析、长文档理解、视觉接地、计数、空间理解、3D识别、代码、视频、STEM 等多个领域。</li>
<li><strong>精细化数据处理：</strong> 采用多阶段的过滤、去重、增强和合成技术，确保数据的质量、多样性和领域覆盖度。例如，针对长文档，通过合并页面构建了长序列数据；针对视频，采用了时间感知视频理解和数据平衡采样策略。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<p>Qwen3-VL 在广泛的多模态基准测试中取得了领先或具有竞争力的性能，尤其在以下方面表现突出：</p>
<ul>
<li><strong>长上下文理解：</strong> 凭借 256K token 的原生上下文窗口，在长文档和长视频任务上展现出强大的信息保留、检索和交叉引用能力。</li>
<li><strong>多模态推理：</strong> 在 MMMU、MathVista、MathVision 等基准上取得了领先的性能，证明了其在图像、多图像和视频任务上的高级推理能力。</li>
<li><strong>纯文本理解：</strong> 在某些情况下，其纯文本理解能力甚至超越了同等规模的文本模型。</li>
<li><strong>模型家族的多样性：</strong> 提供了密集型 (dense) 和混合专家 (MoE) 两种架构的变体，以适应不同的延迟和质量需求。</li>
<li><strong>代理智能的潜力：</strong> 论文强调了 Qwen3-VL 作为图像接地推理、代理决策和多模态代码智能的基础引擎的潜力，能够赋能更复杂的现实世界应用。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<p>论文中虽然没有明确列出局限性，但从其研究方向和评估方法可以推断出一些潜在的方面：</p>
<ul>
<li><strong>计算资源需求：</strong> 训练如此大规模和复杂的模型需要巨大的计算资源。</li>
<li><strong>评估的公平性：</strong> 在某些视频理解任务的比较中，由于资源和 API 的限制，输入帧数可能存在差异，影响了评估的绝对公平性。</li>
<li><strong>“思考”模式的成本：</strong> “Thinking”模式的模型在推理时可能需要更多的计算资源。</li>
</ul>
<p><strong>5. 未来研究方向：</strong></p>
<p>论文展望了 Qwen3-VL 在未来 AI 代理领域的应用，并提出了未来的研究方向：</p>
<ul>
<li><strong>交互式感知 (Interactive Perception)：</strong> 进一步提升模型与环境的交互能力。</li>
<li><strong>工具增强推理 (Tool-Augmented Reasoning)：</strong> 探索更复杂的工具使用和推理能力。</li>
<li><strong>实时多模态控制 (Real-time Multimodal Control)：</strong> 实现更流畅、实时的多模态控制。</li>
<li><strong>统一理解-生成架构 (Unified Understanding-Generation Architectures)：</strong> 探索更通用的模型架构，以提升视觉生成能力。</li>
<li><strong>赋能社区创新：</strong> 通过开源模型家族，促进社区在多模态 AI 领域的创新。</li>
</ul>
<p><strong>总结：</strong></p>
<p>Qwen3-VL 技术报告详细介绍了 Qwen 系列最新一代视觉-语言模型 Qwen3-VL。该模型通过创新的架构设计（如交错 MROPE、DeepStack、文本时间对齐）、优化的训练策略（长上下文、多阶段后训练）以及大规模、高质量的多模态数据集，在多模态理解和推理方面取得了显著的突破。其在长上下文处理、多模态推理、视频理解等方面的优异表现，使其成为构建更强大、更通用的视觉-语言智能体的重要基石，为未来多模态 AI 的发展开辟了新的可能性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks.</li>
<li>Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.21631v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.21631v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.21691v1'></a></p>
<h2 id="canvas-to-image-compositional-image-generation-with-multimodal-controls"><a href="https://arxiv.org/abs/2511.21691v1">Canvas-to-Image: Compositional Image Generation with Multimodal Controls</a></h2>
<p><strong>Authors:</strong> Yusuf Dalva, Guocheng Gordon Qian, Maya Goldenberg, Tsai-Shien Chen, Kfir Aberman, Sergey Tulyakov, Pinar Yanardag, Kuan-Chieh Jackson Wang</p>
<p><strong>Published:</strong> 2025-11-26</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>While modern diffusion models excel at generating high-quality and diverse images, they still struggle with high-fidelity compositional and multimodal control, particularly when users simultaneously specify text prompts, subject references, spatial arrangements, pose constraints, and layout annotations. We introduce Canvas-to-Image, a unified framework that consolidates these heterogeneous controls into a single canvas interface, enabling users to generate images that faithfully reflect their intent. Our key idea is to encode diverse control signals into a single composite canvas image that the model can directly interpret for integrated visual-spatial reasoning. We further curate a suite of multi-task datasets and propose a Multi-Task Canvas Training strategy that optimizes the diffusion model to jointly understand and integrate heterogeneous controls into text-to-image generation within a unified learning paradigm. This joint training enables Canvas-to-Image to reason across multiple control modalities rather than relying on task-specific heuristics, and it generalizes well to multi-control scenarios during inference. Extensive experiments show that Canvas-to-Image significantly outperforms state-of-the-art methods in identity preservation and control adherence across challenging benchmarks, including multi-person composition, pose-controlled composition, layout-constrained generation, and multi-control generation.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：Canvas-to-Image: Compositional Image Generation with Multimodal Controls</strong></p>
<p><strong>1. 论文的主要贡献 (2-3句话的简洁总结)</strong></p>
<p>这篇论文提出了一种名为 Canvas-to-Image 的新颖框架，旨在解决当前扩散模型在处理复杂、多模态图像生成控制方面的不足。其核心贡献在于将文本提示、主体参考、空间布局、姿态约束等异构控制信号统一编码到一个“画布”接口中，使模型能够直接理解并生成高度符合用户意图的图像。通过多任务数据集和联合训练策略，该框架实现了跨模态的集成推理，显著提升了图像生成在身份保持和控制遵循方面的表现。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<ul>
<li><strong>统一的“画布”接口 (Unified Canvas Interface):</strong> 这是最核心的创新点。论文将原本分散、异构的控制信号（文本、主体、空间、姿态、布局）整合成一个单一的、可解释的“画布”图像。这个画布充当了一个通用的输入表示，模型可以直接从中提取和整合所有控制信息，避免了为每种控制类型设计独立的输入机制或复杂的后处理。</li>
<li><strong>集成视觉-空间推理 (Integrated Visual-Spatial Reasoning):</strong> 通过将所有控制信号编码到画布中，模型被训练来执行集成的视觉-空间推理。这意味着模型不再是孤立地处理每一种控制，而是能够理解它们之间的相互关系和约束，从而实现更精细的构图控制。</li>
<li><strong>多任务数据集和训练策略 (Multi-Task Datasets and Training Strategy):</strong> 为了支持这种集成推理，论文专门构建了一套多任务数据集，并提出了“多任务画布训练”策略。这种策略旨在优化扩散模型，使其能够同时理解和整合多种控制模态，从而在统一的学习范式内完成文本到图像的生成。这种联合训练方式比依赖于特定任务的启发式方法更具泛化能力。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<ul>
<li><strong>提升图像生成的可控性与精确度:</strong> Canvas-to-Image 的方法有望极大地提升用户对图像生成过程的控制能力。用户将能够以前所未有的精细度来指定图像的构图、主体特征、空间关系和姿态，从而生成更符合其创意和需求的图像。</li>
<li><strong>推动更复杂的场景生成:</strong> 尤其是在需要精确构图和多主体交互的场景（如多人物合成、复杂场景布局）中，该方法将带来显著的进步。这对于内容创作、虚拟现实、游戏开发等领域具有重要意义。</li>
<li><strong>统一多模态控制的范式:</strong> 该研究为如何有效地整合和利用多种模态的控制信号来指导图像生成提供了一个统一的框架和解决方案，可能成为未来图像生成模型设计的重要参考。</li>
<li><strong>降低用户使用门槛:</strong> 通过提供一个直观的画布接口，用户无需深入了解复杂的模型参数或编程接口，即可实现高级的图像控制，从而降低了高质量图像生成的门槛。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>内容创作与设计:</strong> 艺术家、设计师和营销人员可以更高效地生成符合特定品牌、故事或广告需求的图像。</li>
<li><strong>虚拟现实 (VR) 和增强现实 (AR):</strong> 快速生成逼真且具有特定布局和角色姿态的虚拟场景，用于游戏、模拟训练或沉浸式体验。</li>
<li><strong>游戏开发:</strong> 自动生成具有特定角色、场景和动作的游戏资产。</li>
<li><strong>电影和动画制作:</strong> 辅助概念艺术设计，快速生成具有特定构图和角色交互的场景草图。</li>
<li><strong>个性化内容生成:</strong> 为用户生成高度定制化的图像，例如根据用户描述和偏好生成社交媒体内容。</li>
<li><strong>教育和培训:</strong> 创建具有特定视觉元素的教学材料或模拟场景。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<ul>
<li><strong>数据集的构建和规模:</strong> 论文提到了“精心策划了一套多任务数据集”，但摘要并未详细说明数据集的规模、多样性以及覆盖的控制类型。如果数据集不够全面或存在偏差，可能会影响模型的泛化能力。</li>
<li><strong>计算成本:</strong> 整合多种控制信号并进行联合训练的扩散模型，其训练和推理的计算成本可能会比现有模型更高。</li>
<li><strong>画布表示的复杂性:</strong> 如何将所有异构控制信号有效地编码到单一的画布表示中，以及画布表示的维度和复杂度，可能会影响模型的学习效率和最终效果。例如，如何平衡不同控制信号的重要性，以及如何处理潜在的冲突控制。</li>
<li><strong>对“高保真度”的定义:</strong> 摘要中提到了“高保真度的组合和多模态控制”，但“高保真度”的具体衡量标准和在不同控制模态下的表现可能需要进一步的实验验证。</li>
<li><strong>对新颖控制类型的适应性:</strong> 该框架在多大程度上能够适应未来出现的新型控制模态或更复杂的控制组合，这一点尚未在摘要中明确。</li>
</ul>
<p>总而言之，Canvas-to-Image 是一项非常有前景的研究，它通过创新的画布接口和多任务训练策略，有效地解决了当前扩散模型在多模态、高精度图像生成控制方面的关键挑战。这项工作有望在多个应用领域带来显著的进步，并为未来的图像生成模型研究提供新的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce Canvas-to-Image, a unified framework that consolidates these heterogeneous controls into a single canvas interface, enabling users to generate images that faithfully reflect their intent.</li>
<li>Extensive experiments show that Canvas-to-Image significantly outperforms state-of-the-art methods in identity preservation and control adherence across challenging benchmarks, including multi-person composition, pose-controlled composition, layout-constrained generation, and multi-control generation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.21691v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.21691v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.21690v1'></a></p>
<h2 id="tracegen-world-modeling-in-3d-trace-space-enables-learning-from-cross-embodiment-videos"><a href="https://arxiv.org/abs/2511.21690v1">TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos</a></h2>
<p><strong>Authors:</strong> Seungjae Lee, Yoonkyo Jung, Inkook Chun, Yao-Chih Lee, Zikui Cai, Hongjia Huang, Aayush Talreja, Tan Dat Dao, Yongyuan Liang, Jia-Bin Huang, Furong Huang</p>
<p><strong>Published:</strong> 2025-11-26</p>
<p><strong>Categories:</strong> cs.RO, cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Learning new robot tasks on new platforms and in new scenes from only a handful of demonstrations remains challenging. While videos of other embodiments - humans and different robots - are abundant, differences in embodiment, camera, and environment hinder their direct use. We address the small-data problem by introducing a unifying, symbolic representation - a compact 3D "trace-space" of scene-level trajectories - that enables learning from cross-embodiment, cross-environment, and cross-task videos. We present TraceGen, a world model that predicts future motion in trace-space rather than pixel space, abstracting away appearance while retaining the geometric structure needed for manipulation. To train TraceGen at scale, we develop TraceForge, a data pipeline that transforms heterogeneous human and robot videos into consistent 3D traces, yielding a corpus of 123K videos and 1.8M observation-trace-language triplets. Pretraining on this corpus produces a transferable 3D motion prior that adapts efficiently: with just five target robot videos, TraceGen attains 80% success across four tasks while offering 50-600x faster inference than state-of-the-art video-based world models. In the more challenging case where only five uncalibrated human demonstration videos captured on a handheld phone are available, it still reaches 67.5% success on a real robot, highlighting TraceGen's ability to adapt across embodiments without relying on object detectors or heavy pixel-space generation.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos”论文的全面中文摘要：</p>
<p><strong>论文题目：</strong> TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos</p>
<p><strong>作者：</strong> Seungjae Lee, Yoonkyo Jung, Inkook Chun, Yao-Chih Lee, Zikui Cai, Hongjia Huang, Aayush Talreja, Tan Dat Dao, Yongyuan Liang, Jia-Bin Huang, Furong Huang</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
该论文旨在解决机器人学习新任务和适应新环境时面临的“小样本”问题。尽管人类和其他机器人的视频数据丰富，但由于身体形态、相机视角和环境的差异，直接利用这些数据进行机器人学习非常困难。研究的核心问题是如何有效地利用跨不同实体（embodiment）、跨不同环境和跨不同任务的视频数据来训练机器人，使其在数据稀缺的情况下也能快速掌握新技能。</p>
<p><strong>2. 主要创新点/方法论贡献：</strong>
*   <strong>3D Trace-Space 表示：</strong> 论文提出了一种新颖的、统一的、符号化的表示方法——“trace-space”。它将场景级轨迹抽象为紧凑的3D轨迹序列，捕捉运动的“何时”和“如何”，而忽略了外观和背景信息。这种表示方法使得模型能够跨越不同实体、环境和任务进行学习，并对外观和相机变化具有不变性。
*   <strong>TraceGen 世界模型：</strong> 基于3D trace-space，论文提出了TraceGen，一个世界模型，它直接在3D trace-space中预测未来运动，而不是在像素空间。这使得模型能够专注于对操纵至关重要的几何信号，避免了生成像素的计算开销。
*   <strong>TraceForge 数据引擎：</strong> 为了大规模训练TraceGen，论文开发了一个名为TraceForge的数据管道。该管道能够将异构的、来自不同来源（包括人类视频和机器人数据集）的视频转换为一致的3D轨迹标注。它通过补偿相机运动、从多个视角重建轨迹以及进行速度重定向来处理各种视频数据，最终构建了一个包含123K视频和1.8M“观察-轨迹-语言”三元组的大规模语料库。
*   <strong>高效的迁移学习和适应能力：</strong> TraceGen通过在TraceForge语料库上进行预训练，获得了强大的3D运动先验。这使得模型能够通过极少量的“热身”（warm-up）数据（例如，仅5个目标机器人视频或5个未校准的人类演示视频）快速适应新任务和新环境。</p>
<p><strong>3. 主要研究成果及其意义：</strong>
*   <strong>机器人-机器人适应：</strong> 在仅使用5个目标机器人演示视频进行“热身”的情况下，TraceGen在四个任务上达到了80%的成功率。
*   <strong>人类-机器人迁移：</strong> 在更具挑战性的场景中，仅使用5个未校准的、手持手机拍摄的人类演示视频进行“热身”，TraceGen在真实机器人上实现了67.5%的成功率。这有力地证明了TraceGen跨实体迁移学习的能力，且无需依赖对象检测器或复杂的像素生成。
*   <strong>推理效率：</strong> TraceGen的推理速度比基于视频生成的世界模型快50-600倍，比其他基于轨迹的模型快3.8倍。这对于需要实时规划的机器人应用至关重要。
*   <strong>意义：</strong> 这些结果表明，TraceGen通过其3D trace-space表示和TraceForge数据引擎，有效地解决了机器人学习中的小样本和跨实体迁移问题，为开发更通用、更高效的机器人操纵系统开辟了新途径。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>零样本生成能力：</strong> 尽管TraceGen的零样本生成能力很有前景，但在面对全新的实体或环境时，其可靠性尚未完全达到要求，有时可能会产生物理上看似合理但实际上不可行的轨迹。
*   <strong>精细操纵细节：</strong> 对于需要极高精度的精细操纵任务，生成的轨迹可能缺乏足够的细节来支持机器人执行精确的操纵动作。
*   <strong>数据质量：</strong> 部分源视频包含效率低下或纠正性的动作，这可能引入次优的监督信号。尽管进行了额外的过滤，但仍可能存在一些噪声数据。
*   <strong>插值方法：</strong> 论文采用了线性插值方法，但尚未探索其他插值调度或机制来明确控制模糊任务的轨迹模式。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>改进插值方法：</strong> 探索更高级的插值调度或机制，以更好地控制轨迹生成，尤其是在处理模糊任务时。
*   <strong>提升精细操纵能力：</strong> 研究如何生成更精细、更具可执行性的轨迹，以支持更复杂的操纵任务。
*   <strong>扩展到更广泛的机器人类型：</strong> 测试TraceGen的抽象能力是否能扩展到与人类手臂截然不同的机器人类型。
*   <strong>更大规模的数据集和过滤：</strong> 结合互联网规模的演示数据集，并开发更先进的数据过滤机制，以进一步提高模型的泛化能力和鲁棒性。
*   <strong>结合隐式世界模型：</strong> 将隐式世界模型的思想融入TraceGen的编码器中，以进一步增强预训练效果。</p>
<p>总而言之，这篇论文提出了一种创新的方法，通过引入3D trace-space和TraceForge数据引擎，显著提高了机器人从跨实体视频中学习操纵技能的能力，尤其是在数据稀缺的情况下，并实现了高效的推理和迁移。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Learning new robot tasks on new platforms and in new scenes from only a handful of demonstrations remains challenging.</li>
<li>We present TraceGen, a world model that predicts future motion in trace-space rather than pixel space, abstracting away appearance while retaining the geometric structure needed for manipulation.</li>
<li>To train TraceGen at scale, we develop TraceForge, a data pipeline that transforms heterogeneous human and robot videos into consistent 3D traces, yielding a corpus of 123K videos and 1.8M observation-trace-language triplets.</li>
<li>Pretraining on this corpus produces a transferable 3D motion prior that adapts efficiently: with just five target robot videos, TraceGen attains 80% success across four tasks while offering 50-600x faster inference than state-of-the-art video-based world models.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.21690v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.21690v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.21688v1'></a></p>
<h2 id="g2vlm-geometry-grounded-vision-language-model-with-unified-3d-reconstruction-and-spatial-reasoning"><a href="https://arxiv.org/abs/2511.21688v1">G<script type="math/tex">^2</script>VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning</a></h2>
<p><strong>Authors:</strong> Wenbo Hu, Jingli Lin, Yilin Long, Yunlong Ran, Lihan Jiang, Yifan Wang, Chenming Zhu, Runsen Xu, Tai Wang, Jiangmiao Pang</p>
<p><strong>Published:</strong> 2025-11-26</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.CL</p>
<p><strong>Abstract:</strong></p>
<p>Vision-Language Models (VLMs) still lack robustness in spatial intelligence, demonstrating poor performance on spatial understanding and reasoning tasks. We attribute this gap to the absence of a visual geometry learning process capable of reconstructing 3D space from 2D images. We present G<script type="math/tex">^2</script>VLM, a geometry grounded vision-language model that bridges two fundamental aspects of spatial intelligence: spatial 3D reconstruction and spatial understanding. G<script type="math/tex">^2</script>VLM natively leverages learned 3D visual geometry features to directly predict 3D attributes and enhance spatial reasoning tasks via in-context learning and interleaved reasoning. Our unified design is highly scalable for spatial understanding: it trains on abundant multi-view image and video data, while simultaneously leveraging the benefits of 3D visual priors that are typically only derived from hard-to-collect annotations. Experimental results demonstrate G<script type="math/tex">^2</script>VLM is proficient in both tasks, achieving comparable results to state-of-the-art feed-forward 3D reconstruction models and achieving better or competitive results across spatial understanding and reasoning tasks. By unifying a semantically strong VLM with low-level 3D vision tasks, we hope G<script type="math/tex">^2</script>VLM can serve as a strong baseline for the community and unlock more future applications, such as 3D scene editing.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“G<script type="math/tex">^2</script>VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning”的全面摘要，其中包含了您要求的各个方面：</p>
<p><strong>论文摘要：G<script type="math/tex">^2</script>VLM：统一 3D 重建与空间推理的几何基础视觉语言模型</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
该论文旨在解决当前视觉语言模型（VLMs）在空间智能方面存在的鲁棒性不足问题。现有 VLMs 在空间理解和推理任务上表现不佳，作者认为这主要是由于缺乏一个能够从 2D 图像重建 3D 空间能力的视觉几何学习过程。</p>
<p><strong>2. 关键创新与方法贡献：</strong>
*   <strong>G<script type="math/tex">^2</script>VLM 模型：</strong> 论文提出了 G<script type="math/tex">^2</script>VLM，一个将 3D 重建和空间理解这两个空间智能的关键方面统一起来的几何基础视觉语言模型。
*   <strong>双专家架构（Two-Streams Hypothesis）：</strong> 模型借鉴了人类认知的“双流假说”，采用了一个“几何感知专家”（where pathway）用于视觉几何学习，以及一个“语义感知专家”（what pathway）用于多模态理解。这两个专家通过共享的自注意力机制进行交互，相互促进。
*   <strong>利用 3D 先验：</strong> G<script type="math/tex">^2</script>VLM 原生利用学习到的 3D 视觉几何特征，通过上下文学习（in-context learning）和交错推理（interleaved reasoning）来直接预测 3D 属性并增强空间推理能力。
*   <strong>可扩展性与数据效率：</strong> 该模型设计具有高度可扩展性，能够利用海量的多视图图像和视频数据进行训练，同时也能受益于通常难以获取的 3D 先验信息（如深度图和相机位姿），而无需依赖大量标注的 3D 数据。
*   <strong>统一推理框架：</strong> 模型能够直接从纯 2D 图像输入中推理 3D 几何信息，并将其整合到空间推理任务中。</p>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>视觉几何任务：</strong> G<script type="math/tex">^2</script>VLM 在单目深度估计、点云估计和相机位姿估计等视觉几何任务上取得了与最先进的（SOTA）前馈 3D 重建模型相当的性能，例如在 Sintel 基准上将单目深度估计的绝对相对误差从 0.335 降低到 0.297。
*   <strong>空间理解与推理任务：</strong> 在空间理解和推理任务上，G<script type="math/tex">^2</script>VLM 取得了显著的成果。在 SPAR-Bench 基准上，G<script type="math/tex">^2</script>VLM-SR（专门针对空间推理优化的版本）取得了最佳结果，超越了 GPT-4o 18.5 个百分点。在所有四个空间推理基准上，G<script type="math/tex">^2</script>VLM 的性能都优于或媲美更大规模的模型，尽管其自身规模相对较小（2B）。
*   <strong>几何与语义的协同作用：</strong> 实验证明，几何感知专家性能的提升能够带来空间推理能力的更大改进，证实了几何与语义表示之间的积极协同作用。
*   <strong>作为基线模型：</strong> 论文希望 G<script type="math/tex">^2</script>VLM 能够作为一个强大的基线模型，为社区提供一个统一的框架，以解锁更多未来在语义空间任务上的应用，例如 3D 场景编辑。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>训练稳定性：</strong> 论文提到，对于大规模模型，训练稳定性可能是一个挑战，这需要先进的优化技术、细致的数据整理和大量的计算资源。
*   <strong>数据需求（G<script type="math/tex">^2</script>VLM-SR）：</strong> 虽然 G<script type="math/tex">^2</script>VLM 本身具有数据效率，但其专门为空间推理优化的版本 G<script type="math/tex">^2</script>VLM-SR 在联合训练阶段需要大规模的 3D 标注数据集，这限制了其可扩展性。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>模型规模扩展：</strong> 论文暗示了未来可以探索 G<script type="math/tex">^2</script>VLM 的规模扩展，以期获得更强的性能，尤其是在需要存储大量知识的在线时空场景理解任务上。
*   <strong>3D 场景编辑等应用：</strong> G<script type="math/tex">^2</script>VLM 的统一框架为解锁更多高层级的 3D 应用（如 3D 场景编辑）提供了基础。
*   <strong>优化技术：</strong> 针对大规模模型训练不稳定的问题，可能需要进一步研究更先进的优化技术。</p>
<p><strong>总结：</strong>
G<script type="math/tex">^2</script>VLM 是一个开创性的工作，它成功地将 3D 几何重建和高层级的空间理解与推理能力整合到一个统一的视觉语言模型中。通过借鉴人类的“双流假说”并采用创新的双专家架构，G<script type="math/tex">^2</script>VLM 能够有效地学习和利用 3D 几何信息，显著提升了模型在空间智能任务上的表现。其在视觉几何和空间推理任务上的出色性能，以及对数据的有效利用，使其成为该领域一个重要的基线模型，并为未来的研究和应用开辟了新的可能性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present G<script type="math/tex">^2</script>VLM, a geometry grounded vision-language model that bridges two fundamental aspects of spatial intelligence: spatial 3D reconstruction and spatial understanding.</li>
<li>Experimental results demonstrate G<script type="math/tex">^2</script>VLM is proficient in both tasks, achieving comparable results to state-of-the-art feed-forward 3D reconstruction models and achieving better or competitive results across spatial understanding and reasoning tasks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.21688v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.21688v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.21681v1'></a></p>
<h2 id="seeing-without-pixels-perception-from-camera-trajectories"><a href="https://arxiv.org/abs/2511.21681v1">Seeing without Pixels: Perception from Camera Trajectories</a></h2>
<p><strong>Authors:</strong> Zihui Xue, Kristen Grauman, Dima Damen, Andrew Zisserman, Tengda Han</p>
<p><strong>Published:</strong> 2025-11-26</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Can one perceive a video's content without seeing its pixels, just from the camera trajectory-the path it carves through space? This paper is the first to systematically investigate this seemingly implausible question. Towards this end, we propose a contrastive learning framework to train CamFormer, a dedicated encoder that projects camera pose trajectories into a joint embedding space, aligning them with natural language. We find that, contrary to its apparent simplicity, the camera trajectory is a remarkably informative signal to uncover video content. In other words, "how you move" can indeed reveal "what you are doing" (egocentric) or "observing" (exocentric). We demonstrate the versatility of our learned CamFormer embeddings on a diverse suite of downstream tasks, ranging from cross-modal alignment to classification and temporal analysis. Importantly, our representations are robust across diverse camera pose estimation methods, including both high-fidelity multi-sensored and standard RGB-only estimators. Our findings establish camera trajectory as a lightweight, robust, and versatile modality for perceiving video content.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：Seeing without Pixels: Perception from Camera Trajectories</strong></p>
<p><strong>1. 论文的主要贡献 (2-3句话的简洁总结)</strong></p>
<p>这篇论文首次系统性地探索了仅凭相机轨迹（相机在空间中的运动路径）来感知视频内容的可行性。作者提出了一个名为 CamFormer 的对比学习框架，能够将相机姿态轨迹映射到一个联合嵌入空间，并与自然语言对齐。研究表明，相机轨迹本身是一个极具信息量的信号，能够揭示视频内容，为“如何移动”揭示“在做什么”或“在观察什么”提供了新的视角。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<ul>
<li><strong>核心创新：</strong> 核心创新在于证明了相机轨迹作为一种“无像素”的感知信号，能够承载丰富的视频内容信息。这颠覆了传统计算机视觉依赖像素信息进行分析的范式。</li>
<li><strong>方法论：</strong><ul>
<li><strong>对比学习框架：</strong> 作者提出了一个专门的对比学习框架来训练 CamFormer 模型。这种方法旨在学习一个能够将相机轨迹映射到语义丰富的嵌入空间的编码器。</li>
<li><strong>CamFormer 编码器：</strong> CamFormer 是一个专门设计的模型，用于处理和理解相机姿态轨迹。它将原始的相机轨迹数据转化为一种可用于下游任务的表示。</li>
<li><strong>联合嵌入空间与自然语言对齐：</strong> 将相机轨迹嵌入与自然语言（例如视频描述）对齐是关键。这意味着模型学习到的相机运动模式可以被解释为与特定的视频内容相关联。</li>
<li><strong>跨模态对齐：</strong> 这种对齐本身就是一种重要的跨模态学习任务，展示了相机轨迹的语义潜力。</li>
</ul>
</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<ul>
<li><strong>开辟新的感知范式：</strong> 这项研究可能开辟一个全新的计算机视觉研究方向，即“无像素感知”。它挑战了我们对视觉信息来源的传统认知，并可能催生一系列新的算法和应用。</li>
<li><strong>降低数据和计算成本：</strong> 相机轨迹数据通常比原始像素数据更轻量级，存储和处理成本更低。如果能够有效利用轨迹信息，将显著降低视频分析的门槛。</li>
<li><strong>增强鲁棒性：</strong> 论文强调了其表示对不同相机姿态估计方法的鲁棒性。这意味着即使在相机姿态估计不完美的情况下，模型也能保持较好的性能，这在实际应用中非常重要。</li>
<li><strong>新的特征表示：</strong> CamFormer 学习到的嵌入可以作为一种新的、通用的视频特征表示，可以用于各种下游任务，而无需从头开始训练。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>机器人导航与感知：</strong> 机器人可以通过分析其自身的运动轨迹来推断周围环境的特征，而无需依赖高分辨率的视觉输入。例如，在低光照、雾霾或数据传输受限的环境中，轨迹信息可能成为关键。</li>
<li><strong>增强现实 (AR) 和虚拟现实 (VR)：</strong> 理解用户在虚拟或增强现实环境中的移动方式，可以帮助系统预测用户的意图，并提供更自然的交互体验。</li>
<li><strong>自动驾驶：</strong> 车辆的运动轨迹本身就包含了大量关于道路状况、交通流量和周围车辆行为的信息。结合轨迹信息可以提升自动驾驶系统的感知能力。</li>
<li><strong>视频检索与推荐：</strong> 基于相机运动模式的视频检索，可以发现与用户观看习惯或兴趣相似的视频，即使视频内容本身不完全相同。</li>
<li><strong>体育分析：</strong> 分析运动员或球的运动轨迹，可以揭示战术意图、技术动作等，而无需逐帧分析视频画面。</li>
<li><strong>医疗诊断：</strong> 例如，分析手术器械的运动轨迹，可以帮助评估手术的精细程度或识别潜在的风险。</li>
<li><strong>内容创作：</strong> 艺术家或创作者可以利用相机轨迹来生成具有特定风格或情感的视频内容。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<ul>
<li><strong>对相机姿态估计的依赖：</strong> 虽然论文声称表示对姿态估计方法具有鲁棒性，但其性能的上限仍然可能受到相机姿态估计精度的影响。如果姿态估计本身存在严重偏差，那么基于轨迹的感知也会受到限制。</li>
<li><strong>“无像素”的定义：</strong> 摘要中“Seeing without Pixels”可能是一种比喻，实际训练过程中可能仍然需要某种形式的视觉数据来生成或验证相机轨迹与内容之间的对应关系（例如，通过视频本身来生成轨迹和对应的文本描述）。完全脱离任何视觉信息进行感知可能仍然是一个挑战。</li>
<li><strong>特定场景的有效性：</strong> 摘要提到“diverse suite of downstream tasks”，但并未具体说明这些任务的领域和复杂性。在某些高度依赖精细视觉细节的场景下，纯粹的相机轨迹信息可能不足以进行准确感知。</li>
<li><strong>解释性：</strong> 虽然模型能够将轨迹与内容对齐，但其内部机制如何具体地将特定的运动模式映射到特定的语义信息，可能需要进一步的深入研究来解释。</li>
<li><strong>数据需求：</strong> 训练一个强大的对比学习模型通常需要大量的、多样化的数据。论文中可能隐含了对大规模、标注好的相机轨迹数据集的需求。</li>
</ul>
<p>总而言之，这篇论文的贡献在于提出了一个新颖的视角和方法，将计算机视觉的感知能力从像素扩展到相机运动本身。如果研究成果能够得到充分验证和推广，它将对计算机视觉领域产生深远的影响，并催生一系列创新的应用。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Towards this end, we propose a contrastive learning framework to train CamFormer, a dedicated encoder that projects camera pose trajectories into a joint embedding space, aligning them with natural language.</li>
<li>We demonstrate the versatility of our learned CamFormer embeddings on a diverse suite of downstream tasks, ranging from cross-modal alignment to classification and temporal analysis.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.21681v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.21681v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.21666v1'></a></p>
<h2 id="uncertainty-quantification-for-visual-object-pose-estimation"><a href="https://arxiv.org/abs/2511.21666v1">Uncertainty Quantification for Visual Object Pose Estimation</a></h2>
<p><strong>Authors:</strong> Lorenzo Shaikewitz, Charis Georgiou, Luca Carlone</p>
<p><strong>Published:</strong> 2025-11-26</p>
<p><strong>Categories:</strong> cs.RO, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Quantifying the uncertainty of an object's pose estimate is essential for robust control and planning. Although pose estimation is a well-studied robotics problem, attaching statistically rigorous uncertainty is not well understood without strict distributional assumptions. We develop distribution-free pose uncertainty bounds about a given pose estimate in the monocular setting. Our pose uncertainty only requires high probability noise bounds on pixel detections of 2D semantic keypoints on a known object. This noise model induces an implicit, non-convex set of pose uncertainty constraints. Our key contribution is SLUE (S-Lemma Uncertainty Estimation), a convex program to reduce this set to a single ellipsoidal uncertainty bound that is guaranteed to contain the true object pose with high probability. SLUE solves a relaxation of the minimum volume bounding ellipsoid problem inspired by the celebrated S-lemma. It requires no initial guess of the bound's shape or size and is guaranteed to contain the true object pose with high probability. For tighter uncertainty bounds at the same confidence, we extend SLUE to a sum-of-squares relaxation hierarchy which is guaranteed to converge to the minimum volume ellipsoidal uncertainty bound for a given set of keypoint constraints. We show this pose uncertainty bound can easily be projected to independent translation and axis-angle orientation bounds. We evaluate SLUE on two pose estimation datasets and a real-world drone tracking scenario. Compared to prior work, SLUE generates substantially smaller translation bounds and competitive orientation bounds. We release code at https://github.com/MIT-SPARK/PoseUncertaintySets.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于视觉对象姿态估计不确定性量化的论文的全面中文摘要：</p>
<p><strong>论文题目：</strong> Uncertainty Quantification for Visual Object Pose Estimation (视觉对象姿态估计的不确定性量化)</p>
<p><strong>作者：</strong> Lorenzo Shaikewitz, Charis Georgiou, Luca Carlone</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题：</strong>
该论文旨在解决在单目视觉设置下，为对象姿态估计提供统计上严格且无分布假设的不确定性量化问题。现有的姿态估计方法通常缺乏对估计结果的不确定性进行可靠量化，这对于机器人控制和规划至关重要。现有方法要么依赖于强烈的分布假设，要么提供的是启发式的不确定性度量，缺乏统计学保证。</p>
<p><strong>2. 主要创新与方法贡献：</strong>
该论文的核心贡献是提出了一种名为 <strong>SLUE (S-Lemma Uncertainty Estimation)</strong> 的方法，用于生成具有统计学保证的、分布无关的对象姿态不确定性边界。其关键创新点包括：</p>
<ul>
<li><strong>基于关键点不确定性的姿态约束：</strong> SLUE 从对象 2D 语义关键点的像素检测误差的概率性有界噪声模型出发，推导出隐式的、非凸的姿态不确定性约束集。这种方法仅需关键点检测的高概率噪声界限，对姿态估计器本身是无关的。</li>
<li><strong>S-Lemma 松弛与凸优化：</strong> SLUE 利用 S-Lemma 的推广，将非凸的姿态约束集转化为一个凸优化问题，求解最小体积包围椭球体，该椭球体以高概率包含真实的对象姿态。与以往工作不同，SLUE 直接优化椭球体的形状，而非固定其形状（如球形），从而获得更具表达力的不确定性边界。</li>
<li><strong>松弛层级结构：</strong> 为了获得更紧致的不确定性边界，SLUE 扩展为一种基于和平方（sum-of-squares）松弛的层级结构，该结构保证收敛到最小体积椭球体不确定性边界。</li>
<li><strong>可解释的边界投影：</strong> SLUE 生成的联合姿态不确定性边界可以方便地投影为独立的平移和轴角方向不确定性边界，便于理解和使用。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
通过在两个公开数据集（LM-O 和 YCB-V）以及一个真实世界的无人机跟踪场景（CAST）上的实验，SLUE 取得了显著的成果：</p>
<ul>
<li><strong>更紧致的平移边界：</strong> SLUE 生成的平移不确定性边界比现有方法（如 RANSAG 和 GRCC）小几个数量级，表明其能够更精确地捕捉平移不确定性。</li>
<li><strong>有竞争力的方向边界：</strong> SLUE 的方向不确定性边界也具有竞争力，尤其是在第一阶松弛时表现优异。</li>
<li><strong>统计学保证：</strong> SLUE 提供的是具有统计学保证的边界，这意味着在给定的置信度下，真实姿态有高概率落入该边界内。</li>
<li><strong>计算效率：</strong> SLUE 在计算效率方面表现出色，尤其是在第一阶松弛时，其速度甚至快于关键点检测本身。使用四元数表示进一步提高了计算效率。</li>
<li><strong>意义：</strong> SLUE 为机器人领域提供了一种强大且通用的工具，能够为任何姿态估计器添加统计学上严格的不确定性量化，从而提高系统的鲁棒性和可靠性。</li>
</ul>
<p><strong>4. 提及的局限性：</strong>
*   <strong>关键点不确定性利用不足：</strong> 论文提到，SLUE 没有充分利用关键点不确定性边界的“高概率”性质，未来工作可以考虑这一点，以应对某些关键点超出其不确定性集的情况，这可能与最大似然估计中的离群点鲁棒估计相关。
*   <strong>对象对称性处理：</strong> SLUE 目前不直接处理对象对称性引起的不确定性。
*   <strong>计算成本：</strong> 虽然 SLUE 效率很高，但更高阶的松弛会显著增加计算成本。</p>
<p><strong>5. 未来研究方向：</strong>
*   <strong>利用关键点不确定性的高概率性质：</strong> 探索如何利用关键点不确定性边界的“高概率”性质来改进姿态估计和不确定性量化，可能涉及离群点鲁棒估计。
*   <strong>处理对象对称性：</strong> 开发能够捕捉对象对称性引起的不确定性的方法。
*   <strong>联合估计中心与形状：</strong> 在计算成本允许的情况下，探索联合估计不确定性边界的中心和形状。
*   <strong>序列帧信息融合：</strong> 在跟踪场景中，利用连续帧之间的信息来融合或改进不确定性估计。
*   <strong>更高级的松弛：</strong> 探索更高阶的松弛以获得更精确的边界，并权衡计算成本。</p>
<p>总而言之，这篇论文提出了一种新颖的、基于 S-Lemma 松弛的 SLUE 方法，为视觉对象姿态估计提供了统计学上严格且分布无关的不确定性量化。其核心优势在于能够生成更紧致、更具表达力的不确定性边界，同时保持了计算效率和易于解释的特性，为机器人感知和决策提供了重要的理论和实践基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We develop distribution-free pose uncertainty bounds about a given pose estimate in the monocular setting.</li>
<li>Our key contribution is SLUE (S-Lemma Uncertainty Estimation), a convex program to reduce this set to a single ellipsoidal uncertainty bound that is guaranteed to contain the true object pose with high probability.</li>
<li>We show this pose uncertainty bound can easily be projected to independent translation and axis-angle orientation bounds.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.21666v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.21666v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.21662v1'></a></p>
<h2 id="multi-crit-benchmarking-multimodal-judges-on-pluralistic-criteria-following"><a href="https://arxiv.org/abs/2511.21662v1">Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following</a></h2>
<p><strong>Authors:</strong> Tianyi Xiong, Yi Ge, Ming Li, Zuolong Zhang, Pranav Kulkarni, Kaishen Wang, Qi He, Zeying Zhu, Chenxi Liu, Ruibo Chen, Tong Zheng, Yanshuo Chen, Xiyao Wang, Renrui Zhang, Wenhu Chen, Heng Huang</p>
<p><strong>Published:</strong> 2025-11-26</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Large multimodal models (LMMs) are increasingly adopted as judges in multimodal evaluation systems due to their strong instruction following and consistency with human preferences. However, their ability to follow diverse, fine-grained evaluation criteria remains underexplored. We develop Multi-Crit, a benchmark for evaluating multimodal judges on their capacity to follow pluralistic criteria and produce reliable criterion-level judgments. Covering both open-ended generation and verifiable reasoning tasks, Multi-Crit is built through a rigorous data curation pipeline that gathers challenging response pairs with multi-criterion human annotations. It further introduces three novel metrics for systematically assessing pluralistic adherence, criterion-switching flexibility, and the ability to recognize criterion-level preference conflicts. Comprehensive analysis of 25 LMMs reveals that 1) proprietary models still struggle to maintain consistent adherence to pluralistic criteria--especially in open-ended evaluation; 2) open-source models lag further behind in flexibly following diverse criteria; and 3) critic fine-tuning with holistic judgment signals enhances visual grounding but fails to generalize to pluralistic criterion-level judgment. Additional analyses on reasoning fine-tuning, test-time scaling, and boundary consistency between open-source and proprietary models further probe the limits of current multimodal judges. As a pioneering study, Multi-Crit lays the foundation for building reliable and steerable multimodal AI evaluation.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于评估大型多模态模型（LMM）作为多模态评估裁判的文章的全面中文摘要，重点关注其在遵循多元化标准方面的能力。</p>
<p><strong>论文题目：</strong> Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following</p>
<p><strong>作者：</strong> Tianyi Xiong, Yi Ge, Ming Li, Zuolong Zhang, Pranav Kulkarni, Kaishen Wang, Qi He, Zeying Zhu, Chenxi Liu, Ruibo Chen, Tong Zheng, Yanshuo Chen, Xiyao Wang, Renrui Zhang, Wenhu Chen, Heng Huang</p>
<hr />
<p><strong>全面摘要</strong></p>
<p><strong>1. 主要问题或研究问题：</strong></p>
<p>大型多模态模型（LMM）在多模态评估系统中作为裁判（LMM-as-a-Judge）的应用日益广泛，这得益于它们强大的指令遵循能力和与人类偏好的一致性。然而，它们在遵循<strong>多样化、细粒度的评估标准</strong>方面的能力仍未得到充分探索。现有的大多数多模态评估基准仅提供单一的整体偏好标签，忽略了模型在遵循<strong>标准层面</strong>的判断能力，以及在不同标准之间可能存在的<strong>权衡和冲突</strong>。因此，本文的核心研究问题是：<strong>LMM裁判能否有效地遵循多元化的评估标准，并产生可靠的、符合人类偏好的标准级判断？</strong></p>
<p><strong>2. 关键创新或方法论贡献：</strong></p>
<p>为了解决上述问题，本文提出了 <strong>Multi-Crit</strong>，一个专门用于评估多模态裁判在遵循多元化标准方面的能力的基准。其主要贡献包括：</p>
<ul>
<li><strong>Multi-Crit 基准的构建：</strong><ul>
<li><strong>数据收集与标注：</strong> 通过严格的数据策划流程，收集了包含<strong>开放式生成任务</strong>和<strong>可验证推理任务</strong>的多元化多模态提示。针对这些提示，收集了具有<strong>多标准人类偏好标注</strong>的挑战性响应对。这些标注不仅包含了每个标准的偏好，还揭示了标准之间的冲突。</li>
<li><strong>响应对生成与筛选：</strong> 生成了跨模型和模型内部的响应对，并采用多阶段筛选机制，以保留具有细微标准级差异且能体现权衡和冲突的样本。</li>
</ul>
</li>
<li><strong>新的评估指标：</strong> 引入了三个新颖的指标来系统地评估LMM裁判的能力：<ul>
<li><strong>多元准确率 (Pluralistic Accuracy, PAcc)：</strong> 衡量裁判在所有标准上都做出正确判断的比例。</li>
<li><strong>权衡敏感度 (Trade-off Sensitivity, TOS)：</strong> 衡量裁判识别响应对在不同标准之间权衡的能力。</li>
<li><strong>冲突匹配率 (Conflict Matching Rate, CMR)：</strong> 衡量裁判正确解决标准级偏好冲突的能力。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<p>对 25 个 LMM 的全面分析揭示了当前多模态裁判在遵循多元化标准方面存在的显著挑战：</p>
<ul>
<li><strong>专有模型仍面临挑战：</strong> 最先进的专有模型（如 o4-mini, Claude-3.7-Sonnet）在保持对多元化标准的<strong>一致遵循</strong>方面仍有困难，尤其是在开放式评估任务中。</li>
<li><strong>开源模型表现落后：</strong> 开源模型在灵活遵循多样化标准方面，以及识别权衡和冲突的能力上，<strong>显著落后于专有模型</strong>。</li>
<li><strong>批评微调的局限性：</strong> 使用整体判断信号进行的批评微调（critic fine-tuning）虽然能提升视觉基础（visual grounding）能力，但<strong>未能泛化到多元化、标准驱动的评估</strong>。</li>
<li><strong>其他发现：</strong><ul>
<li>推理微调（reasoning fine-tuning）未能显著提升推理判断能力，反而削弱了模型识别权衡的能力。</li>
<li>测试时缩放（test-time scaling）对模型的影响有限且不稳定。</li>
<li>专有模型的标准级准确率上限与人类标注者的一致性高度相关，而开源模型则不然。</li>
</ul>
</li>
</ul>
<p><strong>意义：</strong> Multi-Crit 基准的提出填补了现有评估体系的空白，为深入理解和改进 LMM 裁判的能力提供了重要工具。研究结果表明，当前 LMM 裁判在处理复杂、多维度的评估场景时仍存在显著局限性，为未来研究指明了方向。</p>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>模型能力差异：</strong> 尽管基准涵盖了多种 LMM，但模型能力的差异可能影响结果的普适性。</li>
<li><strong>数据规模：</strong> 虽然数据收集过程严谨，但与海量数据相比，仍有进一步扩充的空间。</li>
<li><strong>人类标注的潜在主观性：</strong> 尽管努力保证了标注者的一致性，但人类判断本身仍可能存在一定程度的主观性。</li>
<li><strong>评估指标的局限性：</strong> PAcc、TOS、CMR 是对多元化判断能力的有效衡量，但可能无法完全捕捉所有细微之处。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>开发更强的多元化标准遵循能力：</strong> 研究新的训练范式和模型架构，以提升 LMM 在理解和遵循复杂、冲突标准方面的能力。</li>
<li><strong>改进微调策略：</strong> 探索更有效的微调方法，使其能够泛化到多元化、标准驱动的评估任务，而不仅仅是提升特定方面的能力。</li>
<li><strong>更精细化的评估指标：</strong> 开发更具区分度的指标，以更全面地衡量 LMM 在处理标准权衡和冲突时的细微表现。</li>
<li><strong>探索人类与 LMM 裁判的协同：</strong> 研究如何更好地结合人类的判断和 LMM 的效率，构建更可靠、可信赖的多模态评估系统。</li>
<li><strong>扩展到更多模态和任务：</strong> 将 Multi-Crit 的理念和方法扩展到更广泛的模态（如音频、视频）和更复杂的任务中。</li>
</ul>
<p><strong>总结：</strong></p>
<p>Multi-Crit 是一个开创性的工作，它通过构建一个包含多标准人类偏好标注的基准，并引入新的评估指标，系统地揭示了当前 LMM 裁判在遵循多元化评估标准方面的不足。研究结果表明，尽管 LMM 在多模态评估中展现出潜力，但在处理细粒度、冲突的标准时仍需大量改进。Multi-Crit 为未来开发更可靠、可控的多模态人工智能评估系统奠定了基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We develop Multi-Crit, a benchmark for evaluating multimodal judges on their capacity to follow pluralistic criteria and produce reliable criterion-level judgments.</li>
<li>It further introduces three novel metrics for systematically assessing pluralistic adherence, criterion-switching flexibility, and the ability to recognize criterion-level preference conflicts.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.21662v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.21662v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.21652v1'></a></p>
<h2 id="continual-error-correction-on-low-resource-devices"><a href="https://arxiv.org/abs/2511.21652v1">Continual Error Correction on Low-Resource Devices</a></h2>
<p><strong>Authors:</strong> Kirill Paramonov, Mete Ozay, Aristeidis Mystakidis, Nikolaos Tsalikidis, Dimitrios Sotos, Anastasios Drosou, Dimitrios Tzovaras, Hyunjun Kim, Kiseok Chang, Sangdok Mo, Namwoong Kim, Woojong Yoo, Jijoong Moon, Umberto Michieli</p>
<p><strong>Published:</strong> 2025-11-26</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>The proliferation of AI models in everyday devices has highlighted a critical challenge: prediction errors that degrade user experience. While existing solutions focus on error detection, they rarely provide efficient correction mechanisms, especially for resource-constrained devices. We present a novel system enabling users to correct AI misclassifications through few-shot learning, requiring minimal computational resources and storage. Our approach combines server-side foundation model training with on-device prototype-based classification, enabling efficient error correction through prototype updates rather than model retraining. The system consists of two key components: (1) a server-side pipeline that leverages knowledge distillation to transfer robust feature representations from foundation models to device-compatible architectures, and (2) a device-side mechanism that enables ultra-efficient error correction through prototype adaptation. We demonstrate our system's effectiveness on both image classification and object detection tasks, achieving over 50% error correction in one-shot scenarios on Food-101 and Flowers-102 datasets while maintaining minimal forgetting (less than 0.02%) and negligible computational overhead. Our implementation, validated through an Android demonstration app, proves the system's practicality in real-world scenarios.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Continual Error Correction on Low-Resource Devices”的全面中文摘要，其中包含了您要求的各个部分：</p>
<p><strong>论文题目：</strong> Continual Error Correction on Low-Resource Devices
<strong>作者：</strong> Kirill Paramonov, Mete Ozay, Aristeidis Mystakidis, Nikolaos Tsalikidis, Dimitrios Sotos, Anastasios Drosou, Dimitrios Tzovaras, Hyunjun Kim, Kiseok Chang, Sangdok Mo, Namwoong Kim, Woojong Yoo, Jijoong Moon, Umberto Michieli</p>
<hr />
<p><strong>摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
随着人工智能（AI）模型在日常设备中的普及，预测错误成为一个关键挑战，严重影响用户体验。现有解决方案大多侧重于错误检测，而对于资源受限的设备而言，提供高效的纠错机制尤为困难。论文旨在解决在计算和存储资源有限的设备上，如何实现AI模型的持续性错误纠正问题。</p>
<p><strong>2. 主要创新点/方法论贡献：</strong>
该论文提出了一种新颖的系统，使<strong>用户能够通过少量样本（few-shot learning）来纠正AI的误分类</strong>，同时对计算资源和存储空间的要求极低。其核心方法论包含两个关键组件：</p>
<ul>
<li><strong>服务器端流水线：</strong> 利用<strong>知识蒸馏（knowledge distillation）</strong>技术，将强大的基础模型（foundation model）的鲁棒特征表示迁移到设备兼容的轻量级模型架构中。这使得设备能够利用大型模型的强大能力，同时保持较低的部署成本。</li>
<li><strong>设备端机制：</strong> 采用<strong>基于原型的分类（prototype-based classification）</strong>方法，并结合<strong>高效的原型更新机制</strong>，实现超高效的错误纠正。这种方法通过更新原型（prototypes）而非重新训练模型，能够持续适应用户的纠错反馈，有效防止灾难性遗忘（catastrophic forgetting）。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
该系统在图像分类和目标检测任务上均展现了有效性。在Food-101和Flowers-102数据集上，<strong>单样本（one-shot）纠错场景下，错误纠正准确率超过50%</strong>。同时，该系统在保持模型性能的同时，<strong>遗忘率极低（小于0.02%）</strong>，并且计算开销微乎其微。通过Android应用程序的实际演示验证了该系统在真实世界场景中的实用性。</p>
<p>该研究的意义在于：
*   <strong>提升用户体验：</strong> 允许用户直接参与纠错，增强了AI系统的可靠性和用户信任度。
*   <strong>赋能低资源设备：</strong> 使得在智能手机、物联网设备等资源受限的设备上部署具备纠错能力的AI成为可能。
*   <strong>隐私保护：</strong> 错误纠正过程在设备本地进行，无需将用户数据上传至云端，保护了用户隐私。
*   <strong>高效性：</strong> 极低的计算和存储开销，以及无需网络连接即可工作的能力，使其非常适合边缘计算场景。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>检测与分类的计算成本：</strong> 在目标检测任务中，使用独立的检测和分类网络虽然能获得最高精度，但计算成本较高。
*   <strong>原型数量的权衡：</strong> 为了满足设备资源限制，需要选择合适的原型数量（K值），这可能需要在精度和资源占用之间进行权衡。
*   <strong>未来探索方向：</strong> 文章提到，将原型推理直接构建在检测特征之上，可以进一步优化计算效率，但这部分留待未来工作。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>更深度的优化：</strong> 探索将原型推理直接应用于目标检测的特征提取阶段，以进一步降低计算复杂度。
*   <strong>更广泛的应用：</strong> 将该系统扩展到更多类型的AI模型和应用场景。
*   <strong>自动化纠错：</strong> 研究如何进一步减少用户干预，例如通过更智能的错误检测和置信度评估来触发自动纠错。
*   <strong>多模态纠错：</strong> 探索在多模态AI系统中实现类似的持续性错误纠正机制。</p>
<hr />
<p>总而言之，这篇论文提出了一种创新的、面向低资源设备的AI错误纠正方法，通过服务器端知识蒸馏和设备端原型更新，实现了高效、低成本、用户友好的持续性错误纠正，为AI在边缘设备的广泛应用开辟了新的可能性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present a novel system enabling users to correct AI misclassifications through few-shot learning, requiring minimal computational resources and storage.</li>
<li>Our approach combines server-side foundation model training with on-device prototype-based classification, enabling efficient error correction through prototype updates rather than model retraining.</li>
<li>We demonstrate our system's effectiveness on both image classification and object detection tasks, achieving over 50% error correction in one-shot scenarios on Food-101 and Flowers-102 datasets while maintaining minimal forgetting (less than 0.02%) and negligible computational overhead.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.21652v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.21652v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.21635v1'></a></p>
<h2 id="mechanisms-of-non-monotonic-scaling-in-vision-transformers"><a href="https://arxiv.org/abs/2511.21635v1">Mechanisms of Non-Monotonic Scaling in Vision Transformers</a></h2>
<p><strong>Authors:</strong> Anantha Padmanaban Krishna Kumar</p>
<p><strong>Published:</strong> 2025-11-26</p>
<p><strong>Categories:</strong> cs.LG, cs.AI, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Deeper Vision Transformers often perform worse than shallower ones, which challenges common scaling assumptions. Through a systematic empirical analysis of ViT-S, ViT-B, and ViT-L on ImageNet, we identify a consistent three-phase Cliff-Plateau-Climb pattern that governs how representations evolve with depth. We observe that better performance is associated with progressive marginalization of the [CLS] token, originally designed as a global aggregation hub, in favor of distributed consensus among patch tokens. We quantify patterns of information mixing with an Information Scrambling Index, and show that in ViT-L the information-task tradeoff emerges roughly 10 layers later than in ViT-B, and that these additional layers correlate with increased information diffusion rather than improved task performance. Taken together, these results suggest that transformer architectures in this regime may benefit more from carefully calibrated depth that executes clean phase transitions than from simply increasing parameter count. The Information Scrambling Index provides a useful diagnostic for existing models and suggests a potential design target for future architectures. All code is available at: https://github.com/AnanthaPadmanaban-KrishnaKumar/Cliff-Plateau-Climb.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Mechanisms of Non-Monotonic Scaling in Vision Transformers”的全面中文摘要，重点关注其研究问题、创新点、主要发现、局限性和未来方向：</p>
<p><strong>论文题目：</strong> Mechanisms of Non-Monotonic Scaling in Vision Transformers (视觉Transformer中非单调缩放的机制)</p>
<p><strong>作者：</strong> Anantha Padmanaban Krishna Kumar</p>
<p><strong>摘要：</strong></p>
<p>这篇论文深入探讨了视觉Transformer（ViT）模型在深度增加时性能反而下降的“非单调缩放”现象，挑战了普遍认为“更深的模型更好”的缩放假设。研究人员通过对不同规模的ViT模型（ViT-S、ViT-B、ViT-L）在ImageNet数据集上的系统性实证分析，揭示了模型内部表示（representations）随深度的演化规律，并提出了新的分析工具来理解这种现象背后的计算机制。</p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<ul>
<li><strong>非单调缩放现象：</strong> 为什么更深的ViT模型（如ViT-L）在某些情况下性能不如更浅的模型（如ViT-B），这与传统的模型缩放假设相悖。</li>
<li><strong>理解深度与性能的关系：</strong> 视觉Transformer模型内部的信息处理和表示演化机制是什么，以及它们如何影响最终的性能和几何结构。</li>
<li><strong>计算效率与模型性能的权衡：</strong> 如何在增加模型深度和参数量的同时，避免性能下降，并实现更高效的计算。</li>
</ul>
<p><strong>2. 关键创新与方法贡献：</strong></p>
<ul>
<li><strong>Cliff-Plateau-Climb模式的识别：</strong> 首次系统性地识别出ViT模型表示随深度的演化过程中存在一个一致的“Cliff-Plateau-Climb”（悬崖-平台-攀升）三阶段模式。<ul>
<li><strong>Cliff (悬崖):</strong> 初始阶段，表示的相似度急剧下降，主要由位置编码（PE）驱动。</li>
<li><strong>Plateau (平台):</strong> 中间阶段，表示相似度保持在较低水平，是特征提取的关键“工作空间”，其持续时间随模型深度增加而延长。</li>
<li><strong>Climb (攀升):</strong> 最后阶段，表示相似度快速回升，与“神经崩溃”（Neural Collapse）现象相关联，最终形成分类器几何结构。</li>
</ul>
</li>
<li><strong>信息混乱指数 (Information Scrambling Index)：</strong> 提出一个新的量化指标，用于衡量跨Patch Token的信息混合程度。该指数通过比较“自注意力”（self-only）和“全注意力”（all-to-all）解码器在重构输入信息时的性能差异来计算。它揭示了模型在不同计算阶段（局部处理、受控共识、混沌扩散）的信息交互模式。</li>
<li><strong>注意力动力学分析：</strong> 利用图论方法分析注意力权重，量化了[CLS] Token的中心化程度（CLS Centrality, CCC）和全局混合速率（Attention Consensus Index, ACI），以理解信息如何从中心化的[CLS] Token向Patch Token的分布式共识转移。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>性能与[CLS] Token边缘化相关：</strong> 研究发现，更好的性能与[CLS] Token（最初设计为全局聚合中心）的逐步边缘化，以及Patch Token之间形成分布式共识的趋势密切相关。</li>
<li><strong>计算策略而非参数量是关键：</strong> ViT-L模型需要大约10层（约125%的额外深度）才能达到ViT-B在8层时所能达到的信息-任务权衡点。这表明，额外的深度并未带来同等比例的性能提升，反而可能导致信息扩散和计算效率的下降。</li>
<li><strong>信息混乱指数的诊断价值：</strong><ul>
<li><strong>ViT-S (通信崩溃):</strong> 信息混乱指数趋近于零或负值，表明全局信息混合效果不佳，导致局部计算，几何结构较差。</li>
<li><strong>ViT-B (受控共识):</strong> 信息混乱指数保持在较低但稳定的正值范围内（约0.004-0.009），表明存在有效的跨Patch交互，实现了高效的共识形成，并获得了最佳的几何质量。</li>
<li><strong>ViT-L (混沌扩散):</strong> 信息混乱指数随深度单调递增，达到较高值（约0.031），表明过度混合，尽管任务性能有所提升，但几何质量下降，计算效率低下。</li>
</ul>
</li>
<li><strong>对模型设计的启示：</strong> 模型的深度应经过精心校准，以实现清晰的阶段转换，而非简单地增加参数量。未来的Transformer架构应支持显式的分布式决策，并避免[CLS] Token成为瓶颈。信息混乱指数和[CLS] Token中心化指标可作为训练过程中的诊断工具，用于识别和避免低效的深度利用。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>特定设置下的观察：</strong> 研究主要基于在ImageNet-1k数据集上预训练的ViT-S/B/L模型。其发现的“最优”信息混乱范围和[CLS] Token边缘化模式可能依赖于特定的归一化和代理指标，理论上的完全解释仍待完善。</li>
<li><strong>冻结模型分析：</strong> 分析的是预训练的冻结模型，并未在训练过程中进行干预，因此结论是相关性而非因果性。</li>
<li><strong>模型和任务的普适性：</strong> 结论的普适性可能受限于所分析的模型架构、任务（ImageNet分类）和训练设置。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>跨架构、任务和模态的验证：</strong> 将这些发现推广到其他Transformer变体、不同的计算机视觉任务（如检测、分割）以及多模态学习场景。</li>
<li><strong>理论解释的深化：</strong> 对信息混乱指数和Cliff-Plateau-Climb模式进行更深入的理论推导和解释。</li>
<li><strong>主动干预与设计：</strong> 在模型训练过程中主动利用信息混乱指数和[CLS] Token中心化指标作为正则化项或早期预警信号，以指导模型设计和训练。</li>
<li><strong>探索更高效的深度利用策略：</strong> 设计能够实现平滑阶段转换、有效分布式共识和避免过度混合的新型Transformer架构。</li>
</ul>
<p>总而言之，这篇论文通过创新的分析工具和系统性的实证研究，深刻揭示了视觉Transformer模型在深度缩放中的非单调行为，强调了计算策略和信息流动的协调性对于模型性能的重要性，为未来设计更高效、更具可扩展性的Transformer架构提供了宝贵的见解和设计方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>The Information Scrambling Index provides a useful diagnostic for existing models and suggests a potential design target for future architectures.</li>
<li>All code is available at: https://github.com/AnanthaPadmanaban-KrishnaKumar/Cliff-Plateau-Climb.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.21635v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.21635v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.21592v1'></a></p>
<h2 id="mogan-improving-motion-quality-in-video-diffusion-via-few-step-motion-adversarial-post-training"><a href="https://arxiv.org/abs/2511.21592v1">MoGAN: Improving Motion Quality in Video Diffusion via Few-Step Motion Adversarial Post-Training</a></h2>
<p><strong>Authors:</strong> Haotian Xue, Qi Chen, Zhonghao Wang, Xun Huang, Eli Shechtman, Jinrong Xie, Yongxin Chen</p>
<p><strong>Published:</strong> 2025-11-26</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Video diffusion models achieve strong frame-level fidelity but still struggle with motion coherence, dynamics and realism, often producing jitter, ghosting, or implausible dynamics. A key limitation is that the standard denoising MSE objective provides no direct supervision on temporal consistency, allowing models to achieve low loss while still generating poor motion. We propose MoGAN, a motion-centric post-training framework that improves motion realism without reward models or human preference data. Built atop a 3-step distilled video diffusion model, we train a DiT-based optical-flow discriminator to differentiate real from generated motion, combined with a distribution-matching regularizer to preserve visual fidelity. With experiments on Wan2.1-T2V-1.3B, MoGAN substantially improves motion quality across benchmarks. On VBench, MoGAN boosts motion score by +7.3% over the 50-step teacher and +13.3% over the 3-step DMD model. On VideoJAM-Bench, MoGAN improves motion score by +7.4% over the teacher and +8.8% over DMD, while maintaining comparable or even better aesthetic and image-quality scores. A human study further confirms that MoGAN is preferred for motion quality (52% vs. 38% for the teacher; 56% vs. 29% for DMD). Overall, MoGAN delivers significantly more realistic motion without sacrificing visual fidelity or efficiency, offering a practical path toward fast, high-quality video generation. Project webpage is: https://xavihart.github.io/mogan.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于提升视频扩散模型运动质量的论文的全面摘要，其中包含了您要求的各个方面：</p>
<p><strong>论文题目：</strong> MoGAN: Improving Motion Quality in Video Diffusion via Few-Step Motion Adversarial Post-Training</p>
<p><strong>作者：</strong> Haotian Xue, Qi Chen, Zhonghao Wang, Xun Huang, Eli Shechtman, Jinrong Xie, Yongxin Chen</p>
<hr />
<p><strong>全面摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>视频扩散模型在生成高保真度的单帧图像方面取得了显著进展，但它们在模拟逼真的时间动态、运动连贯性和真实感方面仍然存在挑战，常常导致生成视频出现抖动、重影或不合乎物理规律的运动。论文指出，标准扩散模型的目标函数（如均方误差 MSE）主要关注像素级别的重构，缺乏对时间一致性和运动真实感的直接监督，这使得模型即使在低损失下也可能生成质量不佳的运动。</p>
<p><strong>2. 关键创新/方法贡献：</strong></p>
<p>为了解决上述问题，论文提出了 <strong>MoGAN (Motion-GAN Post-training)</strong>，一个以运动为中心的对抗性后训练框架，旨在无需奖励模型或人类偏好数据即可显著提升视频的运动真实感。其核心创新点包括：</p>
<ul>
<li><strong>基于光流的运动对抗性判别器：</strong> MoGAN 构建了一个基于 Diffusion Transformer (DiT) 的判别器，该判别器专门接收密集光流信息（而非像素或潜在空间信息），用于区分真实视频和生成视频的运动。通过冻结一个预训练的光流估计器（如 RAFT）来提取光流，并将其作为判别器的输入。</li>
<li><strong>与蒸馏模型的结合：</strong> MoGAN 构建在经过 3 步蒸馏的视频扩散模型之上。这种蒸馏模型能够生成相对清晰的中间帧，使得光流估计更加可靠。</li>
<li><strong>双重目标函数：</strong><ul>
<li><strong>MoGAN 对抗损失：</strong> 在光流空间中引入对抗性损失，直接训练判别器来区分真实和生成的运动，从而引导生成器学习更真实的运动模式。</li>
<li><strong>分布匹配蒸馏 (DMD) 正则化：</strong> 结合 DMD 损失，以保持生成模型与教师模型（通常是更高步数的模型）的分布一致性，从而保留视觉保真度和文本对齐能力。</li>
</ul>
</li>
<li><strong>稳定的训练策略：</strong> 论文采用了 R1/R2 正则化来稳定对抗训练，并结合了 DMD 正则化，以防止生成器在对抗训练中漂移过远。</li>
<li><strong>高效的推理：</strong> MoGAN 作为一个后训练框架，不改变原有的 3 步采样流程，因此能够保持高效的推理速度。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<p>论文在 Wan2.1-T2V-1.3B 模型上进行了大量实验，并在 VBench 和 VideoJAM-Bench 数据集上取得了显著成果：</p>
<ul>
<li><strong>运动质量大幅提升：</strong><ul>
<li>在 VBench 上，MoGAN 将运动得分比 50 步教师模型提高了 +7.3%，比 3 步 DMD 模型提高了 +13.3%。</li>
<li>在 VideoJAM-Bench 上，MoGAN 将运动得分比教师模型提高了 +7.4%，比 DMD 模型提高了 +8.8%。</li>
</ul>
</li>
<li><strong>保持视觉保真度：</strong> MoGAN 在保持或略微提升美学和图像质量得分的同时，显著提高了运动质量。</li>
<li><strong>人类评估的优势：</strong> 人类偏好研究表明，MoGAN 在运动质量方面受到用户的高度青睐（52% 对比教师模型的 38%，56% 对比 DMD 模型的 29%）。</li>
<li><strong>高效性：</strong> MoGAN 在不牺牲视觉保真度和效率的情况下，实现了更逼真的运动，为快速、高质量的视频生成提供了一条实用的途径。</li>
<li><strong>对比基线：</strong> MoGAN 在运动动态方面显著优于仅使用 DMD 的蒸馏模型，后者虽然提高了平滑度，但往往导致运动过于静态。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<p>论文中也提到了该方法的一些局限性：</p>
<ul>
<li><strong>依赖像素空间解码和冻结的光流估计器：</strong> 方法依赖于将潜在表示解码到像素空间，并使用一个固定的 2D 光流估计器。光流估计器的非物理性质可能无法完全捕捉遮挡、平面外运动或快速运动。</li>
<li><strong>光流的局限性：</strong> 光流在极小的运动或复杂的深度变化下可能变得不可靠。</li>
<li><strong>文本对齐的权衡：</strong> 作为 3 步蒸馏模型，其文本对齐能力可能略低于全步数的教师模型，但与 DMD 模型相当。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<p>基于上述局限性，论文也展望了未来的研究方向：</p>
<ul>
<li><strong>探索更先进的光流估计或运动表示：</strong> 研究更鲁棒、更物理一致的光流估计方法，或者探索 3D 运动表示。</li>
<li><strong>超越像素空间解码：</strong> 探索在潜在空间或更抽象的表示空间中进行对抗性训练，以减少对像素空间解码的依赖。</li>
<li><strong>结合几何信息或物理先验：</strong> 探索将几何信息或物理模拟先验融入运动生成过程，以进一步提升运动的真实感。</li>
<li><strong>更通用的运动判别器：</strong> 研究能够处理更复杂运动场景（如极小运动、复杂深度变化）的运动判别器。</li>
</ul>
<hr />
<p>总而言之，MoGAN 提出了一种新颖且有效的后训练方法，通过在光流空间引入对抗性学习，成功解决了视频扩散模型在运动质量方面的关键瓶颈，并在保持高效性和视觉保真度的前提下，显著提升了视频的运动真实感。这项工作为未来开发更逼真、更具动态性的视频生成模型奠定了基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose MoGAN, a motion-centric post-training framework that improves motion realism without reward models or human preference data.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.21592v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.21592v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-11-28 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
