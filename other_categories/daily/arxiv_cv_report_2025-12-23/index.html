<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-12-23 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-12-22/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-12-24/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-12-23">Arxiv Computer Vision Papers - 2025-12-23</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#the-prism-hypothesis-harmonizing-semantic-and-pixel-representations-via-unified-autoencoding" class="nav-link">The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding</a>
                </li>
                <li class="nav-item">
                    <a href="#pushing-the-frontier-of-audiovisual-perception-with-large-scale-multimodal-correspondence-learning" class="nav-link">Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning</a>
                </li>
                <li class="nav-item">
                    <a href="#visual-aware-cot-achieving-high-fidelity-visual-consistency-in-unified-models" class="nav-link">Visual-Aware CoT: Achieving High-Fidelity Visual Consistency in Unified Models</a>
                </li>
                <li class="nav-item">
                    <a href="#zero-shot-reconstruction-of-in-scene-object-manipulation-from-video" class="nav-link">Zero-shot Reconstruction of In-Scene Object Manipulation from Video</a>
                </li>
                <li class="nav-item">
                    <a href="#from-indoor-to-open-world-revealing-the-spatial-reasoning-gap-in-mllms" class="nav-link">From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs</a>
                </li>
                <li class="nav-item">
                    <a href="#va-variational-policy-alignment-for-pixel-aware-autoregressive-generation" class="nav-link">VA-π: Variational Policy Alignment for Pixel-Aware Autoregressive Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#worldwarp-propagating-3d-geometry-with-asynchronous-video-diffusion" class="nav-link">WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion</a>
                </li>
                <li class="nav-item">
                    <a href="#multimodal-llms-for-historical-dataset-construction-from-archival-image-scans-german-patents-1877-1918" class="nav-link">Multimodal LLMs for Historical Dataset Construction from Archival Image Scans: German Patents (1877-1918)</a>
                </li>
                <li class="nav-item">
                    <a href="#4d-gaussian-splatting-as-a-learned-dynamical-system" class="nav-link">4D Gaussian Splatting as a Learned Dynamical System</a>
                </li>
                <li class="nav-item">
                    <a href="#logoplanner-localization-grounded-navigation-policy-with-metric-aware-visual-geometry" class="nav-link">LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-12-23">Arxiv Computer Vision Papers - 2025-12-23</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，作为一名专注于计算机视觉和机器学习的研究助理，我将为您提供一份关于2025年12月22日 Arxiv 计算机视觉领域论文的简明执行摘要。</p>
<hr />
<p><strong>执行摘要：2025年12月22日 Arxiv 计算机视觉论文精选</strong></p>
<p><strong>主要趋势与主题：</strong></p>
<p>本期论文集聚焦于几个关键领域，展现了计算机视觉研究的几个重要发展方向：</p>
<ul>
<li><strong>多模态融合与理解：</strong> 多个研究深入探讨了将视觉信息与其他模态（如音频、文本）相结合，以实现更强大、更全面的感知能力。</li>
<li><strong>三维视觉与几何表示：</strong> 对三维场景的理解、重建和动态表示是另一大亮点，尤其是在视频和扩散模型方面的进展。</li>
<li><strong>大型多模态模型（LMMs）的进步与挑战：</strong> 研究人员正在探索 LMMs 在不同任务中的应用，同时也揭示了其在空间推理等方面的局限性。</li>
<li><strong>生成模型与表示学习：</strong> 利用自编码器和扩散模型等技术，在像素级和语义级表示的学习与生成方面取得了新进展。</li>
</ul>
<p><strong>亮点与创新：</strong></p>
<ul>
<li><strong>"The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding"</strong> 提出了一种统一的自编码方法，旨在协调语义和像素级别的表示，这可能为更深层次的视觉理解提供新的框架。</li>
<li><strong>"Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning"</strong> 在视听感知领域取得了显著进展，通过大规模多模态对应学习，有望提升模型对跨模态信息的理解能力。</li>
<li><strong>"WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion"</strong> 引入了一种利用异步视频扩散模型传播三维几何的新方法，为动态三维场景的重建和理解提供了创新的解决方案。</li>
</ul>
<p><strong>新兴研究方向与技术：</strong></p>
<ul>
<li><strong>统一表示学习：</strong> 将不同层次的视觉信息（像素、语义）整合到统一的表示空间中，是未来研究的重要方向。</li>
<li><strong>视听感知：</strong> 视听信息的深度融合将成为提升模型感知能力的关键，尤其是在理解复杂场景和交互时。</li>
<li><strong>动态三维几何传播：</strong> 利用扩散模型等生成技术处理视频中的三维几何信息，为实时三维重建和场景理解开辟了道路。</li>
<li><strong>LMMs 的空间推理能力提升：</strong> 解决 LMMs 在处理室内和开放世界场景时存在的空间推理差距，是推动其落地应用的关键。</li>
<li><strong>基于学习的动态系统：</strong> 将四维高斯溅射等技术视为学习到的动态系统，预示着对三维场景动态演化的新理解方式。</li>
</ul>
<p><strong>建议阅读论文：</strong></p>
<p>为了快速了解本期论文的精髓和潜在影响，建议优先阅读以下论文：</p>
<ol>
<li><strong>"The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding"</strong>: 探索统一表示学习的潜力。</li>
<li><strong>"Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning"</strong>: 了解视听融合的最新进展。</li>
<li><strong>"WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion"</strong>: 关注视频中三维几何传播的创新方法。</li>
<li><strong>"From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs"</strong>: 了解 LMMs 在空间推理方面的挑战与机遇。</li>
</ol>
<hr />
<p>这份摘要旨在为忙碌的研究人员提供一个快速了解 Arxiv 计算机视觉领域最新动态的窗口。希望它能帮助您高效地把握该领域的关键进展。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2512.19693v1">The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding</a></li>
<li><a href="#2512.19687v1">Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning</a></li>
<li><a href="#2512.19686v1">Visual-Aware CoT: Achieving High-Fidelity Visual Consistency in Unified Models</a></li>
<li><a href="#2512.19684v1">Zero-shot Reconstruction of In-Scene Object Manipulation from Video</a></li>
<li><a href="#2512.19683v1">From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs</a></li>
<li><a href="#2512.19680v1">VA-<script type="math/tex">π</script>: Variational Policy Alignment for Pixel-Aware Autoregressive Generation</a></li>
<li><a href="#2512.19678v1">WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion</a></li>
<li><a href="#2512.19675v1">Multimodal LLMs for Historical Dataset Construction from Archival Image Scans: German Patents (1877-1918)</a></li>
<li><a href="#2512.19648v1">4D Gaussian Splatting as a Learned Dynamical System</a></li>
<li><a href="#2512.19629v1">LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2512.19693v1'></a></p>
<h2 id="the-prism-hypothesis-harmonizing-semantic-and-pixel-representations-via-unified-autoencoding"><a href="https://arxiv.org/abs/2512.19693v1">The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding</a></h2>
<p><strong>Authors:</strong> Weichen Fan, Haiwen Diao, Quan Wang, Dahua Lin, Ziwei Liu</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Deep representations across modalities are inherently intertwined. In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders. Interestingly, our study uncovers a highly inspiring and rarely explored correspondence between an encoder's feature spectrum and its functional role: semantic encoders primarily capture low-frequency components that encode abstract meaning, whereas pixel encoders additionally retain high-frequency information that conveys fine-grained detail. This heuristic finding offers a unifying perspective that ties encoder behavior to its underlying spectral structure. We define it as the Prism Hypothesis, where each data modality can be viewed as a projection of the natural world onto a shared feature spectrum, just like the prism. Building on this insight, we propose Unified Autoencoding (UAE), a model that harmonizes semantic structure and pixel details via an innovative frequency-band modulator, enabling their seamless coexistence. Extensive experiments on ImageNet and MS-COCO benchmarks validate that our UAE effectively unifies semantic abstraction and pixel-level fidelity into a single latent space with state-of-the-art performance.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding”的全面摘要，重点关注其在计算机视觉和机器学习领域的贡献：</p>
<p><strong>论文摘要：The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>该论文旨在解决多模态深度表示（如图像和文本）之间固有的交织性问题。具体来说，研究人员发现，现有的语义编码器（用于理解抽象概念）和像素编码器（用于捕捉精细细节）在功能上存在显著差异，这导致了它们在表示上的不匹配。这种不匹配阻碍了统一模型的发展，并可能导致训练效率低下和表示冲突。论文的核心问题在于：如何有效地统一不同模态的表示，使其既能捕捉全局语义信息，又能保留精细的像素级细节，从而实现更强大的理解和生成能力。</p>
<p><strong>2. 主要创新点/方法论贡献：</strong></p>
<ul>
<li><strong>Prism Hypothesis（棱镜假说）：</strong> 作者提出了一个核心理论，即自然世界的输入（如图像）可以被视为投影到一个共享的特征频谱上。在这个频谱中，低频分量主要编码抽象的全局语义信息（如类别、属性、关系），而高频分量则捕捉精细的局部细节（如边缘、纹理）。这个假说将不同模态的表示统一在一个连续的频谱框架下。</li>
<li><strong>Unified Autoencoding (UAE) 模型：</strong> 基于棱镜假说，作者提出了一种新颖的统一自编码器（UAE）模型。UAE的核心是一个创新的<strong>频率-频段调制器（frequency-band modulator）</strong>，它能够将输入图像的表示分解成多个频率频段。<ul>
<li><strong>频率分解：</strong> UAE首先将编码器的潜在表示通过FFT（快速傅里叶变换）分解成多个频率频段。</li>
<li><strong>残差分割流（Residual Split Flow）：</strong> 采用迭代分割的方式，将原始特征分解为低频基础频段和多个高频残差频段。</li>
<li><strong>语义感知损失（Semantic-wise Loss）：</strong> 在训练过程中，作者引入了一个语义感知损失，将UAE的低频频段与预训练的语义编码器进行对齐，以保留全局语义信息。</li>
<li><strong>像素级重建损失（Pixel-wise Reconstruction Loss）：</strong> 同时，通过像素解码器进行图像重建，并采用噪声注入等策略来增强模型对高频细节的捕捉能力，确保像素级保真度。</li>
</ul>
</li>
<li><strong>统一的潜在空间：</strong> UAE的目标是学习一个统一的潜在空间，该空间能够和谐地融合语义结构和像素细节，使得低频部分负责语义，高频部分负责细节，并能无缝地与现有的扩散Transformer等模型集成。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<ul>
<li><strong>卓越的重建质量：</strong> 在ImageNet和MS-COCO数据集上，UAE在PSNR、SSIM和rFID等指标上取得了<strong>最先进（state-of-the-art）的性能</strong>。与RAE等现有统一模型相比，UAE在PSNR和SSIM上显著提升，rFID大幅降低，表明其在保留精细细节和全局语义方面表现出色。</li>
<li><strong>强大的语义理解能力：</strong> 通过线性探测实验，UAE在ImageNet-1K上达到了83.0%的top-1准确率，与大型模型相当，证明了其统一潜在空间能够有效保留强大的语义可辨识性。t-SNE可视化也表明，UAE的低频分量保留了原始DINOv2编码器的全局语义结构。</li>
<li><strong>有效的生成基础：</strong> UAE生成的潜在空间被证明是<strong>扩散友好的（diffusion-friendly）</strong>，为大规模视觉生成任务提供了坚实的基础。在类条件生成任务中，UAE取得了与现有SOTA模型相当的性能。</li>
<li><strong>鲁棒性：</strong> 实验表明，UAE的频率分解设计对频段数量不敏感，具有良好的鲁棒性，并且使用更少的频段也能获得接近最优的性能。</li>
</ul>
<p><strong>意义：</strong> 该研究提供了一个统一的视角来理解和处理多模态表示，揭示了特征频谱与模型功能之间的深刻联系。UAE模型通过创新的频率分解和调制机制，成功地解决了语义抽象与像素保真度之间的矛盾，为构建更强大、更通用的视觉模型奠定了基础。</p>
<p><strong>4. 提及的局限性：</strong></p>
<p>论文中并未明确列出局限性，但可以推断出：</p>
<ul>
<li><strong>计算成本：</strong> 虽然UAE在性能上表现优异，但其频率分解和重构过程可能带来一定的计算开销，尤其是在处理高分辨率图像时。</li>
<li><strong>预训练模型的依赖：</strong> UAE的初始化依赖于预训练的语义编码器（如DINOv2），这意味着其性能在一定程度上受限于预训练模型的质量和特性。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>更广泛的模态融合：</strong> 将UAE的频率分解思想扩展到更多模态（如音频、视频、3D数据）的融合，探索跨模态的统一表示。</li>
<li><strong>更精细的频率控制：</strong> 研究更精细的频率频段划分和调制策略，以实现更精细的语义和细节控制。</li>
<li><strong>自监督学习的进一步探索：</strong> 探索完全自监督的UAE模型，无需依赖预训练的语义编码器，以实现更通用的表示学习。</li>
<li><strong>动态频率分配：</strong> 开发能够根据输入内容动态调整频率分配的机制，以更有效地捕捉不同场景下的信息。</li>
<li><strong>更高效的实现：</strong> 优化UAE的计算效率，使其能够更广泛地应用于实时应用和大规模模型训练。</li>
</ul>
<p>总而言之，这篇论文通过提出“棱镜假说”和创新的UAE模型，为统一多模态表示提供了一个新颖且有效的框架，在视觉理解和生成领域取得了显著的进展。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Building on this insight, we propose Unified Autoencoding (UAE), a model that harmonizes semantic structure and pixel details via an innovative frequency-band modulator, enabling their seamless coexistence.</li>
<li>Extensive experiments on ImageNet and MS-COCO benchmarks validate that our UAE effectively unifies semantic abstraction and pixel-level fidelity into a single latent space with state-of-the-art performance.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.19693v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.19693v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.19687v1'></a></p>
<h2 id="pushing-the-frontier-of-audiovisual-perception-with-large-scale-multimodal-correspondence-learning"><a href="https://arxiv.org/abs/2512.19687v1">Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning</a></h2>
<p><strong>Authors:</strong> Apoorv Vyas, Heng-Jui Chang, Cheng-Fu Yang, Po-Yao Huang, Luya Gao, Julius Richter, Sanyuan Chen, Matt Le, Piotr Dollár, Christoph Feichtenhofer, Ann Lee, Wei-Ning Hsu</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Categories:</strong> cs.SD, cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>We introduce Perception Encoder Audiovisual, PE-AV, a new family of encoders for audio and video understanding trained with scaled contrastive learning. Built on PE, PE-AV makes several key contributions to extend representations to audio, and natively support joint embeddings across audio-video, audio-text, and video-text modalities. PE-AV's unified cross-modal embeddings enable novel tasks such as speech retrieval, and set a new state of the art across standard audio and video benchmarks. We unlock this by building a strong audiovisual data engine that synthesizes high-quality captions for O(100M) audio-video pairs, enabling large-scale supervision consistent across modalities. Our audio data includes speech, music, and general sound effects-avoiding single-domain limitations common in prior work. We exploit ten pairwise contrastive objectives, showing that scaling cross-modality and caption-type pairs strengthens alignment and improves zero-shot performance. We further develop PE-A-Frame by fine-tuning PE-AV with frame-level contrastive objectives, enabling fine-grained audio-frame-to-text alignment for tasks such as sound event detection.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“推动视听感知前沿：大规模多模态对应学习”的论文的全面摘要，由Apoorv Vyas等人撰写。</p>
<p><strong>论文题目：</strong> Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning (推动视听感知前沿：大规模多模态对应学习)</p>
<p><strong>作者：</strong> Apoorv Vyas, Heng-Jui Chang, Cheng-Fu Yang, Po-Yao Huang, Luya Gao, Julius Richter, Sanyuan Chen, Matt Le, Piotr Dollár, Christoph Feichtenhofer, Ann Lee, Wei-Ning Hsu</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题/研究问题：</strong>
该研究旨在解决当前多模态（音频、视频、文本）学习中的一个关键挑战：<strong>音频-视频-文本模态之间的对齐和表示学习不足，尤其是在大规模、多样化的数据集上。</strong> 现有方法在整合这些模态时存在数据规模不均衡、跨模态对齐效果有限、以及音频模态在某些任务上表现滞后等问题。因此，研究的核心问题是如何构建一个能够有效整合音频、视频和文本信息，并在广泛的下游任务中实现最先进（SOTA）性能的统一多模态编码器。</p>
<p><strong>2. 关键创新或方法论贡献：</strong></p>
<ul>
<li><strong>Perception Encoder Audiovisual (PE-AV) 系列编码器：</strong> 论文引入了PE-AV，这是一个全新的音频-视频-文本多模态编码器家族。它基于现有的PE（Perception Encoder）模型，并将其扩展到音频模态，实现了音频-视频、音频-文本和视频-文本模态的联合嵌入。</li>
<li><strong>大规模视听数据引擎：</strong> 为了实现大规模的跨模态学习，研究者构建了一个强大的视听数据引擎。该引擎能够合成高质量的、跨模态一致的文本描述（约1亿个音频-视频对），从而解决了数据稀疏性和标注成本高的问题。这种合成数据引擎能够生成包含语音、音乐和通用音效等多样化音频内容的数据，避免了以往工作中常见的单领域限制。</li>
<li><strong>多样的对比学习目标：</strong> 论文利用了多达十种成对的对比学习目标，涵盖了多种模态对和文本描述类型。研究表明，扩大对比学习的范围和类型能够显著增强模态间的对齐，并提升零样本（zero-shot）性能。</li>
<li><strong>PE-A-Frame：细粒度的音频-帧-文本对齐：</strong> 论文进一步提出了PE-A-Frame，通过在PE-AV基础上引入帧级别的对比学习目标，实现了音频信号中特定帧与文本描述之间的细粒度对齐。这使得模型能够处理诸如声音事件检测（SED）等需要精确时间对齐的任务。</li>
<li><strong>统一的跨模态嵌入：</strong> PE-AV能够生成统一的跨模态嵌入，使得音频、视频和文本信息能够在一个共享的嵌入空间中被有效表示和关联，从而支持新颖的任务，如语音检索。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<ul>
<li><strong>SOTA性能：</strong> PE-AV在多个音频-视频基准测试中取得了最先进的零样本性能，显著优于现有的音频-文本和音频-视频-文本模型。例如，在AudioCaps上，文本到音频检索的R@1得分从35.4提升到45.8；在VGGSound上，分类准确率从36.0提升到47.1。</li>
<li><strong>语音检索的突破：</strong> PE-AV是第一个能够实现语音检索（85.6 R@1）的模型，而其他模型在此任务上得分接近于零。</li>
<li><strong>视频任务的提升：</strong> 在视频检索任务上，PE-AVL在ActivityNet上将文本到视频检索的R@1得分从60.4提升到66.5；在Kinetics-400上，视频分类准确率从76.9提升到78.9，超越了参数量大2-4倍的模型。</li>
<li><strong>跨模态能力的展现：</strong> PE-AV能够有效地捕捉不同模态之间的对应关系，例如在视频-文本检索中，它能够利用音频信息来打破视频和文本之间的歧义，从而更准确地检索结果。</li>
<li><strong>广泛的音频覆盖：</strong> 其音频编码器能够覆盖语音、音乐和通用音效等多种音频类型，克服了以往模型在单一领域上表现优异但泛化能力不足的缺点。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong></p>
<ul>
<li><strong>数据规模和计算成本：</strong> 尽管论文通过数据引擎解决了数据规模问题，但训练如此大规模的多模态模型仍然需要巨大的计算资源。</li>
<li><strong>特定任务的性能差异：</strong> 虽然在许多任务上取得了SOTA，但论文也提到，在某些特定场景下，如需要更精细时间对齐的音频事件检测，仍有进一步优化的空间。</li>
<li><strong>模型大小与性能的权衡：</strong> 论文展示了模型大小（参数量）对性能的影响，但达到最佳性能的模型（PEAVL）参数量较大，这可能限制其在资源受限环境下的应用。</li>
</ul>
<p><strong>5. 未来研究方向：</strong></p>
<ul>
<li><strong>更广泛的模态整合：</strong> 将PE-AV的框架扩展到更多模态，如触觉、气味等，构建更全面的全模态感知模型。</li>
<li><strong>更高效的训练方法：</strong> 探索更高效的训练策略和模型架构，以降低大规模多模态模型的训练成本。</li>
<li><strong>更精细的跨模态理解：</strong> 进一步研究如何实现更深层次的跨模态理解，例如理解模态间的因果关系和更复杂的交互。</li>
<li><strong>下游应用的拓展：</strong> 将PE-AV的能力应用于更广泛的下游应用，如多模态内容生成、人机交互、机器人感知等。</li>
<li><strong>鲁棒性和公平性：</strong> 进一步研究模型在不同环境、不同人群数据下的鲁棒性和公平性问题。</li>
</ul>
<p><strong>对计算机视觉领域的意义：</strong></p>
<p>这篇论文对计算机视觉领域具有重要意义，主要体现在以下几个方面：</p>
<ul>
<li><strong>推动了多模态学习的边界：</strong> 通过引入大规模的视听数据引擎和创新的对比学习方法，论文极大地推动了音频、视频和文本模态的有效融合，为构建更全面、更强大的多模态理解模型奠定了基础。</li>
<li><strong>提升了零样本学习能力：</strong> PE-AV在广泛的零样本任务上取得了SOTA性能，证明了大规模、多样化的跨模态预训练能够赋予模型强大的泛化能力，使其能够应对未见过的数据和任务。</li>
<li><strong>为音频模态的整合提供了新思路：</strong> 论文成功地将音频模态整合到多模态学习框架中，并取得了显著的性能提升，这对于过去在多模态研究中相对被忽视的音频模态具有重要启示。</li>
<li><strong>为声音事件检测等任务带来了突破：</strong> PE-A-Frame在声音事件检测等需要精细时间对齐的任务上取得了显著进展，为相关领域的应用提供了新的可能性。</li>
<li><strong>提供了可复现的研究基础：</strong> 论文公开了代码和模型，为后续研究者提供了宝贵的资源，有助于推动该领域的进一步发展。</li>
</ul>
<p>总而言之，这篇论文通过大规模数据、创新的模型架构和训练策略，显著提升了视听文本多模态学习的能力，为构建更接近人类感知能力的通用人工智能模型迈出了重要一步。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce Perception Encoder Audiovisual, PE-AV, a new family of encoders for audio and video understanding trained with scaled contrastive learning.</li>
<li>Built on PE, PE-AV makes several key contributions to extend representations to audio, and natively support joint embeddings across audio-video, audio-text, and video-text modalities.</li>
<li>PE-AV's unified cross-modal embeddings enable novel tasks such as speech retrieval, and set a new state of the art across standard audio and video benchmarks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.19687v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.19687v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.19686v1'></a></p>
<h2 id="visual-aware-cot-achieving-high-fidelity-visual-consistency-in-unified-models"><a href="https://arxiv.org/abs/2512.19686v1">Visual-Aware CoT: Achieving High-Fidelity Visual Consistency in Unified Models</a></h2>
<p><strong>Authors:</strong> Zixuan Ye, Quande Liu, Cong Wei, Yuanxing Zhang, Xintao Wang, Pengfei Wan, Kun Gai, Wenhan Luo</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recently, the introduction of Chain-of-Thought (CoT) has largely improved the generation ability of unified models. However, it is observed that the current thinking process during generation mainly focuses on the text consistency with the text prompt, ignoring the \textbf{visual context consistency} with the visual reference images during the multi-modal generation, e.g., multi-reference generation. The lack of such consistency results in the failure in maintaining key visual features (like human ID, object attribute, style). To this end, we integrate the visual context consistency into the reasoning of unified models, explicitly motivating the model to sustain such consistency by 1) Adaptive Visual Planning: generating structured visual check list to figure out the visual element of needed consistency keeping, and 2) Iterative Visual Correction: performing self-reflection with the guidance of check lists and refining the generated result in an iterative manner. To achieve this, we use supervised finetuning to teach the model how to plan the visual checking, conduct self-reflection and self-refinement, and use flow-GRPO to further enhance the visual consistency through a customized visual checking reward. The experiments show that our method outperforms both zero-shot unified models and those with text CoTs in multi-modal generation, demonstrating higher visual context consistency.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Visual-Aware CoT: Achieving High-Fidelity Visual Consistency in Unified Models”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Visual-Aware CoT: Achieving High-Fidelity Visual Consistency in Unified Models (视觉感知思维链：在统一模型中实现高保真视觉一致性)</p>
<p><strong>作者：</strong> Zixuan Ye, Quande Liu, Cong Wei, Yuanxing Zhang, Xintao Wang, Pengfei Wan, Kun Gai, Wenhan Luo</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题/研究问题：</strong>
该论文指出，尽管链式思维（Chain-of-Thought, CoT）在统一模型（unified models）的生成能力方面取得了显著进展，但现有的 CoT 方法主要关注文本与提示（prompt）的一致性，而忽略了在多模态生成（如多参考图像生成）中与视觉参考图像的<strong>视觉上下文一致性</strong>。这种视觉一致性的缺失导致关键视觉特征（如人物身份、物体属性、风格）无法得到有效保持，从而影响生成结果的质量。因此，研究的核心问题是如何在统一模型中实现高保真的视觉上下文一致性。</p>
<p><strong>2. 关键创新/方法贡献：</strong>
为了解决上述问题，作者提出了 <strong>Visual-Aware CoT (VACOT)</strong> 框架，该框架将视觉上下文一致性显式地整合到统一模型的推理过程中。其核心创新点在于：</p>
<ul>
<li><strong>自适应视觉规划 (Adaptive Visual Planning)：</strong> 生成结构化的视觉检查清单（checklist），系统地识别需要保持一致性的视觉元素，使模型能够明确地推理哪些视觉特征需要被保留。</li>
<li><strong>迭代视觉纠正 (Iterative Visual Correction)：</strong> 利用视觉检查清单进行自我反思和迭代式精炼。模型在检查清单的指导下评估其生成结果，并逐步改进。</li>
<li><strong>两阶段训练策略：</strong><ul>
<li><strong>第一阶段：</strong> 使用监督微调（SFT）来训练模型进行视觉规划和自我反思，构建了专门的视觉规划和纠正数据集。</li>
<li><strong>第二阶段：</strong> 采用 flow-GRPO（一种强化学习框架），并设计了一个定制化的视觉一致性奖励函数，以进一步增强视觉一致性。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
通过广泛的实验，VACOT 在多模态生成任务上取得了显著的成果：</p>
<ul>
<li><strong>性能优越：</strong> VACOT 在多参考图像生成任务上显著优于零样本（zero-shot）的统一模型以及仅关注文本 CoT 的方法，在视觉上下文一致性方面表现出更高的水平。</li>
<li><strong>身份和风格保持：</strong> 在定性比较中，VACOT 能够更稳定、更出色地保持人物身份和视觉风格，而基线模型（如 BAGEL）和仅关注文本对齐的方法（如 UiG, UniCoT）则存在明显不足。</li>
<li><strong>不牺牲文本一致性：</strong> 实验表明，VACOT 在增强视觉一致性的同时，并未损害其在文本到图像（T2I）生成任务上的基本能力，甚至在某些组合生成任务上有所提升。这表明视觉感知能力的训练能够促进更连贯、更结构化的图像生成。</li>
<li><strong>重要性验证：</strong> 消融研究表明，自适应视觉规划和迭代视觉纠正这两个核心组件都对提升性能至关重要。</li>
</ul>
<p><strong>4. 提及的局限性：</strong>
论文中提到，虽然 VACOT 在大多数情况下能有效解决问题，但<strong>迭代次数过多（超过 2-3 次）可能会导致性能下降</strong>。这可能是因为：
*   重复的编辑会累积噪声和伪影，降低图像质量。
*   对于一些根本性的、模型难以识别或有效解决的问题，过多的迭代可能导致模型误判或应用不恰当的修正。</p>
<p><strong>5. 未来研究方向（隐含）：</strong>
虽然论文没有明确列出未来研究方向，但从其提出的方法和发现的局限性中可以推断出以下潜在方向：
*   <strong>更鲁棒的迭代机制：</strong> 研究如何设计更智能的迭代策略，避免因过度迭代而引入的负面影响，特别是在处理复杂或根本性问题时。
*   <strong>更精细的视觉元素识别与评估：</strong> 探索更高级的技术来识别和评估更细粒度的视觉特征，以应对更复杂的视觉一致性要求。
*   <strong>通用性与泛化能力：</strong> 进一步探索 VACOT 在更多样化的多模态生成任务（如视频生成、图像编辑等）中的应用和泛化能力。
*   <strong>效率优化：</strong> 尽管 flow-GRPO 是一种有效的强化学习方法，但其计算成本可能较高，未来可以探索更高效的训练和推理方法。</p>
<p><strong>总结：</strong>
该论文的核心贡献在于提出了一种名为 VACOT 的新颖框架，通过引入<strong>自适应视觉规划</strong>和<strong>迭代视觉纠正</strong>，有效地解决了现有统一模型在多模态生成中忽视视觉上下文一致性的问题。VACOT 通过结构化的检查清单和迭代式精炼，显著提升了生成图像的视觉保真度，特别是在人物身份和风格保持方面。实验结果表明，VACOT 在多参考图像生成任务上取得了最先进的性能，并且不牺牲文本一致性，为实现更可靠、更高质量的多模态生成提供了新的思路。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>The experiments show that our method outperforms both zero-shot unified models and those with text CoTs in multi-modal generation, demonstrating higher visual context consistency.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.19686v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.19686v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.19684v1'></a></p>
<h2 id="zero-shot-reconstruction-of-in-scene-object-manipulation-from-video"><a href="https://arxiv.org/abs/2512.19684v1">Zero-shot Reconstruction of In-Scene Object Manipulation from Video</a></h2>
<p><strong>Authors:</strong> Dixuan Lin, Tianyou Wang, Zhuoyang Pan, Yufu Wang, Lingjie Liu, Kostas Daniilidis</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Categories:</strong> cs.CV, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>We build the first system to address the problem of reconstructing in-scene object manipulation from a monocular RGB video. It is challenging due to ill-posed scene reconstruction, ambiguous hand-object depth, and the need for physically plausible interactions. Existing methods operate in hand centric coordinates and ignore the scene, hindering metric accuracy and practical use. In our method, we first use data-driven foundation models to initialize the core components, including the object mesh and poses, the scene point cloud, and the hand poses. We then apply a two-stage optimization that recovers a complete hand-object motion from grasping to interaction, which remains consistent with the scene information observed in the input video.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Zero-shot Reconstruction of In-Scene Object Manipulation from Video”的全面摘要，重点关注其研究问题、方法创新、结果、局限性和未来方向：</p>
<p><strong>论文题目：</strong> Zero-shot Reconstruction of In-Scene Object Manipulation from Video</p>
<p><strong>作者：</strong> Dixuan Lin, Tianyou Wang, Zhuoyang Pan, Yufu Wang, Lingjie Liu, Kostas Daniilidis</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题：</strong>
该论文旨在解决从单目RGB视频中重建场景内物体操控（例如，抓取、移动、放置物体）的挑战性问题。现有方法通常在手部坐标系下进行，忽略了场景信息，导致重建的度量精度不足且难以在实际应用中使用。主要挑战包括：单目场景重建的固有不适定性、手部与物体深度估计的模糊性，以及实现物理上可信的手部-物体交互的困难。</p>
<p><strong>2. 关键创新与方法贡献：</strong>
该研究提出了<strong>首个</strong>能够从单目RGB视频中重建场景内物体操控的系统。其核心创新在于：</p>
<ul>
<li><strong>场景对齐的重建：</strong> 与以往仅关注手部或物体的方法不同，该系统能够将手部和物体的运动重建到全局场景坐标系中，提高了度量精度和实用性。</li>
<li><strong>两阶段优化框架：</strong><ul>
<li><strong>初始化：</strong> 利用数据驱动的<strong>基础模型</strong>（如SpatialTrackerV2、Hi3DGen/Amodal3R、Foundationpose、HaPTIC）来初始化核心组件，包括场景点云、物体网格和姿态、以及手部姿态。</li>
<li><strong>两阶段优化：</strong> 将手部-物体运动分解为两个阶段进行优化：<ul>
<li><strong>交互阶段（Interaction Stage）：</strong> 重点在于实现深度一致的手部-物体运动，通过接触点匹配、物理碰撞约束（SDF损失）、运动平滑性和正则化来优化手部姿态和物体姿态。</li>
<li><strong>抓取阶段（Grasping Stage）：</strong> 专注于优化手部接近和抓取物体的过程，利用人类运动先验（Egoallo）来完成手部运动的补全，并进一步优化抓取姿态，避免手指与物体的穿透。</li>
</ul>
</li>
</ul>
</li>
<li><strong>接触点约束：</strong> 提出了一种有效的接触点匹配算法，通过采样手部指尖顶点作为接触候选点，并将其与物体表面进行匹配，以实现精确的手部-物体对齐。</li>
<li><strong>物理一致性：</strong> 引入了多种损失函数（如接触损失、穿透损失、平滑损失、正则化损失）来确保重建的运动在物理上是可信的。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>性能提升：</strong> 在DexYCB和HOI4D等标准数据集上，该系统在手部姿态准确性、轨迹偏差以及物理交互指标（如穿透体积、穿透深度、运动平滑度）方面均取得了显著的改进，尤其是在场景对齐和物理交互方面。
*   <strong>“零样本”能力：</strong> 该系统能够处理之前未见过的物体和场景，展现了其“零样本”泛化能力。
*   <strong>实际应用潜力：</strong> 通过实现场景对齐的物体操控重建，该系统为机器人抓取、增强现实/虚拟现实交互等应用提供了更准确、更实用的基础。
*   <strong>定性结果：</strong> 在“in-the-wild”视频上的定性结果也表明，该方法能够生成逼真且与场景一致的手部-物体运动。</p>
<p><strong>4. 局限性：</strong>
*   <strong>对场景分割和物体重建的依赖：</strong> 系统的性能在很大程度上依赖于初始场景分割和物体重建的准确性。在低光照或严重运动模糊的情况下，这些步骤可能失败，从而影响后续的接触点识别。
*   <strong>物体重建的单帧依赖：</strong> 目前，物体重建主要依赖于视频的第一帧。作者指出，利用整个视频序列的信息进行物体重建是一个有待改进的方向。
*   <strong>手部可见性限制：</strong> 手部检测算法在手部不可见时效果不佳，这使得在抓取阶段需要依赖人类运动先验来补全运动。</p>
<p><strong>5. 未来研究方向：</strong>
*   <strong>利用多帧信息进行物体重建：</strong> 改进物体重建模块，使其能够从整个视频序列中提取信息，提高重建的鲁棒性。
*   <strong>处理更复杂的场景和交互：</strong> 扩展系统以处理多物体交互、更精细的物体操作以及更具挑战性的场景。
*   <strong>端到端的学习：</strong> 探索端到端的学习方法，以减少对预训练基础模型的依赖，并可能进一步提升性能。
*   <strong>实时性提升：</strong> 进一步优化算法以实现实时或近实时的场景内物体操控重建。</p>
<p>总而言之，该论文在从单目视频重建场景内物体操控这一具有挑战性的领域取得了重要进展，通过创新的两阶段优化框架和对基础模型的有效利用，实现了场景对齐、物理可信的手部-物体运动重建，为机器人和人机交互领域开辟了新的可能性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In our method, we first use data-driven foundation models to initialize the core components, including the object mesh and poses, the scene point cloud, and the hand poses.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.19684v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.19684v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.19683v1'></a></p>
<h2 id="from-indoor-to-open-world-revealing-the-spatial-reasoning-gap-in-mllms"><a href="https://arxiv.org/abs/2512.19683v1">From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs</a></h2>
<p><strong>Authors:</strong> Mingrui Wu, Zhaozhi Wang, Fangjinhua Wang, Jiaolong Yang, Marc Pollefeys, Tong Zhang</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>While Multimodal Large Language Models (MLLMs) have achieved impressive performance on semantic tasks, their spatial intelligence--crucial for robust and grounded AI systems--remains underdeveloped. Existing benchmarks fall short of diagnosing this limitation: they either focus on overly simplified qualitative reasoning or rely on domain-specific indoor data, constrained by the lack of outdoor datasets with verifiable metric ground truth. To bridge this gap, we introduce a large-scale benchmark built from pedestrian-perspective videos captured with synchronized stereo cameras, LiDAR, and IMU/GPS sensors. This dataset provides metrically precise 3D information, enabling the automatic generation of spatial reasoning questions that span a hierarchical spectrum--from qualitative relational reasoning to quantitative metric and kinematic understanding. Evaluations reveal that the performance gains observed in structured indoor benchmarks vanish in open-world settings. Further analysis using synthetic abnormal scenes and blinding tests confirms that current MLLMs depend heavily on linguistic priors instead of grounded visual reasoning. Our benchmark thus provides a principled platform for diagnosing these limitations and advancing physically grounded spatial intelligence.</p>
<p><strong>Analysis:</strong></p>
<p>作为计算机视觉和机器学习领域的专家，我对这篇论文摘要进行了深入分析，并为您提供以下中文解读：</p>
<p><strong>1. 论文的主要贡献（2-3句话）</strong></p>
<p>该论文指出，当前多模态大语言模型（MLLMs）在空间智能方面存在显著不足，尤其是在开放世界场景下。为了解决这一问题，作者构建了一个大规模、真实世界的基准数据集，该数据集包含同步的立体相机、LiDAR 和 IMU/GPS 数据，能够生成具有度量精度和层次化的空间推理问题。通过该基准的评估，揭示了 MLLMs 在开放世界中空间推理能力的局限性，并证明它们过度依赖语言先验而非视觉推理。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<ul>
<li><strong>构建大规模、真实世界的开放世界空间推理基准：</strong> 这是论文的核心贡献。该基准数据集的创新之处在于：<ul>
<li><strong>数据来源：</strong> 使用行人视角捕捉的视频，并同步了多种传感器（立体相机、LiDAR、IMU/GPS），这使得数据更具真实性和开放性。</li>
<li><strong>度量精度：</strong> 强调了“metrically precise 3D information”，这意味着数据集提供了精确的几何和尺度信息，这是以往许多室内基准所缺乏的。</li>
<li><strong>问题生成：</strong> 能够自动生成跨越定性关系推理、定量度量和运动学理解的层次化空间推理问题。这种自动化和层次化设计有助于更全面地诊断 MLLMs 的能力。</li>
</ul>
</li>
<li><strong>揭示 MLLMs 的空间推理局限性：</strong> 通过在新的开放世界基准上进行评估，论文有效地揭示了现有 MLLMs 在从室内到开放世界的迁移中性能急剧下降的现象。</li>
<li><strong>诊断 MLLMs 的推理机制：</strong> 通过合成异常场景和“blinding tests”（可能指一种剥离语言先验的测试方法），论文进一步证实了 MLLMs 依赖于语言先验而非真正的视觉推理。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<ul>
<li><strong>推动 MLLMs 的空间智能发展：</strong> 该研究直接指出了 MLLMs 在空间理解上的短板，并提供了一个强有力的工具（基准数据集和评估方法）来衡量和改进这一能力。这将促使研究人员更加关注如何让 MLLMs 真正“理解”物理世界。</li>
<li><strong>为更鲁棒和具身 AI 系统奠定基础：</strong> 空间智能是构建能够与物理世界交互的 AI 系统的关键。该研究的成果将有助于开发更可靠的机器人、自动驾驶系统以及其他需要精确空间感知的 AI 应用。</li>
<li><strong>重新审视 MLLMs 的评估方法：</strong> 论文表明，现有的室内、定性或领域特定的基准不足以全面评估 MLLMs 的真实空间推理能力。这可能会促使社区开发更多样化、更具挑战性的开放世界基准。</li>
<li><strong>促进语言模型与物理世界的连接：</strong> 该研究强调了将语言模型与真实的物理世界进行“接地”（grounding）的重要性，避免模型仅仅是“背诵”语言知识。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>机器人学：</strong> 需要机器人能够理解和导航复杂的、动态的开放世界环境，进行物体抓取、路径规划等。</li>
<li><strong>自动驾驶：</strong> 车辆需要精确的空间感知能力来理解道路状况、其他车辆和行人的位置及运动。</li>
<li><strong>增强现实/虚拟现实 (AR/VR)：</strong> 需要将虚拟内容准确地叠加到真实世界中，或在虚拟环境中进行逼真的交互。</li>
<li><strong>三维重建与场景理解：</strong> 提升对真实世界场景的几何和语义理解能力。</li>
<li><strong>地理信息系统 (GIS) 和遥感：</strong> 分析和理解大范围的地理空间数据。</li>
<li><strong>智能助手和问答系统：</strong> 使 AI 能够回答更复杂的关于空间关系和物理过程的问题。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<ul>
<li><strong>基准数据集的规模和多样性：</strong> 虽然被称为“large-scale”，但具体规模和覆盖的场景多样性仍需进一步了解。开放世界是极其复杂的，一个基准可能无法完全捕捉所有挑战。</li>
<li><strong>“自动生成”问题的质量和覆盖范围：</strong> 自动生成问题的能力很强大，但生成问题的质量、难度分布以及是否能完全覆盖所有重要的空间推理类型，可能需要进一步验证。</li>
<li><strong>“blinding tests”的具体实现：</strong> 摘要中提到的“blinding tests”具体是如何设计的，以及它在多大程度上能够完全剥离语言先验，是评估其有效性的关键。</li>
<li><strong>评估指标的全面性：</strong> 摘要提到了性能增益的消失，但具体的评估指标和分析的深度（例如，模型在哪些类型的空间推理上表现最差）需要论文正文来详细说明。</li>
<li><strong>计算资源需求：</strong> 处理和推理大规模、高精度的 3D 数据通常需要大量的计算资源，这可能成为模型部署和训练的挑战。</li>
</ul>
<p>总而言之，这篇论文通过构建一个创新性的开放世界空间推理基准，有力地揭示了当前 MLLMs 在理解物理世界方面的关键瓶颈，并为未来的研究指明了方向。其对“接地”AI 的关注，以及对模型推理机制的深入分析，使其在计算机视觉和机器学习领域具有重要的理论和实践意义。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To bridge this gap, we introduce a large-scale benchmark built from pedestrian-perspective videos captured with synchronized stereo cameras, LiDAR, and IMU/GPS sensors.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.19683v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.19683v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.19680v1'></a></p>
<h2 id="va-variational-policy-alignment-for-pixel-aware-autoregressive-generation"><a href="https://arxiv.org/abs/2512.19680v1">VA-<script type="math/tex">π</script>: Variational Policy Alignment for Pixel-Aware Autoregressive Generation</a></h2>
<p><strong>Authors:</strong> Xinyao Liao, Qiyuan He, Kai Xu, Xiaoye Qu, Yicong Li, Wei Wei, Angela Yao</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Autoregressive (AR) visual generation relies on tokenizers to map images to and from discrete sequences. However, tokenizers are trained to reconstruct clean images from ground-truth tokens, while AR generators are optimized only for token likelihood. This misalignment leads to generated token sequences that may decode into low-quality images, without direct supervision from the pixel space. We propose VA-<script type="math/tex">π</script>, a lightweight post-training framework that directly optimizes AR models with a principled pixel-space objective. VA-<script type="math/tex">π</script> formulates the generator-tokenizer alignment as a variational optimization, deriving an evidence lower bound (ELBO) that unifies pixel reconstruction and autoregressive modeling. To optimize under the discrete token space, VA-<script type="math/tex">π</script> introduces a reinforcement-based alignment strategy that treats the AR generator as a policy, uses pixel-space reconstruction quality as its intrinsic reward. The reward is measured by how well the predicted token sequences can reconstruct the original image under teacher forcing, giving the model direct pixel-level guidance without expensive free-running sampling. The regularization term of the ELBO serves as a natural regularizer, maintaining distributional consistency of tokens. VA-<script type="math/tex">π</script> enables rapid adaptation of existing AR generators, without neither tokenizer retraining nor external reward models. With only 1% ImageNet-1K data and 25 minutes of tuning, it reduces FID from 14.36 to 7.65 and improves IS from 86.55 to 116.70 on LlamaGen-XXL, while also yielding notable gains in the text-to-image task on GenEval for both visual generation model (LlamaGen: from 0.306 to 0.339) and unified multi-modal model (Janus-Pro: from 0.725 to 0.744). Code is available at https://github.com/Lil-Shake/VA-Pi.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“VA-π: Variational Policy Alignment for Pixel-Aware Autoregressive Generation”的全面中文摘要，重点关注其研究问题、创新点、主要结果、局限性和未来方向：</p>
<p><strong>论文题目：</strong> VA-π: Variational Policy Alignment for Pixel-Aware Autoregressive Generation</p>
<p><strong>作者：</strong> Xinyao Liao, Qiyuan He, Kai Xu, Xiaoye Qu, Yicong Li, Wei Wei, Angela Yao</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题/研究问题：</strong></p>
<p>该论文主要解决了当前自回归（AR）视觉生成模型在生成图像质量方面存在的问题。核心挑战在于，AR生成器仅针对离散的token序列进行优化（最大化token似然度），而用于将图像编码为token的tokenizer则是在重构真实图像的监督下训练的。这种“token似然度”与“像素空间真实性”之间的不匹配，导致AR生成器产生的token序列在解码后会生成低质量、带有伪影的图像，即使这些token序列在token层面具有高似然度。论文的研究问题是：<strong>能否设计一个目标函数，将token级别的建模与像素级别的分布对齐起来，从而直接优化AR生成器以生成更高质量的图像？</strong></p>
<p><strong>2. 关键创新点/方法论贡献：</strong></p>
<ul>
<li><strong>像素感知ELBO（Evidence Lower Bound）框架：</strong> 论文提出了一种新的变分优化目标，将生成器-tokenizer的对齐问题形式化为一个ELBO，该ELBO统一了像素重构和自回归建模。这提供了一个原则性的像素空间目标。</li>
<li><strong>基于强化学习的对齐策略：</strong> 为了在离散的token空间中进行优化，VA-π引入了一种基于强化学习（RL）的对齐策略。它将AR生成器视为一个策略，并利用像素空间重构质量作为内在奖励。这种奖励是通过在“教师强制”（teacher forcing）模式下评估预测token序列重构原始图像的能力来获得的，从而为模型提供了直接的像素级指导，避免了计算成本高昂的“自由运行”（free-running）采样。</li>
<li><strong>轻量级后训练框架：</strong> VA-π是一个轻量级的后训练（post-training）框架，可以直接优化现有的AR生成器，而无需重新训练tokenizer或引入外部奖励模型。</li>
<li><strong>结合了像素重构奖励和先验正则化：</strong> ELBO中的正则化项自然地充当了保持token分布一致性的角色，而像素重构奖励则直接驱动模型生成更符合像素空间分布的图像。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<ul>
<li><strong>显著提升图像质量：</strong> 在ImageNet-1K数据集上，VA-π对LlamaGen-XXL模型进行后训练，仅使用1%的数据和25分钟的调优时间，就将FID（Fréchet Inception Distance）从14.36降低到7.65，IS（Inception Score）从86.55提高到116.70。这表明VA-π在图像保真度和多样性方面取得了显著提升。</li>
<li><strong>文本到图像生成任务的改进：</strong> 在GenEval基准上，VA-π在文本到图像生成任务中也取得了显著的性能提升。对于纯视觉生成模型LlamaGen，其整体得分从0.306提升到0.339；对于统一的多模态模型Janus-Pro，得分从0.725提升到0.744。这证明了VA-π的泛化能力。</li>
<li><strong>效率和成本效益：</strong> VA-π的后训练过程非常高效，仅需少量数据和计算资源（25分钟），并且不需要外部奖励模型，这使其成为一种实用的方法。</li>
<li><strong>理论基础：</strong> 论文提供了扎实的理论分析，将像素感知对齐问题形式化为ELBO，并解释了其与VAE和VQVAE的关系。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>对教师强制的依赖：</strong> 论文中提出的像素重构奖励是基于“教师强制”模式下的重构质量。虽然这避免了自由运行采样，但可能无法完全捕捉自由运行模式下的所有挑战。</li>
<li><strong>潜在的“过平滑”问题（通过消融实验间接提及）：</strong> 在消融研究中，论文提到仅对tokenizer进行后训练可能会导致生成图像的纹理过于平滑，FID和IS反而恶化。这暗示了仅优化重构路径可能不足以解决所有问题，需要AR生成器本身的像素级对齐。</li>
<li><strong>对现有AR模型的依赖：</strong> VA-π是一个后训练框架，其效果依赖于预训练AR模型的质量。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>更精细的奖励函数设计：</strong> 探索更复杂的像素空间奖励函数，可能包含更细粒度的感知损失或对抗性损失，以进一步提升生成质量。</li>
<li><strong>自由运行模式下的对齐：</strong> 研究如何在自由运行模式下直接优化像素空间目标，以克服教师强制的局限性。</li>
<li><strong>与其他生成模型的结合：</strong> 将VA-π的对齐思想应用于其他类型的生成模型，如扩散模型或GANs。</li>
<li><strong>更广泛的多模态应用：</strong> 探索VA-π在更复杂的跨模态生成任务中的应用，例如视频生成或3D内容生成。</li>
<li><strong>更高效的正则化策略：</strong> 研究更有效的正则化方法，以进一步稳定训练并提升模型性能。</li>
</ul>
<p>总而言之，VA-π提出了一种新颖且高效的后训练框架，通过将自回归生成器的优化目标与像素空间重构质量相结合，显著提升了AR视觉生成模型的图像质量和文本对齐能力。其基于强化学习的像素感知对齐策略，为解决AR生成模型中的核心挑战提供了一个有前景的解决方案。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose VA-<script type="math/tex">π</script>, a lightweight post-training framework that directly optimizes AR models with a principled pixel-space objective.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.19680v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.19680v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.19678v1'></a></p>
<h2 id="worldwarp-propagating-3d-geometry-with-asynchronous-video-diffusion"><a href="https://arxiv.org/abs/2512.19678v1">WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion</a></h2>
<p><strong>Authors:</strong> Hanyang Kong, Xingyi Yang, Xiaoxu Zheng, Xinchao Wang</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Generating long-range, geometrically consistent video presents a fundamental dilemma: while consistency demands strict adherence to 3D geometry in pixel space, state-of-the-art generative models operate most effectively in a camera-conditioned latent space. This disconnect causes current methods to struggle with occluded areas and complex camera trajectories. To bridge this gap, we propose WorldWarp, a framework that couples a 3D structural anchor with a 2D generative refiner. To establish geometric grounding, WorldWarp maintains an online 3D geometric cache built via Gaussian Splatting (3DGS). By explicitly warping historical content into novel views, this cache acts as a structural scaffold, ensuring each new frame respects prior geometry. However, static warping inevitably leaves holes and artifacts due to occlusions. We address this using a Spatio-Temporal Diffusion (ST-Diff) model designed for a "fill-and-revise" objective. Our key innovation is a spatio-temporal varying noise schedule: blank regions receive full noise to trigger generation, while warped regions receive partial noise to enable refinement. By dynamically updating the 3D cache at every step, WorldWarp maintains consistency across video chunks. Consequently, it achieves state-of-the-art fidelity by ensuring that 3D logic guides structure while diffusion logic perfects texture. Project page: \href{https://hyokong.github.io/worldwarp-page/}{https://hyokong.github.io/worldwarp-page/}.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion”的全面中文摘要：</p>
<p><strong>论文题目：</strong> WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion</p>
<p><strong>作者：</strong> Hanyang Kong, Xingyi Yang, Xiaoxu Zheng, Xinchao Wang</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
论文旨在解决生成长距离、几何一致性视频的根本性难题。现有生成模型在像素空间中严格遵循3D几何信息时表现不佳，而它们在相机条件下的潜在空间中表现最佳。这种脱节导致当前方法在处理遮挡区域和复杂相机轨迹时遇到困难，容易产生视觉伪影和几何不一致。</p>
<p><strong>2. 关键创新与方法贡献：</strong>
作者提出了 <strong>WorldWarp</strong> 框架，该框架巧妙地结合了3D结构锚点和2D生成精炼器，以弥合几何一致性与生成模型能力之间的差距。其核心创新包括：</p>
<ul>
<li><strong>在线3D几何缓存 (Online 3D Geometric Cache)：</strong> 利用3D Gaussian Splatting (3DGS) 技术构建一个动态更新的3D几何缓存。这个缓存充当结构支架，通过将历史内容向前投影到新视角，确保每一帧都尊重先前的几何信息。</li>
<li><strong>时空自适应扩散模型 (Spatio-Temporal Diffusion - ST-Diff)：</strong> 设计了一个专门用于“填充与修正”任务的扩散模型。其关键创新在于 <strong>时空变化的噪声调度</strong>：<ul>
<li><strong>填充（空白区域）：</strong> 接收全噪声，以触发新内容的生成。</li>
<li><strong>修正（已投影区域）：</strong> 接收部分噪声，以进行精炼和修正，保留原有几何细节。</li>
</ul>
</li>
<li><strong>异步推理管线 (Asynchronous Inference Pipeline)：</strong> 采用分块（chunk-by-chunk）的自回归推理方式生成视频。这种方法通过动态更新3D缓存，在每一步都将模型“锚定”在最新的、准确的几何信息上，从而避免了传统方法中不可逆的错误累积。</li>
<li><strong>非因果（Non-causal）注意力机制：</strong> ST-Diff模型利用了强大的双向注意力机制，这得益于能够利用未来相机位姿进行前向投影的“未来”图像作为条件。这使得模型能够同时处理所有帧，摆脱了传统视频生成中严格的因果约束。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
WorldWarp 在具有挑战性的长序列新视角外推任务上取得了 <strong>最先进（state-of-the-art）的性能</strong>。
*   <strong>几何一致性与视觉保真度：</strong> 在 RealEstate10K 和 DL3DV 数据集上，WorldWarp 在所有评估指标（包括PSNR, SSIM, LPIPS, Rdist, Tdist）上均显著优于现有方法，尤其是在长距离合成中，其质量衰减最小。
*   <strong>鲁棒性：</strong> 该方法在复杂相机轨迹和不同场景下表现出强大的泛化能力和几何稳定性，有效解决了相机漂移问题。
*   <strong>3D逻辑指导结构，扩散逻辑完善纹理：</strong> 论文强调，WorldWarp 实现了3D几何逻辑对结构生成的指导，同时利用扩散模型对纹理进行精细化处理，从而达到高保真度的结果。
*   <strong>对艺术风格的泛化能力：</strong> 通过文本提示，模型能够生成具有不同艺术风格（如“梵高风格”、“吉卜力工作室风格”）的视频，同时保持严格的3D几何一致性，验证了其在语义和美学上的泛化能力。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>长时序生成中的误差累积：</strong> 尽管采用了异步扩散和动态缓存，但生成无限长视频序列并保持完美保真度仍然是一个挑战。分块生成中微小的视觉伪影或几何不一致可能会累积，导致长序列（超过1000帧）的视觉质量或几何稳定性下降。
*   <strong>对几何先验的依赖：</strong> WorldWarp 的性能高度依赖于上游3D几何基础模型（如 TTT3R 或 VGGT）提供的深度图和相机位姿估计的准确性。在复杂环境（如极端光照下的户外场景）中，如果这些估计不准确，可能导致错误的投影结果，从而影响生成质量。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>进一步提升长时序生成能力：</strong> 探索更有效的机制来抑制或纠正分块生成中累积的误差，以实现更长、更稳定的视频生成。
*   <strong>增强对复杂几何场景的鲁棒性：</strong> 研究如何提高模型在极端光照、透明物体或纹理稀疏等复杂场景下的几何估计和生成能力，以减少对上游3D估计的敏感性。
*   <strong>探索更高效的3D几何表示和更新策略：</strong> 尽管3DGS 表现出色，但研究更轻量级或更快速的3D几何表示和更新方法，可能有助于进一步加速推理过程。
*   <strong>结合更丰富的条件信息：</strong> 除了文本提示，探索结合其他模态（如音频、用户交互）来指导视频生成，以实现更具交互性和多样性的内容创作。</p>
<p><strong>总结：</strong>
WorldWarp 是一项重要的工作，它通过创新的3D几何缓存和时空自适应扩散模型，成功地解决了长距离、几何一致性视频生成中的关键挑战。该方法在保持高视觉保真度的同时，实现了对3D几何的精确控制，为未来在虚拟现实、内容创作和电影制作等领域的应用奠定了坚实基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Generating long-range, geometrically consistent video presents a fundamental dilemma: while consistency demands strict adherence to 3D geometry in pixel space, state-of-the-art generative models operate most effectively in a camera-conditioned latent space.</li>
<li>To bridge this gap, we propose WorldWarp, a framework that couples a 3D structural anchor with a 2D generative refiner.</li>
<li>By explicitly warping historical content into novel views, this cache acts as a structural scaffold, ensuring each new frame respects prior geometry.</li>
<li>Consequently, it achieves state-of-the-art fidelity by ensuring that 3D logic guides structure while diffusion logic perfects texture.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.19678v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.19678v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.19675v1'></a></p>
<h2 id="multimodal-llms-for-historical-dataset-construction-from-archival-image-scans-german-patents-1877-1918"><a href="https://arxiv.org/abs/2512.19675v1">Multimodal LLMs for Historical Dataset Construction from Archival Image Scans: German Patents (1877-1918)</a></h2>
<p><strong>Authors:</strong> Niclas Griesshaber, Jochen Streb</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Categories:</strong> econ.GN, cs.CV, cs.DL</p>
<p><strong>Abstract:</strong></p>
<p>We leverage multimodal large language models (LLMs) to construct a dataset of 306,070 German patents (1877-1918) from 9,562 archival image scans using our LLM-based pipeline powered by Gemini-2.5-Pro and Gemini-2.5-Flash-Lite. Our benchmarking exercise provides tentative evidence that multimodal LLMs can create higher quality datasets than our research assistants, while also being more than 795 times faster and 205 times cheaper in constructing the patent dataset from our image corpus. About 20 to 50 patent entries are embedded on each page, arranged in a double-column format and printed in Gothic and Roman fonts. The font and layout complexity of our primary source material suggests to us that multimodal LLMs are a paradigm shift in how datasets are constructed in economic history. We open-source our benchmarking and patent datasets as well as our LLM-based data pipeline, which can be easily adapted to other image corpora using LLM-assisted coding tools, lowering the barriers for less technical researchers. Finally, we explain the economics of deploying LLMs for historical dataset construction and conclude by speculating on the potential implications for the field of economic history.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Multimodal LLMs for Historical Dataset Construction from Archival Image Scans: German Patents (1877-1918)”的全面中文摘要：</p>
<p><strong>论文摘要：</strong></p>
<p><strong>1. 研究问题/研究目标：</strong>
该论文旨在解决经济史研究中构建大规模微观数据集的瓶颈问题，特别是如何高效、准确地从复杂的历史图像扫描（如德国专利档案）中提取结构化数据。传统的手动数据提取方法耗时、昂贵且容易出错，限制了经济史研究的规模和深度。研究的核心问题是如何利用新兴的多模态大语言模型（LLMs）来自动化这一过程，并评估其与人工方法的性能对比。</p>
<p><strong>2. 主要创新点/方法论贡献：</strong>
*   <strong>LLM驱动的数据集构建流水线：</strong> 作者开发了一个创新的、基于多模态LLMs（Gemini-2.5-Pro和Gemini-2.5-Flash-Lite）的两阶段流水线，用于从9,562张德国专利档案图像扫描中提取306,070个专利条目。
    *   <strong>第一阶段（Patent Entry Extraction）：</strong> 利用Gemini-2.5-Pro识别和提取图像中的专利条目和技术类别，并处理了跨页和跨栏的截断条目。
    *   <strong>第二阶段（Variable Extraction）：</strong> 利用Gemini-2.5-Flash-Lite从每个提取的专利条目中提取关键变量，包括专利ID、申请人、地点、标题和日期。
*   <strong>LLM辅助的提示工程与迭代优化：</strong> 作者强调了精心设计的提示（prompts）在指导LLM准确提取信息中的关键作用，并通过迭代优化过程来提高流水线的性能。
*   <strong>构建并公开了高质量的基准数据集：</strong> 为了评估LLM的性能，作者构建了一个“完美基准数据集”（perfect benchmarking dataset），并与研究助理手动创建的“学生构建数据集”（student-constructed dataset）进行了对比。这些数据集和LLM流水线均已开源。
*   <strong>对复杂历史文档的处理能力：</strong> 该研究特别关注了处理具有双栏布局、哥特体和罗马体混合字体以及字体和布局复杂性等挑战性历史源材料的能力，这对于计算机视觉在历史档案处理领域具有重要意义。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>效率和成本效益：</strong> LLM流水线比研究助理手动构建数据集快795倍，成本降低205倍。这极大地降低了大规模历史数据集构建的门槛。
*   <strong>数据质量：</strong> 基准测试结果表明，多模态LLMs在专利条目转录（Character Error Rate - CER）方面表现优于研究助理，并且在变量提取方面也达到了很高的准确率（整体变量提取准确率达到95.07%）。这为LLMs生成高质量历史数据集提供了初步证据。
*   <strong>范式转变：</strong> 研究认为，多模态LLMs代表了经济史数据集构建方式的范式转变，使得研究人员能够按需生成数据集，并可能促进数据集的开放共享。
*   <strong>开源贡献：</strong> 开源的流水线、数据集和基准测试结果，为其他研究人员提供了工具和参考，加速了经济史领域的研究。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>幻觉（Hallucinations）：</strong> LLMs可能产生“输入冲突型幻觉”，即生成与源图像不符的信息。虽然作者通过基准测试评估了幻觉问题，但仍存在一些轻微的幻觉（如错误识别字符、历史长s问题）。
*   <strong>对复杂布局的挑战：</strong> 尽管流水线能够处理双栏布局，但跨页和跨栏的条目截断仍需要额外的修复步骤。
*   <strong>模型选择的依赖性：</strong> 作者选择了专有的Gemini模型，并指出其性能和成本是选择的关键因素。未来随着开源模型的发展，可能会有新的选择。
*   <strong>对历史长s的识别困难：</strong> Gemini模型在处理历史长s方面存在一定困难，这影响了某些变量的提取准确率。
*   <strong>数据集规模的验证挑战：</strong> 对于如此大规模的数据集（306,070个专利条目），对所有数据进行人工验证几乎不可能，研究结果的泛化能力依赖于基准测试的代表性。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>处理手写体和低资源语言：</strong> 评估LLMs在处理手写体和非英语等低资源语言的历史文档时的性能。
*   <strong>端到端处理多页PDF：</strong> 开发能够直接处理多页PDF文件而无需分块的LLM方法，以简化流程。
*   <strong>图像即数据（Image-as-Data）方法：</strong> 探索直接处理视频等更复杂的媒体格式，以捕捉更细微的信息。
*   <strong>开源模型的进一步评估：</strong> 随着开源LLMs性能的提升，评估其在历史数据集构建中的可行性。
*   <strong>对LLM幻觉的深入研究和缓解：</strong> 进一步研究LLM幻觉的机制，并开发更有效的缓解策略。
*   <strong>对经济史研究范式的影响：</strong> 探讨LLMs如何改变经济史研究的实践、研究问题和研究助理的角色。</p>
<p><strong>对计算机视觉领域的意义：</strong>
这篇论文展示了多模态LLMs在处理复杂、低质量、非结构化历史图像数据方面的强大能力，这对于计算机视觉在历史档案数字化、信息提取和知识发现等领域的应用具有重要启示。它证明了LLMs不仅能进行文本识别（OCR），还能理解图像布局、上下文信息，并从中提取结构化数据，为计算机视觉技术在人文社科领域的深度融合开辟了新的道路。特别是其在处理双栏布局、混合字体等挑战性场景下的成功应用，为其他需要处理类似复杂文档的计算机视觉任务提供了宝贵的经验和方法论参考。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We open-source our benchmarking and patent datasets as well as our LLM-based data pipeline, which can be easily adapted to other image corpora using LLM-assisted coding tools, lowering the barriers for less technical researchers.</li>
<li>Finally, we explain the economics of deploying LLMs for historical dataset construction and conclude by speculating on the potential implications for the field of economic history.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.19675v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.19675v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.19648v1'></a></p>
<h2 id="4d-gaussian-splatting-as-a-learned-dynamical-system"><a href="https://arxiv.org/abs/2512.19648v1">4D Gaussian Splatting as a Learned Dynamical System</a></h2>
<p><strong>Authors:</strong> Arnold Caleb Asiimwe, Carl Vondrick</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We reinterpret 4D Gaussian Splatting as a continuous-time dynamical system, where scene motion arises from integrating a learned neural dynamical field rather than applying per-frame deformations. This formulation, which we call EvoGS, treats the Gaussian representation as an evolving physical system whose state evolves continuously under a learned motion law. This unlocks capabilities absent in deformation-based approaches:(1) sample-efficient learning from sparse temporal supervision by modeling the underlying motion law; (2) temporal extrapolation enabling forward and backward prediction beyond observed time ranges; and (3) compositional dynamics that allow localized dynamics injection for controllable scene synthesis. Experiments on dynamic scene benchmarks show that EvoGS achieves better motion coherence and temporal consistency compared to deformation-field baselines while maintaining real-time rendering</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“4D Gaussian Splatting as a Learned Dynamical System”的全面中文摘要，其中包含了您要求的各个方面：</p>
<p><strong>论文题目：</strong> 4D Gaussian Splatting as a Learned Dynamical System (将4D高斯泼溅视为一个学习到的动力学系统)</p>
<p><strong>作者：</strong> Arnold Caleb Asiimwe, Carl Vondrick</p>
<hr />
<p><strong>摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>传统动态场景重建方法（如基于变形场的方法）通常将时间视为离散的帧序列，并依赖于每帧的变形来捕捉场景的运动。这种方法在处理稀疏、不规则或缺失的帧时存在局限性，容易导致运动不连贯、时间上不一致，并且难以进行时间上的外插（预测未来或回溯过去）。此外，这些方法通常需要密集的时序监督，并且计算成本较高。</p>
<p><strong>2. 主要创新点/方法贡献：</strong></p>
<p>本文提出了一种名为 <strong>EvoGS</strong> 的新框架，将4D高斯泼溅（4D Gaussian Splatting）重新诠释为一个<strong>连续时间动力学系统</strong>。其核心创新在于：</p>
<ul>
<li><strong>连续时间动力学建模：</strong> EvoGS将场景中的高斯原语视为一个物理系统中的粒子，其状态（位置、旋转、尺度、颜色、不透明度）由一个学习到的<strong>连续时间速度场</strong>直接驱动。运动不再是离散的帧间变形，而是通过数值积分（使用RK4求解器）来连续演化。</li>
<li><strong>学习运动定律而非变形：</strong> 模型直接学习一个<strong>神经网络动力学定律</strong>，预测高斯属性的时间导数，而不是学习每帧的独立变形。</li>
<li><strong>样本效率和鲁棒性：</strong> 通过建模底层的运动定律，EvoGS能够从稀疏的时序监督中学习，并且对不规则的帧采样更加鲁棒。</li>
<li><strong>时间外插能力：</strong> 连续积分使得模型能够进行<strong>前向和后向的时间外插</strong>，预测未见过的帧，甚至在观察时间范围之外进行推断。</li>
<li><strong>可控的动态合成：</strong> EvoGS支持<strong>组合式动力学</strong>，允许通过注入外部速度场来对局部动态进行可控的编辑和合成，而无需重新训练。</li>
<li><strong>高斯航点（Gaussian Waypoints）用于运动稳定：</strong> 为了缓解长期积分可能产生的漂移，引入了稀疏的“航点”作为伪观测，用于定期重新初始化和约束积分过程。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>性能提升：</strong> 在动态场景基准测试中，EvoGS在运动连贯性和时间一致性方面优于基于变形场的方法。</li>
<li><strong>时间外插能力：</strong> 实验证明了EvoGS在预测未来帧和回溯过去帧方面的出色能力，尤其是在稀疏帧训练的情况下。</li>
<li><strong>实时渲染：</strong> 在实现这些高级功能的同时，EvoGS仍然保持了高斯泼溅的实时渲染效率。</li>
<li><strong>可控编辑：</strong> 通过向量场代数，EvoGS能够实现对动态场景的局部编辑和新动态的注入，为动态场景的合成和控制提供了新的可能性。</li>
<li><strong>统一的表示：</strong> EvoGS提供了一个统一的框架，将重建、插值、外插和可控动态合成整合到一个连续的时间动力学空间中。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong></p>
<ul>
<li><strong>数据驱动的局限性：</strong> EvoGS作为一种数据驱动的方法，会继承训练视频中的偏见和模糊性。</li>
<li><strong>物理推理的不足：</strong> 在需要真正因果或物理推理的场景中，例如液体填充玻璃的例子（图11），EvoGS可以推断刚体（手和杯子）的运动，但无法预测流体行为或水杯相互作用，这些现象超出了训练数据的时空模式。</li>
<li><strong>极端稀疏情况下的退化：</strong> 在极端的时间稀疏性下，动力学变得欠约束，模型会逐渐退化到类似变形的行为。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>更强的物理理解：</strong> 探索如何将更强的物理规律或因果推理能力融入到动力学模型中，以处理更复杂的物理现象。</li>
<li><strong>生成式4D场景：</strong> 将EvoGS的连续时间动力学模型与生成模型相结合，以生成具有物理可信度、可编辑的4D场景。</li>
<li><strong>跨模态控制：</strong> 探索将文本、音频或其他外部信号作为输入，来控制动态场景的演化。</li>
<li><strong>大规模应用：</strong> 将EvoGS的框架应用于更大规模的数据集，或作为视频生成模型与3D表示之间的接口。</li>
</ul>
<p><strong>总结：</strong></p>
<p>EvoGS通过将4D高斯泼溅建模为一个连续时间动力学系统，成功地解决了传统动态场景重建方法在处理稀疏数据、时间外插和可控动态合成方面的挑战。其核心贡献在于学习一个连续的速度场来驱动高斯原语的演化，从而实现了更鲁棒、更灵活、更具表现力的动态场景表示和生成。这为未来的动态场景理解和生成研究开辟了新的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This unlocks capabilities absent in deformation-based approaches:(1) sample-efficient learning from sparse temporal supervision by modeling the underlying motion law; (2) temporal extrapolation enabling forward and backward prediction beyond observed time ranges; and (3) compositional dynamics that allow localized dynamics injection for controllable scene synthesis.</li>
<li>Experiments on dynamic scene benchmarks show that EvoGS achieves better motion coherence and temporal consistency compared to deformation-field baselines while maintaining real-time rendering</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.19648v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.19648v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.19629v1'></a></p>
<h2 id="logoplanner-localization-grounded-navigation-policy-with-metric-aware-visual-geometry"><a href="https://arxiv.org/abs/2512.19629v1">LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry</a></h2>
<p><strong>Authors:</strong> Jiaqi Peng, Wenzhe Cai, Yuqiang Yang, Tai Wang, Yuan Shen, Jiangmiao Pang</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Categories:</strong> cs.RO, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Trajectory planning in unstructured environments is a fundamental and challenging capability for mobile robots. Traditional modular pipelines suffer from latency and cascading errors across perception, localization, mapping, and planning modules. Recent end-to-end learning methods map raw visual observations directly to control signals or trajectories, promising greater performance and efficiency in open-world settings. However, most prior end-to-end approaches still rely on separate localization modules that depend on accurate sensor extrinsic calibration for self-state estimation, thereby limiting generalization across embodiments and environments. We introduce LoGoPlanner, a localization-grounded, end-to-end navigation framework that addresses these limitations by: (1) finetuning a long-horizon visual-geometry backbone to ground predictions with absolute metric scale, thereby providing implicit state estimation for accurate localization; (2) reconstructing surrounding scene geometry from historical observations to supply dense, fine-grained environmental awareness for reliable obstacle avoidance; and (3) conditioning the policy on implicit geometry bootstrapped by the aforementioned auxiliary tasks, thereby reducing error propagation.We evaluate LoGoPlanner in both simulation and real-world settings, where its fully end-to-end design reduces cumulative error while metric-aware geometry memory enhances planning consistency and obstacle avoidance, leading to more than a 27.3\% improvement over oracle-localization baselines and strong generalization across embodiments and environments. The code and models have been made publicly available on the \href{https://steinate.github.io/logoplanner.github.io/}{project page}.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry”的全面中文摘要：</p>
<p><strong>论文题目：</strong> LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry</p>
<p><strong>作者：</strong> Jiaqi Peng, Wenzhe Cai, Yuqiang Yang, Tai Wang, Yuan Shen, Jiangmiao Pang</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题：</strong>
本文旨在解决移动机器人在非结构化环境中进行轨迹规划的根本性挑战。传统的模块化导航方法（感知、定位、建图、规划）存在延迟和级联误差的问题。虽然端到端学习方法有所改进，但它们通常仍依赖于需要精确传感器外参标定的独立定位模块，这限制了其在不同机器人形态和环境下的泛化能力。此外，现有方法在处理长期历史信息、提供精细几何感知以及避免误差累积方面存在不足。</p>
<p><strong>2. 主要创新与方法贡献：</strong>
LoGoPlanner 提出了一种<strong>定位驱动的端到端导航框架</strong>，通过以下关键创新来解决上述问题：</p>
<ul>
<li><strong>隐式状态估计与度量尺度感知：</strong> 通过微调一个长时序的视觉-几何骨干网络，并注入深度信息作为场景度量尺度先验，实现了对绝对度量尺度的感知。这使得模型能够进行隐式的自状态估计，从而实现精确的定位，而无需外部定位模块。</li>
<li><strong>度量感知场景几何重建：</strong> 利用历史视觉观测重建周围场景的密集、精细几何信息，为可靠的避障提供支持。</li>
<li><strong>基于隐式几何的策略条件化：</strong> 将上述辅助任务（度量尺度感知和几何重建）产生的隐式几何信息作为策略的条件，从而减少误差传播。</li>
<li><strong>查询驱动的统一框架：</strong> 采用查询驱动的设计，通过状态查询和几何查询提取隐式状态表示和环境几何信息，并将其与目标嵌入融合，形成统一的规划上下文。</li>
<li><strong>扩散模型轨迹生成：</strong> 使用扩散模型作为策略的末端，迭代地优化噪声动作，生成可行且无碰撞的轨迹。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>仿真实验：</strong> LoGoPlanner 在仿真环境中取得了显著的性能提升，相比于依赖“神谕式”定位的基线方法，成功率（SR）提升了 27.3%。
*   <strong>真实世界实验：</strong> 在不同机器人平台（TurtleBot, Unitree Go2, Unitree G1）和多样化的真实世界场景（办公室、家庭、工业）中，LoGoPlanner 展现了强大的泛化能力，能够实现准确的自定位和可靠的无碰撞轨迹规划，即使在相机抖动等挑战性条件下也能表现良好。
*   <strong>意义：</strong> LoGoPlanner 证明了将导航策略与度量感知的几何先验相结合的潜力，为在非结构化真实世界环境中实现更自主、可靠和适应性强的机器人导航指明了方向。它减少了对外部定位模块的依赖，降低了部署复杂性，并缩小了仿真到现实的差距。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>真实世界场景数量有限：</strong> 论文提到，由于可用的真实世界导航场景数量有限（约 2k），在真实世界环境中的重建性能尚不理想。作者正在训练使用真实世界度量尺度数据集来增强这方面的性能。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>增强真实世界场景下的几何重建：</strong> 通过在真实世界度量尺度数据集上进行训练，进一步提升模型在复杂真实世界环境中的几何重建能力。
*   <strong>更广泛的泛化性探索：</strong> 进一步探索在更多样化的机器人平台、传感器配置和更具挑战性的动态环境中 LoGoPlanner 的泛化能力。
*   <strong>实时性优化：</strong> 尽管 LoGoPlanner 效率较高，但对于资源受限的嵌入式系统，进一步优化计算效率以实现更快的实时响应可能是一个方向。
*   <strong>多模态融合：</strong> 探索将其他传感器信息（如激光雷达）与视觉信息融合，以进一步提升定位和导航的鲁棒性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce LoGoPlanner, a localization-grounded, end-to-end navigation framework that addresses these limitations by: (1) finetuning a long-horizon visual-geometry backbone to ground predictions with absolute metric scale, thereby providing implicit state estimation for accurate localization; (2) reconstructing surrounding scene geometry from historical observations to supply dense, fine-grained environmental awareness for reliable obstacle avoidance; and (3) conditioning the policy on implicit geometry bootstrapped by the aforementioned auxiliary tasks, thereby reducing error propagation.We evaluate LoGoPlanner in both simulation and real-world settings, where its fully end-to-end design reduces cumulative error while metric-aware geometry memory enhances planning consistency and obstacle avoidance, leading to more than a 27.3\% improvement over oracle-localization baselines and strong generalization across embodiments and environments.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.19629v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.19629v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-12-23 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
