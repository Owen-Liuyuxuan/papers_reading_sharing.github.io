<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-11-27 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-27
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-11-26/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-11-28/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-11-27">Arxiv Computer Vision Papers - 2025-11-27</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#qwen3-vl-technical-report" class="nav-link">Qwen3-VL Technical Report</a>
                </li>
                <li class="nav-item">
                    <a href="#canvas-to-image-compositional-image-generation-with-multimodal-controls" class="nav-link">Canvas-to-Image: Compositional Image Generation with Multimodal Controls</a>
                </li>
                <li class="nav-item">
                    <a href="#tracegen-world-modeling-in-3d-trace-space-enables-learning-from-cross-embodiment-videos" class="nav-link">TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos</a>
                </li>
                <li class="nav-item">
                    <a href="#g2vlm-geometry-grounded-vision-language-model-with-unified-3d-reconstruction-and-spatial-reasoning" class="nav-link">G^2VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning</a>
                </li>
                <li class="nav-item">
                    <a href="#seeing-without-pixels-perception-from-camera-trajectories" class="nav-link">Seeing without Pixels: Perception from Camera Trajectories</a>
                </li>
                <li class="nav-item">
                    <a href="#uncertainty-quantification-for-visual-object-pose-estimation" class="nav-link">Uncertainty Quantification for Visual Object Pose Estimation</a>
                </li>
                <li class="nav-item">
                    <a href="#multi-crit-benchmarking-multimodal-judges-on-pluralistic-criteria-following" class="nav-link">Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following</a>
                </li>
                <li class="nav-item">
                    <a href="#continual-error-correction-on-low-resource-devices" class="nav-link">Continual Error Correction on Low-Resource Devices</a>
                </li>
                <li class="nav-item">
                    <a href="#mechanisms-of-non-monotonic-scaling-in-vision-transformers" class="nav-link">Mechanisms of Non-Monotonic Scaling in Vision Transformers</a>
                </li>
                <li class="nav-item">
                    <a href="#mogan-improving-motion-quality-in-video-diffusion-via-few-step-motion-adversarial-post-training" class="nav-link">MoGAN: Improving Motion Quality in Video Diffusion via Few-Step Motion Adversarial Post-Training</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-11-27">Arxiv Computer Vision Papers - 2025-11-27</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，作为一名专注于计算机视觉和机器学习的研究助理，我将为您提供一份简明的 Arxiv 计算机视觉领域论文的每日报告执行摘要。</p>
<hr />
<p><strong>Arxiv 计算机视觉领域论文每日报告 - 执行摘要 (2025-11-26)</strong></p>
<p><strong>报告日期：</strong> 2025-11-26
<strong>涵盖论文数量：</strong> 10 篇</p>
<p><strong>1. 主要主题与趋势：</strong></p>
<p>本期 Arxiv 论文集中体现了计算机视觉领域在以下几个关键方向的深入探索：</p>
<ul>
<li><strong>多模态理解与生成：</strong> 视觉语言模型 (VLM) 的发展依然是热点，尤其是在结合更丰富的模态（如 3D 信息、几何约束）以实现更强大的理解和生成能力方面。</li>
<li><strong>3D 理解与重建：</strong> 从不同视角或不同载体（如跨具身视频）学习 3D 世界模型，以及利用 3D 重建来增强视觉语言模型的能力，是重要的研究方向。</li>
<li><strong>生成模型的进步：</strong> 在图像生成和视频生成领域，研究人员正致力于提高生成质量、可控性以及处理更复杂的场景（如运动质量）。</li>
<li><strong>模型鲁棒性与效率：</strong> 针对低资源设备上的持续学习、不确定性量化以及对模型内部机制的理解（如 Vision Transformers 的非单调缩放），也展现了对模型实用性和可解释性的关注。</li>
</ul>
<p><strong>2. 突出亮点与创新：</strong></p>
<ul>
<li><strong>Qwen3-VL Technical Report (1):</strong> 作为一项技术报告，它很可能代表了当前大型视觉语言模型在能力和架构上的最新进展，是了解行业前沿的重要参考。</li>
<li><strong>G<script type="math/tex">^2</script>VLM: Geometry Grounded Vision Language Model (4):</strong> 该工作将几何约束和 3D 重建深度整合到 VLM 中，有望显著提升模型在空间推理和理解方面的能力，是多模态融合的一个重要突破。</li>
<li><strong>TraceGen: World Modeling in 3D Trace Space (3):</strong> 提出了一种在 3D 轨迹空间中进行世界建模的方法，能够从跨具身视频中学习，这为机器人感知和通用世界模型提供了新的视角。</li>
<li><strong>Seeing without Pixels: Perception from Camera Trajectories (5):</strong> 这是一个非常新颖的思路，探索了仅凭相机轨迹信息进行感知，可能为低功耗或特殊场景下的感知任务带来颠覆性解决方案。</li>
</ul>
<p><strong>3. 新兴研究方向与技术：</strong></p>
<ul>
<li><strong>几何与 3D 感知驱动的 VLM：</strong> 将 3D 重建、几何推理等技术深度融入 VLM，以实现更具物理现实感的理解和生成。</li>
<li><strong>跨具身学习：</strong> 利用不同机器人或载体产生的视频数据进行通用世界模型的学习，克服单一样本的局限性。</li>
<li><strong>轨迹信息作为感知信号：</strong> 探索非像素信息（如相机运动）在感知任务中的潜力。</li>
<li><strong>生成模型中的运动质量提升：</strong> 针对视频生成，研究如何更有效地控制和提升运动的真实感。</li>
<li><strong>模型可解释性与鲁棒性：</strong> 深入理解 Transformer 类模型的内部工作机制，以及在低资源环境下的持续学习能力。</li>
</ul>
<p><strong>4. 建议阅读全文的论文：</strong></p>
<p>考虑到其潜在的创新性和对领域的影响力，以下论文强烈建议阅读全文：</p>
<ul>
<li><strong>Qwen3-VL Technical Report (1):</strong> 了解当前最先进 VLM 的技术细节和能力边界。</li>
<li><strong>G<script type="math/tex">^2</script>VLM: Geometry Grounded Vision Language Model (4):</strong> 探索几何与 VLM 结合的强大潜力。</li>
<li><strong>TraceGen: World Modeling in 3D Trace Space (3):</strong> 学习如何从多样的具身数据中构建世界模型。</li>
<li><strong>Seeing without Pixels: Perception from Camera Trajectories (5):</strong> 了解一种全新的感知范式。</li>
</ul>
<hr />
<p>这份摘要旨在帮助您快速把握本期 Arxiv 论文的核心内容和发展趋势。希望对您的研究工作有所助益！</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2511.21631v1">Qwen3-VL Technical Report</a></li>
<li><a href="#2511.21691v1">Canvas-to-Image: Compositional Image Generation with Multimodal Controls</a></li>
<li><a href="#2511.21690v1">TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos</a></li>
<li><a href="#2511.21688v1">G<script type="math/tex">^2</script>VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning</a></li>
<li><a href="#2511.21681v1">Seeing without Pixels: Perception from Camera Trajectories</a></li>
<li><a href="#2511.21666v1">Uncertainty Quantification for Visual Object Pose Estimation</a></li>
<li><a href="#2511.21662v1">Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following</a></li>
<li><a href="#2511.21652v1">Continual Error Correction on Low-Resource Devices</a></li>
<li><a href="#2511.21635v1">Mechanisms of Non-Monotonic Scaling in Vision Transformers</a></li>
<li><a href="#2511.21592v1">MoGAN: Improving Motion Quality in Video Diffusion via Few-Step Motion Adversarial Post-Training</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2511.21631v1'></a></p>
<h2 id="qwen3-vl-technical-report"><a href="https://arxiv.org/abs/2511.21631v1">Qwen3-VL Technical Report</a></h2>
<p><strong>Authors:</strong> Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, Wenbin Ge, Zhifang Guo, Qidong Huang, Jie Huang, Fei Huang, Binyuan Hui, Shutong Jiang, Zhaohai Li, Mingsheng Li, Mei Li, Kaixin Li, Zicheng Lin, Junyang Lin, Xuejing Liu, Jiawei Liu, Chenglong Liu, Yang Liu, Dayiheng Liu, Shixuan Liu, Dunjie Lu, Ruilin Luo, Chenxu Lv, Rui Men, Lingchen Meng, Xuancheng Ren, Xingzhang Ren, Sibo Song, Yuchong Sun, Jun Tang, Jianhong Tu, Jianqiang Wan, Peng Wang, Pengfei Wang, Qiuyue Wang, Yuxuan Wang, Tianbao Xie, Yiheng Xu, Haiyang Xu, Jin Xu, Zhibo Yang, Mingkun Yang, Jianxin Yang, An Yang, Bowen Yu, Fei Zhang, Hang Zhang, Xi Zhang, Bo Zheng, Humen Zhong, Jingren Zhou, Fan Zhou, Jing Zhou, Yuanzhi Zhu, Ke Zhu</p>
<p><strong>Published:</strong> 2025-11-26</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-text understanding, surpassing comparable text-only backbones in several cases; (ii) robust long-context comprehension with a native 256K-token window for both text and interleaved multimodal inputs, enabling faithful retention, retrieval, and cross-referencing across long documents and videos; and (iii) advanced multimodal reasoning across single-image, multi-image, and video tasks, demonstrating leading performance on comprehensive evaluations such as MMMU and visual-math benchmarks (e.g., MathVista and MathVision). Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding. Under comparable token budgets and latency constraints, Qwen3-VL achieves superior performance in both dense and Mixture-of-Experts (MoE) architectures. We envision Qwen3-VL serving as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world workflows.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对“Qwen3-VL Technical Report”论文的全面中文摘要，重点关注其在计算机视觉领域的创新和重要性：</p>
<p><strong>论文标题：</strong> Qwen3-VL Technical Report</p>
<p><strong>作者：</strong> Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, Wenbin Ge, Zhifang Guo, Qidong Huang, Jie Huang, Fei Huang, Binyuan Hui, Shutong Jiang, Zhaohai Li, Mingsheng Li, Mei Li, Kaixin Li, Zicheng Lin, Junyang Lin, Xuejing Liu, Jiawei Liu, Chenglong Liu, Yang Liu, Dayiheng Liu, Shixuan Liu, Dunjie Lu, Ruilin Luo, Chenxu Lv, Rui Men, Lingchen Meng, Xuancheng Ren, Xingzhang Ren, Sibo Song, Yuchong Sun, Jun Tang, Jianhong Tu, Jianqiang Wan, Peng Wang, Pengfei Wang, Qiuyue Wang, Yuxuan Wang, Tianbao Xie, Yiheng Xu, Haiyang Xu, Jin Xu, Zhibo Yang, Mingkun Yang, Jianxin Yang, An Yang, Bowen Yu, Fei Zhang, Hang Zhang, Xi Zhang, Bo Zheng, Humen Zhong, Jingren Zhou, Fan Zhou, Jing Zhou, Yuanzhi Zhu, Ke Zhu</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决当前多模态大模型在理解和生成能力上的局限性，特别是在长上下文理解、跨模态推理以及在复杂现实世界任务中的应用方面。研究的核心问题是如何构建一个更强大、更通用、性能更优越的视觉-语言模型（VLM），使其能够无缝地整合文本、图像和视频，并在广泛的多模态基准测试中取得领先地位，同时保持甚至超越纯文本模型的性能。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
Qwen3-VL 的主要创新体现在以下三个核心支柱和三个关键架构升级：</p>
<ul>
<li>
<p><strong>核心支柱：</strong></p>
<ul>
<li><strong>更强的纯文本理解能力：</strong> 在多模态训练的同时，模型在纯文本任务上也能达到甚至超越同等规模的纯文本模型。</li>
<li><strong>强大的长上下文理解能力：</strong> 原生支持高达 256K token 的上下文窗口，能够忠实地保留、检索和交叉引用长文档和视频中的信息。</li>
<li><strong>先进的多模态推理能力：</strong> 在单图像、多图像和视频任务上展现出领先性能，尤其是在 MMMU 和视觉数学等综合评估中。</li>
</ul>
</li>
<li>
<p><strong>关键架构升级：</strong></p>
<ul>
<li><strong>增强的交错式 MROPE (Interleaved MROPE)：</strong> 改进了位置编码机制，通过将时间 (t)、水平 (h) 和垂直 (w) 组件交错分布在嵌入维度中，实现了更均衡的频率谱，从而提升了对长视频的时空建模能力。</li>
<li><strong>DeepStack 集成：</strong> 引入了 DeepStack 机制，将视觉编码器多层级的特征注入到 LLM 的对应层中，增强了多层级融合，从而更紧密地对齐视觉和语言信息。</li>
<li><strong>文本时间对齐 (Text-based Time Alignment)：</strong> 针对视频，从 T-ROPE 演进到显式的文本时间戳对齐，为视频提供更精确的时间定位。</li>
</ul>
</li>
</ul>
<p>此外，论文还强调了以下方法论上的改进：
*   <strong>优化的训练策略：</strong> 采用分阶段的预训练（包括冻结部分层进行对齐，然后进行全参数训练，并逐步扩展上下文长度）和多阶段的后训练（SFT、知识蒸馏、强化学习）。
*   <strong>高质量、多样化的训练数据：</strong> 大幅扩展和精炼了图像-文本对、交错式文本-图像序列、OCR、文档解析、长文档理解、3D/空间理解、多模态代码、视频理解以及 STEM 等领域的数据集。
*   <strong>模型变体：</strong> 提供了多种模型变体，包括密集模型（2B/4B/8B/32B）和混合专家（MoE）模型（30B-A3B/235B-A22B），以适应不同的延迟-质量权衡需求。
*   <strong>“思考”与“非思考”变体：</strong> 区分了两种后训练模式，以满足不同的应用需求。</p>
<p><strong>3. 主要结果及其意义：</strong>
Qwen3-VL 在广泛的多模态基准测试中取得了显著的性能提升，并在许多任务上达到了最先进（SOTA）的水平。
*   在多模态推理方面，Qwen3-VL 在 MMMU、MathVista 等基准上表现出色。
*   在长上下文理解方面，其 256K token 的上下文窗口使其在处理长文档和视频时具有显著优势，并在“Needle-in-a-Haystack”任务中展现了完美的准确率。
*   在视觉-语言对齐和指令遵循方面，Qwen3-VL 在 MM-MT-Bench、HallusionBench 和 MIA-Bench 等基准上超越了其他模型。
*   在 OCR、文档理解、2D/3D 视觉接地、视频理解等领域，Qwen3-VL 也取得了非常有竞争力的结果。
*   其多语言 OCR 能力也得到了显著增强，在 39 种语言中实现了超过 70% 的准确率。</p>
<p>这些结果表明 Qwen3-VL 是一个非常强大和通用的多模态基础模型，为现实世界中的图像理解推理、智能体决策和多模态代码智能等应用奠定了坚实的基础。</p>
<p><strong>4. 论文中提到的局限性：</strong>
论文中虽然没有明确列出局限性，但可以推断出一些潜在的挑战：
*   <strong>计算资源需求：</strong> 训练如此大规模的模型需要巨大的计算资源，这可能限制了其在资源受限环境中的部署和进一步研究。
*   <strong>模型规模与延迟的权衡：</strong> 尽管提供了不同规模的模型，但在某些对延迟要求极高的场景下，即使是较小的模型也可能存在挑战。
*   <strong>“思考”模式的解释性：</strong> 虽然“思考”模式提升了推理能力，但其内部的推理过程可能仍然是一个“黑箱”，需要进一步的解释和理解。
*   <strong>数据偏差和公平性：</strong> 尽管努力构建多样化的数据集，但任何大规模数据集都可能存在潜在的偏差，这可能影响模型在某些特定场景下的表现。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
论文展望了 Qwen3-VL 在未来研究中的应用和发展方向：
*   <strong>作为智能体（Agent）的基础引擎：</strong> Qwen3-VL 将作为智能体（Agent）的基础，实现数字世界和物理世界的无缝连接，能够感知、推理并执行复杂的、上下文感知的动作。
*   <strong>交互式感知（Interactive Perception）：</strong> 进一步提升模型与环境的交互能力，使其能够主动探索和理解信息。
*   <strong>工具增强推理（Tool-Augmented Reasoning）：</strong> 探索更高级的工具使用和集成，以解决更复杂的问题。
*   <strong>实时多模态控制（Real-time Multimodal Control）：</strong> 实现更流畅、实时的多模态控制能力，以指导机器人系统。
*   <strong>统一的理解-生成架构（Unified Understanding-Generation Architectures）：</strong> 探索能够同时进行深度理解和高质量生成的模型架构。
*   <strong>跨领域泛化和适应性：</strong> 进一步提升模型在未见过领域和任务上的泛化能力和适应性。</p>
<p><strong>总结：</strong></p>
<p>Qwen3-VL 技术报告详细介绍了 Qwen 系列最新一代的视觉-语言模型。该模型通过创新的架构设计（如交错式 MROPE、DeepStack 集成和文本时间对齐）以及海量高质量的多模态数据训练，在长上下文理解、多模态推理和跨模态对齐等方面取得了突破性进展，并在众多基准测试中刷新了最先进水平。Qwen3-VL 的出现不仅是多模态大模型领域的重要里程碑，也为未来构建更强大、更通用的智能体和实现更深层次的人机交互奠定了坚实的基础。其在多种模型规模和架构上的可用性，以及在多语言支持上的显著进步，都使其成为一个极具潜力的通用多模态基础模型。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks.</li>
<li>Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.21631v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.21631v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.21691v1'></a></p>
<h2 id="canvas-to-image-compositional-image-generation-with-multimodal-controls"><a href="https://arxiv.org/abs/2511.21691v1">Canvas-to-Image: Compositional Image Generation with Multimodal Controls</a></h2>
<p><strong>Authors:</strong> Yusuf Dalva, Guocheng Gordon Qian, Maya Goldenberg, Tsai-Shien Chen, Kfir Aberman, Sergey Tulyakov, Pinar Yanardag, Kuan-Chieh Jackson Wang</p>
<p><strong>Published:</strong> 2025-11-26</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>While modern diffusion models excel at generating high-quality and diverse images, they still struggle with high-fidelity compositional and multimodal control, particularly when users simultaneously specify text prompts, subject references, spatial arrangements, pose constraints, and layout annotations. We introduce Canvas-to-Image, a unified framework that consolidates these heterogeneous controls into a single canvas interface, enabling users to generate images that faithfully reflect their intent. Our key idea is to encode diverse control signals into a single composite canvas image that the model can directly interpret for integrated visual-spatial reasoning. We further curate a suite of multi-task datasets and propose a Multi-Task Canvas Training strategy that optimizes the diffusion model to jointly understand and integrate heterogeneous controls into text-to-image generation within a unified learning paradigm. This joint training enables Canvas-to-Image to reason across multiple control modalities rather than relying on task-specific heuristics, and it generalizes well to multi-control scenarios during inference. Extensive experiments show that Canvas-to-Image significantly outperforms state-of-the-art methods in identity preservation and control adherence across challenging benchmarks, including multi-person composition, pose-controlled composition, layout-constrained generation, and multi-control generation.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：Canvas-to-Image: Compositional Image Generation with Multimodal Controls</strong></p>
<p><strong>1. 论文的主要贡献 (2-3句话的简洁总结)</strong></p>
<p>这篇论文提出了一种名为 Canvas-to-Image 的新颖框架，旨在解决当前扩散模型在处理复杂、多模态图像生成控制方面的不足。其核心贡献在于将文本提示、对象参考、空间布局、姿态约束等异构控制信号统一编码到一个“画布”接口中，使模型能够直接理解并生成高度符合用户意图的图像。通过多任务数据集和训练策略，该框架实现了跨模态的联合推理，显著提升了图像生成在身份保持和控制遵循方面的表现。</p>
<p><strong>2. 关键创新点或方法论</strong></p>
<ul>
<li><strong>统一的“画布”接口 (Unified Canvas Interface):</strong> 这是最核心的创新。论文将原本分散、异构的控制信号（文本、对象、空间、姿态等）整合成一个单一的、可解释的“画布”图像。这个画布充当了模型理解和整合所有控制指令的媒介。</li>
<li><strong>异构控制信号的编码 (Encoding Heterogeneous Control Signals):</strong> 论文的关键在于如何将这些不同类型的控制信息有效地编码到画布中，使其能够被扩散模型直接理解。这可能涉及到多通道的图像表示，其中不同的通道或区域代表不同的控制类型。</li>
<li><strong>集成视觉-空间推理 (Integrated Visual-Spatial Reasoning):</strong> 通过将所有控制信号整合到画布中，模型被训练来执行“跨多个控制模态的联合推理”，而不是依赖于特定任务的启发式方法。这意味着模型能够同时考虑文本内容、对象位置、姿态要求等，并进行全局的、协同的推理。</li>
<li><strong>多任务数据集和训练策略 (Multi-Task Datasets and Training Strategy):</strong> 为了支持这种联合推理，论文专门构建了多任务数据集，并提出了“多任务画布训练”策略。这种策略旨在优化扩散模型，使其能够同时理解和整合这些异构控制信号，从而实现文本到图像生成。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<ul>
<li><strong>提升图像生成的可控性与精确性:</strong> Canvas-to-Image 的方法有望显著提高用户对生成图像的控制能力，尤其是在需要精确布局、对象交互和特定姿态的场景下。这将使扩散模型从“生成高质量但难以精确控制”的工具，转变为“能够精确实现用户创意”的强大助手。</li>
<li><strong>推动更复杂的场景生成:</strong> 论文的成果将为生成包含多个对象、复杂交互和精确空间关系的场景提供更可靠的解决方案，例如故事板生成、虚拟环境创建、产品设计等。</li>
<li><strong>统一多模态控制的范式:</strong> 将多种控制模态统一到一个画布接口中，可能为未来多模态图像生成的研究提供一种新的、更通用的范式，简化了用户与模型的交互方式。</li>
<li><strong>促进人机协作的创意工具:</strong> 这种高度可控的生成能力将极大地赋能设计师、艺术家和内容创作者，使他们能够更高效地将想法转化为视觉作品。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>内容创作与设计:</strong> 广告设计、插画创作、游戏资产生成、电影场景预演等。</li>
<li><strong>虚拟现实与增强现实 (VR/AR):</strong> 快速生成符合特定场景布局和对象要求的虚拟环境或AR叠加内容。</li>
<li><strong>机器人与自动化:</strong> 生成机器人执行任务的场景模拟，或根据指令生成特定布局的图像用于训练。</li>
<li><strong>教育与培训:</strong> 创建包含特定元素和布局的教学材料。</li>
<li><strong>个性化内容生成:</strong> 根据用户详细的描述和布局要求生成个性化图像。</li>
<li><strong>医学影像:</strong> 在特定解剖结构和相对位置要求下生成合成医学影像。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<ul>
<li><strong>画布表示的复杂性:</strong> 如何设计一个既能有效编码所有控制信息，又能被扩散模型高效处理的画布表示，本身就是一个技术挑战。摘要中未详细说明具体的编码机制。</li>
<li><strong>数据集的构建难度:</strong> 论文提到了“精心策划的多任务数据集”，这表明构建能够覆盖多种控制模态和组合的训练数据可能非常耗时且复杂。</li>
<li><strong>计算成本:</strong> 联合训练一个能够处理如此复杂多模态输入的扩散模型，很可能需要巨大的计算资源和训练时间。</li>
<li><strong>泛化能力边界:</strong> 虽然摘要提到“泛化性好”，但对于非常极端或前所未有的控制组合，其泛化能力仍需在实际应用中验证。例如，如果用户尝试将完全不相关的对象放置在非常规的布局中，模型是否仍能生成合理结果。</li>
<li><strong>“高保真度”的定义:</strong> 摘要中提到“高保真度的组合和多模态控制”，但“高保真度”的具体衡量标准和在不同控制模态下的表现差异，需要通过实验结果来进一步确认。</li>
</ul>
<p>总而言之，这篇论文提出的 Canvas-to-Image 框架通过创新的画布接口和多任务训练策略，有望解决当前扩散模型在复杂多模态控制方面的瓶颈，为生成式AI在精确控制和创意实现方面开辟新的可能性。其核心在于将异构信息转化为统一的视觉-空间语言，实现更深层次的模型理解和推理。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce Canvas-to-Image, a unified framework that consolidates these heterogeneous controls into a single canvas interface, enabling users to generate images that faithfully reflect their intent.</li>
<li>Extensive experiments show that Canvas-to-Image significantly outperforms state-of-the-art methods in identity preservation and control adherence across challenging benchmarks, including multi-person composition, pose-controlled composition, layout-constrained generation, and multi-control generation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.21691v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.21691v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.21690v1'></a></p>
<h2 id="tracegen-world-modeling-in-3d-trace-space-enables-learning-from-cross-embodiment-videos"><a href="https://arxiv.org/abs/2511.21690v1">TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos</a></h2>
<p><strong>Authors:</strong> Seungjae Lee, Yoonkyo Jung, Inkook Chun, Yao-Chih Lee, Zikui Cai, Hongjia Huang, Aayush Talreja, Tan Dat Dao, Yongyuan Liang, Jia-Bin Huang, Furong Huang</p>
<p><strong>Published:</strong> 2025-11-26</p>
<p><strong>Categories:</strong> cs.RO, cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Learning new robot tasks on new platforms and in new scenes from only a handful of demonstrations remains challenging. While videos of other embodiments - humans and different robots - are abundant, differences in embodiment, camera, and environment hinder their direct use. We address the small-data problem by introducing a unifying, symbolic representation - a compact 3D "trace-space" of scene-level trajectories - that enables learning from cross-embodiment, cross-environment, and cross-task videos. We present TraceGen, a world model that predicts future motion in trace-space rather than pixel space, abstracting away appearance while retaining the geometric structure needed for manipulation. To train TraceGen at scale, we develop TraceForge, a data pipeline that transforms heterogeneous human and robot videos into consistent 3D traces, yielding a corpus of 123K videos and 1.8M observation-trace-language triplets. Pretraining on this corpus produces a transferable 3D motion prior that adapts efficiently: with just five target robot videos, TraceGen attains 80% success across four tasks while offering 50-600x faster inference than state-of-the-art video-based world models. In the more challenging case where only five uncalibrated human demonstration videos captured on a handheld phone are available, it still reaches 67.5% success on a real robot, highlighting TraceGen's ability to adapt across embodiments without relying on object detectors or heavy pixel-space generation.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos”的全面中文摘要：</p>
<p><strong>论文题目：</strong> TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos</p>
<p><strong>作者：</strong> Seungjae Lee, Yoonkyo Jung, Inkook Chun, Yao-Chih Lee, Zikui Cai, Hongjia Huang, Aayush Talreja, Tan Dat Dao, Yongyuan Liang, Jia-Bin Huang, Furong Huang</p>
<p><strong>摘要：</strong></p>
<p>这篇论文提出了一种名为 <strong>TraceGen</strong> 的新颖世界模型，旨在解决机器人学习新任务时面临的<strong>小样本数据挑战</strong>，尤其是在跨不同机器人平台、环境和任务时。研究的核心问题是：<strong>如何有效地利用海量但异构的视频数据（包括人类演示和不同机器人演示）来训练一个能够快速适应新机器人和新任务的世界模型？</strong></p>
<p><strong>主要创新和方法贡献：</strong></p>
<ol>
<li><strong>3D Trace-Space 表示：</strong> 论文引入了一个核心概念——<strong>3D Trace-Space</strong>。这是一种紧凑、符号化的表示，将场景中的轨迹（包括物体和末端执行器的运动）编码为一系列3D轨迹，从而抽象掉了外观和相机变化，只保留了对操作至关重要的几何结构信息。这种表示方式使得模型能够学习到<strong>跨具身（cross-embodiment）、跨环境（cross-environment）和跨任务（cross-task）</strong>的运动先验。</li>
<li><strong>TraceGen 世界模型：</strong> TraceGen 是一个基于流的模型（flow-based world model），它直接在3D Trace-Space 中预测未来的运动，而不是在像素空间中生成视频。这大大提高了计算效率，并避免了像素空间模型在背景和纹理上分配不必要算力的问题。</li>
<li><strong>TraceForge 数据引擎：</strong> 为了大规模训练 TraceGen，作者开发了一个名为 <strong>TraceForge</strong> 的数据处理流水线。TraceForge 能够将异构的视频数据（包括人类演示和各种机器人数据集）转换为一致的3D Trace标注。它通过补偿相机运动、从多视角重建轨迹以及进行速度重定向等技术，生成了一个包含123K个视频和1.8M个（观察-轨迹-语言）三元组的大规模语料库。</li>
<li><strong>高效的迁移学习和适应能力：</strong> 通过在 TraceForge-123K 语料库上进行预训练，TraceGen 获得了一个强大的3D运动先验。这使得模型能够通过<strong>轻量级的“热身”（warm-up）阶段</strong>，仅用少量（例如5个）目标机器人演示，就能快速适应新任务，并在机器人-机器人（Robot-Robot）适应设置中达到80%的成功率。</li>
</ol>
<p><strong>主要结果及其意义：</strong></p>
<ul>
<li><strong>机器人-机器人适应：</strong> 在仅有5个目标机器人演示的情况下，TraceGen 在四个任务上取得了 <strong>80% 的成功率</strong>，并且其推理速度比最先进的视频生成模型快 <strong>50-600倍</strong>。</li>
<li><strong>人类-机器人迁移：</strong> 在更具挑战性的人类-机器人迁移场景中，即使只使用5个未校准的、在手持手机上拍摄的人类演示视频（这些视频在背景、光照和物体位置上与机器人环境差异很大），TraceGen 也能在真实机器人上达到 <strong>67.5% 的成功率</strong>。这证明了 TraceGen 的3D Trace-Space 表示能够有效地桥接人类演示和机器人执行之间的鸿沟，且无需依赖物体检测器或复杂的像素生成。</li>
<li><strong>效率优势：</strong> TraceGen 的3D Trace-Space 推理比基于视频生成的基线模型快 <strong>3.8倍</strong>，比大型视频生成模型快 <strong>50倍以上</strong>。这对于需要实时闭环规划的机器人应用至关重要。</li>
<li><strong>预训练的重要性：</strong> 实验表明，跨具身预训练对于 TraceGen 的成功至关重要。与从头开始训练的模型相比，预训练模型在少量热身数据下表现出压倒性的优势。</li>
</ul>
<p><strong>论文中提到的局限性：</strong></p>
<ul>
<li><strong>零样本泛化能力：</strong> 尽管 TraceGen 的零样本生成能力很有前景，但在面对全新的具身（embodiment）或未知的环境时，其泛化能力尚未完全可靠，有时会产生物理上看似可行但实际上不可行的轨迹。</li>
<li><strong>精细操作的细节：</strong> 对于需要极高精度的精细操作任务，生成的轨迹可能缺乏足够的细节，导致机器人难以精确执行。</li>
<li><strong>数据质量：</strong> 训练数据中存在一些效率低下或包含纠正性动作的演示，尽管进行了额外的过滤，但仍可能存在一些噪声。</li>
<li><strong>插值方法：</strong> 模型使用了线性插值（ODE集成）来生成轨迹，但尚未探索其他插值调度或机制来明确控制模糊任务的轨迹模式。</li>
</ul>
<p><strong>潜在的未来研究方向：</strong></p>
<ul>
<li><strong>改进插值方法：</strong> 探索更先进的插值调度或机制，以更好地控制轨迹生成，尤其是在处理模糊任务时。</li>
<li><strong>提高零样本泛化能力：</strong> 进一步提升 TraceGen 在新具身和未知环境下的零样本泛化能力，使其能够更可靠地处理未见过的场景。</li>
<li><strong>增强精细操作的细节：</strong> 探索如何生成更精细、更具可执行性的轨迹，以支持更复杂的精细操作任务。</li>
<li><strong>扩展到更广泛的机器人类型：</strong> 测试 TraceGen 的 Trace-Space 抽象能力是否能推广到与人类手臂截然不同的机器人类型。</li>
<li><strong>更大规模的数据集和过滤：</strong> 利用互联网规模的数据集，并开发更强大的数据过滤机制，以解决数据质量问题，并进一步提升模型的鲁棒性。</li>
<li><strong>结合隐式世界模型：</strong> TraceGen 的编码器可以与隐式世界模型结合，以进一步增强预训练效果。</li>
</ul>
<p>总而言之，这篇论文通过引入3D Trace-Space 和 TraceForge 数据引擎，显著推动了机器人学习在小样本和跨具身场景下的发展，为实现更高效、更通用的机器人技能学习提供了新的途径。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Learning new robot tasks on new platforms and in new scenes from only a handful of demonstrations remains challenging.</li>
<li>We present TraceGen, a world model that predicts future motion in trace-space rather than pixel space, abstracting away appearance while retaining the geometric structure needed for manipulation.</li>
<li>To train TraceGen at scale, we develop TraceForge, a data pipeline that transforms heterogeneous human and robot videos into consistent 3D traces, yielding a corpus of 123K videos and 1.8M observation-trace-language triplets.</li>
<li>Pretraining on this corpus produces a transferable 3D motion prior that adapts efficiently: with just five target robot videos, TraceGen attains 80% success across four tasks while offering 50-600x faster inference than state-of-the-art video-based world models.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.21690v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.21690v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.21688v1'></a></p>
<h2 id="g2vlm-geometry-grounded-vision-language-model-with-unified-3d-reconstruction-and-spatial-reasoning"><a href="https://arxiv.org/abs/2511.21688v1">G<script type="math/tex">^2</script>VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning</a></h2>
<p><strong>Authors:</strong> Wenbo Hu, Jingli Lin, Yilin Long, Yunlong Ran, Lihan Jiang, Yifan Wang, Chenming Zhu, Runsen Xu, Tai Wang, Jiangmiao Pang</p>
<p><strong>Published:</strong> 2025-11-26</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.CL</p>
<p><strong>Abstract:</strong></p>
<p>Vision-Language Models (VLMs) still lack robustness in spatial intelligence, demonstrating poor performance on spatial understanding and reasoning tasks. We attribute this gap to the absence of a visual geometry learning process capable of reconstructing 3D space from 2D images. We present G<script type="math/tex">^2</script>VLM, a geometry grounded vision-language model that bridges two fundamental aspects of spatial intelligence: spatial 3D reconstruction and spatial understanding. G<script type="math/tex">^2</script>VLM natively leverages learned 3D visual geometry features to directly predict 3D attributes and enhance spatial reasoning tasks via in-context learning and interleaved reasoning. Our unified design is highly scalable for spatial understanding: it trains on abundant multi-view image and video data, while simultaneously leveraging the benefits of 3D visual priors that are typically only derived from hard-to-collect annotations. Experimental results demonstrate G<script type="math/tex">^2</script>VLM is proficient in both tasks, achieving comparable results to state-of-the-art feed-forward 3D reconstruction models and achieving better or competitive results across spatial understanding and reasoning tasks. By unifying a semantically strong VLM with low-level 3D vision tasks, we hope G<script type="math/tex">^2</script>VLM can serve as a strong baseline for the community and unlock more future applications, such as 3D scene editing.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“G<script type="math/tex">^2</script>VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning”的全面中文摘要：</p>
<p><strong>论文摘要：G<script type="math/tex">^2</script>VLM：统一3D重建与空间推理的几何基础视觉语言模型</strong></p>
<p><strong>1. 研究问题与动机</strong></p>
<p>本文指出，当前主流的视觉语言模型（VLMs）在空间智能方面仍显不足，尤其在空间理解和推理任务上表现不佳。作者认为，这种不足源于VLMs缺乏一个能够从2D图像重建3D空间的可视几何学习过程。现有的VLMs主要依赖于海量的、非结构化的2D图像-文本数据集进行训练，其空间理解能力主要通过语言和2D视觉先验隐式获得，而未能显式地学习和利用3D几何信息。</p>
<p><strong>2. 关键创新与方法论贡献</strong></p>
<p>为了解决上述问题，作者提出了 <strong>G<script type="math/tex">^2</script>VLM</strong>（Geometry Grounded Vision Language Model），一个将3D重建和空间理解这两个空间智能的核心方面统一起来的模型。其主要创新点包括：</p>
<ul>
<li><strong>统一的几何基础框架：</strong> G<script type="math/tex">^2</script>VLM将视觉几何学习（3D重建）和语义感知（多模态理解）整合到一个统一的框架中。它借鉴了人类认知的“双流假说”，设计了一个包含<strong>几何感知专家（"where pathway"）</strong>和<strong>语义感知专家（"what pathway"）</strong>的<strong>混合专家（Mixture-of-Transformer-Experts, MoT）</strong>架构。</li>
<li><strong>利用3D视觉先验：</strong> 模型能够直接从纯2D图像输入中学习3D几何信息，无需依赖难以获取的3D标注数据（如深度图和相机位姿），从而大大提高了模型的可扩展性。</li>
<li><strong>交错推理（Interleaved Reasoning）：</strong> G<script type="math/tex">^2</script>VLM能够直接预测3D属性，并利用这些学习到的3D几何特征，通过<strong>上下文学习（in-context learning）</strong>和<strong>交错推理</strong>来增强空间推理任务的表现。</li>
<li><strong>两阶段训练策略：</strong><ul>
<li><strong>第一阶段：</strong> 冻结预训练的VLM（如Qwen2-VL），并从头开始训练几何感知专家，使其学习几何丰富的表示。</li>
<li><strong>第二阶段：</strong> 解冻语义感知专家，并与几何感知专家联合训练，以学习如何有效地整合几何特征，从而提升空间推理能力。</li>
</ul>
</li>
<li><strong>模型架构：</strong> 采用MoT架构，其中几何感知专家利用DINOv2编码器提取低级视觉信息，并与LLM（Qwen2-VL-2B）的隐藏状态对齐。语义感知专家则基于预训练的VLM。两个专家通过共享的自注意力机制进行交互。</li>
</ul>
<p><strong>3. 主要结果与意义</strong></p>
<ul>
<li><strong>视觉几何任务：</strong> G<script type="math/tex">^2</script>VLM在单目深度估计、点云估计和相机位姿估计等视觉几何任务上取得了与最先进的（SOTA）前馈3D重建模型相当的性能，例如在Sintel基准上显著降低了单目深度估计的绝对相对误差。</li>
<li><strong>空间理解与推理任务：</strong> 在SPAR-Bench、OmniSpatial、MindCube和OST-Bench等多个空间理解和推理基准上，G<script type="math/tex">^2</script>VLM（特别是G<script type="math/tex">^2</script>VLM-SR变体）取得了<strong>最佳性能</strong>，显著超越了包括GPT-4o在内的许多模型，并且在2B参数规模下表现优异。</li>
<li><strong>模型有效性：</strong> 实验证明，几何感知专家和语义感知专家之间的<strong>正向互动</strong>是至关重要的，几何特征的提升能带来空间推理能力的更大进步。</li>
<li><strong>意义：</strong> G<script type="math/tex">^2</script>VLM成功地弥合了VLMs在空间智能方面的差距，展示了将3D几何学习与多模态理解相结合的强大潜力。它为社区提供了一个强大的基线模型，有望解锁3D场景编辑等更多未来应用。</li>
</ul>
<p><strong>4. 局限性</strong></p>
<ul>
<li><strong>训练稳定性：</strong> 作者提到，在处理大规模模型时，训练可能存在不稳定性，这需要先进的优化技术、精细的数据处理和大量的计算资源。</li>
<li><strong>数据需求：</strong> 尽管模型能够从2D数据中学习3D几何，但某些训练策略（如VG + CE Loss）需要大规模的3D标注数据集，这限制了其可扩展性。</li>
</ul>
<p><strong>5. 未来研究方向</strong></p>
<ul>
<li><strong>模型规模扩展：</strong> 作者指出，模型规模的扩展是一个有前景的方向，可以进一步解锁更强的性能。</li>
<li><strong>3D场景编辑等应用：</strong> G<script type="math/tex">^2</script>VLM的统一框架为开发更高级的3D空间相关应用（如3D场景编辑）奠定了基础。</li>
<li><strong>优化技术：</strong> 针对大规模模型训练不稳定的问题，需要进一步研究先进的优化技术。</li>
</ul>
<p>总而言之，G<script type="math/tex">^2</script>VLM通过创新地将3D几何重建能力融入视觉语言模型，显著提升了模型在空间理解和推理方面的表现，为构建更具空间智能的AI系统开辟了新的道路。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present G<script type="math/tex">^2</script>VLM, a geometry grounded vision-language model that bridges two fundamental aspects of spatial intelligence: spatial 3D reconstruction and spatial understanding.</li>
<li>Experimental results demonstrate G<script type="math/tex">^2</script>VLM is proficient in both tasks, achieving comparable results to state-of-the-art feed-forward 3D reconstruction models and achieving better or competitive results across spatial understanding and reasoning tasks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.21688v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.21688v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.21681v1'></a></p>
<h2 id="seeing-without-pixels-perception-from-camera-trajectories"><a href="https://arxiv.org/abs/2511.21681v1">Seeing without Pixels: Perception from Camera Trajectories</a></h2>
<p><strong>Authors:</strong> Zihui Xue, Kristen Grauman, Dima Damen, Andrew Zisserman, Tengda Han</p>
<p><strong>Published:</strong> 2025-11-26</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Can one perceive a video's content without seeing its pixels, just from the camera trajectory-the path it carves through space? This paper is the first to systematically investigate this seemingly implausible question. Towards this end, we propose a contrastive learning framework to train CamFormer, a dedicated encoder that projects camera pose trajectories into a joint embedding space, aligning them with natural language. We find that, contrary to its apparent simplicity, the camera trajectory is a remarkably informative signal to uncover video content. In other words, "how you move" can indeed reveal "what you are doing" (egocentric) or "observing" (exocentric). We demonstrate the versatility of our learned CamFormer embeddings on a diverse suite of downstream tasks, ranging from cross-modal alignment to classification and temporal analysis. Importantly, our representations are robust across diverse camera pose estimation methods, including both high-fidelity multi-sensored and standard RGB-only estimators. Our findings establish camera trajectory as a lightweight, robust, and versatile modality for perceiving video content.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：Seeing without Pixels: Perception from Camera Trajectories</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话的简洁总结）</strong></p>
<p>这篇论文首次系统性地探索了仅凭相机轨迹（相机在空间中的运动路径）来感知视频内容的可行性。作者提出了一个名为 CamFormer 的对比学习框架，该框架能够将相机姿态轨迹映射到一个联合嵌入空间，并与自然语言对齐。研究表明，相机轨迹本身是一个极具信息量的信号，能够揭示视频内容，从而为“如何移动”能够反映“在做什么”或“在观察什么”提供了证据。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<ul>
<li><strong>核心创新：</strong> 核心创新在于提出并验证了“仅凭相机轨迹感知视频内容”这一看似反直觉的假设。这挑战了传统计算机视觉依赖像素信息进行理解的范式。</li>
<li><strong>方法论：</strong><ul>
<li><strong>对比学习框架：</strong> 作者设计了一个对比学习框架来训练 CamFormer 模型。这种方法通常用于学习具有区分性的表示，通过最大化相似样本（例如，同一视频的不同相机轨迹片段）的嵌入相似度，同时最小化不相似样本的相似度。</li>
<li><strong>CamFormer 编码器：</strong> 这是一个专门设计的编码器，其任务是将输入的相机姿态轨迹（通常是三维空间中的一系列位姿信息）转换为一个低维的、具有语义信息的嵌入向量。</li>
<li><strong>与自然语言对齐：</strong> 将相机轨迹嵌入与自然语言（如视频描述）对齐是关键。这意味着学习到的嵌入能够捕捉到与文本描述相对应的视频内容信息。这通常通过多模态对比学习实现，例如使用 CLIP 类似的损失函数。</li>
<li><strong>跨模态对齐：</strong> 这种对齐能力是实现“感知”的关键，它允许模型将抽象的相机运动与具体的视频内容联系起来。</li>
</ul>
</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<ul>
<li><strong>范式转变的潜力：</strong> 如果研究成果得到广泛认可和应用，可能会开启一个全新的研究方向，即“无像素视觉感知”。这可能促使研究人员重新思考视觉信息的来源和表示方式。</li>
<li><strong>降低计算和存储成本：</strong> 相机轨迹数据通常比原始像素视频小得多，存储和处理成本也更低。这对于资源受限的环境（如嵌入式设备、低功耗传感器）或需要处理海量视频数据的场景具有重要意义。</li>
<li><strong>鲁棒性提升：</strong> 摘要提到其表示对不同相机姿态估计方法（包括低成本的 RGB-only 估计器）具有鲁棒性。这意味着该方法可以在不依赖高精度、多传感器系统的情况下工作，大大提高了实际应用的可行性。</li>
<li><strong>新的跨模态交互方式：</strong> 这种方法为相机运动与内容理解之间的联系提供了新的视角，可能催生新的交互式应用，例如通过控制相机运动来查询视频内容。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>机器人导航与感知：</strong> 机器人可以通过其运动轨迹来推断周围环境的性质和正在进行的活动，而无需持续处理高分辨率图像。</li>
<li><strong>自动驾驶：</strong> 车辆的运动模式本身可能包含关于道路状况、交通流量甚至其他车辆意图的信息。</li>
<li><strong>虚拟现实/增强现实 (VR/AR)：</strong> 用户在虚拟或增强环境中的移动方式可以用来推断其意图或理解其所处的场景。</li>
<li><strong>视频检索与分析：</strong> 可以基于相机运动模式来检索特定类型的视频，或者分析视频中发生的事件。</li>
<li><strong>内容创作与编辑：</strong> 了解相机运动如何影响内容感知，可以帮助创作者更好地设计镜头语言。</li>
<li><strong>人机交互：</strong> 通过分析用户在空间中的移动来理解用户意图，实现更自然的交互。</li>
<li><strong>运动分析：</strong> 在体育、医疗等领域，相机轨迹可以作为分析人体或物体运动的辅助信息。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<ul>
<li><strong>信息丢失的可能性：</strong> 尽管摘要强调了相机轨迹的丰富性，但不可避免地会丢失大量像素信息，例如物体的具体外观、纹理、颜色、细节等。对于需要精细视觉识别的任务，仅凭轨迹可能不足够。</li>
<li><strong>对相机轨迹质量的依赖：</strong> 尽管摘要提到对不同姿态估计方法的鲁棒性，但如果相机轨迹本身存在严重的噪声、不准确或不完整，其感知能力将受到严重影响。</li>
<li><strong>“感知”的定义和范围：</strong> 摘要中“感知”的含义可能相对宽泛。它可能更侧重于理解场景的整体性质、活动类型或相机视角下的主要对象，而不是对场景中所有细节进行精确识别。</li>
<li><strong>训练数据的需求：</strong> 对比学习框架通常需要大量的训练数据来学习有效的嵌入。如何获取足够多样化且标注良好的相机轨迹-内容对数据是一个潜在的挑战。</li>
<li><strong>解释性问题：</strong> 虽然模型能够学习到轨迹与内容之间的关联，但具体是轨迹的哪些特征（例如，速度、加速度、转弯半径、平移方向等）与特定内容相关联，其内在的解释性可能需要进一步研究。</li>
<li><strong>“无像素”的绝对性：</strong> 严格来说，相机轨迹的获取本身可能依赖于传感器（如 IMU、视觉里程计），这些传感器在某种程度上也间接与物理世界互动。因此，“无像素”可能更多是指不直接处理原始图像像素数据。</li>
</ul>
<p>总而言之，这篇论文提出了一个非常新颖且具有挑战性的研究方向，如果其方法能够有效且鲁棒地实现，将对计算机视觉领域产生深远的影响，并可能催生一系列新的应用。其核心价值在于证明了“运动”本身蕴含着丰富的语义信息，为我们理解和处理视觉数据提供了新的维度。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Towards this end, we propose a contrastive learning framework to train CamFormer, a dedicated encoder that projects camera pose trajectories into a joint embedding space, aligning them with natural language.</li>
<li>We demonstrate the versatility of our learned CamFormer embeddings on a diverse suite of downstream tasks, ranging from cross-modal alignment to classification and temporal analysis.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.21681v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.21681v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.21666v1'></a></p>
<h2 id="uncertainty-quantification-for-visual-object-pose-estimation"><a href="https://arxiv.org/abs/2511.21666v1">Uncertainty Quantification for Visual Object Pose Estimation</a></h2>
<p><strong>Authors:</strong> Lorenzo Shaikewitz, Charis Georgiou, Luca Carlone</p>
<p><strong>Published:</strong> 2025-11-26</p>
<p><strong>Categories:</strong> cs.RO, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Quantifying the uncertainty of an object's pose estimate is essential for robust control and planning. Although pose estimation is a well-studied robotics problem, attaching statistically rigorous uncertainty is not well understood without strict distributional assumptions. We develop distribution-free pose uncertainty bounds about a given pose estimate in the monocular setting. Our pose uncertainty only requires high probability noise bounds on pixel detections of 2D semantic keypoints on a known object. This noise model induces an implicit, non-convex set of pose uncertainty constraints. Our key contribution is SLUE (S-Lemma Uncertainty Estimation), a convex program to reduce this set to a single ellipsoidal uncertainty bound that is guaranteed to contain the true object pose with high probability. SLUE solves a relaxation of the minimum volume bounding ellipsoid problem inspired by the celebrated S-lemma. It requires no initial guess of the bound's shape or size and is guaranteed to contain the true object pose with high probability. For tighter uncertainty bounds at the same confidence, we extend SLUE to a sum-of-squares relaxation hierarchy which is guaranteed to converge to the minimum volume ellipsoidal uncertainty bound for a given set of keypoint constraints. We show this pose uncertainty bound can easily be projected to independent translation and axis-angle orientation bounds. We evaluate SLUE on two pose estimation datasets and a real-world drone tracking scenario. Compared to prior work, SLUE generates substantially smaller translation bounds and competitive orientation bounds. We release code at https://github.com/MIT-SPARK/PoseUncertaintySets.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Uncertainty Quantification for Visual Object Pose Estimation”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Uncertainty Quantification for Visual Object Pose Estimation (视觉物体姿态估计中的不确定性量化)</p>
<p><strong>作者：</strong> Lorenzo Shaikewitz, Charis Georgiou, Luca Carlone</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题：</strong>
该论文旨在解决在单目视觉设置下，为物体姿态估计提供统计上严格且无分布假设的不确定性量化问题。现有的姿态估计方法虽然成熟，但为其估计结果附加具有统计保证的不确定性仍然是一个挑战，尤其是在不引入严格分布假设的情况下。准确量化姿态不确定性对于机器人控制和规划至关重要，例如在自动驾驶中，不确定的物体位置可能需要车辆减速。</p>
<p><strong>2. 主要创新与方法贡献：</strong>
该论文的核心贡献是提出了一种名为 <strong>SLUE (S-Lemma Uncertainty Estimation)</strong> 的方法，用于生成分布无关的、具有高概率保证的物体姿态不确定性边界。</p>
<ul>
<li><strong>核心思想：</strong> SLUE 的基本思想是将物体姿态的不确定性约束表示为一个隐式、非凸的二次约束集，该约束集源于对 2D 语义关键点像素检测的高概率噪声边界的假设。然后，SLUE 利用 <strong>S-lemma 的推广</strong>，将这个非凸约束集转化为一个凸优化问题，求解一个<strong>最小体积的椭球体边界</strong>，该椭球体能够以高概率包含真实的物体姿态。</li>
<li><strong>关键技术：</strong><ul>
<li><strong>分布无关的姿态不确定性边界：</strong> SLUE 不依赖于对关键点噪声或姿态分布的特定假设，使其具有广泛的适用性。</li>
<li><strong>隐式非凸约束到显式凸边界的转化：</strong> 通过 S-lemma 的推广，将由关键点不确定性诱导的非凸约束集，转化为一个可解的凸优化问题，从而得到一个明确的椭球体不确定性边界。</li>
<li><strong>最小体积椭球体：</strong> SLUE 旨在找到最小体积的椭球体，这提供了更紧凑、信息量更大的不确定性估计。</li>
<li><strong>松弛层级结构：</strong> 为了获得更精确的边界，SLUE 扩展为一种 <strong>sum-of-squares (SOS) 松弛层级结构</strong>，该结构保证能够收敛到真正的最小体积椭球体边界。</li>
<li><strong>可解释的边界：</strong> SLUE 生成的联合姿态不确定性椭球体可以方便地投影到独立的平移和轴角方向不确定性边界，便于理解和使用。</li>
<li><strong>效率：</strong> SLUE 在计算效率上表现出色，尤其是在第一阶松弛时，甚至比一些关键点检测器本身还要快。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
论文在两个公开数据集 (LineMOD-Occlusion, YCB-Video) 和一个真实世界的无人机跟踪场景 (CAST) 上对 SLUE 进行了评估。</p>
<ul>
<li><strong>紧凑的边界：</strong> 与现有方法 (如 RANSAG 和 GRCC) 相比，SLUE 生成的<strong>平移不确定性边界显著更小</strong>，而<strong>方向不确定性边界也具有竞争力</strong>。这表明 SLUE 能够更精确地捕捉姿态不确定性，尤其是在处理尺度模糊性方面。</li>
<li><strong>统计保证：</strong> SLUE 提供了统计上严格的保证，即生成的椭球体以高概率包含真实姿态。实验结果也验证了其经验覆盖率接近理论保证。</li>
<li><strong>灵活性：</strong> SLUE 对姿态估计器是无关的，并且可以与不同的关键点检测器结合使用。</li>
<li><strong>效率优势：</strong> SLUE 在计算效率上优于现有方法，尤其是在第一阶松弛时，这对于实时应用非常重要。</li>
<li><strong>意义：</strong> 该方法为机器人领域提供了一种强大且可靠的工具，用于量化视觉物体姿态估计的不确定性，从而支持更鲁棒的决策和控制。</li>
</ul>
<p><strong>4. 提及的局限性：</strong>
*   <strong>关键点不确定性利用不足：</strong> 论文提到，虽然 SLUE 利用了关键点不确定性的高概率性质，但并未完全利用其“组合多重检验”的特性，这可能导致在某些情况下（如存在对称性物体）可以进一步改进姿态估计和不确定性。
*   <strong>方向不确定性边界的保守性：</strong> 在某些情况下，尤其是在使用无穷范数（∞-norm）关键点不确定性时，SLUE 的方向不确定性边界可能略显保守。
*   <strong>合成数据校准的潜在问题：</strong> 在 YCB-V 数据集上使用合成图像进行校准时，可能存在与真实数据交换性（exchangeability）的潜在问题，导致覆盖率略有偏差。</p>
<p><strong>5. 未来研究方向：</strong>
*   <strong>利用关键点不确定性的组合多重检验特性：</strong> 探索如何更好地利用关键点不确定性的组合多重检验问题，以改进姿态估计和不确定性量化，尤其是在处理对称性物体时。
*   <strong>捕捉物体对称性：</strong> 目前的方法未直接处理物体对称性带来的不确定性，未来的工作可以探索如何将此纳入考量。
*   <strong>融合序列信息：</strong> 在跟踪场景中，可以利用连续帧之间的信息来融合或改进不确定性估计。
*   <strong>更高效的松弛方法：</strong> 尽管 SLUE 已非常高效，但对于更高阶的松弛，仍然存在计算成本的权衡。探索更高效的松弛方法或近似技术可能是有益的。
*   <strong>主动感知：</strong> 将 SLUE 的不确定性量化结果用于主动感知任务，例如通过规划相机运动来减少不确定性。</p>
<p>总而言之，这篇论文提出了一种新颖且高效的方法 (SLUE) 来解决视觉物体姿态估计中的不确定性量化问题。通过利用 S-lemma 的推广和 SOS 松弛技术，SLUE 能够生成统计上严格、分布无关且紧凑的姿态不确定性边界，为机器人应用提供了重要的理论和实践贡献。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We develop distribution-free pose uncertainty bounds about a given pose estimate in the monocular setting.</li>
<li>Our key contribution is SLUE (S-Lemma Uncertainty Estimation), a convex program to reduce this set to a single ellipsoidal uncertainty bound that is guaranteed to contain the true object pose with high probability.</li>
<li>We show this pose uncertainty bound can easily be projected to independent translation and axis-angle orientation bounds.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.21666v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.21666v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.21662v1'></a></p>
<h2 id="multi-crit-benchmarking-multimodal-judges-on-pluralistic-criteria-following"><a href="https://arxiv.org/abs/2511.21662v1">Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following</a></h2>
<p><strong>Authors:</strong> Tianyi Xiong, Yi Ge, Ming Li, Zuolong Zhang, Pranav Kulkarni, Kaishen Wang, Qi He, Zeying Zhu, Chenxi Liu, Ruibo Chen, Tong Zheng, Yanshuo Chen, Xiyao Wang, Renrui Zhang, Wenhu Chen, Heng Huang</p>
<p><strong>Published:</strong> 2025-11-26</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Large multimodal models (LMMs) are increasingly adopted as judges in multimodal evaluation systems due to their strong instruction following and consistency with human preferences. However, their ability to follow diverse, fine-grained evaluation criteria remains underexplored. We develop Multi-Crit, a benchmark for evaluating multimodal judges on their capacity to follow pluralistic criteria and produce reliable criterion-level judgments. Covering both open-ended generation and verifiable reasoning tasks, Multi-Crit is built through a rigorous data curation pipeline that gathers challenging response pairs with multi-criterion human annotations. It further introduces three novel metrics for systematically assessing pluralistic adherence, criterion-switching flexibility, and the ability to recognize criterion-level preference conflicts. Comprehensive analysis of 25 LMMs reveals that 1) proprietary models still struggle to maintain consistent adherence to pluralistic criteria--especially in open-ended evaluation; 2) open-source models lag further behind in flexibly following diverse criteria; and 3) critic fine-tuning with holistic judgment signals enhances visual grounding but fails to generalize to pluralistic criterion-level judgment. Additional analyses on reasoning fine-tuning, test-time scaling, and boundary consistency between open-source and proprietary models further probe the limits of current multimodal judges. As a pioneering study, Multi-Crit lays the foundation for building reliable and steerable multimodal AI evaluation.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following</p>
<p><strong>作者：</strong> Tianyi Xiong, Yi Ge, Ming Li, Zuolong Zhang, Pranav Kulkarni, Kaishen Wang, Qi He, Zeying Zhu, Chenxi Liu, Ruibo Chen, Tong Zheng, Yanshuo Chen, Xiyao Wang, Renrui Zhang, Wenhu Chen, Heng Huang</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
大型多模态模型（LMMs）在多模态评估系统中作为“裁判”的应用日益广泛，这得益于它们强大的指令遵循能力和与人类偏好的一致性。然而，它们遵循多样化、细粒度评估标准的能力仍未得到充分探索。现有的大多数量化评估方法仅提供单一的整体偏好标签，忽略了在同一评估样本中不同标准之间可能存在的权衡和冲突。这引发了一个关键问题：<strong>LMM裁判能否有效地遵循多重、细粒度的评估标准，并准确识别标准间的偏好冲突？</strong></p>
<p><strong>2. 主要创新与方法贡献：</strong>
为了解决这一挑战，作者提出了 <strong>Multi-Crit</strong>，一个专门用于评估多模态裁判模型遵循“多元化标准”（pluralistic criteria）能力的新基准。其核心贡献包括：</p>
<ul>
<li>
<p><strong>Multi-Crit 基准数据集：</strong></p>
<ul>
<li><strong>数据构建流程：</strong> 采用严格的数据策选和偏好标注流程，收集了包含开放式生成和可验证推理任务的多元化多模态提示。</li>
<li><strong>挑战性样本对：</strong> 精心筛选了来自不同 LMM 的响应对，这些响应对在细粒度标准上存在微妙的质量差异，并包含多标准的人工标注偏好，以揭示标准间的冲突。</li>
<li><strong>多元化评估标准：</strong> 为开放式生成任务定义了五项标准（完整性与覆盖度、视觉接地与细节、事实性/无幻觉、创造性与表现力、清晰度与连贯性），为可验证推理任务定义了五项标准（视觉接地、逻辑连贯性与一致性、事实性/无幻觉、反思与探索、简洁性与效率）。</li>
</ul>
</li>
<li>
<p><strong>新的评估指标：</strong> 引入了三个新颖的指标来系统评估 LMM 裁判的能力：</p>
<ul>
<li><strong>多元准确率 (PAcc)：</strong> 衡量裁判模型在每个评估实例的所有标准上都做出正确判断的比例。</li>
<li><strong>权衡敏感度 (TOS)：</strong> 衡量裁判模型识别同一实例内不同标准之间权衡的能力。</li>
<li><strong>冲突匹配率 (CMR)：</strong> 衡量裁判模型正确解决标准间冲突偏好的能力。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
对 25 个 LMM 的全面分析揭示了当前多模态裁判模型的局限性：</p>
<ul>
<li><strong>专有模型仍面临挑战：</strong> 最先进的专有模型（如 o4-mini, Claude-3.7-Sonnet）在保持多元标准的一致性方面仍有困难，尤其是在开放式评估中。</li>
<li><strong>开源模型表现落后：</strong> 开源模型在灵活遵循多样化标准方面，与专有模型存在显著差距。</li>
<li><strong>批评微调效果有限：</strong> 使用整体判断信号进行的批评微调（critic fine-tuning）可以提升视觉接地能力，但未能泛化到多元化、标准驱动的评估。</li>
<li><strong>推理微调的影响：</strong> 推理微调未能显著提升推理判断能力，反而削弱了模型识别权衡的能力。</li>
<li><strong>测试时缩放效果不一：</strong> 测试时缩放（test-time scaling）对模型的影响因模型而异，且在多标准判断中效果有限。</li>
<li><strong>人类标注一致性是上限：</strong> 专有模型的标准级上限与人类标注者的一致性高度吻合，而开源模型则未能展现出这种趋势。</li>
</ul>
<p><strong>意义：</strong> Multi-Crit 是首个专门评估多模态裁判模型遵循多元化标准能力的基准。其研究结果揭示了当前 LMM 裁判在理解和执行复杂、细粒度评估指令方面的不足，为开发更可靠、可控的多模态人工智能评估系统奠定了基础。</p>
<p><strong>4. 局限性：</strong>
*   <strong>模型能力差异：</strong> 论文指出，即使是顶尖模型，在开放式评估中的多元准确率也仅能达到 32.78%，这表明当前 LMM 在处理主观性强、依赖细粒度视觉感知的评估任务时仍有很大提升空间。
*   <strong>开源模型差距：</strong> 开源模型在标准级准确率和识别权衡、冲突方面的能力明显落后于专有模型。
*   <strong>微调的局限性：</strong> 批评微调虽然能提升特定能力（如视觉接地），但未能有效泛化到多元化标准评估。</p>
<p><strong>5. 未来研究方向：</strong>
*   <strong>提升多元标准遵循能力：</strong> 需要开发新的训练方法和模型架构，以使 LMM 能够更好地理解和执行复杂的、相互关联的评估标准。
*   <strong>更精细的评估指标：</strong> 进一步探索能够更准确地量化 LMM 在处理标准权衡和冲突方面的能力的指标。
*   <strong>跨模型一致性：</strong> 研究如何缩小开源模型与专有模型在多元标准遵循能力上的差距。
*   <strong>可解释性与可控性：</strong> 探索如何使 LMM 裁判的评估过程更具可解释性，并能够根据特定需求进行更精细地控制。
*   <strong>更具挑战性的基准：</strong> 持续构建更复杂、更细粒度的多模态评估基准，以推动 LMM 裁判能力的进步。</p>
<p><strong>对计算机视觉领域的贡献：</strong></p>
<p>Multi-Crit 基准的提出，<strong>显著推动了多模态模型评估领域的发展</strong>。它不仅提供了一个更具挑战性的评估平台，能够深入剖析 LMM 在理解和执行细粒度、多元化评估指令方面的能力，还通过引入新的评估指标，为量化 LMM 在处理复杂评估场景（如标准间的权衡与冲突）下的表现提供了更精确的工具。这对于<strong>提升多模态模型（尤其是视觉-语言模型）的可靠性、可信度和可控性</strong>至关重要，尤其是在需要精确、细致评估的下游应用场景中，如自动驾驶、内容审核、辅助诊断等。该研究揭示了当前 LMM 裁判的局限性，为未来开发更强大、更鲁棒的多模态评估系统指明了方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We develop Multi-Crit, a benchmark for evaluating multimodal judges on their capacity to follow pluralistic criteria and produce reliable criterion-level judgments.</li>
<li>It further introduces three novel metrics for systematically assessing pluralistic adherence, criterion-switching flexibility, and the ability to recognize criterion-level preference conflicts.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.21662v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.21662v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.21652v1'></a></p>
<h2 id="continual-error-correction-on-low-resource-devices"><a href="https://arxiv.org/abs/2511.21652v1">Continual Error Correction on Low-Resource Devices</a></h2>
<p><strong>Authors:</strong> Kirill Paramonov, Mete Ozay, Aristeidis Mystakidis, Nikolaos Tsalikidis, Dimitrios Sotos, Anastasios Drosou, Dimitrios Tzovaras, Hyunjun Kim, Kiseok Chang, Sangdok Mo, Namwoong Kim, Woojong Yoo, Jijoong Moon, Umberto Michieli</p>
<p><strong>Published:</strong> 2025-11-26</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>The proliferation of AI models in everyday devices has highlighted a critical challenge: prediction errors that degrade user experience. While existing solutions focus on error detection, they rarely provide efficient correction mechanisms, especially for resource-constrained devices. We present a novel system enabling users to correct AI misclassifications through few-shot learning, requiring minimal computational resources and storage. Our approach combines server-side foundation model training with on-device prototype-based classification, enabling efficient error correction through prototype updates rather than model retraining. The system consists of two key components: (1) a server-side pipeline that leverages knowledge distillation to transfer robust feature representations from foundation models to device-compatible architectures, and (2) a device-side mechanism that enables ultra-efficient error correction through prototype adaptation. We demonstrate our system's effectiveness on both image classification and object detection tasks, achieving over 50% error correction in one-shot scenarios on Food-101 and Flowers-102 datasets while maintaining minimal forgetting (less than 0.02%) and negligible computational overhead. Our implementation, validated through an Android demonstration app, proves the system's practicality in real-world scenarios.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Continual Error Correction on Low-Resource Devices”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Continual Error Correction on Low-Resource Devices
<strong>作者：</strong> Kirill Paramonov, Mete Ozay, Aristeidis Mystakidis, Nikolaos Tsalikidis, Dimitrios Sotos, Anastasios Drosou, Dimitrios Tzovaras, Hyunjun Kim, Kiseok Chang, Sangdok Mo, Namwoong Kim, Woojong Yoo, Jijoong Moon, Umberto Michieli</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
随着人工智能（AI）模型在日常设备中的普及，预测错误成为一个关键挑战，严重影响用户体验。现有解决方案大多侧重于错误检测，而缺乏高效的纠错机制，尤其是在资源受限的设备上。论文旨在解决这一问题，即如何在计算和存储资源有限的设备上实现持续、高效的AI模型错误纠正。</p>
<p><strong>2. 主要创新点/方法论贡献：</strong>
该论文提出了一种新颖的系统，使<strong>用户能够通过少量样本（few-shot learning）来纠正AI模型的误分类</strong>，同时对计算资源和存储空间的要求极低。其核心创新在于：</p>
<ul>
<li><strong>服务器端基础模型训练与知识蒸馏：</strong> 利用强大的基础模型（如DINO-v2）提取鲁棒的特征表示，并通过知识蒸馏技术将其迁移到设备兼容的轻量级模型（如MobileNet-V2）上。这使得设备端模型能够继承大型模型的强大能力，同时保持紧凑的尺寸和高效的推理速度。</li>
<li><strong>设备端原型（Prototype）为基础的分类与高效纠错机制：</strong><ul>
<li><strong>原型计算：</strong> 在服务器端，利用训练好的轻量级模型为每个类别计算一组代表性的原型（prototypes）。</li>
<li><strong>原型更新：</strong> 在设备端，当模型发生误分类时，用户可以通过提供少量（例如，仅一个）正确标签来更新这些原型。这种<strong>基于原型更新的纠错方式</strong>，而非模型重训练，实现了超高效的错误纠正，并有效避免了灾难性遗忘。</li>
<li><strong>轻量级部署：</strong> 整个系统设计充分考虑了设备的资源限制，确保了低功耗、低内存占用和快速推理。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
论文在图像分类（Food-101, Flowers-102）和物体检测任务上验证了系统的有效性。主要结果包括：</p>
<ul>
<li><strong>高效的错误纠正能力：</strong> 在单样本（one-shot）场景下，错误纠正准确率（ACCE）超过50%。</li>
<li><strong>极低的遗忘率：</strong> 遗忘率（For）非常低，仅为0.02%，表明系统在纠正新错误的同时，不会显著损害对已有类别的识别能力。</li>
<li><strong>极低的计算开销：</strong> 纠错过程对设备资源的要求极低，实现了近乎零的计算开销。</li>
<li><strong>实际应用可行性：</strong> 通过Android演示应用程序验证了系统的实用性，能够在真实世界的场景中运行。</li>
</ul>
<p>该研究的意义在于，它为在资源受限的边缘设备上实现个性化、自适应的AI模型提供了切实可行的解决方案，极大地提升了用户体验和AI系统的可靠性，为普及AI应用迈出了重要一步。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>计算成本权衡：</strong> 在物体检测任务中，采用独立的检测和分类网络虽然能获得最高精度，但计算成本较高。
*   <strong>未来优化方向：</strong> 论文提到，可以将原型推理直接构建在检测特征上，以进一步降低计算复杂度，但这留待未来工作。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>更深度的集成：</strong> 将原型推理直接应用于物体检测的特征提取阶段，以进一步优化计算效率。
*   <strong>更广泛的应用：</strong> 探索该系统在更多类型的AI模型和应用场景中的潜力。
*   <strong>自动化用户反馈：</strong> 研究如何更智能地收集和利用用户反馈，进一步减少用户干预。
*   <strong>更复杂的错误类型：</strong> 探索处理更复杂的错误类型，如定位错误或属性识别错误。</p>
<p>总而言之，这篇论文提出了一种创新的、面向低资源设备的持续错误纠正框架，通过服务器端知识蒸馏和设备端原型更新机制，实现了用户友好的、高效的AI模型错误纠正，为提升AI在日常设备中的可靠性和用户体验提供了重要贡献。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present a novel system enabling users to correct AI misclassifications through few-shot learning, requiring minimal computational resources and storage.</li>
<li>Our approach combines server-side foundation model training with on-device prototype-based classification, enabling efficient error correction through prototype updates rather than model retraining.</li>
<li>We demonstrate our system's effectiveness on both image classification and object detection tasks, achieving over 50% error correction in one-shot scenarios on Food-101 and Flowers-102 datasets while maintaining minimal forgetting (less than 0.02%) and negligible computational overhead.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.21652v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.21652v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.21635v1'></a></p>
<h2 id="mechanisms-of-non-monotonic-scaling-in-vision-transformers"><a href="https://arxiv.org/abs/2511.21635v1">Mechanisms of Non-Monotonic Scaling in Vision Transformers</a></h2>
<p><strong>Authors:</strong> Anantha Padmanaban Krishna Kumar</p>
<p><strong>Published:</strong> 2025-11-26</p>
<p><strong>Categories:</strong> cs.LG, cs.AI, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Deeper Vision Transformers often perform worse than shallower ones, which challenges common scaling assumptions. Through a systematic empirical analysis of ViT-S, ViT-B, and ViT-L on ImageNet, we identify a consistent three-phase Cliff-Plateau-Climb pattern that governs how representations evolve with depth. We observe that better performance is associated with progressive marginalization of the [CLS] token, originally designed as a global aggregation hub, in favor of distributed consensus among patch tokens. We quantify patterns of information mixing with an Information Scrambling Index, and show that in ViT-L the information-task tradeoff emerges roughly 10 layers later than in ViT-B, and that these additional layers correlate with increased information diffusion rather than improved task performance. Taken together, these results suggest that transformer architectures in this regime may benefit more from carefully calibrated depth that executes clean phase transitions than from simply increasing parameter count. The Information Scrambling Index provides a useful diagnostic for existing models and suggests a potential design target for future architectures. All code is available at: https://github.com/AnanthaPadmanaban-KrishnaKumar/Cliff-Plateau-Climb.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Mechanisms of Non-Monotonic Scaling in Vision Transformers”的全面中文摘要，重点关注其研究问题、创新点、主要发现、局限性和未来方向：</p>
<p><strong>论文题目：</strong> Mechanisms of Non-Monotonic Scaling in Vision Transformers (视觉 Transformer 中非单调缩放的机制)</p>
<p><strong>作者：</strong> Anantha Padmanaban Krishna Kumar</p>
<p><strong>摘要：</strong></p>
<p>这篇论文深入探讨了视觉 Transformer (ViT) 模型在深度增加时性能反而下降的“非单调缩放”现象，挑战了普遍认为“更深即更好”的缩放假设。研究人员通过对不同规模的 ViT 模型（ViT-S、ViT-B、ViT-L）在 ImageNet 数据集上的系统性实证分析，揭示了模型内部表示如何随深度演变，并提出了新的解释框架。</p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<ul>
<li><strong>非单调缩放现象：</strong> 为什么更深的 ViT 模型（如 ViT-L）在某些情况下性能不如更浅的模型（如 ViT-B），尽管它们拥有更多的参数和计算量？这与传统的模型缩放假设相悖。</li>
<li><strong>理解 Transformer 的内部机制：</strong> 如何量化和理解 Transformer 模型在不同层级中信息处理和混合的方式，以及这种机制如何影响最终的性能和表示几何？</li>
</ul>
<p><strong>2. 主要创新点/方法论贡献：</strong></p>
<ul>
<li><strong>Cliff-Plateau-Climb 三阶段模式：</strong> 首次识别并系统性地描述了 ViT 模型中表示演化的一个一致的三阶段模式：Cliff（悬崖，初始快速去相关）、Plateau（平台期，低相似性处理）和 Climb（攀升期，最终重相关）。该模式在不同模型规模下均成立，但持续时间和特征有所不同。</li>
<li><strong>信息混乱度指数 (Information Scrambling Index)：</strong> 引入了一个新的量化指标，用于衡量跨 patch token 的信息混合程度。该指数通过比较全局解码器和自注意力解码器的重构能力差异来计算，能够区分模型内部计算策略的三个不同计算模式：局部处理（Local Processing）、受控共识（Controlled Consensus）和混沌扩散（Chaotic Diffusion）。</li>
<li><strong>注意力动力学分析：</strong> 利用图论方法分析了注意力权重，量化了 [CLS] token 的中心化程度（Hub Marginalization）和全局混合速率（Attention Consensus Index, ACI），以理解信息如何从中心化的 [CLS] token 转移到分布式的 patch token 共识。</li>
<li><strong>信息平面分析：</strong> 将模型在信息保留（InfoX）和任务信号（Probe Accuracy）之间的权衡可视化，揭示了不同模型在信息处理效率上的差异。</li>
</ul>
<p><strong>3. 主要研究结果及其意义：</strong></p>
<ul>
<li><strong>性能与 [CLS] token 边缘化相关：</strong> 研究发现，更好的模型性能（几何质量）与 [CLS] token 的逐渐边缘化（即不再作为唯一的全局聚合中心）以及 patch token 之间形成分布式共识密切相关。ViT-B 在这方面表现尤为突出，其 [CLS] token 的中心性急剧下降。</li>
<li><strong>计算策略而非参数量：</strong> ViT-B 在信息-任务权衡上比 ViT-L 更早达到最优状态（约 8 层 vs 18 层）。ViT-L 的额外层数并没有带来更好的任务性能，反而导致了信息过度扩散（高信息混乱度）和不稳定的表示几何。这表明，<strong>精心校准的深度和清晰的阶段过渡比单纯增加参数量更能提升 Transformer 的性能。</strong></li>
<li><strong>信息混乱度指数的诊断价值：</strong> 该指数能够区分不同模型的计算策略。ViT-S 表现为“通信崩溃”，ViT-B 为“受控共识”，而 ViT-L 则陷入“混沌扩散”。这种区分与模型的最终性能和几何质量高度相关。</li>
<li><strong>非单调缩放的解释：</strong> 研究结果为 ViT 模型中出现的非单调缩放现象提供了机制性解释：过度的深度可能导致信息处理策略的失调（如 ViT-L 的过度混合），从而损害了表示的质量和最终的性能。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>特定设置下的观察：</strong> 研究主要集中在预训练的 ViT-S/B/L 模型在 ImageNet-1k 分类任务上的表现。其发现的“最优”信息混乱度范围和 Hub Marginalization 模式可能依赖于特定的归一化和代理指标，理论上的普适性仍需进一步验证。</li>
<li><strong>相关性而非因果性：</strong> 研究主要基于对冻结模型的分析，观察到的是相关性而非直接的因果干预。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>跨架构、任务和模态的泛化：</strong> 将这些发现推广到其他 Transformer 架构、不同的计算机视觉任务（如检测、分割）、以及其他模态（如自然语言处理、多模态）的研究。</li>
<li><strong>训练过程中的干预：</strong> 在模型训练过程中主动利用信息混乱度指数和 Hub Marginalization 指标作为正则化项或早期预警信号，以指导模型设计和训练。</li>
<li><strong>理论解释的深化：</strong> 进一步探索信息混乱度、Hub Marginalization 与 Transformer 内部机制（如注意力机制、位置编码）之间的理论联系。</li>
<li><strong>设计更高效的 Transformer 架构：</strong> 基于这些发现，设计能够实现更平滑、更高效阶段过渡的 Transformer 架构，以避免深度带来的负面影响。</li>
</ul>
<p>总而言之，这篇论文通过创新的量化工具和系统的实证分析，揭示了视觉 Transformer 模型深度缩放的复杂性，强调了计算策略和信息流动的协调性在模型性能中的关键作用，为未来设计更高效、更可预测的 Transformer 架构提供了重要的见解和方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>The Information Scrambling Index provides a useful diagnostic for existing models and suggests a potential design target for future architectures.</li>
<li>All code is available at: https://github.com/AnanthaPadmanaban-KrishnaKumar/Cliff-Plateau-Climb.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.21635v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.21635v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.21592v1'></a></p>
<h2 id="mogan-improving-motion-quality-in-video-diffusion-via-few-step-motion-adversarial-post-training"><a href="https://arxiv.org/abs/2511.21592v1">MoGAN: Improving Motion Quality in Video Diffusion via Few-Step Motion Adversarial Post-Training</a></h2>
<p><strong>Authors:</strong> Haotian Xue, Qi Chen, Zhonghao Wang, Xun Huang, Eli Shechtman, Jinrong Xie, Yongxin Chen</p>
<p><strong>Published:</strong> 2025-11-26</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Video diffusion models achieve strong frame-level fidelity but still struggle with motion coherence, dynamics and realism, often producing jitter, ghosting, or implausible dynamics. A key limitation is that the standard denoising MSE objective provides no direct supervision on temporal consistency, allowing models to achieve low loss while still generating poor motion. We propose MoGAN, a motion-centric post-training framework that improves motion realism without reward models or human preference data. Built atop a 3-step distilled video diffusion model, we train a DiT-based optical-flow discriminator to differentiate real from generated motion, combined with a distribution-matching regularizer to preserve visual fidelity. With experiments on Wan2.1-T2V-1.3B, MoGAN substantially improves motion quality across benchmarks. On VBench, MoGAN boosts motion score by +7.3% over the 50-step teacher and +13.3% over the 3-step DMD model. On VideoJAM-Bench, MoGAN improves motion score by +7.4% over the teacher and +8.8% over DMD, while maintaining comparable or even better aesthetic and image-quality scores. A human study further confirms that MoGAN is preferred for motion quality (52% vs. 38% for the teacher; 56% vs. 29% for DMD). Overall, MoGAN delivers significantly more realistic motion without sacrificing visual fidelity or efficiency, offering a practical path toward fast, high-quality video generation. Project webpage is: https://xavihart.github.io/mogan.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于提升视频扩散模型运动质量的论文的全面中文摘要：</p>
<p><strong>论文题目：</strong> MoGAN: Improving Motion Quality in Video Diffusion via Few-Step Motion Adversarial Post-Training (MoGAN：通过几步运动对抗性后训练提升视频扩散模型的运动质量)</p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>视频扩散模型在生成高质量的单帧图像方面表现出色，但仍然难以实现连贯、动态且逼真的运动。现有模型通常采用像素级重建的均方误差（MSE）损失函数，这导致模型在时间一致性和运动真实性方面缺乏直接监督，从而生成带有抖动、重影或不合理动态的视频。</p>
<p><strong>2. 主要创新点/方法贡献：</strong></p>
<p>作者提出了 <strong>MoGAN (Motion-GAN Post-training)</strong>，一个新颖的、以运动为中心的对抗性后训练框架，旨在无需奖励模型或人类偏好数据即可显著提升视频扩散模型的运动真实感。其核心贡献包括：</p>
<ul>
<li><strong>基于光流的运动对抗性判别器：</strong> MoGAN 构建了一个基于 Diffusion Transformer (DiT) 的判别器，该判别器专门接收密集光流信息（而非像素或潜在空间信息），用于区分真实视频和生成视频的运动。这使得判别器能够更直接地关注运动的动态和一致性。</li>
<li><strong>几步蒸馏模型作为基础：</strong> MoGAN 在一个经过3步蒸馏的视频扩散模型之上进行后训练。这种蒸馏模型能够生成相对清晰的中间帧，从而保证了光流估计的可靠性。</li>
<li><strong>结合分布匹配蒸馏（DMD）正则化：</strong> 为了在提升运动质量的同时保持视觉保真度，MoGAN 集成了 DMD 作为一种正则化项，确保学生模型（MoGAN）的输出分布接近教师模型。</li>
<li><strong>稳定的训练策略：</strong> 论文提出了包括判别器正则化（R1/R2）和交替训练 DMD 与 MoGAN 损失在内的稳定训练技术，以防止训练不稳定和过拟合。</li>
<li><strong>高效的光流提取：</strong> 针对全分辨率视频解码光流的计算成本问题，MoGAN 采用了截断反向传播（BPTT）、梯度检查点和分块子采样等技术来高效地提取光流。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>显著提升运动质量：</strong> 在 VBench 和 VideoJAM-Bench 数据集上，MoGAN 在运动得分（Motion Score）上均取得了显著提升。与50步的教师模型相比，MoGAN 提升了约7.5%，而与3步的 DMD 模型相比，则提升了约10.5%。</li>
<li><strong>保持视觉保真度：</strong> MoGAN 在提升运动质量的同时，保持了与基线模型相当甚至更好的美学（Aesthetic）和图像质量（Image Quality）得分。</li>
<li><strong>人类评估的优势：</strong> 人类偏好研究表明，MoGAN 生成的视频在运动质量方面更受青睐（例如，在与教师模型对比中，52%的用户更喜欢 MoGAN，而38%喜欢教师模型；与DMD模型对比中，56%喜欢MoGAN，29%喜欢DMD）。</li>
<li><strong>高效生成：</strong> MoGAN 保持了3步蒸馏模型的快速推理速度，为实现快速、高质量的视频生成提供了一条实用路径。</li>
<li><strong>克服了现有方法的局限性：</strong> MoGAN 避免了依赖外部运动先验（限制灵活性）或基于像素/潜在空间的 GAN 损失（难以捕捉时间动态）的缺点。</li>
</ul>
<p><strong>4. 论文提及的局限性：</strong></p>
<ul>
<li><strong>依赖像素空间解码和2D光流估计器：</strong> 方法依赖于像素空间解码和预训练的2D光流估计器。2D光流估计器本身存在非物理性的局限，可能无法准确解释遮挡、平面外运动或快速运动。</li>
<li><strong>对小运动和复杂深度变化的敏感性：</strong> 光流在极小运动或复杂深度变化时可能变得不可靠。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li>探索潜在空间中的运动代理（latent-space motion surrogates）。</li>
<li>研究3D一致性或几何感知的运动场。</li>
<li>结合混合物理运动先验以进一步提升运动真实感。</li>
</ul>
<p><strong>总结：</strong></p>
<p>MoGAN 提出了一种创新的后训练方法，通过引入基于光流的运动对抗性判别器，有效地解决了视频扩散模型在运动质量方面的不足。该方法在不牺牲视觉保真度和生成效率的前提下，显著提升了视频的运动连贯性、动态性和真实感，为实现更逼真的视频生成开辟了新的途径。其对光流空间的直接利用以及与分布匹配蒸馏的结合，是其成功的关键。论文也指出了该方法在处理复杂运动和深度变化时的局限性，并提出了未来研究的可能方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose MoGAN, a motion-centric post-training framework that improves motion realism without reward models or human preference data.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.21592v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.21592v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-11-27 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
