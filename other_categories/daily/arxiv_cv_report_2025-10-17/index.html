<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-10-17 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DEâ«¶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Birdâs-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">è®ºæéè¯»</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-Î±
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-10-16/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-10-20/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-10-17">Arxiv Computer Vision Papers - 2025-10-17</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#prompt-based-adaptation-in-large-scale-vision-models-a-survey" class="nav-link">Prompt-based Adaptation in Large-scale Vision Models: A Survey</a>
                </li>
                <li class="nav-item">
                    <a href="#from-pixels-to-words-towards-native-vision-language-primitives-at-scale" class="nav-link">From Pixels to Words -- Towards Native Vision-Language Primitives at Scale</a>
                </li>
                <li class="nav-item">
                    <a href="#changinggrounding-3d-visual-grounding-in-changing-scenes" class="nav-link">ChangingGrounding: 3D Visual Grounding in Changing Scenes</a>
                </li>
                <li class="nav-item">
                    <a href="#c4d-4d-made-from-3d-through-dual-correspondences" class="nav-link">C4D: 4D Made from 3D through Dual Correspondences</a>
                </li>
                <li class="nav-item">
                    <a href="#mathcanvas-intrinsic-visual-chain-of-thought-for-multimodal-mathematical-reasoning" class="nav-link">MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning</a>
                </li>
                <li class="nav-item">
                    <a href="#omnimotion-multimodal-motion-generation-with-continuous-masked-autoregression" class="nav-link">OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression</a>
                </li>
                <li class="nav-item">
                    <a href="#3d-scene-prompting-for-scene-consistent-camera-controllable-video-generation" class="nav-link">3D Scene Prompting for Scene-Consistent Camera-Controllable Video Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#maskcaptioner-learning-to-jointly-segment-and-caption-object-trajectories-in-videos" class="nav-link">MaskCaptioner : Learning to Jointly Segment and Caption Object Trajectories in Videos</a>
                </li>
                <li class="nav-item">
                    <a href="#badas-context-aware-collision-prediction-using-real-world-dashcam-data" class="nav-link">BADAS: Context Aware Collision Prediction Using Real-World Dashcam Data</a>
                </li>
                <li class="nav-item">
                    <a href="#touch-text-guided-controllable-generation-of-free-form-hand-object-interactions" class="nav-link">TOUCH: Text-guided Controllable Generation of Free-Form Hand-Object Interactions</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-10-17">Arxiv Computer Vision Papers - 2025-10-17</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>å¥½çï¼è¿æ¯ä¸ä»½éå¯¹2025å¹´10æ15æ¥Arxivè®¡ç®æºè§è§è®ºæçæ¯æ¥æ¥åæ§è¡æè¦ï¼æ¨å¨å¸®å©å¿ç¢çç ç©¶äººåå¿«éäºè§£é¢åææ°è¿å±ã</p>
<hr />
<p><strong>æ¯æ¥Arxivè®¡ç®æºè§è§è®ºææ§è¡æè¦ (2025å¹´10æ15æ¥)</strong></p>
<p><strong>æ¦è¿°ä¸ä¸»è¦è¶å¿ï¼</strong></p>
<p>ä»æ¥Arxivè®ºæå±ç°äºè®¡ç®æºè§è§é¢åå¨<strong>å¤æ¨¡æçè§£ä¸çæã3Dåºæ¯çè§£ä¸éå»ºãä»¥åå·èº«æºè½ä¸äº¤äº</strong>æ¹é¢çæ¾èè¿å±ãæ ¸å¿è¶å¿åæ¬ï¼</p>
<ol>
<li><strong>å¤æ¨¡æèåä¸ç»ä¸ï¼</strong> è§è§-è¯­è¨æ¨¡åç»§ç»­åæ´æ·±å±æ¬¡çèåè¿è¿ï¼æ¨å¨å®ç°åççº§çè§è§-è¯­è¨åè¯­ï¼å¹¶æ¢ç´¢å¨æ°å­¦æ¨çãè¿å¨çæç­å¤æä»»å¡ä¸­çåºç¨ã</li>
<li><strong>3Dåºæ¯çå¨æä¸äº¤äºçè§£ï¼</strong> ç ç©¶éç¹ä»éæ3Déå»ºæ©å±å°å¨æåºæ¯ççè§£ï¼4Déå»ºï¼ã3Dè§è§å®ä½ä»¥åä¸åºæ¯äº¤äºççæã</li>
<li><strong>å¯æ§çæä¸å·èº«æºè½ï¼</strong> è®ºæå¼ºè°äºå¨è§é¢ãè¿å¨åæç©äº¤äºç­çæä»»å¡ä¸­å®ç°ç²¾ç»åæ§å¶çéè¦æ§ï¼å¹¶å¼å§å©ç¨çå®ä¸çæ°æ®è¿è¡å·èº«æºè½ï¼å¦ç¢°æé¢æµï¼çç ç©¶ã</li>
<li><strong>Promptå·¥ç¨çæ·±åï¼</strong> Prompt-basedæ¹æ³å¨å¤§åè§è§æ¨¡åä¸­çåºç¨æ¥çå¹¿æ³ï¼è¡¨æå¶ä½ä¸ºä¸ç§é«æéåºç­ç¥çéè¦æ§ã</li>
</ol>
<p><strong>ç¹å«æ¾èæåæ°çè®ºæï¼</strong></p>
<ul>
<li><strong>"From Pixels to Words -- Towards Native Vision-Language Primitives at Scale" (Haiwen Diao et al.)</strong>: è¿ç¯è®ºæå¯è½ä»£è¡¨äºè§è§-è¯­è¨æ¨¡ååå±çä¸ä¸ªéè¦æ¹åï¼å³è¶è¶ç®åçç¹å¾èåï¼æ¢ç´¢æ´æ·±å±æ¬¡çâåçâè§è§-è¯­è¨çè§£ååï¼ææä¸ºæªæ¥çå¤æ¨¡æAIå¥ å®åºç¡ã</li>
<li><strong>"C4D: 4D Made from 3D through Dual Correspondences" (Shizun Wang et al.)</strong>: æåºäºä¸ç§ä»3Dæ°æ®æå»º4Dï¼3D+æ¶é´ï¼åºæ¯çæ°æ¹æ³ï¼è¿å¯¹äºçè§£åå»ºæ¨¡å¨æä¸çå·æå¼åæ§æä¹ï¼å¯è½å¨èªå¨é©¾é©¶ãæºå¨äººåèæç°å®ç­é¢åæå¹¿æ³åºç¨ã</li>
<li><strong>"MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning" (Weikang Shi et al.)</strong>: å°è§è§é¾å¼æèï¼Visual Chain-of-Thoughtï¼å¼å¥å¤æ¨¡ææ°å­¦æ¨çï¼è¿å¯¹äºæåAIå¨å¤ææ¨çä»»å¡ä¸­çå¯è§£éæ§ååç¡®æ§è³å³éè¦ï¼æ¯è¿åæ´é«çº§è®¤ç¥æºè½çä¸æ­¥ã</li>
<li><strong>"OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression" (Zhe Li et al.)</strong>: æåºäºä¸ç§å¤æ¨¡æè¿å¨çææ¡æ¶ï¼å¶è¿ç»­æ©ç èªåå½æºå¶å¯è½å¨çæé«è´¨éãå¤æ ·åè¿å¨æ¹é¢è¡¨ç°åºè²ï¼å¯¹å¨ç»ãæ¸¸æåæºå¨äººé¢åæç´æ¥å½±åã</li>
</ul>
<p><strong>æ°å´ç ç©¶æ¹åæææ¯ï¼</strong></p>
<ul>
<li><strong>åçè§è§-è¯­è¨åè¯­ (Native Vision-Language Primitives):</strong> æ¨å¨æå»ºæ´æ·±å±æ¬¡ãæ´ç»ä¸çè§è§-è¯­è¨çè§£ååï¼èéç®åçç¹å¾æ¼æ¥ã</li>
<li><strong>4Dåºæ¯éå»ºä¸çè§£ (4D Scene Reconstruction &amp; Understanding):</strong> ä»éæ3Dåå¨æ3D+æ¶é´ç»´åº¦æ©å±ï¼ä»¥æ´å¥½å°ææçå®ä¸ççå¤ææ§ã</li>
<li><strong>è§è§é¾å¼æè (Visual Chain-of-Thought):</strong> å°æ¨çè¿ç¨å¯è§åï¼æé«å¤æ¨¡ææ¨¡åå¨å¤æä»»å¡ï¼å¦æ°å­¦æ¨çï¼ä¸­çå¯è§£éæ§åæ§è½ã</li>
<li><strong>è¿ç»­æ©ç èªåå½ (Continuous Masked Autoregression):</strong> ä¸ç§å¨çæä»»å¡ä¸­å®ç°é«è´¨éãå¤æ ·åè¾åºçæææºå¶ï¼å°¤å¶éç¨äºè¿å¨çæã</li>
<li><strong>3Dåºæ¯Prompting (3D Scene Prompting):</strong> å°Promptingææ¯ä»2Då¾åæ©å±å°3Dåºæ¯ï¼ä»¥å®ç°æ´ç²¾ç»çè§é¢çææ§å¶ã</li>
<li><strong>å·èº«æºè½ä¸­ççå®ä¸çæ°æ®åºç¨ (Real-World Data for Embodied AI):</strong> å©ç¨è¡è½¦è®°å½ä»ªç­çå®ä¸çæ°æ®è¿è¡ç¢°æé¢æµï¼æ¨å¨å·èº«æºè½å¨å®éåºç¨ä¸­çé²æ£æ§ã</li>
</ul>
<p><strong>å»ºè®®éè¯»å¨æçè®ºæï¼</strong></p>
<p>å¯¹äºå¸ææ·±å¥äºè§£åæ²¿è¿å±çç ç©¶äººåï¼å»ºè®®ä¼åéè¯»ä»¥ä¸è®ºæï¼</p>
<ol>
<li><strong>"From Pixels to Words -- Towards Native Vision-Language Primitives at Scale" (Haiwen Diao et al.)</strong>: äºè§£å¤æ¨¡æèåçæªæ¥æ¹åã</li>
<li><strong>"C4D: 4D Made from 3D through Dual Correspondences" (Shizun Wang et al.)</strong>: ææ¡4Dåºæ¯éå»ºçæ°èå¼ã</li>
<li><strong>"MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning" (Weikang Shi et al.)</strong>: æ¢ç´¢å¤æ¨¡ææ¨çåå¯è§£éæ§çæ°æ¹æ³ã</li>
<li><strong>"OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression" (Zhe Li et al.)</strong>: å³æ³¨å¤æ¨¡æçæåæ°åèªåå½æºå¶ã</li>
<li><strong>"Prompt-based Adaptation in Large-scale Vision Models: A Survey" (Xi Xiao et al.)</strong>: å¯¹äºå¸æäºè§£Promptå·¥ç¨å¨å¤§åè§è§æ¨¡åä¸­åºç¨ç°ç¶åæªæ¥è¶å¿çç ç©¶äººåï¼è¿ç¯ç»¼è¿°æ¯æä½³çèµ·ç¹ã</li>
</ol>
<hr />
<p>è¿ä»½æè¦æ¨å¨æä¾ä¸ä¸ªé«å±æ¬¡çæ¦è§ï¼å¸®å©æ¨å¿«éè¯å«ä¸æ¨ç ç©¶å´è¶£æç¸å³çè®ºæã</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2510.13219v1">Prompt-based Adaptation in Large-scale Vision Models: A Survey</a></li>
<li><a href="#2510.14979v1">From Pixels to Words -- Towards Native Vision-Language Primitives at Scale</a></li>
<li><a href="#2510.14965v1">ChangingGrounding: 3D Visual Grounding in Changing Scenes</a></li>
<li><a href="#2510.14960v1">C4D: 4D Made from 3D through Dual Correspondences</a></li>
<li><a href="#2510.14958v1">MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning</a></li>
<li><a href="#2510.14954v1">OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression</a></li>
<li><a href="#2510.14945v1">3D Scene Prompting for Scene-Consistent Camera-Controllable Video Generation</a></li>
<li><a href="#2510.14904v1">MaskCaptioner : Learning to Jointly Segment and Caption Object Trajectories in Videos</a></li>
<li><a href="#2510.14876v1">BADAS: Context Aware Collision Prediction Using Real-World Dashcam Data</a></li>
<li><a href="#2510.14874v1">TOUCH: Text-guided Controllable Generation of Free-Form Hand-Object Interactions</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2510.13219v1'></a></p>
<h2 id="prompt-based-adaptation-in-large-scale-vision-models-a-survey"><a href="https://arxiv.org/abs/2510.13219v1">Prompt-based Adaptation in Large-scale Vision Models: A Survey</a></h2>
<p><strong>Authors:</strong> Xi Xiao, Yunbei Zhang, Lin Zhao, Yiyang Liu, Xiaoying Liao, Zheda Mai, Xingjian Li, Xiao Wang, Hao Xu, Jihun Hamm, Xue Lin, Min Xu, Qifan Wang, Tianyang Wang, Cheng Han</p>
<p><strong>Published:</strong> 2025-10-15</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>In computer vision, Visual Prompting (VP) and Visual Prompt Tuning (VPT) have
recently emerged as lightweight and effective alternatives to full fine-tuning
for adapting large-scale vision models within the ``pretrain-then-finetune''
paradigm. However, despite rapid progress, their conceptual boundaries remain
blurred, as VP and VPT are frequently used interchangeably in current research,
reflecting a lack of systematic distinction between these techniques and their
respective applications. In this survey, we revisit the designs of VP and VPT
from first principles, and conceptualize them within a unified framework termed
Prompt-based Adaptation (PA). We provide a taxonomy that categorizes existing
methods into learnable, generative, and non-learnable prompts, and further
organizes them by injection granularity -- pixel-level and token-level. Beyond
the core methodologies, we examine PA's integrations across diverse domains,
including medical imaging, 3D point clouds, and vision-language tasks, as well
as its role in test-time adaptation and trustworthy AI. We also summarize
current benchmarks and identify key challenges and future directions. To the
best of our knowledge, we are the first comprehensive survey dedicated to PA's
methodologies and applications in light of their distinct characteristics. Our
survey aims to provide a clear roadmap for researchers and practitioners in all
area to understand and explore the evolving landscape of PA-related research.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºâPrompt-based Adaptation in Large-scale Vision Models: A Surveyâçä¸­ææè¦ï¼æ¶µçäºæ¨è¦æ±çææè¦ç¹ï¼</p>
<p><strong>è®ºææè¦ï¼å¤§åè§è§æ¨¡åä¸­çåºäºæç¤ºçéåºæ§ï¼ä¸é¡¹ç»¼è¿°</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³è®¡ç®æºè§è§é¢åä¸­ï¼è§è§æç¤ºï¼Visual Prompting, VPï¼åè§è§æç¤ºå¾®è°ï¼Visual Prompt Tuning, VPTï¼è¿ä¸¤ç§è½»éçº§æ¨¡åéåºæ¹æ³ä¹é´æ¦å¿µè¾¹çæ¨¡ç³çé®é¢ãå°½ç®¡å®ä»¬å¨å¤§åè§è§æ¨¡åçâé¢è®­ç»-å¾®è°âèå¼ä¸­åå¾äºå¿«éè¿å±å¹¶è¢«å¹¿æ³åºç¨ï¼ä½å½åç ç©¶ä¸­å¸¸å°äºèäºæ¢ä½¿ç¨ï¼ç¼ºä¹ç³»ç»æ§çåºåï¼è¿é»ç¢äºå¯¹å®ä»¬åèªææ¯ç¹ç¹åéç¨åºæ¯çæ·±å¥çè§£ãæ¬ç»¼è¿°çæ ¸å¿ç®æ æ¯æä¾ä¸ä¸ªç»ä¸çæ¡æ¶ï¼æ¸æ°å°çå®ååç±»è¿äºæ¹æ³ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>ç»ä¸æ¡æ¶ä¸åç±»æ³ï¼</strong> è®ºæå°VPåVPTä»ç¬¬ä¸æ§åçåºåï¼æ¦å¿µåä¸ºä¸ä¸ªç»ä¸çæ¡æ¶ï¼ç§°ä¹ä¸ºâåºäºæç¤ºçéåºæ§âï¼Prompt-based Adaptation, PAï¼ã
*   <strong>è¯¦ç»åç±»ï¼</strong> æåºäºä¸ç§å¨é¢çåç±»æ³ï¼å°ç°ææ¹æ³åä¸ºå¯å­¦ä¹ ï¼learnableï¼ãçæå¼ï¼generativeï¼åä¸å¯å­¦ä¹ ï¼non-learnableï¼æç¤ºã
*   <strong>æ³¨å¥ç²åº¦åºåï¼</strong> è¿ä¸æ­¥æ ¹æ®æç¤ºçæ³¨å¥ç²åº¦ï¼åç´ çº§åTokençº§ï¼å¯¹æ¹æ³è¿è¡ç»ç»ï¼è¿æ¯åºåVPåVPTçå³é®ãVPä¸»è¦å¨è¾å¥ç©ºé´è¿è¡åç´ çº§ä¿®æ¹ï¼èVPTåå¨æ¨¡ååé¨çTokenåºåä¸­æ³¨å¥å¯å­¦ä¹ çæç¤ºã
*   <strong>åºç¨é¢åæ´åï¼</strong> ç»¼è¿°äºPAå¨åç§ä¸åé¢åä¸­çåºç¨ï¼åæ¬å»å­¦å½±åã3Dç¹äºãè§è§-è¯­è¨ä»»å¡ï¼ä»¥åå¶å¨æµè¯æ¶é´éåºï¼Test-Time Adaptationï¼åå¯ä¿¡èµAIï¼Trustworthy AIï¼ä¸­çä½ç¨ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>PAçæææ§ï¼</strong> è®ºæå¼ºè°PAä½ä¸ºå¨éå¾®è°çè½»éçº§ä¸æææ¿ä»£æ¹æ¡ï¼å¨æ°æ®åéãèµæºåéåå¨æéåºç­å¤ç§åºæ¯ä¸è¡¨ç°åºæ¾èææã
*   <strong>æçæåï¼</strong> PAéè¿ä»æ´æ°å°éåæ°ï¼VPå¯è½ä¸æ´æ°åæ°ï¼VPTæ´æ°å°éæç¤ºTokenåå¤´é¨ï¼ï¼æ¾èéä½äºè®¡ç®åå­å¨ææ¬ï¼å°¤å¶éç¨äºååç¡¬ä»¶åå»¶è¿ææçé¨ç½²ç¯å¢ã
*   <strong>é²æ£æ§ä¸æ³åè½åï¼</strong> æç¤ºæºå¶è½å¤å¸®å©æ¨¡åæ´å¥½å°åºå¯¹é¢åæ¼ç§»ãåå¸ååï¼å¹¶å¨å°æ ·æ¬ãé¶æ ·æ¬ç­æ°æ®åéåºæ¯ä¸æé«æ³åè½åã
*   <strong>å¯ä¿¡èµAIçè´¡ç®ï¼</strong> PAå¨æåæ¨¡åé²æ£æ§ãç¼è§£åè§åç¡®ä¿éç§å®å¨æ¹é¢åæ¥ä½ç¨ï¼æ¯æå»ºå¯ä¿¡èµAIç³»ç»çéè¦ç»æé¨åã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>è®­ç»å¼éä¸ç¨³å®æ§ï¼</strong> å°½ç®¡PAåå°äºåæ°æçï¼ä½æ»è®­ç»æ¶é¿å¯è½å è¶åæ°æç´¢ååå§åä¸ç¨³å®æ§èå¢å ãVPæ¹æ³å¯è½è¡¨ç°åºä¸ç¨³å®æ§ï¼å¯¹æç¤ºéç½®çå¾®å°æ°å¨ææã
*   <strong>æ¨çå»¶è¿ï¼</strong> é¢å¤çæç¤ºç»ä»¶ï¼æ è®ºæ¯VPçè¾å¥ç©ºé´ä¿®æ¹è¿æ¯VPTçTokenæ³¨å¥ï¼å¯è½å¯¼è´æ¨çå»¶è¿åé¢å¤çåå­æ¶èã
*   <strong>çå®ä¸çç¯å¢è¯ä¼°ä¸è¶³ï¼</strong> å½åPAæ¹æ³çè¯ä¼°ä¸»è¦ä¾èµæ ååå­¦æ¯åºåï¼è¿äºåºåå¯è½æ æ³åç¡®åæ çå®ä¸çåºæ¯çå¤ææ§ååå¸ååã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>è®­ç»æçä¸ç¨³å®æ§ï¼</strong> è¿ä¸æ­¥ç ç©¶è®­ç»æ·å¾ãæ£æµåçº æ­£è®­ç»ä¸ç¨³å®æ§çç­ç¥ã
*   <strong>æ··åæ¹æ³ï¼</strong> æ¢ç´¢ç»åVPåVPTä¼å¿çæ··åæ¹æ³ï¼ä¾å¦ä½¿ç¨çæå¨æä¾åå§ç©ºé´æç¤ºï¼VPï¼ï¼åæ¶ä½¿ç¨å¯å­¦ä¹ çæ¡ä»¶æç¥Tokenï¼VPTï¼æ¥å¼å¯¼æ¨¡ååé¨ç¹å¾æåã
*   <strong>çå®ä¸çé¨ç½²ï¼</strong> ä¼åå¼åè½å¤åºå¯¹å¤æå¼æè§è§ä¸ä¸æçé²æ£æ¹æ³ï¼ä»¥å¼¥åå­¦æ¯ç ç©¶ä¸å®éé¨ç½²ä¹é´çå·®è·ã
*   <strong>å®å¨å¯¹é½ï¼</strong> æç»­ç ç©¶å¦ä½æ£æµåçº æ­£æ¨¡åè¡ä¸ºä¸­çåå·®ãæ¶æåå®¹çæç­é®é¢ï¼ç¡®ä¿PAæ¹æ³ç¬¦åäººç±»ä»·å¼è§åç®æ ã
*   <strong>çè®ºåºç¡ï¼</strong> æ·±å¥æ¢ç´¢PAå¦ä½è¯±å¯¼æ¨¡åè¡ä¸ºååãè§è§æç¤ºå­¦ä¹ äºä»ä¹ä»¥åPAæ¹æ³å¨ä¸åéåºè®¾ç½®ä¸­çæææ§ç­çè®ºé®é¢ã</p>
<p>æ»èè¨ä¹ï¼è¿ç¯ç»¼è¿°ä¸ºç ç©¶äººååå®è·µèæä¾äºä¸ä¸ªå³äºå¤§åè§è§æ¨¡åä¸­åºäºæç¤ºçéåºæ§æ¹æ³çæ¸æ°è·¯çº¿å¾ï¼ç³»ç»å°æ¢³çäºå¶æ¦å¿µãæ¹æ³ãåºç¨ãææåæªæ¥æ¹åï¼å¯¹äºæ¨å¨è¯¥é¢åçåå±å·æéè¦æä¹ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To the
best of our knowledge, we are the first comprehensive survey dedicated to PA's
methodologies and applications in light of their distinct characteristics.</li>
<li>Our
survey aims to provide a clear roadmap for researchers and practitioners in all
area to understand and explore the evolving landscape of PA-related research.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.13219v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.13219v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.14979v1'></a></p>
<h2 id="from-pixels-to-words-towards-native-vision-language-primitives-at-scale"><a href="https://arxiv.org/abs/2510.14979v1">From Pixels to Words -- Towards Native Vision-Language Primitives at Scale</a></h2>
<p><strong>Authors:</strong> Haiwen Diao, Mingxuan Li, Silei Wu, Linjun Dai, Xiaohua Wang, Hanming Deng, Lewei Lu, Dahua Lin, Ziwei Liu</p>
<p><strong>Published:</strong> 2025-10-16</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>The edifice of native Vision-Language Models (VLMs) has emerged as a rising
contender to typical modular VLMs, shaped by evolving model architectures and
training paradigms. Yet, two lingering clouds cast shadows over its widespread
exploration and promotion: (-) What fundamental constraints set native VLMs
apart from modular ones, and to what extent can these barriers be overcome? (-)
How to make research in native VLMs more accessible and democratized, thereby
accelerating progress in the field. In this paper, we clarify these challenges
and outline guiding principles for constructing native VLMs. Specifically, one
native VLM primitive should: (i) effectively align pixel and word
representations within a shared semantic space; (ii) seamlessly integrate the
strengths of formerly separate vision and language modules; (iii) inherently
embody various cross-modal properties that support unified vision-language
encoding, aligning, and reasoning. Hence, we launch NEO, a novel family of
native VLMs built from first principles, capable of rivaling top-tier modular
counterparts across diverse real-world scenarios. With only 390M image-text
examples, NEO efficiently develops visual perception from scratch while
mitigating vision-language conflicts inside a dense and monolithic model
crafted from our elaborate primitives. We position NEO as a cornerstone for
scalable and powerful native VLMs, paired with a rich set of reusable
components that foster a cost-effective and extensible ecosystem. Our code and
models are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO.</p>
<p><strong>Analysis:</strong></p>
<p>è¿ç¯è®ºæâFrom Pixels to Words -- Towards Native Vision-Language Primitives at Scaleâç±Haiwen Diaoç­äººæ°åï¼æ¨å¨è§£å³åçè§è§-è¯­è¨æ¨¡åï¼VLMsï¼å¨ä¸ä¼ ç»æ¨¡ååVLMç«äºæ¶æé¢ä¸´çææï¼å¹¶æ¨å¨è¯¥é¢åçç ç©¶è¿å±ã</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è®ºæä¸»è¦æ¢è®¨äºåçVLMå¨å¹¿æ³æ¢ç´¢åæ¨å¹¿ä¸­é¢ä¸´çä¸¤ä¸ªæ ¸å¿é®é¢ï¼
*   åçVLMä¸æ¨¡ååVLMä¹é´çæ ¹æ¬åºå«æ¯ä»ä¹ï¼ä»¥åå¦ä½åæè¿äºéç¢ï¼
*   å¦ä½ä½¿åçVLMçç ç©¶æ´æäºè®¿é®åæ°ä¸»åï¼ä»èå éè¯¥é¢åçè¿å±ï¼</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è®ºææåºäºæå»ºåçVLMçæå¯¼ååï¼å¹¶æ¨åºäºä¸ä¸ªåä¸º<strong>NEO</strong>çæ°ååçVLMå®¶æï¼å¶å³é®åæ°åæ¬ï¼
*   <strong>åçVLMåºåï¼Native VLM Primitiveï¼ï¼</strong> NEOè®¾è®¡äºä¸ä¸ªç»ä¸çè§è§-è¯­è¨åºåï¼è½å¤å¨ä¸ä¸ªæ¨¡åä¸­åæ¶æ´åè·¨æ¨¡æçç¼ç ãå¯¹é½åæ¨çãè¿åæ¬ï¼
    *   çµæ´»çä½ç½®ç¼ç æ¹æ¡ï¼æææ³åå°å¨æç©ºé´ç»æã
    *   å¤å¤´åçæ³¨æåï¼MHNAï¼ï¼å±åå¤çè§è§-ææ¬è¿æ¥ã
    *   åçæè½¬ä½ç½®åµå¥ï¼Native-RoPEï¼ï¼å·ææ¨¡æç¹å®é¢çï¼å¼å®¹é¢è®­ç»LLMæéå¹¶å¸æ¶åå§VEçäº¤äºæ¨¡å¼ã
*   <strong>é¢ç¼å²åºï¼Pre-Bufferï¼ååLLMï¼Post-LLMï¼æ¶æï¼</strong> ä¸ºäºé«æå°æ©å±è§è§è®­ç»å¹¶ç¡®ä¿åç´ -è¯è¯­å¯¹é½çä¸è´æ§ï¼NEOå°éª¨å¹²ç½ç»ååä¸ºé¢ç¼å²åºååLLMå±ãè¿ç§è®¾è®¡å¨é¢è®­ç»é¶æ®µä½¿é¢è®­ç»LLMè½å¤å¼å¯¼è§è§å­¦ä¹ ï¼å¹¶å¨åç»­é¶æ®µå»ºç«è¿è´¯çç¸å³æ§ãå¨è®­ç»åæï¼è¿ç§ååä¼æ¶å¤±ï¼å½¢æä¸ä¸ªç»ä¸çæ¶æã
*   <strong>ç«¯å°ç«¯è®­ç»èå¼ï¼</strong> NEOéè¿ç®åçç«¯å°ç«¯è®­ç»ï¼å¨ä»3.9äº¿å¾å-ææ¬ç¤ºä¾ä¸ï¼ä»å¤´å¼å§é«æå°åå±è§è§æç¥è½åï¼åæ¶ç¼è§£å¯éãåçæ¨¡ååé¨çè§è§-è¯­è¨å²çªã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>ç«äºæ§è¡¨ç°ï¼</strong> å°½ç®¡è®­ç»æ°æ®åè®¡ç®èµæºç¸å¯¹æéï¼NEOå¨2Bå8Bè§æ¨¡ä¸åå±ç°åºé«åº¦ç«äºæ§çæ§è½ï¼å¨å¤ä¸ªåºåæµè¯ä¸­ä¸é¡¶çº§çæ¨¡ååVLMï¼å¦Qwen2-VLãInternVL2.5ç­ï¼ç¸åª²ç¾ï¼çè³è¶è¶äºè®¸å¤ä½¿ç¨æ´å¤è®­ç»èµæºçåçVLMã
*   <strong>é«æçåç´ -è¯è¯­å¯¹é½åæ¨çï¼</strong> NEOéè¿å¶ç»ä¸çåºåè®¾è®¡åè®­ç»ç­ç¥ï¼è½å¤ä»å¤´å¼å§å°è§è§è¾å¥ä¸ææ¬ç¹å¾å¯¹é½ï¼å¹¶æ¯æå¤æçè§è§æ¨çã
*   <strong>å¯éç¨ç»ä»¶åçæç³»ç»ï¼</strong> NEOæä¾äºä¸°å¯çå¯éç¨ç»ä»¶ï¼ä¿è¿äºææ¬æçåå¯æ©å±ççæç³»ç»ï¼éä½äºåçVLMå¼åçé¨æ§ã
*   <strong>å¯¹æªæ¥å¤æ¨¡æç³»ç»çå¯ç¤ºï¼</strong> è®ºæè¡¨æï¼ä¸ä¸ä»£å¤æ¨¡æç³»ç»å¯ä»¥æºäºåçãç»ä¸ä¸æ¬è´¨ä¸å¤æ¨¡æçæ¶æã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>æ°æ®åè®¡ç®èµæºéå¶ï¼</strong> NEOçæ§è½ä»åéäºç¨ç¼ºçè®­ç»æ°æ®åæéçè®¡ç®èµæºï¼å°¤å¶æ¯å¨ç¥è¯å¯éååOCRé¢åã
*   <strong>æ æ³å®å¨ä»å¤´è®­ç»ï¼</strong> åéäºå½åçææ¬è¯­æåºåè®¡ç®èµæºï¼NEOæ æ³å¨ä¸ä¾èµç°æLLMåå§åçæåµä¸å®å¨ä»å¤´è®­ç»ä¸ä¸ªå®å¨åççæ¨¡åãè¿éå¶äºç¼è§£è¯­è¨æ¨¡æä¸»å¯¼å°ä½å¯è½å¸¦æ¥çæ½å¨åå·®çè½åã
*   <strong>å¨æäºä»»å¡ä¸çä¸è¶³ï¼</strong> NEOå¨ç¥è¯/OCRå¯éåä»»å¡ï¼å¦MMMUãInfoVQAåTextVQAï¼ä¸è¡¨ç°ç¥éä¸ç­¹ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æç»­æå¥åæ©å±ï¼</strong> æç»­æå¥å¤§éèµæºï¼å°¤å¶æ¯å¨é¢è®­ç»é¶æ®µï¼ä»¥ååéæ¾NEOçæ§è½æ½åã
*   <strong>å¼æ¾å³é®ç»ä»¶ï¼</strong> å¨ä¸­é´å¼åé¶æ®µéæ©æ§å°å¼æ¾å³é®ç»ä»¶ï¼ä»¥éä½æªæ¥ç ç©¶äººåçåç»­è®­ç»ææ¬ï¼å¹¶å¸å¼æ´å¤åçè§è§-è¯­è¨æ¨¡åç ç©¶ã
*   <strong>æ¢ç´¢å¨è°±æ¨¡åè½åï¼</strong> æ©å±æ¨¡åè§æ¨¡æ¯æ¨å¨å®éåºç¨çå³é®å ç´ ï¼NEO-2.2Bçæ§è½å·²æ¥è¿åç­å®¹éçæ¨¡ååVLMï¼è¡¨æ0.6å°80äº¿åæ°èå´åçæ¨¡åè®¾è®¡å²å­¦å·²è¶äºæçã
*   <strong>æ¶æååºç¨åçº§ï¼</strong> å°NEOè§ä¸ºä¸ä¸ªè§è§-è¯­è¨æºè½çæ°èå¼ï¼å©ç¨ç«¯å°ç«¯è®­ç»åç»ä¸æ¶æï¼æ¶é¤æå¨æ½å çåå·®åæ©å±å¤ææ§ãå°NEOæ©å±å°è§é¢çæãé¿è§é¢çè§£åå·èº«AIç­é¢åã
*   <strong>å»ä¸­å¿åå¤æ¨¡æè®­ç»ï¼</strong> æ¢ç´¢ä»å¤´å¼å§è¿è¡å¤æ¨¡æè®­ç»ï¼ä»¥è¿ä¸æ­¥æ­ç¤ºåçVLMæ¶æçæ§è½ä¸éã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Hence, we launch NEO, a novel family of
native VLMs built from first principles, capable of rivaling top-tier modular
counterparts across diverse real-world scenarios.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.14979v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.14979v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.14965v1'></a></p>
<h2 id="changinggrounding-3d-visual-grounding-in-changing-scenes"><a href="https://arxiv.org/abs/2510.14965v1">ChangingGrounding: 3D Visual Grounding in Changing Scenes</a></h2>
<p><strong>Authors:</strong> Miao Hu, Zhiwei Huang, Tai Wang, Jiangmiao Pang, Dahua Lin, Nanning Zheng, Runsen Xu</p>
<p><strong>Published:</strong> 2025-10-16</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Real-world robots localize objects from natural-language instructions while
scenes around them keep changing. Yet most of the existing 3D visual grounding
(3DVG) method still assumes a reconstructed and up-to-date point cloud, an
assumption that forces costly re-scans and hinders deployment. We argue that
3DVG should be formulated as an active, memory-driven problem, and we introduce
ChangingGrounding, the first benchmark that explicitly measures how well an
agent can exploit past observations, explore only where needed, and still
deliver precise 3D boxes in changing scenes. To set a strong reference point,
we also propose Mem-ChangingGrounder, a zero-shot method for this task that
marries cross-modal retrieval with lightweight multi-view fusion: it identifies
the object type implied by the query, retrieves relevant memories to guide
actions, then explores the target efficiently in the scene, falls back when
previous operations are invalid, performs multi-view scanning of the target,
and projects the fused evidence from multi-view scans to get accurate object
bounding boxes. We evaluate different baselines on ChangingGrounding, and our
Mem-ChangingGrounder achieves the highest localization accuracy while greatly
reducing exploration cost. We hope this benchmark and method catalyze a shift
toward practical, memory-centric 3DVG research for real-world applications.
Project page: https://hm123450.github.io/CGB/ .</p>
<p><strong>Analysis:</strong></p>
<p>ä»¥ä¸æ¯Miao Huç­äººæ°åçè®ºæâChangingGrounding: 3D Visual Grounding in Changing Scenesâçå¨é¢æè¦ï¼</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³ç°å®ä¸çæºå¨äººé¢ä¸´çä¸ä¸ªæ ¸å¿ææï¼å¦ä½å¨åºæ¯ä¸æ­ååçç¯å¢ä¸­ï¼æ ¹æ®èªç¶è¯­è¨æä»¤åç¡®ææå°å®ä½ç®æ ç©ä½ãç°æç3Dè§è§å®ä½ï¼3DVGï¼æ¹æ³å¤§å¤åè®¾åºæ¯æ¯éæçï¼ä¾èµäºå®æ´ä¸ææ°çç¹äºï¼è¿å¯¼è´äºæè´µçéå¤æ«æåé¨ç½²éç¢ãå æ­¤ï¼ç ç©¶é®é¢æ¯å¦ä½å°3DVGéæ°å®ä¹ä¸ºä¸ä¸ªä¸»å¨çãè®°å¿é©±å¨çé®é¢ï¼ä½¿æºå¨äººè½å¤å©ç¨è¿å»çè§å¯ãä»å¨éè¦æ¶è¿è¡æ¢ç´¢ï¼å¹¶ä»è½å¨å¨æåºæ¯ä¸­æä¾ç²¾ç¡®ç3Dè¾¹çæ¡ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>ChangingGroundingåºåï¼</strong> è®ºæå¼å¥äºé¦ä¸ªä¸é¨ç¨äºè¡¡éæºå¨äººå¨åååºæ¯ä¸­3DVGæ§è½çåºåãè¯¥åºåæç¡®è¯ä¼°äºä»£çå©ç¨è¿å»è§å¯ãé«ææ¢ç´¢åç²¾ç¡®3Då®ä½çè½åã
*   <strong>Mem-ChangingGrounderæ¹æ³ï¼</strong> æåºäºä¸ç§é¶æ ·æ¬ï¼zero-shotï¼æ¹æ³æ¥è§£å³æ­¤ä»»å¡ï¼è¯¥æ¹æ³ç»åäºè·¨æ¨¡ææ£ç´¢åè½»éçº§å¤è§å¾èåãå¶æ ¸å¿æµç¨åæ¬ï¼
    *   <strong>æ¥è¯¢åç±»ï¼</strong> è¯å«æ¥è¯¢ä¸­éå«çç©ä½ç±»åã
    *   <strong>è®°å¿æ£ç´¢ï¼</strong> æ£ç´¢ç¸å³è®°å¿ä»¥æå¯¼è¡å¨ã
    *   <strong>é«ææ¢ç´¢ï¼</strong> å¨åºæ¯ä¸­é«ææ¢ç´¢ç®æ ï¼å¹¶å¨ååæä½æ ææ¶è¿è¡åéã
    *   <strong>å¤è§å¾æ«æä¸èåï¼</strong> å¯¹ç®æ è¿è¡å¤è§å¾æ«æï¼å¹¶å°èåçè¯æ®æå½±ä»¥è·å¾åç¡®çç©ä½è¾¹çæ¡ã
*   <strong>è¡å¨ç­ç¥ï¼OSSåSRASï¼ï¼</strong> å¼å¥äºå¨ååºæ¯æ«æå¨ï¼Omnidirectional Scene Scanner, OSSï¼åç©ºé´å³ç³»æç¥æ«æå¨ï¼Spatial Relation Aware Scanner, SRASï¼ä¸¤ç§è¡å¨ç­ç¥ï¼ç¨äºå¨æªç¥åºæ¯ä¸­æ¢ç´¢åå®ä½ç®æ ç©ä½ã
*   <strong>æ°æ®éæå»ºï¼</strong> åºäº3RScanæ°æ®éæå»ºäºä¸ä¸ªæ°çChangingGroundingæ°æ®éï¼åå«ç©ºé´å³ç³»æè¿°ãRGB-Då¾åãç¸æºå§¿æåç½æ ¼æä»¶ï¼ä»¥æ¨¡æç©ä½ç§»å¨åçææ°è§å¯ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>Mem-ChangingGrounderçä¼è¶æ§ï¼</strong> å¨ChangingGroundingåºåæµè¯ä¸­ï¼Mem-ChangingGrounderå¨ä½åè¾¨çåé«åè¾¨çè®¾ç½®ä¸åå®ç°äºæé«çå®ä½ç²¾åº¦ï¼Acc@0.25åå«ä¸º29.2%å36.8%ï¼ï¼åæ¶æ¾èéä½äºæ¢ç´¢ææ¬ï¼å¨ä½ææ¬Caåè¿å¨ææ¬Cmï¼ã
*   <strong>æçä¸åç¡®æ§çå¹³è¡¡ï¼</strong> å®éªç»æè¡¨æï¼è¯¥æ¹æ³å¨åç¡®æ§åæçä¹é´åå¾äºåè¶çå¹³è¡¡ï¼éè¿å¨ç§»å¨åå¨è¯¢è®°å¿å¹¶æ§è¡æéå¯¹æ§çç­å¨ä½ï¼é¿åäºé¿æ¶é´çæ¢ç´¢å¾ªç¯ã
*   <strong>åºçº¿æ¯è¾ï¼</strong> ä¸âæ¼«æ¸¸å®ä½âï¼Wandering Groundingï¼ãâä¸­å¿æè½¬å®ä½âï¼Central Rotation Groundingï¼åâä»è®°å¿å®ä½âï¼Memory-Only Groundingï¼ç­åºçº¿æ¹æ³ç¸æ¯ï¼Mem-ChangingGrounderå¨åç¡®æ§åææ¬æ¹é¢åè¡¨ç°åºæ¾èä¼å¿ã
*   <strong>è®°å¿åæ¢ç´¢çéè¦æ§ï¼</strong> æ¶èç ç©¶è¯å®äºè®°å¿ç­ç¥ååéæºå¶çæææ§ï¼ä»¥åå¤è§å¾æå½±å¯¹æé«å®ä½åç¡®æ§çè´¡ç®ã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
*   <strong>CGBåºåçå±éæ§ï¼</strong> å½åæ°æ®éä»æ¨¡æç®æ åå¶å¨å´ç¯å¢çç¸å¯¹ä½ç½®ååï¼æªèèåç§ååãç©ä½å¤è§å±æ§ï¼é¢è²ãææãåå½¢ï¼æå¨æåºæ¯äº¤äºç­å³é®å ç´ ãæ­¤å¤ï¼ç¼ºä¹âç©ä½Aå¨ç©ä½Båé¢âç­ç»å¯¹ç©ºé´å³ç³»æè¿°ã
*   <strong>MCGæ¹æ³çå±éæ§ï¼</strong>
    *   <strong>VLMè½åä¾èµï¼</strong> MCGä¸¥éä¾èµåºå±è§è§-è¯­è¨æ¨¡åï¼VLMï¼çè½åï¼å¶æ§è½åVLMè½ååç°å®ä¸çåºæ¯å¤ææ§çå½±åã
    *   <strong>æ¸²æå¾åçåªå£°ï¼</strong> æ¸²æè¿ç¨å¼å¥çåªå£°ï¼å¦RGBå¾åä¸­çä¼ªå½±ãæ·±åº¦å¾çä¸åç¡®æ§ï¼ä»¥åæ¸²æå¾åä¸çå®å¾åä¹é´çåºæå·®å¼ï¼å¯è½å½±åå®ä½åç¡®æ§ã
    *   <strong>2Dæ¨¡åå¼å¥çåªå£°ï¼</strong> MCGä¾èµ2Dç©ä½æ£æµå¨ååå²ç½ç»ï¼è¿äºæ¨¡åå¯è½å­å¨æ¼æ£ãè¯¯æ¥ãè¾¹çæ¡ä¸ç²¾ç¡®ååå²éè¯¯ï¼è¿äºç¼ºé·ä¼å½±åæç»çå®ä½åç¡®æ§ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æé«VLMçé²æ£æ§ï¼</strong> å¼åæ´é²æ£çè§è§-è¯­è¨æ¨¡åï¼ä»¥å¤çå¤æçç°å®ä¸çè§è§ä¿¡æ¯å¹¶åå°åªå£°å½±åã
*   <strong>å¢å¼ºå¤æ¨¡æéæï¼</strong> æ¢ç´¢æ´å¥½å°éæå¤æ¨¡ææ°æ®ï¼è§è§ãè¯­è¨åç©ºé´ä¿¡æ¯ï¼ä»¥æé«å®ä½åç¡®æ§çæ¹æ³ã
*   <strong>æ©å±åºåå¤æ ·æ§ï¼</strong> éè¿å¢å æ´å¤æ ·åçåºæ¯ï¼åæ¬åç§ååãç©ä½å¤è§åå¨æäº¤äºï¼æ¥æ©å±CGBåºåã
*   <strong>åå°æ¸²ææ°æ®ä¸­çåªå£°ï¼</strong> ç ç©¶æå°åæ¸²æè¿ç¨ä¸­å¼å¥çåªå£°ï¼å¹¶å¼¥åçå®å¾åä¸æ¸²æå¾åä¹é´å·®è·çæ¹æ³ã
*   <strong>æ¨è¿2Då°3Dæå½±ææ¯ï¼</strong> æé«2Dç©ä½æ£æµååå²æ¨¡åçåç¡®æ§åå¯é æ§ï¼ä»¥å¢å¼ºæ´ä½å®ä½æ§è½ã</p>
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæéè¿å¼å¥ChangingGroundingåºååMem-ChangingGrounderæ¹æ³ï¼ä¸ºå¨æåºæ¯ä¸­ç3Dè§è§å®ä½ä»»å¡å¼è¾äºæ°æ¹åãå®å¼ºè°äºè®°å¿åé«ææ¢ç´¢å¨å®éæºå¨äººåºç¨ä¸­çéè¦æ§ï¼å¹¶ä¸ºæªæ¥è¯¥é¢åçç ç©¶å¥ å®äºåå®åºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We argue that
3DVG should be formulated as an active, memory-driven problem, and we introduce
ChangingGrounding, the first benchmark that explicitly measures how well an
agent can exploit past observations, explore only where needed, and still
deliver precise 3D boxes in changing scenes.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.14965v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.14965v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.14960v1'></a></p>
<h2 id="c4d-4d-made-from-3d-through-dual-correspondences"><a href="https://arxiv.org/abs/2510.14960v1">C4D: 4D Made from 3D through Dual Correspondences</a></h2>
<p><strong>Authors:</strong> Shizun Wang, Zhenxiang Jiang, Xingyi Yang, Xinchao Wang</p>
<p><strong>Published:</strong> 2025-10-16</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Recovering 4D from monocular video, which jointly estimates dynamic geometry
and camera poses, is an inevitably challenging problem. While recent
pointmap-based 3D reconstruction methods (e.g., DUSt3R) have made great
progress in reconstructing static scenes, directly applying them to dynamic
scenes leads to inaccurate results. This discrepancy arises because moving
objects violate multi-view geometric constraints, disrupting the
reconstruction. To address this, we introduce C4D, a framework that leverages
temporal Correspondences to extend existing 3D reconstruction formulation to
4D. Specifically, apart from predicting pointmaps, C4D captures two types of
correspondences: short-term optical flow and long-term point tracking. We train
a dynamic-aware point tracker that provides additional mobility information,
facilitating the estimation of motion masks to separate moving elements from
the static background, thus offering more reliable guidance for dynamic scenes.
Furthermore, we introduce a set of dynamic scene optimization objectives to
recover per-frame 3D geometry and camera parameters. Simultaneously, the
correspondences lift 2D trajectories into smooth 3D trajectories, enabling
fully integrated 4D reconstruction. Experiments show that our framework
achieves complete 4D recovery and demonstrates strong performance across
multiple downstream tasks, including depth estimation, camera pose estimation,
and point tracking. Project Page: https://littlepure2333.github.io/C4D</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºShizun Wang, Zhenxiang Jiang, Xingyi Yang, Xinchao Wangæ°åçè®ºæâC4D: 4D Made from 3D through Dual Correspondencesâçå¨é¢æè¦ã</p>
<p><strong>è®ºææè¦ï¼C4D: éè¿åéå¯¹åºä»3Dæå»º4D</strong></p>
<p>è¿ç¯è®ºæãC4D: 4D Made from 3D through Dual Correspondencesãæåºäºä¸ç§æ°é¢çæ¡æ¶C4Dï¼æ¨å¨è§£å³ä»åç®è§é¢ä¸­æ¢å¤å¨æåºæ¯4Dè¡¨ç¤ºçæææ§é®é¢ã4Déå»ºä¸ä»æ¶åä¼°è®¡å¨æåºæ¯çå ä½å½¢ç¶ï¼è¿åæ¬ç¸æºå§¿æå3Dç¹è·è¸ªã</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢</strong>
ç°æçåºäºç¹å¾ï¼pointmap-basedï¼ç3Déå»ºæ¹æ³ï¼å¦DUSt3Rï¼å¨éæåºæ¯ä¸­åå¾äºæ¾èè¿å±ï¼ä½ç´æ¥åºç¨äºå¨æåºæ¯æ¶ï¼ç±äºç§»å¨ç©ä½è¿åäºå¤è§è§å ä½çº¦æï¼å¯¼è´éå»ºç»æä¸åç¡®ãæ ¸å¿é®é¢æ¯å¦ä½å¨å¨æåºæ¯ä¸­å®ç°åç¡®ãå¹³æ»ä¸æ¶é´ä¸è´ç4Déå»ºï¼åæ¬æ¯å¸§3Då ä½ãç¸æºå§¿æå3Dç¹è½¨è¿¹ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®</strong>
C4Dæ¡æ¶éè¿å¼å¥âåéå¯¹åºâï¼Dual Correspondencesï¼å°ç°æç3Déå»ºæ©å±å°4Dï¼å¶ä¸»è¦åæ°åæ¬ï¼</p>
<ul>
<li><strong>å¨ææç¥ç¹è·è¸ªå¨ï¼DynPTï¼</strong>ï¼C4Dè®­ç»äºä¸ä¸ªè½å¤é¢æµç¹å¨ä¸çåæ ç³»ä¸­æ¯å¦å¨æçè·è¸ªå¨ãè¿è¶è¶äºä¼ ç»2Dç¹è·è¸ªå¨ä»é¢æµä½ç½®åé®æ¡çè½åï¼ä¸ºåºåç¸æºè¿å¨åç©ä½èªèº«è¿å¨æä¾äºå³é®ä¿¡æ¯ã</li>
<li><strong>å¯¹åºå¼å¯¼çè¿å¨æ©ç ä¼°è®¡</strong>ï¼å©ç¨ç­æ¶åæµï¼optical flowï¼åé¿æ¶ç¹è·è¸ªï¼DynPTï¼ä¸¤ç§å¯¹åºï¼C4Dè½å¤çæå¯é çè¿å¨æ©ç ãè¿äºæ©ç ç¨äºå°å¨æåç´ ä»éæèæ¯ä¸­åç¦»åºæ¥ï¼ä»èå¨éæåºåè¿è¡æ´åç¡®çç¸æºåæ°ä¼°è®¡åå ä½éå»ºã</li>
<li><strong>å¯¹åºè¾å©çå¨æåºæ¯ä¼åç®æ </strong>ï¼C4Då¼å¥äºä¸ç³»åæ°çä¼åç®æ ï¼åæ¬ï¼<ul>
<li><strong>ç¸æºè¿å¨å¯¹é½ï¼CMAï¼</strong>ï¼ç¡®ä¿ä¼°è®¡çèªæè¿å¨ä¸éæåºåçåæµä¸è´ã</li>
<li><strong>ç¸æºè½¨è¿¹å¹³æ»åº¦ï¼CTSï¼</strong>ï¼éè¿æ©ç½è¿ç»­å¸§ä¹é´ç¸æºæè½¬åå¹³ç§»ççªç¶ååï¼å¼ºå¶ç¸æºè¿å¨å¹³æ»ã</li>
<li><strong>ç¹è½¨è¿¹å¹³æ»åº¦ï¼PTSï¼</strong>ï¼éè¿å¯¹ç¨ç3Dç¹è½¨è¿¹è¿è¡èªéåºå æçä¸ç»´å·ç§¯å¹³æ»ï¼ç¶åéè¿çº¿æ§æ··åä½ç§»ï¼LBDï¼å°å¶ä¼ æ­å°ææç¹ï¼ç¡®ä¿3Dç¹è½¨è¿¹çæ¶é´å¹³æ»æ§ã</li>
</ul>
</li>
<li><strong>å®å¨éæ4Déå»º</strong>ï¼éè¿èåé¢æµç¹å¾åä¸è¿°åéå¯¹åºï¼C4Då°2Dè½¨è¿¹æåä¸ºå¹³æ»ç3Dè½¨è¿¹ï¼å®ç°äºæ¯å¸§3Då ä½åç¸æºåæ°çå®å¨éæ4Dæ¢å¤ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹</strong>
å®éªç»æè¡¨æï¼C4Dæ¡æ¶å¨å¨æåºæ¯éå»ºæ¹é¢è¡¨ç°åºè²ï¼å¹¶å¨å¤ä¸ªä¸æ¸¸ä»»å¡ä¸­å±ç¤ºäºå¼ºå¤§çæ§è½ï¼</p>
<ul>
<li><strong>æ·±åº¦ä¼°è®¡</strong>ï¼C4Då¨SintelãBonnåKITTIæ°æ®éä¸å®ç°äºç«äºæ§çæ·±åº¦ä¼°è®¡æ§è½ï¼å°¤å¶å¨å°ºåº¦ä¸åå¯¹é½æ¹é¢è¡¨ç°æä½³ã</li>
<li><strong>ç¸æºå§¿æä¼°è®¡</strong>ï¼C4Då¨SintelãTUM-dynamicsåScanNetæ°æ®éä¸æ¾èæé«äºç¸æºå§¿æä¼°è®¡çåç¡®æ§ï¼çè³ä¼äºä¸äºä¸é¨çè§è§éç¨è®¡æ¹æ³ã</li>
<li><strong>ç¹è·è¸ª</strong>ï¼å°½ç®¡DynPTéè¦é¢æµé¢å¤çâç§»å¨æ§âä¿¡æ¯ï¼ä½å¶å¨TAP-VidåKubricæ°æ®éä¸ä»è½ä¸æåè¿çTAPæ¹æ³ä¿æç«äºæ§æ§è½ï¼å¹¶åç¡®é¢æµç¹çå¨æç¶æã</li>
<li><strong>æ¶é´å¹³æ»æ§</strong>ï¼C4Déè¿PTSç®æ æ¾èæ¹åäºè§é¢æ·±åº¦å3Dç¹è½¨è¿¹çæ¶é´å¹³æ»æ§ï¼ææåå°äºç°ææ¹æ³ä¸­å¸¸è§çéªçä¼ªå½±ã</li>
<li><strong>è¿å¨æ©ç åç¡®æ§</strong>ï¼C4Dçæçè¿å¨æ©ç æ¯MonST3Rç­æ¹æ³æ´åç¡®åå®æ´ï¼å°¤å¶å¨å¤æå¨æåºæ¯ä¸­ã</li>
</ul>
<p>è¿äºç»æè¯æäºC4Då¨å¤çå¨æåºæ¯æ¶çæææ§åé²æ£æ§ï¼ä¸ºåç®è§é¢4Déå»ºé¢åæ ç«äºæ°çåºåã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§</strong>
è®ºæä¸­æ²¡ææç¡®æåC4Dæ¡æ¶çæ¾èå±éæ§ãç¶èï¼ä»æ¹æ³è®ºåå®éªè®¾ç½®ä¸­å¯ä»¥æ¨æ­åºä¸äºæ½å¨çéå¶ï¼</p>
<ul>
<li><strong>è®¡ç®ææ¬</strong>ï¼è½ç¶è®ºææå°éè¿ç¨çåºæ¯å¾åæ»å¨çªå£ç­ç¥æ¥éä½è®¡ç®ææ¬ï¼ä½èåä¼åç¹å¾ãåæµãç¹è·è¸ªåå¤ä¸ªä¼åç®æ ï¼å¯è½ä»ç¶å·æè¾é«çè®¡ç®å¤æåº¦ï¼å°¤å¶æ¯å¨å¤çè¶é¿è§é¢æ¶ã</li>
<li><strong>åææ°æ®ä¾èµ</strong>ï¼DynPTçè®­ç»ä¾èµäºKubricç­åææ°æ®éï¼è¿äºæ°æ®éæä¾äºå°é¢çå®ç§»å¨æ§æ ç­¾ãå°½ç®¡åææ°æ®æå©äºæ§å¶åéï¼ä½å¶å¨çå®ä¸çå¤æåºæ¯ä¸­çæ³åè½åå¯è½ä»éè¿ä¸æ­¥éªè¯ã</li>
<li><strong>æ¨¡åæéåå§å</strong>ï¼C4Då©ç¨äºé¢è®­ç»çDUSt3Ræ¨¡åæéï¼è¿æå³çå¶æ§è½å¯è½é¨åä¾èµäºè¿äºåºç¡æ¨¡åçè´¨éåæ³åè½åã</li>
<li><strong>è¶åæ°æææ§</strong>ï¼ä¼åè¿ç¨ä¸­æ¶åå¤ä¸ªæå¤±æéï¼WGA, WCMA, WCTS, WPTSï¼åä¸äºè¶åæ°ï¼å¦å¹³æ»å å­Î»ãæ ¸å¤§å°kï¼ï¼è¿äºåæ°çéæ©å¯è½å¯¹æç»æ§è½æå½±åã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å</strong>
åºäºC4Dçè´¡ç®åæ½å¨å±éæ§ï¼æªæ¥çç ç©¶æ¹åå¯è½åæ¬ï¼</p>
<ul>
<li><strong>å®æ¶æ§è½æå</strong>ï¼è¿ä¸æ­¥ä¼åC4Dçè®¡ç®æçï¼ä½¿å¶è½å¤å®ç°æ´æ¥è¿å®æ¶ç4Déå»ºï¼è¿å¯¹äºæºå¨äººåå¢å¼ºç°å®ç­åºç¨è³å³éè¦ã</li>
<li><strong>æ´å¼ºçæ³åè½å</strong>ï¼æ¢ç´¢å¨æ´å¤æ ·åãæ´å·æææ§ççå®ä¸çå¨æåºæ¯æ°æ®éä¸è®­ç»åè¯ä¼°C4Dï¼ä»¥æé«å¶å¨æªè§åºæ¯ä¸­çæ³åè½åã</li>
<li><strong>èªçç£æå¼±çç£å­¦ä¹ </strong>ï¼åå°å¯¹å°é¢çå®ç§»å¨æ§æ ç­¾çä¾èµï¼å¼åæ´å¼ºå¤§çèªçç£æå¼±çç£æ¹æ³æ¥è®­ç»å¨ææç¥ç¹è·è¸ªå¨åè¿å¨æ©ç ä¼°è®¡ã</li>
<li><strong>éæè¯­ä¹ä¿¡æ¯</strong>ï¼å°è¯­ä¹åå²æç©ä½æ£æµç­é«çº§è¯­ä¹ä¿¡æ¯éæå°C4Dæ¡æ¶ä¸­ï¼ä»¥æ´å¥½å°çè§£åºæ¯ä¸­çå¨æåç´ ï¼ä»èå¯è½æé«è¿å¨æ©ç åç¹è½¨è¿¹çåç¡®æ§ã</li>
<li><strong>å¤æ¨¡æè¾å¥</strong>ï¼æ¢ç´¢ç»åå¶ä»ä¼ æå¨æ°æ®ï¼å¦IMUãæ¿åé·è¾¾ï¼æ¥å¢å¼º4Déå»ºçé²æ£æ§ååç¡®æ§ï¼å°¤å¶æ¯å¨åç§ä¸è¶³æçº¹çç¨ççæææ§ç¯å¢ä¸­ã</li>
<li><strong>äº¤äºå¼4Déå»º</strong>ï¼å¼ååè®¸ç¨æ·å¨4Déå»ºè¿ç¨ä¸­è¿è¡äº¤äºåä¿®æ­£çå·¥å·ï¼ä»¥å¤çå¤æææ¨¡ç³çå¨æåºæ¯ã</li>
</ul>
<p>æ»èè¨ä¹ï¼C4Dä¸ºåç®è§é¢4Déå»ºæä¾äºä¸ä¸ªå¨é¢ä¸é«æ§è½çè§£å³æ¹æ¡ï¼éè¿å·§å¦å°ç»åç­æ¶åé¿æ¶å¯¹åºï¼ææè§£å³äºå¨æåºæ¯ä¸­çææï¼å¹¶ä¸ºæªæ¥çç ç©¶å¥ å®äºåå®çåºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we introduce C4D, a framework that leverages
temporal Correspondences to extend existing 3D reconstruction formulation to
4D.</li>
<li>Furthermore, we introduce a set of dynamic scene optimization objectives to
recover per-frame 3D geometry and camera parameters.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.14960v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.14960v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.14958v1'></a></p>
<h2 id="mathcanvas-intrinsic-visual-chain-of-thought-for-multimodal-mathematical-reasoning"><a href="https://arxiv.org/abs/2510.14958v1">MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning</a></h2>
<p><strong>Authors:</strong> Weikang Shi, Aldrich Yu, Rongyao Fang, Houxing Ren, Ke Wang, Aojun Zhou, Changyao Tian, Xinyu Fu, Yuxuan Hu, Zimu Lu, Linjiang Huang, Si Liu, Rui Liu, Hongsheng Li</p>
<p><strong>Published:</strong> 2025-10-16</p>
<p><strong>Categories:</strong> cs.CV, cs.CL</p>
<p><strong>Abstract:</strong></p>
<p>While Large Language Models (LLMs) have excelled in textual reasoning, they
struggle with mathematical domains like geometry that intrinsically rely on
visual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often
limited by rigid external tools or fail to generate the high-fidelity,
strategically-timed diagrams necessary for complex problem-solving. To bridge
this gap, we introduce MathCanvas, a comprehensive framework designed to endow
unified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for
mathematics. Our approach consists of two phases. First, a Visual Manipulation
stage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M
caption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing
trajectories (MathCanvas-Edit), to master diagram generation and editing.
Second, a Strategic Visual-Aided Reasoning stage fine-tunes the model on
MathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual
reasoning paths, teaching it when and how to leverage visual aids. To
facilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging
benchmark with 3K problems that require models to produce interleaved
visual-textual solutions. Our model, BAGEL-Canvas, trained under this
framework, achieves an 86% relative improvement over strong LMM baselines on
MathCanvas-Bench, demonstrating excellent generalization to other public math
benchmarks. Our work provides a complete toolkit-framework, datasets, and
benchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project
Page: https://mathcanvas.github.io/</p>
<p><strong>Analysis:</strong></p>
<p>ä»¥ä¸æ¯Weikang Shiç­äººæ°åçè®ºæâMathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoningâçæè¦ï¼</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
å¤§åè¯­è¨æ¨¡åï¼LLMsï¼å¨ææ¬æ¨çæ¹é¢è¡¨ç°åºè²ï¼ä½å¨å ä½å­¦ç­æ°å­¦é¢åä¸­ï¼ç±äºå¶æ¬è´¨ä¸ä¾èµè§è§è¾å©ï¼LLMsè¡¨ç°ä¸ä½³ãç°æçè§è§æç»´é¾ï¼VCoTï¼æ¹æ³éå¸¸åéäºåµç¡¬çå¤é¨å·¥å·ï¼ææ æ³çæé«ä¿çåº¦ãç­ç¥æ§éæ¶çå¾è¡¨ï¼ä»èé¾ä»¥è§£å³å¤æçæ°å­¦é®é¢ãè¯¥ç ç©¶æ¨å¨å¼¥åè¿ä¸é¸¿æ²ï¼èµäºç»ä¸å¤§åå¤æ¨¡ææ¨¡åï¼LMMsï¼åå¨çVCoTè½åï¼ä»¥è¿è¡æ°å­¦æ¨çã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥è®ºæå¼å¥äºMathCanvasï¼ä¸ä¸ªå¨é¢çæ¡æ¶ï¼æ¨å¨èµäºLMMsåå¨çVCoTè½åãå¶æ¹æ³åä¸ºä¸¤ä¸ªé¶æ®µï¼
*   <strong>è§è§æä½ï¼Visual Manipulationï¼é¶æ®µï¼</strong> éè¿å¨ä¸ä¸ªåå«15.2Må¯¹æ°æ®çæ°åè¯­æåºä¸é¢è®­ç»æ¨¡åï¼ææ¡å¾è¡¨çæåç¼è¾è½åãè¯¥è¯­æåºåæ¬10Mçâæ é¢-å¾è¡¨å¯¹âï¼MathCanvas-Imagenï¼å5.2Mçâéæ­¥ç¼è¾è½¨è¿¹âï¼MathCanvas-Editï¼ã
*   <strong>ç­ç¥æ§è§è§è¾å©æ¨çï¼Strategic Visual-Aided Reasoningï¼é¶æ®µï¼</strong> å¨MathCanvas-Instructæ°æ®éä¸å¯¹æ¨¡åè¿è¡å¾®è°ï¼è¯¥æ°æ®éæ¯ä¸ä¸ªåå«219Kç¤ºä¾çäº¤éè§è§-ææ¬æ¨çè·¯å¾æ°æ®éï¼æ¨å¨æææ¨¡åä½æ¶ä»¥åå¦ä½å©ç¨è§è§è¾å©ã
*   <strong>MathCanvas-Benchåºåæµè¯ï¼</strong> ä¸ºäºè¿è¡ä¸¥æ ¼è¯ä¼°ï¼å¼å¥äºä¸ä¸ªåå«3Ké®é¢çæææ§åºåæµè¯ï¼è¦æ±æ¨¡åçæäº¤éçè§è§-ææ¬è§£å³æ¹æ¡ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
è¯¥ç ç©¶è®­ç»çæ¨¡åBAGEL-Canvasï¼å¨è¯¥æ¡æ¶ä¸ï¼å¨MathCanvas-Benchä¸æ¯å¼ºå¤§çLMMåºçº¿åå¾äº86%çç¸å¯¹æ¹è¿ï¼å¹¶å±ç¤ºäºå¯¹å¶ä»å¬å±æ°å­¦åºåçä¼ç§æ³åè½åãè¿è¡¨æMathCanvasæåå°è§£éäºLMMsä¸­å¤æãç±»äººè§è§è¾å©æ¨ççè½åã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
æè¦ä¸­æªæç¡®æåè®ºæçå±éæ§ï¼ä½ä»å¶å¼ºè°âè§£éå¤æãç±»äººè§è§è¾å©æ¨çâä»¥åâå¯¹å¶ä»å¬å±æ°å­¦åºåçä¼ç§æ³åè½åâæ¥çï¼å¯è½æç¤ºäºç°ææ¨¡åå¨è¿äºæ¹é¢ä»ææåç©ºé´ï¼æèå¨æäºç¹å®æ°å­¦é¢åï¼å¦å¾®ç§¯åååéï¼çæ¹è¿ç¸å¯¹è¾å°ï¼æ­£æä¸­æå°å¾®ç§¯åååéé¢åçå¢çè¾å°ï¼å¯è½è¶åºå½åè§è§å¢å¼ºææ¯çèå´ï¼ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è¯¥å·¥ä½æä¾äºä¸ä¸ªå®æ´çå·¥å·åãæ¡æ¶ãæ°æ®éååºåæµè¯ï¼ä¸ºLMMsä¸­å¤æãç±»äººè§è§è¾å©æ¨ççæªæ¥ç ç©¶å¥ å®äºåå®åºç¡ãæªæ¥çç ç©¶å¯ä»¥è¿ä¸æ­¥æ¢ç´¢å¦ä½ä¼åæ¨¡åå¨ç¹å®æ°å­¦é¢åçè¡¨ç°ï¼ææ©å±å¶å¨æ´å¹¿æ³ãæ´å¤æçå¤æ¨¡ææ¨çä»»å¡ä¸­çåºç¨ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To bridge
this gap, we introduce MathCanvas, a comprehensive framework designed to endow
unified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for
mathematics.</li>
<li>Our approach consists of two phases.</li>
<li>First, a Visual Manipulation
stage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M
caption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing
trajectories (MathCanvas-Edit), to master diagram generation and editing.</li>
<li>Second, a Strategic Visual-Aided Reasoning stage fine-tunes the model on
MathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual
reasoning paths, teaching it when and how to leverage visual aids.</li>
<li>To
facilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging
benchmark with 3K problems that require models to produce interleaved
visual-textual solutions.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.14958v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.14958v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.14954v1'></a></p>
<h2 id="omnimotion-multimodal-motion-generation-with-continuous-masked-autoregression"><a href="https://arxiv.org/abs/2510.14954v1">OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression</a></h2>
<p><strong>Authors:</strong> Zhe Li, Weihao Yuan, Weichao Shen, Siyu Zhu, Zilong Dong, Chang Xu</p>
<p><strong>Published:</strong> 2025-10-16</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Whole-body multi-modal human motion generation poses two primary challenges:
creating an effective motion generation mechanism and integrating various
modalities, such as text, speech, and music, into a cohesive framework. Unlike
previous methods that usually employ discrete masked modeling or autoregressive
modeling, we develop a continuous masked autoregressive motion transformer,
where a causal attention is performed considering the sequential nature within
the human motion. Within this transformer, we introduce a gated linear
attention and an RMSNorm module, which drive the transformer to pay attention
to the key actions and suppress the instability caused by either the abnormal
movements or the heterogeneous distributions within multi-modalities. To
further enhance both the motion generation and the multimodal generalization,
we employ the DiT structure to diffuse the conditions from the transformer
towards the targets. To fuse different modalities, AdaLN and cross-attention
are leveraged to inject the text, speech, and music signals. Experimental
results demonstrate that our framework outperforms previous methods across all
modalities, including text-to-motion, speech-to-gesture, and music-to-dance.
The code of our method will be made public.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Zhe Liç­äººæ°åçè®ºæâOmniMotion: Multimodal Motion Generation with Continuous Masked Autoregressionâçå¨é¢æè¦ã</p>
<hr />
<h3 id="omnimotion">è®ºææè¦ï¼OmniMotion: å¤æ¨¡æè¿ç»­æ©ç èªåå½è¿å¨çæ</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³å¨èº«å¤æ¨¡æäººä½è¿å¨çæé¢åçä¸¤å¤§ææï¼ä¸æ¯å¦ä½æå»ºä¸ä¸ªé«æçè¿å¨çææºå¶ï¼äºæ¯å¦ä½å°ææ¬ãè¯­é³åé³ä¹ç­å¤ç§æ¨¡æææå°æ´åå°ä¸ä¸ªç»ä¸çæ¡æ¶ä¸­ãç°æçæ¹æ³éå¸¸ä¸æ³¨äºåä¸æ¨¡ææéç¨ç¦»æ£çæ©ç æèªåå½å»ºæ¨¡ï¼è¿éå¶äºå¶æ³åè½ååçæè´¨éã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
OmniMotion æåºäºä¸ä¸ªæ°é¢çãç»ä¸çæ¡æ¶ï¼ç¨äºä»å¤ç§æ¨¡æï¼ææ¬ãè¯­é³ãé³ä¹ï¼çæå¨èº«äººä½è¿å¨ãå¶æ ¸å¿åæ°åæ¬ï¼</p>
<ul>
<li><strong>è¿ç»­æ©ç èªåå½è¿å¨Transformer (Continuous Masked Autoregressive Motion Transformer)ï¼</strong> ä¸ä»¥å¾ç¦»æ£å»ºæ¨¡ä¸åï¼è¯¥æ¹æ³éç¨è¿ç»­çæ©ç èªåå½Transformerãå®éè¿å ææ³¨æåæºå¶å¤çåºåè¿å¨æ°æ®ï¼ä»¥ææäººä½è¿å¨çé¡ºåºæ§ï¼å¹¶é¢æµè¢«æ©ç çè¿å¨çæ®µã</li>
<li><strong>é¨æ§çº¿æ§æ³¨æå (Gated Linear Attention) å RMSNorm æ¨¡åï¼</strong> å¨Transformeråé¨ï¼å¼å¥äºé¨æ§çº¿æ§æ³¨æåæºå¶ä½ä¸ºèªéåºç¹å¾éæ©å¨ï¼ä½¿æ¨¡åè½å¤å³æ³¨å³é®å¨ä½ï¼å¦æå¿åæ¢ãå¤§å¹åº¦è¿å¨ï¼ï¼åæ¶æå¶ä¸ç¸å³æåä½å¨ä½ï¼å¦éæ­¢è¿å¨ï¼ãRMSNormæ¨¡ååç¨äºå¤çå¤æ¨¡æè¾å¥ä¸­å¼æåå¸å¸¦æ¥çä¸ç¨³å®æ§ï¼å¹¶ç¼è§£å¼å¸¸è¿å¨ï¼å¦çªç¶è·³è·ï¼å¼èµ·çæ¢¯åº¦ä¸ç¨³å®æ§ã</li>
<li><strong>DiT (Diffusion Transformer) ç»æçåºç¨ï¼</strong> ä¸ºäºè¿ä¸æ­¥æåè¿å¨çæè´¨éåå¤æ¨¡ææ³åè½åï¼æ¨¡åéç¨DiTç»æï¼å°Transformerçæçæ¡ä»¶ä¿¡æ¯æ©æ£å°ç®æ è¿å¨ãå¨å¤æ¨¡æå­¦ä¹ é¶æ®µï¼DiTæ¨¡åç»æä¿æä¸åå¹¶è¢«å»ç»ï¼ä»å¯¹æ©ç Transformerè¿è¡å¾®è°ã</li>
<li><strong>å¤æ¨¡æä¿¡å·èåæºå¶ï¼</strong> å©ç¨ AdaLN (Adaptive Layer Normalization) åäº¤åæ³¨æåæºå¶ï¼å°ææ¬ãè¯­é³åé³ä¹ä¿¡å·ææå°æ³¨å¥å°Transformerä¸­ï¼å®ç°å¤æ¨¡ææ¡ä»¶çèåã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
å®éªç»æè¡¨æï¼OmniMotion æ¡æ¶å¨æææ¨¡æï¼åæ¬ææ¬å°è¿å¨ãè¯­é³å°æå¿åé³ä¹å°èè¹ï¼ä¸åä¼äºç°ææ¹æ³ã
*   <strong>ææ¬å°è¿å¨çæï¼</strong> å¨HumanML3Dæ°æ®éä¸ï¼æ¨¡åå¨R-Precision (Top-1, 2, 3) ä¸åå«æåäº19.3%ã13.5%å11.7%ï¼FIDåæ°æåäº75.2%ï¼è¡¨æå¶çæçè¿å¨å·æåè¶çä¿çåº¦åä¸ææ¬æè¿°çé«åº¦å¯¹é½ã
*   <strong>è¯­é³å°æå¿çæï¼</strong> å¨BEAT2æ°æ®éä¸ï¼æ¨¡åå¨æé¨åèº«ä½è¿å¨çææ¹é¢è¡¨ç°åºè¯å¥½çè´¨éåå¤æ ·æ§ï¼å¹¶è½æ´å¥½å°ä¸ç¬¬ä¸äººç§°è¯­é³çèå¥å¯¹é½ã
*   <strong>é³ä¹å°èè¹çæï¼</strong> å¨FineDanceæ°æ®éä¸ï¼æ¨¡åå¨çææé¨å¨ä½åèº«ä½è¿å¨æ¹é¢ç¥ä¼äºç°ææ¹æ³ã
*   <strong>æ¶èç ç©¶ï¼</strong> éªè¯äºå ææ³¨æåãDiTãé¨æ§çº¿æ§æºå¶ãRMSNormåäº¤åæ³¨æåæ¨¡åå¯¹æ¨¡åæ§è½çç§¯æè´¡ç®ï¼å°¤å¶æ¯å¨å¤çå¤æå¤æ¨¡æä¸ä¸ææ¶ã</p>
<p>è¿äºç»æçªæ¾äºOmniMotionå¨ç»ä¸æ¡æ¶ä¸å¤çå¤ç§æ¨¡æè¾å¥ãçæé«è´¨éå¨èº«äººä½è¿å¨çå¼ºå¤§è½åï¼ä¸ºè®¡ç®æºè§è§é¢åçè¿å¨çæä»»å¡æ ç«äºæ°çåºåã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºææåºï¼ç±äºæ°æ®éçéå¶ï¼è¿å¨çææ¨¡åçèªç¶æ§åæ³åè½åä»ç¶æéï¼å°¤å¶æ¯å¨è¯­é³åé³ä¹é©±å¨çè¿å¨çææ¹é¢ãè¿æå³çæ¨¡åå¯è½å¨é¢å¯¹æªè§è¿ææ´å¤æãæ´ç»è´çè¯­é³/é³ä¹è¾å¥æ¶ï¼å¶çæææä»ææåç©ºé´ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
å°½ç®¡è®ºææªæç¡®ååºæªæ¥ç ç©¶æ¹åï¼ä½ä»å¶å±éæ§å¯ä»¥æ¨æ­åºä»¥ä¸å ä¸ªæ¹åï¼
*   <strong>æ©å±æ°æ®éè§æ¨¡åå¤æ ·æ§ï¼</strong> è§£å³å½åæ°æ®ééå¶ï¼æ¶éæ´å¤§è§æ¨¡ãæ´ä¸°å¯å¤æ ·ï¼åå«æ´å¤å¤æå¨ä½ãææè¡¨è¾¾åé£æ ¼ï¼çå¤æ¨¡æè¿å¨æ°æ®éï¼ä»¥è¿ä¸æ­¥æåæ¨¡åçèªç¶æ§åæ³åè½åã
*   <strong>æ´ç²¾ç»åçå¤æ¨¡æå¯¹é½ï¼</strong> æ¢ç´¢æ´åè¿çæºå¶ï¼ä»¥å®ç°è¯­é³ãé³ä¹ä¸è¿å¨ä¹é´æ´ç²¾ç»ãæ´åç¡®çæ¶é´åè¯­ä¹å¯¹é½ï¼å°¤å¶æ¯å¨å¤çå¤æèå¥åææè¡¨è¾¾æ¶ã
*   <strong>å®æ¶çæåäº¤äºï¼</strong> ä¼åæ¨¡åæçï¼ä½¿å¶è½å¤æ¯æå®æ¶è¿å¨çæï¼å¹¶æ¢ç´¢ä¸ç¨æ·è¿è¡äº¤äºå¼è¿å¨çæçæ¹æ³ã
*   <strong>ä¸ªæ§ååé£æ ¼åè¿å¨çæï¼</strong> è¿ä¸æ­¥ç ç©¶å¦ä½æ ¹æ®ç¨æ·çåå¥½æç¹å®é£æ ¼çæä¸ªæ§ååé£æ ¼åçè¿å¨ï¼ä¾å¦éè¿å­¦ä¹ ä¸åèèçé£æ ¼æç¹å®è§è²çå¨ä½ç¹å¾ã
*   <strong>ç»åç©ççº¦æï¼</strong> å¼å¥ç©ççº¦ææå¨åå­¦æ¨¡åï¼ä»¥çææ´çå®ãæ´ç¬¦åç©çè§å¾çè¿å¨ï¼é¿åä¸èªç¶çå¨ä½ã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>Unlike
previous methods that usually employ discrete masked modeling or autoregressive
modeling, we develop a continuous masked autoregressive motion transformer,
where a causal attention is performed considering the sequential nature within
the human motion.</li>
<li>Within this transformer, we introduce a gated linear
attention and an RMSNorm module, which drive the transformer to pay attention
to the key actions and suppress the instability caused by either the abnormal
movements or the heterogeneous distributions within multi-modalities.</li>
<li>Experimental
results demonstrate that our framework outperforms previous methods across all
modalities, including text-to-motion, speech-to-gesture, and music-to-dance.</li>
<li>The code of our method will be made public.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.14954v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.14954v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.14945v1'></a></p>
<h2 id="3d-scene-prompting-for-scene-consistent-camera-controllable-video-generation"><a href="https://arxiv.org/abs/2510.14945v1">3D Scene Prompting for Scene-Consistent Camera-Controllable Video Generation</a></h2>
<p><strong>Authors:</strong> JoungBin Lee, Jaewoo Jung, Jisang Han, Takuya Narihira, Kazumi Fukuda, Junyoung Seo, Sunghwan Hong, Yuki Mitsufuji, Seungryong Kim</p>
<p><strong>Published:</strong> 2025-10-16</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We present 3DScenePrompt, a framework that generates the next video chunk
from arbitrary-length input while enabling precise camera control and
preserving scene consistency. Unlike methods conditioned on a single image or a
short clip, we employ dual spatio-temporal conditioning that reformulates
context-view referencing across the input video. Our approach conditions on
both temporally adjacent frames for motion continuity and spatially adjacent
content for scene consistency. However, when generating beyond temporal
boundaries, directly using spatially adjacent frames would incorrectly preserve
dynamic elements from the past. We address this by introducing a 3D scene
memory that represents exclusively the static geometry extracted from the
entire input video. To construct this memory, we leverage dynamic SLAM with our
newly introduced dynamic masking strategy that explicitly separates static
scene geometry from moving elements. The static scene representation can then
be projected to any target viewpoint, providing geometrically consistent warped
views that serve as strong 3D spatial prompts while allowing dynamic regions to
evolve naturally from temporal context. This enables our model to maintain
long-range spatial coherence and precise camera control without sacrificing
computational efficiency or motion realism. Extensive experiments demonstrate
that our framework significantly outperforms existing methods in scene
consistency, camera controllability, and generation quality. Project page :
https://cvlab-kaist.github.io/3DScenePrompt/</p>
<p><strong>Analysis:</strong></p>
<p>ä»¥ä¸æ¯è®ºæâ3D Scene Prompting for Scene-Consistent Camera-Controllable Video Generationâçå¨é¢æè¦ï¼</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³åºæ¯ä¸è´ãç¸æºå¯æ§çè§é¢çæä»»å¡ãç°ææ¹æ³å¨å¤çä»»æé¿åº¦çè¾å¥è§é¢æ¶ï¼é¾ä»¥å¨ä¿æé¿æç©ºé´ä¸è´æ§ãç²¾ç¡®ç¸æºæ§å¶åçæè´¨éçåæ¶ï¼çæä¸è¾å¥è§é¢åºæ¯ä¸è´çåç»­è§é¢çæ®µãå·ä½æ¥è¯´ï¼å½çæè¶åºæ¶é´è¾¹ççè§é¢æ¶ï¼ç´æ¥ä½¿ç¨è¿å»çç©ºé´ç¸é»å¸§ä¼å¯¼è´å¨æåç´ ï¼å¦ç§»å¨ç©ä½ï¼è¢«éè¯¯å°ä¿çï¼ä»èç ´ååºæ¯çèªç¶æ¼åã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è´¡ç®ï¼</strong>
*   <strong>åéæ¶ç©ºæ¡ä»¶ä½ç¨ï¼Dual Spatio-Temporal Conditioningï¼ï¼</strong> è®ºææåºäºä¸ç§æ°é¢çåéæ¶ç©ºæ¡ä»¶ä½ç¨ç­ç¥ï¼éè¿éæ°æå»ºè¾å¥è§é¢ä¸­çä¸ä¸æè§å¾å¼ç¨ï¼åæ¶èèæ¶é´ç¸é»å¸§ï¼ç¨äºè¿å¨è¿ç»­æ§ï¼åç©ºé´ç¸é»åå®¹ï¼ç¨äºåºæ¯ä¸è´æ§ï¼ã
*   <strong>3D åºæ¯è®°å¿ï¼3D Scene Memoryï¼ï¼</strong> ä¸ºäºè§£å³å¨æåç´ ä¿çé®é¢ï¼è®ºæå¼å¥äºä¸ä¸ª3Dåºæ¯è®°å¿ï¼å®ä¸é¨è¡¨ç¤ºä»æ´ä¸ªè¾å¥è§é¢ä¸­æåçéæå ä½ç»æãè¿ç¡®ä¿äºç©ºé´æ¡ä»¶ä½ç¨åªæä¾æä¹çéæåºæ¯ç»æï¼èå¨æåå®¹å¯ä»¥ä»æ¶é´ä¸ä¸æä¸­èªç¶æ¼åã
*   <strong>å¨ææ©è½ç­ç¥ï¼Dynamic Masking Strategyï¼ï¼</strong> ä¸ºäºæå»º3Dåºæ¯è®°å¿ï¼è®ºæå©ç¨å¨æSLAMï¼å¹¶å¼å¥äºä¸ç§æ°çå¨ææ©è½ç­ç¥ï¼æç¡®å°å°éæåºæ¯å ä½ä¸ç§»å¨åç´ åç¦»ãè¯¥ç­ç¥éè¿åç´ çº§è¿å¨æ£æµãååè·è¸ªèåè¿å¨è¯æ®ä»¥åä½¿ç¨SAM2è¿è¡å¯¹è±¡çº§æ©è½ï¼ç¡®ä¿åªæåéæåå®¹ã
*   <strong>å ä½ä¸è´çæ­æ²è§å¾ä½ä¸ºç©ºé´æç¤ºï¼</strong> éæåºæ¯è¡¨ç¤ºå¯ä»¥æå½±å°ä»»ä½ç®æ è§ç¹ï¼çæå ä½ä¸è´çæ­æ²è§å¾ï¼ä½ä¸ºå¼ºå¤§ç3Dç©ºé´æç¤ºãè¿ä½¿å¾æ¨¡åè½å¤å¨ä¸çºç²è®¡ç®æçæè¿å¨çå®æçæåµä¸ï¼ä¿æé¿è·ç¦»ç©ºé´è¿è´¯æ§åç²¾ç¡®ç¸æºæ§å¶ã
*   <strong>åºäºé¢è®­ç»è§é¢çæå¨çæ¶æï¼</strong> è®ºæå¨å¼ºå¤§çé¢è®­ç»è§é¢çæå¨ï¼å¦CogVideoX-I2V-5Bï¼çåºç¡ä¸è¿è¡æå»ºï¼éè¿éæ°è®¾è®¡åå®¹å¼ç¨æ¹å¼ï¼ä¿çäºå¶å­¦ä¹ å°çåéªç¥è¯åè®­ç»æçã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>åè¶çåºæ¯ä¸è´æ§ï¼</strong> 3DScenePromptå¨RealEstate10KåDynPose-100Kæ°æ®éä¸ï¼å¨PSNRãSSIMãLPIPSåMEt3Rç­ææææ ä¸åæ¾èä¼äºç°ææ¹æ³ï¼å¦DFoTï¼ãå°¤å¶å¨MEt3Rå ä½ä¸è´æ§è¯¯å·®ä¸ï¼ä¸éäº77%ï¼è¡¨æå¶å¨å¤è§å¾å ä½å¯¹é½æ¹é¢çåè¶æ§è½ã
*   <strong>ç²¾ç¡®çç¸æºå¯æ§æ§ï¼</strong> è®ºææ¹æ³å¨mRotErrãmTransErråmCamMCç­ç¸æºå¯æ§æ§ææ ä¸åä¼äºMotionCtrlãCameraCtrlãFloVDåAC3Dç­åºçº¿æ¹æ³ï¼è¯æäºå¶è½å¤ç²¾ç¡®éµå¾ªç»å®çç¸æºè½¨è¿¹ã
*   <strong>é«è´¨éçè§é¢çæï¼</strong> å¨FVDåVBench++ï¼åæ¬ä¸»ä½ä¸è´æ§ãèæ¯ä¸è´æ§ãç¾å­¦è´¨éãæåè´¨éãæ¶é´éªçãè¿å¨å¹³æ»åº¦åå¨æç¨åº¦ï¼ç­è§é¢çæè´¨éææ ä¸ï¼è®ºææ¹æ³ä¹åå¾äºæä½³æ§è½ã
*   <strong>è®¡ç®æçåè¿å¨çå®æï¼</strong> éè¿éæ©æ§å°æ£ç´¢æç¸å³çå¸§ï¼æ¶ç©ºä¸ï¼ï¼è¯¥æ¡æ¶å®ç°äºå¯¹ä»»æé¿åº¦è§é¢çè®¡ç®é«æå¤çï¼åæ¶ä¿æäºè¿å¨è¿ç»­æ§ååºæ¯ä¸è´æ§ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   è®ºæä¸­æ²¡ææç¡®æåå½åæ¹æ³çå·ä½å±éæ§ãç¶èï¼éè¿ä¸ç°ææ¹æ³çæ¯è¾ï¼å¯ä»¥æ¨æ­åºï¼å¨å¤çé¿åºåæ¶ï¼ç°ææ¹æ³ï¼å¦DFoTï¼ç±äºåå­éå¶ï¼é¾ä»¥ä¿æé¿æç©ºé´è¿è´¯æ§ã3DScenePromptéè¿å¶3Dåºæ¯è®°å¿åå¨ææ©è½ç­ç¥è§£å³äºè¿ä¸é®é¢ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   è®ºææç¡®æåºï¼è¯¥æ¹æ³ä¸ºé¿ç¯è§é¢åæåºç¨å¼è¾äºæ°çå¯è½æ§ï¼è¿äºåºç¨ä¸­ä¿æç©ºé´ä¸è´æ§åç²¾ç¡®ç¸æºæ§å¶è³å³éè¦ãè¿æç¤ºäºæªæ¥ç ç©¶å¯ä»¥è¿ä¸æ­¥æ¢ç´¢è¯¥æ¡æ¶å¨çµå½±å¶ä½ãèæç°å®ååææ°æ®çæç­é¢åçåºç¨ã
*   è¿ä¸æ­¥ä¼åå¨ææ©è½ç­ç¥ï¼ä»¥å¤çæ´å¤æææ´ç»å¾®çå¨æåºæ¯ï¼ä¾å¦å¿«éååçå¨ææåéæç©ä½ã
*   æ¢ç´¢å°3Dåºæ¯è®°å¿ä¸æ´é«çº§çè§é¢æ©æ£æ¨¡åæçææ¶æç¸ç»åï¼ä»¥è¿ä¸æ­¥æåçæè´¨éåæçã
*   ç ç©¶å¦ä½å°ç¨æ·äº¤äºï¼ä¾å¦ï¼éè¿ææ¬æç¤ºæèå¾ï¼æ´æ·±å¥å°æ´åå°3Dåºæ¯è®°å¿çæå»ºåå©ç¨ä¸­ï¼ä»¥å®ç°æ´çµæ´»çåä½æ§å¶ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present 3DScenePrompt, a framework that generates the next video chunk
from arbitrary-length input while enabling precise camera control and
preserving scene consistency.</li>
<li>Our approach conditions on
both temporally adjacent frames for motion continuity and spatially adjacent
content for scene consistency.</li>
<li>To construct this memory, we leverage dynamic SLAM with our
newly introduced dynamic masking strategy that explicitly separates static
scene geometry from moving elements.</li>
<li>Extensive experiments demonstrate
that our framework significantly outperforms existing methods in scene
consistency, camera controllability, and generation quality.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.14945v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.14945v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.14904v1'></a></p>
<h2 id="maskcaptioner-learning-to-jointly-segment-and-caption-object-trajectories-in-videos"><a href="https://arxiv.org/abs/2510.14904v1">MaskCaptioner : Learning to Jointly Segment and Caption Object Trajectories in Videos</a></h2>
<p><strong>Authors:</strong> Gabriel Fiastre, Antoine Yang, Cordelia Schmid</p>
<p><strong>Published:</strong> 2025-10-16</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Dense Video Object Captioning (DVOC) is the task of jointly detecting,
tracking, and captioning object trajectories in a video, requiring the ability
to understand spatio-temporal details and describe them in natural language.
Due to the complexity of the task and the high cost associated with manual
annotation, previous approaches resort to disjoint training strategies,
potentially leading to suboptimal performance. To circumvent this issue, we
propose to generate captions about spatio-temporally localized entities
leveraging a state-of-the-art VLM. By extending the LVIS and LV-VIS datasets
with our synthetic captions (LVISCap and LV-VISCap), we train MaskCaptioner, an
end-to-end model capable of jointly detecting, segmenting, tracking and
captioning object trajectories. Moreover, with pretraining on LVISCap and
LV-VISCap, MaskCaptioner achieves state-of-the-art DVOC results on three
existing benchmarks, VidSTG, VLN and BenSMOT. The datasets and code are
available at https://www.gabriel.fiastre.fr/maskcaptioner/.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºâMaskCaptioner : Learning to Jointly Segment and Caption Object Trajectories in Videosâè®ºæçå¨é¢æè¦ï¼åå®¹å°ç¨ä¸­æåç°ï¼å¹¶æ¶µçæ¨è¦æ±çææè¦ç¹ã</p>
<hr />
<h3 id="maskcaptioner">MaskCaptioner: å­¦ä¹ èååå²åæè¿°è§é¢ä¸­ç©ä½è½¨è¿¹</h3>
<p><strong>1. è®ºæä¸»è¦é®é¢æç ç©¶é®é¢</strong></p>
<p>è¯¥è®ºææ¨å¨è§£å³<strong>å¯éè§é¢ç©ä½æè¿° (Dense Video Object Captioning, DVOC)</strong> ä»»å¡ä¸­çæ ¸å¿ææãDVOCè¦æ±æ¨¡åè½å¤èåæ£æµãè·è¸ªå¹¶ç¨èªç¶è¯­è¨æè¿°è§é¢ä¸­ææç©ä½çè½¨è¿¹ãè¿é¡¹ä»»å¡çå¤ææ§ä»¥åæå¨æ æ³¨çé«æææ¬å¯¼è´ç°ææ¹æ³éå¸¸éç¨åç¦»çè®­ç»ç­ç¥ï¼è¿å¯è½å¯¼è´æ¬¡ä¼æ§è½ãå æ­¤ï¼è®ºæçæ ¸å¿é®é¢æ¯å¦ä½å®ç°ç«¯å°ç«¯çDVOCè®­ç»ï¼ä»¥åæå¯¹å¯éæ æ³¨æ°æ®ç¨ç¼ºçä¾èµï¼å¹¶æé«æ´ä½æ§è½ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®</strong></p>
<ul>
<li><strong>VLMé©±å¨çåææ°æ®çæï¼</strong> è®ºææä¸»è¦çåæ°æ¯æåºäºä¸ç§å©ç¨æåè¿çè§è§è¯­è¨æ¨¡åï¼VLMï¼å·ä½æ¯Gemini 2.0 Flashï¼æ¥çæç©ä½çº§åææè¿°çæ¹æ³ãéè¿å¤æ¨¡ææç¤ºç­ç¥ï¼VLMè½å¤ä¸ºè§é¢ä¸­ç©ºé´å±é¨åçç©ä½çæè¯¦ç»çãä»¥ç©ä½ä¸ºä¸­å¿çæè¿°ã</li>
<li><strong>æ©å±ç°ææ°æ®éï¼</strong> è®ºæå°LVISï¼å¾ååå²ï¼åLV-VISï¼è§é¢å®ä¾åå²ï¼æ°æ®éæ©å±ä¸ºLVISCapåLV-VISCapï¼é¦æ¬¡ä¸ºDVOCä»»å¡æä¾äºåå«ï¼æ©ç ãè¾¹çæ¡ãç±»å«ãæè¿°ï¼æ æ³¨çç»ä¸è®­ç»éãè¿äºåææ°æ®æå¤§å°å¼¥è¡¥äºDVOCä»»å¡æéå¯éæ æ³¨çä¸è¶³ã</li>
<li><strong>ç«¯å°ç«¯MaskCaptioneræ¨¡åï¼</strong> è®ºææåºäºMaskCaptionerï¼è¿æ¯ä¸ä¸ªè½å¤èåæ§è¡ç©ä½æ£æµãåå²ãè·è¸ªåæè¿°çç«¯å°ç«¯æ¨¡åãè¯¥æ¶æåºäºOpen-Vocabulary Video Instance Segmentation (OV-VIS) æ¨¡åOVFormerï¼å¹¶æ©å±äºæè¿°å¤´ï¼è½å¤ä»è§é¢çæ®µçº§å«çé¢æµä¸­èåä¿¡æ¯ï¼çæè½¨è¿¹çº§å«çæè¿°ã</li>
<li><strong>ç»ä¸è®­ç»ç­ç¥ï¼</strong> éè¿ä½¿ç¨çæçLVISCapåLV-VISCapæ°æ®éï¼MaskCaptionerè½å¤è¿è¡ç«¯å°ç«¯è®­ç»ï¼é¿åäºä»¥å¾æ¹æ³ä¸­åç¦»è®­ç»ç­ç¥çæ¬¡ä¼æ§ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹</strong></p>
<ul>
<li><strong>æåè¿çDVOCæ§è½ï¼</strong> MaskCaptionerå¨ä¸ä¸ªç°æåºåæµè¯ï¼VidSTGãVLNåBenSMOTï¼ä¸åå¾äºæåè¿çDVOCç»æãè¿è¯æäºå¶æ¹æ³å¨èåæ£æµãè·è¸ªåæè¿°ç©ä½è½¨è¿¹æ¹é¢çæææ§ã</li>
<li><strong>åææ°æ®çéè¦æ§ï¼</strong> å®éªç»æè¡¨æï¼LVISCapåLV-VISCapç­åææ°æ®éæå¤§å°æåäºMaskCaptionerçDVOCæ§è½ãCapAï¼æè¿°åç¡®æ§ï¼ä¸è®­ç»æè¿°æ°éåå¯¹æ°ç¸å³ï¼è¡¨æçææ´å¤æ°æ®å¯è½å¸¦æ¥è¿ä¸æ­¥æ¹è¿ã</li>
<li><strong>åå²ä»»å¡çæ©å±ï¼</strong> è®ºææåå°å°DVOCä»»å¡æ©å±å°åå²æ©ç ï¼èä¸ä»ä»æ¯è¾¹çæ¡ï¼è¿æä¾äºæ´ç²¾ç»çç©ä½å®ä½åæè¿°ã</li>
<li><strong>é²æ£æ§ï¼</strong> MaskCaptionerå¯¹è§è§éª¨å¹²ç½ç»çéæ©è¡¨ç°åºé²æ£æ§ã</li>
<li><strong>æ¶é´èåçä¼å¿ï¼</strong> å¼å¥æ¶é´èåæ¨¡åæ¾èæé«äºæè¿°æ§è½ï¼éè¿æ´åæ¥èªå¤ä¸ªè§é¢çæ®µçä¿¡æ¯ï¼ä¸°å¯äºå¯¹æ¶é´ä¸æ©å±å¨ä½çæè¿°ã</li>
</ul>
<p><strong>4. è®ºæä¸­æåçå±éæ§</strong></p>
<ul>
<li><strong>å®ä½åæè¿°çæ¹è¿ç©ºé´ï¼</strong> å°½ç®¡MaskCaptionerè¡¨ç°åºè²ï¼ä½å¨å®ä½åæè¿°æ¹é¢ä»ææ¹è¿ç©ºé´ï¼ç¹å«æ¯å¯¹äºå°ç©ä½ã</li>
<li><strong>æè¿°çéç¨æ§ï¼</strong> èªå¨çæçæè¿°ææ¶å¯è½è¿äºéç¨ï¼æèå¨è§é¢ä¸­æ··æ·åä¸ç±»çä¸¤ä¸ªç©ä½ã</li>
<li><strong>å¤æå¨ä½çéå¶ï¼</strong> å½ååºåæµè¯ä¸­å¯è§å¯å°çå¨ä½å¤ææ§æéï¼æ¨¡åå¨å¤çæ´å¤æçç©ä½äº¤äºåå¤å¨ä½çæ®µæ¶å¯è½é¢ä¸´ææã</li>
<li><strong>è¯å«éè¯¯ï¼</strong> å¨æ¨¡ç³ä¸ä¸æãæ¨¡ç³å®ä¾æç¨æç±»å«çæåµä¸ï¼MaskCaptionerå¯è½æ æ³æ­£ç¡®è¯å«ææè¿°çç©ä½ï¼ææ¶ä¼å¯¼è´éè¯¯çå½åï¼ä¾å¦ï¼å°âé³å­âéè¯¯å°è¯å«ä¸ºâåâï¼ã</li>
<li><strong>æè¿°ä¸ä¸è´ï¼</strong> å¨ç±»ä¼¼æåµä¸ï¼MaskCaptionerçæçæè¿°å¨æä»£åä¸ç©ä½æ¶å¯è½ä¸ä¸è´ã</li>
<li><strong>æ£æµ/åå²éè¯¯ï¼</strong> å¨å¤æè¿å¨ãå¤è§ååæé®æ¡çæåµä¸ï¼MaskCaptionerææ¶ä¼æªè½æ£æµãåå²æè·è¸ªç©ä½ï¼å¯¼è´ç¼ºå°æè¿°ï¼ä¾å¦ï¼æªè½æ£æµå°èé¹­åä¸­çé±¼ï¼ã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å</strong></p>
<ul>
<li><strong>æ´åè¿çèªå¨æè¿°ææ¯ï¼</strong> æªæ¥çå·¥ä½å¯ä»¥æ¢ç´¢ä¸åçèªå¨æè¿°ææ¯ï¼ä¾å¦åºäºRef-SAVï¼Yuan et al., 2025ï¼çæ¹æ³ï¼è¯¥æ¹æ³åå¤æ­¥çææè¿°ï¼ä»¥åºåå¤è§åè¿å¨æè¿°ã</li>
<li><strong>æå»ºæ´å¤æçåºåæµè¯ï¼</strong> éå¯¹å·ææ´å¤æç©ä½äº¤äºåå¤ä¸ªå¨ä½çæ®µçè§é¢ï¼æå»ºæ°çåºåæµè¯ï¼ä»¥æ¨å¨DVOCä»»å¡çåå±ã</li>
<li><strong>æåå°ç©ä½å®ä½åæè¿°ï¼</strong> è¿ä¸æ­¥ç ç©¶å¦ä½æé«æ¨¡åå¯¹è§é¢ä¸­å°ç©ä½çå®ä½åæè¿°è½åã</li>
<li><strong>è§£å³æè¿°éç¨æ§åä¸è´æ§é®é¢ï¼</strong> æ¢ç´¢æ¹æ³ä»¥çææ´å·ä½ãæ´ä¸è´çç©ä½æè¿°ï¼é¿åæ··æ·ç¸ä¼¼ç©ä½ã</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To circumvent this issue, we
propose to generate captions about spatio-temporally localized entities
leveraging a state-of-the-art VLM.</li>
<li>Moreover, with pretraining on LVISCap and
LV-VISCap, MaskCaptioner achieves state-of-the-art DVOC results on three
existing benchmarks, VidSTG, VLN and BenSMOT.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.14904v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.14904v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.14876v1'></a></p>
<h2 id="badas-context-aware-collision-prediction-using-real-world-dashcam-data"><a href="https://arxiv.org/abs/2510.14876v1">BADAS: Context Aware Collision Prediction Using Real-World Dashcam Data</a></h2>
<p><strong>Authors:</strong> Roni Goldshmidt, Hamish Scott, Lorenzo Niccolini, Shizhan Zhu, Daniel Moura, Orly Zvitia</p>
<p><strong>Published:</strong> 2025-10-16</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Existing collision prediction methods often fail to distinguish between
ego-vehicle threats and random accidents not involving the ego vehicle, leading
to excessive false alerts in real-world deployment. We present BADAS, a family
of collision prediction models trained on Nexar's real-world dashcam collision
dataset -- the first benchmark designed explicitly for ego-centric evaluation.
We re-annotate major benchmarks to identify ego involvement, add consensus
alert-time labels, and synthesize negatives where needed, enabling fair AP/AUC
and temporal evaluation. BADAS uses a V-JEPA2 backbone trained end-to-end and
comes in two variants: BADAS-Open (trained on our 1.5k public videos) and
BADAS1.0 (trained on 40k proprietary videos). Across DAD, DADA-2000, DoTA, and
Nexar, BADAS achieves state-of-the-art AP/AUC and outperforms a
forward-collision ADAS baseline while producing more realistic time-to-accident
estimates. We release our BADAS-Open model weights and code, along with
re-annotations of all evaluation datasets to promote ego-centric collision
prediction research.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Roni Goldshmidtç­äººæ°åçè®ºæâBADAS: Context Aware Collision Prediction Using Real-World Dashcam Dataâçå¨é¢æè¦ã</p>
<hr />
<h3 id="badas-context-aware-collision-prediction-using-real-world-dashcam-data_1">è®ºææè¦ï¼BADAS: Context Aware Collision Prediction Using Real-World Dashcam Data</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
ç°æçç¢°æé¢æµæ¹æ³å¨å®éé¨ç½²ä¸­å­å¨ä¸¥éç¼ºé·ï¼å³æ æ³ææåºåâèªæè½¦è¾âï¼ego-vehicleï¼é¢ä¸´çå¨èä¸ä¸æ¶åèªæè½¦è¾çéæºäºæãè¿å¯¼è´äºè¿å¤çèåè­¦æ¥ï¼éä½äºç³»ç»çå®ç¨æ§åç¨æ·æ¥ååº¦ãè®ºææ¨å¨è§£å³è¿ä¸æ ¸å¿é®é¢ï¼å³å¦ä½å¼åä¸ç§è½å¤åç¡®é¢æµèªæè½¦è¾ç¸å³ç¢°æï¼åæ¶æ¾èåå°èåè­¦æ¥çç¢°æé¢æµç³»ç»ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>ä»¥èªæè½¦è¾ä¸ºä¸­å¿çç¢°æé¢æµèå¼ï¼</strong> è®ºæéæ°å®ä¹äºç¢°æé¢æµä»»å¡ï¼ä½¿å¶ä¸æ³¨äºèªæè½¦è¾çå®å¨æ§ãéè¿å¯¹DADãDoTAåDADA-2000ç­ä¸»è¦åºåæ°æ®éè¿è¡éæ°æ æ³¨ï¼æç¡®åºåäºèªæè½¦è¾æ¯å¦å·å¥äºæï¼åç°ç°ææ°æ®éä¸­40-92%çäºæä¸èªæè½¦è¾æ å³ãè¿äºéæ°æ æ³¨çæ°æ®éï¼åæ¬å±è¯è­¦æ¥æ¶é´æ ç­¾ååæçè´æ ·æ¬ï¼å·²å¬å¼ï¼ä»¥ä¿è¿ä»¥èªæè½¦è¾ä¸ºä¸­å¿çç¢°æé¢æµç ç©¶ã
*   <strong>V-JEPA2éª¨å¹²ç½ç»çåºç¨ï¼</strong> BADASæ¨¡åéç¨V-JEPA2ï¼ä¸ç§åè¿çè§é¢åºç¡æ¨¡åï¼ä½ä¸ºéª¨å¹²ç½ç»ï¼å¹¶è¿è¡ç«¯å°ç«¯è®­ç»ãV-JEPA2å¨çè§£æ¶é´å¨æåè§è§æ¨¡å¼æ¹é¢è¡¨ç°åºè²ï¼ç¹å«éåé¢æµæ§ä»»å¡ã
*   <strong>åºäºçå®ä¸çè¡è½¦è®°å½ä»ªæ°æ®çè®­ç»ï¼</strong> BADASæ¨¡åå¨Nexarççå®ä¸çè¡è½¦è®°å½ä»ªç¢°ææ°æ®éä¸è¿è¡è®­ç»ï¼è¯¥æ°æ®éæ¯ç¬¬ä¸ä¸ªä¸é¨ä¸ºèªæè½¦è¾è¯ä¼°è®¾è®¡çåºåãè¯¥æ°æ®éåå«èªæè½¦è¾å·å¥çç¢°æåé©æäºä»¶ï¼ä»¥åéè¿ç´§æ¥æä½æåé¿åå±é©æåµçè¿ç¢°æäºä»¶ï¼æä¾äºä¸°å¯çè®­ç»ä¿¡å·ã
*   <strong>æ ååæ¶é´è¯ä¼°ï¼</strong> è®ºæéè¿10ä½æ æ³¨åçå±è¯ï¼å»ºç«äºè­¦æ¥æ¶é´çè¿è´¯å®ä¹ï¼å¹¶ä¸ºæææµè¯éæä¾äºç²¾ç¡®çæ¶é´æ æ³¨ï¼è§£å³äºç°ææ°æ®éä¸­å®ä¹ä¸ä¸è´åä¸»è§æ§é®é¢ã
*   <strong>æ°æ®å¢å¼ºåéæ ·ç­ç¥ï¼</strong> éç¨æ°æ®å¢å¼ºåæ­£æ ·æ¬2åè¿éæ ·ç­ç¥ï¼æ¾èæé«äºæ¨¡åçæ§è½åå¯¹çå®ä¸çååçé²æ£æ§ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æåè¿çæ§è½ï¼</strong> BADASæ¨¡åå¨DADãDADA-2000ãDoTAåNexarç­ææä¸»è¦åºåæµè¯ä¸­ååå¾äºæåè¿çAP/AUCæ§è½ãBADAS1.0ï¼åºäº40kä¸æè§é¢è®­ç»ï¼å¨DADæ°æ®éä¸è¾¾å°äº0.94 APï¼è¿è¶åºçº¿æ¹æ³ç0.06 APãå¨Nexaræ°æ®éä¸ï¼BADAS-Openï¼åºäº1.5kå¬å¼è§é¢è®­ç»ï¼çAPè¾¾å°0.86ï¼æ¾èä¼äºå­¦æ¯åºçº¿ï¼0.48-0.53ï¼ååç¨FCWç³»ç»ï¼0.58ï¼ã
*   <strong>æ´çå®çäºæåçæ¶é´ä¼°è®¡ï¼</strong> BADASæ¨¡åçæçå¹³åäºæåçæ¶é´ï¼mTTAï¼ä¼°è®¡å¼æ´ç¬¦åäººç±»é¢æµè½åï¼3-5ç§ï¼ï¼èåºçº¿æ¹æ³åæ¥åäºä¸åå®éç9-10ç§é¢æµã
*   <strong>é²æ£æ§åæ³åè½åï¼</strong> BADASæ¨¡åå¨ä¸åæ°æ®éä¸ä¿æäºä¸è´çæ§è½ï¼è¡¨æå¶å·æå¼ºå¤§çæ³åè½åã
*   <strong>æ°æ®è§æ¨¡æåºï¼</strong> éçè®­ç»æ°æ®éçå¢å ï¼ä»1.5kå°40kè§é¢ï¼ï¼Nexaréªè¯APåå¯¹æ°çº§æ¹è¿ï¼è¯å®äºå©ç¨å¤§è§æ¨¡çå®ä¸çæ°æ®è½å¤æç»­æåæ§è½ã
*   <strong>å®æ§åæï¼</strong> BADAS-Opençé¢æµåæ°å¨ç¢°æä¸´è¿æ¶æ¥å§ä¸åï¼ä¸é¢æµç¨³å®ãèªä¿¡ï¼èåºçº¿æ¹æ³åè¡¨ç°åºä¸è§å¾çæ¨¡å¼åè¿æ©æä¸ä¸è´çè­¦æ¥ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>é¿å°¾æ§è½ä¸éï¼</strong> BADASæ¨¡åå¨å°æ°ç±»å«ï¼å¦è¡äººãéªèªè¡è½¦èãæ©æè½¦åå¨ç©ï¼ä¸çå¬åçæ¾èä¸éï¼è¡¨ææ¨¡åå¨å¤çç½è§ãè§è§å¤æ ·ä¸ä½é¢äºä»¶ç±»åæ¶å­å¨æ³åææãè¿æ¯ç±äºè®­ç»æ°æ®éä¸­ç¼ºä¹è¿äºé¿å°¾äºä»¶æè´ã
*   <strong>æ°æ®ééå¶ï¼</strong> å°½ç®¡BADAS1.0ä½¿ç¨äº40kä¸æè§é¢ï¼ä½è®ºææåºæ°æ®è§æ¨¡çæ½åå°æªå®å¨é¥±åï¼æç¤ºä»ææåç©ºé´ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ©å±æ°æ®éï¼</strong> è¿ä¸æ­¥æ©å¤§æ°æ®éï¼ä»¥å¢å¼ºæ¨¡åçæ³åè½åï¼å°¤å¶æ¯å¨é¿å°¾ç±»å«æ¹é¢ã
*   <strong>æ¹è¿å¹³åè­¦æ¥æ¶é´ï¼mTTAï¼ï¼</strong> ä¼åæ¨¡åä»¥åå°èåè­¦æ¥ï¼å¹¶æä¾æ´åç¡®çè­¦æ¥æ¶é´ã
*   <strong>è§£å³é¿å°¾ç±»å«é®é¢ï¼</strong> å¼åä¸é¨çç­ç¥æ¥æ´å¥½å°è¯ä¼°åé¢æµå¤æ ·ä¸ç½è§çé©¾é©¶åºæ¯ä¸­çé¿å°¾ç±»å«ã
*   <strong>å¤çº§åç±»ï¼</strong> å°ç¢°æé¢æµæ¨¡åä»äºååç±»ï¼ç¢°æ/éç¢°æï¼æ©å±å°ä¸çº§åç±»ï¼æ­£å¸¸ãè­¦åãè­¦æ¥ï¼ï¼ä»¥å®ç°æ´ç²¾ç»çé£é©è¯ä¼°åèªéåºå³ç­ã
*   <strong>æ´åå°ADASåèªå¨é©¾é©¶ç³»ç»ï¼</strong> è¿ä¸æ­¥ç ç©¶å¦ä½å°BADASçå¯é ãä¸ä¸ææç¥ç¢°æé¢æµè½åæ´åå°æ´å¹¿æ³çADASåå¨èªå¨é©¾é©¶ç³»ç»ä¸­ï¼ä»¥æé«å®å¨æ§ã</p>
<hr />
<p>æ»èè¨ä¹ï¼BADASè®ºæéè¿å¼å¥ä»¥èªæè½¦è¾ä¸ºä¸­å¿çèå¼ãå©ç¨V-JEPA2éª¨å¹²ç½ç»åNexarçå®ä¸çè¡è½¦è®°å½ä»ªæ°æ®ï¼æ¾èæåäºç¢°æé¢æµçæ§è½åå®ç¨æ§ãå¶å³é®è´¡ç®å¨äºéæ°æ æ³¨äºç°ææ°æ®éãæ ååäºè¯ä¼°åè®®ï¼å¹¶å±ç¤ºäºå¨åå°èåè­¦æ¥åæä¾æ´åç¡®è­¦æ¥æ¶é´æ¹é¢çåè¶è½åãå°½ç®¡å¨é¿å°¾äºä»¶å¤çä¸ä»æææï¼ä½BADASä¸ºæªæ¥æ´å®å¨ãæ´æºè½çé©¾é©¶è¾å©ç³»ç»åèªå¨é©¾é©¶ææ¯å¥ å®äºåå®åºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present BADAS, a family
of collision prediction models trained on Nexar's real-world dashcam collision
dataset -- the first benchmark designed explicitly for ego-centric evaluation.</li>
<li>Across DAD, DADA-2000, DoTA, and
Nexar, BADAS achieves state-of-the-art AP/AUC and outperforms a
forward-collision ADAS baseline while producing more realistic time-to-accident
estimates.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.14876v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.14876v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.14874v1'></a></p>
<h2 id="touch-text-guided-controllable-generation-of-free-form-hand-object-interactions"><a href="https://arxiv.org/abs/2510.14874v1">TOUCH: Text-guided Controllable Generation of Free-Form Hand-Object Interactions</a></h2>
<p><strong>Authors:</strong> Guangyi Han, Wei Zhai, Yuhang Yang, Yang Cao, Zheng-Jun Zha</p>
<p><strong>Published:</strong> 2025-10-16</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Hand-object interaction (HOI) is fundamental for humans to express intent.
Existing HOI generation research is predominantly confined to fixed grasping
patterns, where control is tied to physical priors such as force closure or
generic intent instructions, even when expressed through elaborate language.
Such an overly general conditioning imposes a strong inductive bias for stable
grasps, thus failing to capture the diversity of daily HOI. To address these
limitations, we introduce Free-Form HOI Generation, which aims to generate
controllable, diverse, and physically plausible HOI conditioned on fine-grained
intent, extending HOI from grasping to free-form interactions, like pushing,
poking, and rotating. To support this task, we construct WildO2, an in-the-wild
diverse 3D HOI dataset, which includes diverse HOI derived from internet
videos. Specifically, it contains 4.4k unique interactions across 92 intents
and 610 object categories, each with detailed semantic annotations. Building on
this dataset, we propose TOUCH, a three-stage framework centered on a
multi-level diffusion model that facilitates fine-grained semantic control to
generate versatile hand poses beyond grasping priors. This process leverages
explicit contact modeling for conditioning and is subsequently refined with
contact consistency and physical constraints to ensure realism. Comprehensive
experiments demonstrate our method's ability to generate controllable, diverse,
and physically plausible hand interactions representative of daily activities.
The project page is <script type="math/tex">\href{https://guangyid.github.io/hoi123touch}{here}</script>.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Guangyi Hanç­äººæ°åçè®ºæâTOUCH: Text-guided Controllable Generation of Free-Form Hand-Object Interactionsâçå¨é¢æè¦ã</p>
<hr />
<h3 id="touch">TOUCH: ææ¬å¼å¯¼çå¯æ§èªç±å½¢å¼æç©äº¤äºçæ</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
ç°æçæç©äº¤äºï¼HOIï¼çæç ç©¶ä¸»è¦å±éäºåºå®çæåæ¨¡å¼ï¼å¶æ§å¶éå¸¸ä¾èµäºç©çåéªï¼å¦åé­åï¼æéç¨æå¾æä»¤ï¼å³ä½¿éè¿å¤æçè¯­è¨è¡¨è¾¾ï¼ä¹æªè½ææå°æ¥å¸¸HOIçå¤æ ·æ§ãè¿ç§è¿åº¦éç¨çæ¡ä»¶è®¾å®å¯¼è´æ¨¡åååäºçæç¨³å®çæåï¼ä»èéå¶äºäº¤äºçå¤æ ·æ§ãè¯¥è®ºææ¨å¨è§£å³è¿ä¸éå¶ï¼å¼å¥âèªç±å½¢å¼HOIçæâä»»å¡ï¼ç®æ æ¯çæå¯æ§ãå¤æ ·ä¸ç©çä¸åççæç©äº¤äºï¼è¿äºäº¤äºç±ç»ç²åº¦æå¾æ¡ä»¶åï¼å¹¶å°HOIä»æåæ©å±å°æ¨ãæ³ãæè½¬ç­èªç±å½¢å¼äº¤äºã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong></p>
<ul>
<li><strong>èªç±å½¢å¼HOIçæä»»å¡çå¼å¥ï¼</strong> è®ºæé¦æ¬¡æåºäºâèªç±å½¢å¼HOIçæâä»»å¡ï¼æ¨å¨æç ´ä»¥æåä¸ºä¸­å¿çéå¶ï¼çææ´å¹¿æ³ãæ´å·è¯­ä¹è¡¨è¾¾åçæ¥å¸¸äº¤äºï¼åæ¬åç§éæåæä½ã</li>
<li><strong>WildO2æ°æ®éçæå»ºï¼</strong> ä¸ºæ¯ææ°ä»»å¡ï¼è®ºææå»ºäºä¸ä¸ªåä¸ºWildO2çéå¤å¤æ ·å3D HOIæ°æ®éãè¯¥æ°æ®éåå«ä»äºèç½è§é¢ä¸­æåç4.4kç¬ç¹äº¤äºï¼æ¶µç92ç§æå¾å610ä¸ªå¯¹è±¡ç±»å«ï¼å¹¶éæè¯¦ç»çè¯­ä¹æ æ³¨ãæ°æ®éçæå»ºéè¿ä¸ä¸ªåèªå¨åæµç¨å®ç°ï¼è¯¥æµç¨å©ç¨O2HOIï¼Object-only to Hand-Object Interactionï¼å¸§éå¯¹ç­ç¥ï¼è§£å³äºéå¤è§é¢ä¸­æé¨é®æ¡å¯¼è´çå¯¹è±¡éå»ºé¾é¢ã</li>
<li><strong>TOUCHä¸é¶æ®µæ¡æ¶ï¼</strong> è®ºææåºäºä¸ä¸ªåä¸ºTOUCHçä¸é¶æ®µæ¡æ¶ï¼ç¨äºå¯æ§çèªç±å½¢å¼HOIçæï¼<ul>
<li><strong>æ¥è§¦å¾é¢æµï¼Contact Map Predictionï¼ï¼</strong> å©ç¨ä¸¤ä¸ªç¬ç«çCVAEæ¨¡åï¼æ ¹æ®ææ¬åå¯¹è±¡å ä½ä¿¡æ¯ï¼é¢æµæåå¯¹è±¡è¡¨é¢çäºå¼æ¥è§¦å¾ï¼ä¸ºäº¤äºä½ç½®åå§¿ææä¾å¼ºççç©ºé´åéªã</li>
<li><strong>å¤çº§æ¡ä»¶æ©æ£æ¨¡åï¼Multi-Level Conditioned Diffusionï¼ï¼</strong> éç¨åºäºTransformerçå»åªæ©æ£æ¦çæ¨¡åï¼DDPMï¼ï¼éè¿æ³¨æåæºå¶èåè¯­ä¹åå ä½ä¿¡æ¯ãå¨æ©ææ©æ£é¶æ®µï¼ç²ç²åº¦æå¾åå¨å±å¯¹è±¡å ä½ä¿¡æ¯å¼å¯¼æ´ä½å§¿æï¼å¨åæé¶æ®µï¼ç»ç²åº¦ææ¬åå±é¨æ¥è§¦ç¹å¾è¿ä¸æ­¥ç»åç»èå¨ä½ï¼å®ç°ç»ç²åº¦è¯­ä¹æ§å¶ã</li>
<li><strong>ç©ççº¦æç»åï¼Physical Constraints Refinementï¼ï¼</strong> å¼å¥èªçç£å¾ªç¯ä¸è´æ§æå¤±åç©ççº¦æï¼å¯¹çæçå§¿æè¿è¡ä¼åï¼ä»¥ç¡®ä¿äº¤äºççå®æ§åç©çå¯è¡æ§ï¼è§£å³å¨å±å§¿ææ¼ç§»é®é¢ã</li>
</ul>
</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
å¨é¢çå®éªç»æè¡¨æï¼TOUCHæ¹æ³å¨çæå¯æ§ãå¤æ ·ä¸ç©çä¸åççæç©äº¤äºæ¹é¢è¡¨ç°åºè²ï¼è¿äºäº¤äºè½ä»£è¡¨æ¥å¸¸æ´»å¨ãä¸ç°æåºçº¿æ¹æ³ï¼å¦ContactGenåText2HOIï¼ç¸æ¯ï¼TOUCHå¨æ¥è§¦åç¡®æ§ï¼P-IoU, P-F1ï¼ãç©çåçæ§ï¼MPVPE, PD, PVï¼ãå¤æ ·æ§ï¼çµ, èç±»å¤§å°ï¼åè¯­ä¹ä¸è´æ§ï¼P-FID, VLMè¾å©è¯ä¼°, æç¥åæ°ï¼ç­ææè¯ä¼°ææ ä¸åä¼äºåºçº¿ãæ¶èç ç©¶è¿ä¸æ­¥éªè¯äºæ¥è§¦å¼å¯¼åç²å°ç»ææ¬æ§å¶è®¾è®¡çæææ§ãWildO2æ°æ®éçæå»ºä¸ºè¯¥é¢åæªæ¥çç ç©¶æä¾äºå³é®èµæºã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>éæHOIå¿«ç§ï¼</strong> å½åæ¡æ¶ä¸»è¦å³æ³¨éæHOIå¿«ç§ï¼è¿éå¶äºå¶ææäº¤äºè¿ç¨æ¶é´å¨æçè½åã
*   <strong>æ°æ®éè§æ¨¡ï¼</strong> å°½ç®¡WildO2æ°æ®éæä¾äºå¿«éæ©å±çæ½åï¼ä½å½åæ°æ®éçè§æ¨¡ä»æå¢é¿ç©ºé´ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>å¨æåºåæ©å±ï¼</strong> æªæ¥å·¥ä½è®¡åå°ç ç©¶æ©å±å°å¨æåºåï¼å©ç¨å¤§è§æ¨¡è§é¢æ°æ®éå6-DoFå¯¹è±¡å§¿æä¼°è®¡ï¼ä»èå»ºæ¨¡å®æ´çäºº-ç¯å¢äº¤äºè¿ç¨ã
*   <strong>æ´ç²¾ç»çç©çæ¨¡æï¼</strong> è¿ä¸æ­¥æ¢ç´¢æ´ç²¾ç»çç©çæ¨¡æï¼ä»¥æé«äº¤äºççå®æ§åç¨³å®æ§ã
*   <strong>æ´å¹¿æ³çäº¤äºç±»åï¼</strong> æç»­æ©å±èªç±å½¢å¼HOIçèå´ï¼æ¶µçæ´å¤å¤æåå¤æ ·çæ¥å¸¸äº¤äºã</p>
<hr />
<p>è¿ç¯è®ºæéè¿å¼å¥èªç±å½¢å¼HOIçæä»»å¡åæå»ºWildO2æ°æ®éï¼æ¾èæ¨å¨äºæç©äº¤äºçæé¢åçåå±ãå¶æåºçTOUCHæ¡æ¶ï¼éè¿å¤çº§æ©æ£æ¨¡ååç©ççº¦æç»åï¼å®ç°äºå¯¹ç»ç²åº¦æå¾çå¯æ§çæï¼ä¸ºAR/VRãæºå¨äººåå·èº«AIç­åºç¨æä¾äºæ°çå¯è½æ§ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these
limitations, we introduce Free-Form HOI Generation, which aims to generate
controllable, diverse, and physically plausible HOI conditioned on fine-grained
intent, extending HOI from grasping to free-form interactions, like pushing,
poking, and rotating.</li>
<li>Building on
this dataset, we propose TOUCH, a three-stage framework centered on a
multi-level diffusion model that facilitates fine-grained semantic control to
generate versatile hand poses beyond grasping priors.</li>
<li>Comprehensive
experiments demonstrate our method's ability to generate controllable, diverse,
and physically plausible hand interactions representative of daily activities.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.14874v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.14874v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-10-17 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
