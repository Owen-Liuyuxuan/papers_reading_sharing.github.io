<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-10-17 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-10-16/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-10-20/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-10-17">Arxiv Computer Vision Papers - 2025-10-17</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#prompt-based-adaptation-in-large-scale-vision-models-a-survey" class="nav-link">Prompt-based Adaptation in Large-scale Vision Models: A Survey</a>
                </li>
                <li class="nav-item">
                    <a href="#from-pixels-to-words-towards-native-vision-language-primitives-at-scale" class="nav-link">From Pixels to Words -- Towards Native Vision-Language Primitives at Scale</a>
                </li>
                <li class="nav-item">
                    <a href="#changinggrounding-3d-visual-grounding-in-changing-scenes" class="nav-link">ChangingGrounding: 3D Visual Grounding in Changing Scenes</a>
                </li>
                <li class="nav-item">
                    <a href="#c4d-4d-made-from-3d-through-dual-correspondences" class="nav-link">C4D: 4D Made from 3D through Dual Correspondences</a>
                </li>
                <li class="nav-item">
                    <a href="#mathcanvas-intrinsic-visual-chain-of-thought-for-multimodal-mathematical-reasoning" class="nav-link">MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning</a>
                </li>
                <li class="nav-item">
                    <a href="#omnimotion-multimodal-motion-generation-with-continuous-masked-autoregression" class="nav-link">OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression</a>
                </li>
                <li class="nav-item">
                    <a href="#3d-scene-prompting-for-scene-consistent-camera-controllable-video-generation" class="nav-link">3D Scene Prompting for Scene-Consistent Camera-Controllable Video Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#maskcaptioner-learning-to-jointly-segment-and-caption-object-trajectories-in-videos" class="nav-link">MaskCaptioner : Learning to Jointly Segment and Caption Object Trajectories in Videos</a>
                </li>
                <li class="nav-item">
                    <a href="#badas-context-aware-collision-prediction-using-real-world-dashcam-data" class="nav-link">BADAS: Context Aware Collision Prediction Using Real-World Dashcam Data</a>
                </li>
                <li class="nav-item">
                    <a href="#touch-text-guided-controllable-generation-of-free-form-hand-object-interactions" class="nav-link">TOUCH: Text-guided Controllable Generation of Free-Form Hand-Object Interactions</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-10-17">Arxiv Computer Vision Papers - 2025-10-17</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，这是一份针对2025年10月15日Arxiv计算机视觉论文的每日报告执行摘要，旨在帮助忙碌的研究人员快速了解领域最新进展。</p>
<hr />
<p><strong>每日Arxiv计算机视觉论文执行摘要 (2025年10月15日)</strong></p>
<p><strong>概述与主要趋势：</strong></p>
<p>今日Arxiv论文展现了计算机视觉领域在<strong>多模态理解与生成、3D场景理解与重建、以及具身智能与交互</strong>方面的显著进展。核心趋势包括：</p>
<ol>
<li><strong>多模态融合与统一：</strong> 视觉-语言模型继续向更深层次的融合迈进，旨在实现原生级的视觉-语言原语，并探索在数学推理、运动生成等复杂任务中的应用。</li>
<li><strong>3D场景的动态与交互理解：</strong> 研究重点从静态3D重建扩展到动态场景的理解（4D重建）、3D视觉定位以及与场景交互的生成。</li>
<li><strong>可控生成与具身智能：</strong> 论文强调了在视频、运动和手物交互等生成任务中实现精细化控制的重要性，并开始利用真实世界数据进行具身智能（如碰撞预测）的研究。</li>
<li><strong>Prompt工程的深化：</strong> Prompt-based方法在大型视觉模型中的应用日益广泛，表明其作为一种高效适应策略的重要性。</li>
</ol>
<p><strong>特别显著或创新的论文：</strong></p>
<ul>
<li><strong>"From Pixels to Words -- Towards Native Vision-Language Primitives at Scale" (Haiwen Diao et al.)</strong>: 这篇论文可能代表了视觉-语言模型发展的一个重要方向，即超越简单的特征融合，探索更深层次的“原生”视觉-语言理解单元，有望为未来的多模态AI奠定基础。</li>
<li><strong>"C4D: 4D Made from 3D through Dual Correspondences" (Shizun Wang et al.)</strong>: 提出了一种从3D数据构建4D（3D+时间）场景的新方法，这对于理解和建模动态世界具有开创性意义，可能在自动驾驶、机器人和虚拟现实等领域有广泛应用。</li>
<li><strong>"MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning" (Weikang Shi et al.)</strong>: 将视觉链式思考（Visual Chain-of-Thought）引入多模态数学推理，这对于提升AI在复杂推理任务中的可解释性和准确性至关重要，是迈向更高级认知智能的一步。</li>
<li><strong>"OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression" (Zhe Li et al.)</strong>: 提出了一种多模态运动生成框架，其连续掩码自回归机制可能在生成高质量、多样化运动方面表现出色，对动画、游戏和机器人领域有直接影响。</li>
</ul>
<p><strong>新兴研究方向或技术：</strong></p>
<ul>
<li><strong>原生视觉-语言原语 (Native Vision-Language Primitives):</strong> 旨在构建更深层次、更统一的视觉-语言理解单元，而非简单的特征拼接。</li>
<li><strong>4D场景重建与理解 (4D Scene Reconstruction &amp; Understanding):</strong> 从静态3D向动态3D+时间维度扩展，以更好地捕捉真实世界的复杂性。</li>
<li><strong>视觉链式思考 (Visual Chain-of-Thought):</strong> 将推理过程可视化，提高多模态模型在复杂任务（如数学推理）中的可解释性和性能。</li>
<li><strong>连续掩码自回归 (Continuous Masked Autoregression):</strong> 一种在生成任务中实现高质量、多样化输出的有效机制，尤其适用于运动生成。</li>
<li><strong>3D场景Prompting (3D Scene Prompting):</strong> 将Prompting技术从2D图像扩展到3D场景，以实现更精细的视频生成控制。</li>
<li><strong>具身智能中的真实世界数据应用 (Real-World Data for Embodied AI):</strong> 利用行车记录仪等真实世界数据进行碰撞预测，推动具身智能在实际应用中的鲁棒性。</li>
</ul>
<p><strong>建议阅读全文的论文：</strong></p>
<p>对于希望深入了解前沿进展的研究人员，建议优先阅读以下论文：</p>
<ol>
<li><strong>"From Pixels to Words -- Towards Native Vision-Language Primitives at Scale" (Haiwen Diao et al.)</strong>: 了解多模态融合的未来方向。</li>
<li><strong>"C4D: 4D Made from 3D through Dual Correspondences" (Shizun Wang et al.)</strong>: 掌握4D场景重建的新范式。</li>
<li><strong>"MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning" (Weikang Shi et al.)</strong>: 探索多模态推理和可解释性的新方法。</li>
<li><strong>"OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression" (Zhe Li et al.)</strong>: 关注多模态生成和新型自回归机制。</li>
<li><strong>"Prompt-based Adaptation in Large-scale Vision Models: A Survey" (Xi Xiao et al.)</strong>: 对于希望了解Prompt工程在大型视觉模型中应用现状和未来趋势的研究人员，这篇综述是极佳的起点。</li>
</ol>
<hr />
<p>这份摘要旨在提供一个高层次的概览，帮助您快速识别与您研究兴趣最相关的论文。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2510.13219v1">Prompt-based Adaptation in Large-scale Vision Models: A Survey</a></li>
<li><a href="#2510.14979v1">From Pixels to Words -- Towards Native Vision-Language Primitives at Scale</a></li>
<li><a href="#2510.14965v1">ChangingGrounding: 3D Visual Grounding in Changing Scenes</a></li>
<li><a href="#2510.14960v1">C4D: 4D Made from 3D through Dual Correspondences</a></li>
<li><a href="#2510.14958v1">MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning</a></li>
<li><a href="#2510.14954v1">OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression</a></li>
<li><a href="#2510.14945v1">3D Scene Prompting for Scene-Consistent Camera-Controllable Video Generation</a></li>
<li><a href="#2510.14904v1">MaskCaptioner : Learning to Jointly Segment and Caption Object Trajectories in Videos</a></li>
<li><a href="#2510.14876v1">BADAS: Context Aware Collision Prediction Using Real-World Dashcam Data</a></li>
<li><a href="#2510.14874v1">TOUCH: Text-guided Controllable Generation of Free-Form Hand-Object Interactions</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2510.13219v1'></a></p>
<h2 id="prompt-based-adaptation-in-large-scale-vision-models-a-survey"><a href="https://arxiv.org/abs/2510.13219v1">Prompt-based Adaptation in Large-scale Vision Models: A Survey</a></h2>
<p><strong>Authors:</strong> Xi Xiao, Yunbei Zhang, Lin Zhao, Yiyang Liu, Xiaoying Liao, Zheda Mai, Xingjian Li, Xiao Wang, Hao Xu, Jihun Hamm, Xue Lin, Min Xu, Qifan Wang, Tianyang Wang, Cheng Han</p>
<p><strong>Published:</strong> 2025-10-15</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>In computer vision, Visual Prompting (VP) and Visual Prompt Tuning (VPT) have
recently emerged as lightweight and effective alternatives to full fine-tuning
for adapting large-scale vision models within the ``pretrain-then-finetune''
paradigm. However, despite rapid progress, their conceptual boundaries remain
blurred, as VP and VPT are frequently used interchangeably in current research,
reflecting a lack of systematic distinction between these techniques and their
respective applications. In this survey, we revisit the designs of VP and VPT
from first principles, and conceptualize them within a unified framework termed
Prompt-based Adaptation (PA). We provide a taxonomy that categorizes existing
methods into learnable, generative, and non-learnable prompts, and further
organizes them by injection granularity -- pixel-level and token-level. Beyond
the core methodologies, we examine PA's integrations across diverse domains,
including medical imaging, 3D point clouds, and vision-language tasks, as well
as its role in test-time adaptation and trustworthy AI. We also summarize
current benchmarks and identify key challenges and future directions. To the
best of our knowledge, we are the first comprehensive survey dedicated to PA's
methodologies and applications in light of their distinct characteristics. Our
survey aims to provide a clear roadmap for researchers and practitioners in all
area to understand and explore the evolving landscape of PA-related research.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“Prompt-based Adaptation in Large-scale Vision Models: A Survey”的中文摘要，涵盖了您要求的所有要点：</p>
<p><strong>论文摘要：大型视觉模型中的基于提示的适应性：一项综述</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决计算机视觉领域中，视觉提示（Visual Prompting, VP）和视觉提示微调（Visual Prompt Tuning, VPT）这两种轻量级模型适应方法之间概念边界模糊的问题。尽管它们在大型视觉模型的“预训练-微调”范式中取得了快速进展并被广泛应用，但当前研究中常将二者互换使用，缺乏系统性的区分，这阻碍了对它们各自技术特点和适用场景的深入理解。本综述的核心目标是提供一个统一的框架，清晰地界定和分类这些方法。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>统一框架与分类法：</strong> 论文将VP和VPT从第一性原理出发，概念化为一个统一的框架，称之为“基于提示的适应性”（Prompt-based Adaptation, PA）。
*   <strong>详细分类：</strong> 提出了一种全面的分类法，将现有方法分为可学习（learnable）、生成式（generative）和不可学习（non-learnable）提示。
*   <strong>注入粒度区分：</strong> 进一步根据提示的注入粒度（像素级和Token级）对方法进行组织，这是区分VP和VPT的关键。VP主要在输入空间进行像素级修改，而VPT则在模型内部的Token序列中注入可学习的提示。
*   <strong>应用领域整合：</strong> 综述了PA在各种不同领域中的应用，包括医学影像、3D点云、视觉-语言任务，以及其在测试时间适应（Test-Time Adaptation）和可信赖AI（Trustworthy AI）中的作用。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>PA的有效性：</strong> 论文强调PA作为全量微调的轻量级且有效替代方案，在数据受限、资源受限和动态适应等多种场景下表现出显著效果。
*   <strong>效率提升：</strong> PA通过仅更新少量参数（VP可能不更新参数，VPT更新少量提示Token和头部），显著降低了计算和存储成本，尤其适用于商品硬件和延迟敏感的部署环境。
*   <strong>鲁棒性与泛化能力：</strong> 提示机制能够帮助模型更好地应对领域漂移、分布变化，并在少样本、零样本等数据受限场景下提高泛化能力。
*   <strong>可信赖AI的贡献：</strong> PA在提升模型鲁棒性、缓解偏见和确保隐私安全方面发挥作用，是构建可信赖AI系统的重要组成部分。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>训练开销与稳定性：</strong> 尽管PA减少了参数效率，但总训练时长可能因超参数搜索和初始化不稳定性而增加。VP方法可能表现出不稳定性，对提示配置的微小扰动敏感。
*   <strong>推理延迟：</strong> 额外的提示组件（无论是VP的输入空间修改还是VPT的Token注入）可能导致推理延迟和额外的内存消耗。
*   <strong>真实世界环境评估不足：</strong> 当前PA方法的评估主要依赖标准化学术基准，这些基准可能无法准确反映真实世界场景的复杂性和分布变化。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>训练效率与稳定性：</strong> 进一步研究训练捷径、检测和纠正训练不稳定性的策略。
*   <strong>混合方法：</strong> 探索结合VP和VPT优势的混合方法，例如使用生成器提供初始空间提示（VP），同时使用可学习的条件感知Token（VPT）来引导模型内部特征提取。
*   <strong>真实世界部署：</strong> 优先开发能够应对复杂异构视觉上下文的鲁棒方法，以弥合学术研究与实际部署之间的差距。
*   <strong>安全对齐：</strong> 持续研究如何检测和纠正模型行为中的偏差、恶意内容生成等问题，确保PA方法符合人类价值观和目标。
*   <strong>理论基础：</strong> 深入探索PA如何诱导模型行为变化、视觉提示学习了什么以及PA方法在不同适应设置中的有效性等理论问题。</p>
<p>总而言之，这篇综述为研究人员和实践者提供了一个关于大型视觉模型中基于提示的适应性方法的清晰路线图，系统地梳理了其概念、方法、应用、挑战和未来方向，对于推动该领域的发展具有重要意义。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To the
best of our knowledge, we are the first comprehensive survey dedicated to PA's
methodologies and applications in light of their distinct characteristics.</li>
<li>Our
survey aims to provide a clear roadmap for researchers and practitioners in all
area to understand and explore the evolving landscape of PA-related research.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.13219v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.13219v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.14979v1'></a></p>
<h2 id="from-pixels-to-words-towards-native-vision-language-primitives-at-scale"><a href="https://arxiv.org/abs/2510.14979v1">From Pixels to Words -- Towards Native Vision-Language Primitives at Scale</a></h2>
<p><strong>Authors:</strong> Haiwen Diao, Mingxuan Li, Silei Wu, Linjun Dai, Xiaohua Wang, Hanming Deng, Lewei Lu, Dahua Lin, Ziwei Liu</p>
<p><strong>Published:</strong> 2025-10-16</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>The edifice of native Vision-Language Models (VLMs) has emerged as a rising
contender to typical modular VLMs, shaped by evolving model architectures and
training paradigms. Yet, two lingering clouds cast shadows over its widespread
exploration and promotion: (-) What fundamental constraints set native VLMs
apart from modular ones, and to what extent can these barriers be overcome? (-)
How to make research in native VLMs more accessible and democratized, thereby
accelerating progress in the field. In this paper, we clarify these challenges
and outline guiding principles for constructing native VLMs. Specifically, one
native VLM primitive should: (i) effectively align pixel and word
representations within a shared semantic space; (ii) seamlessly integrate the
strengths of formerly separate vision and language modules; (iii) inherently
embody various cross-modal properties that support unified vision-language
encoding, aligning, and reasoning. Hence, we launch NEO, a novel family of
native VLMs built from first principles, capable of rivaling top-tier modular
counterparts across diverse real-world scenarios. With only 390M image-text
examples, NEO efficiently develops visual perception from scratch while
mitigating vision-language conflicts inside a dense and monolithic model
crafted from our elaborate primitives. We position NEO as a cornerstone for
scalable and powerful native VLMs, paired with a rich set of reusable
components that foster a cost-effective and extensible ecosystem. Our code and
models are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO.</p>
<p><strong>Analysis:</strong></p>
<p>这篇论文“From Pixels to Words -- Towards Native Vision-Language Primitives at Scale”由Haiwen Diao等人撰写，旨在解决原生视觉-语言模型（VLMs）在与传统模块化VLM竞争时所面临的挑战，并推动该领域的研究进展。</p>
<p><strong>1. 主要问题或研究问题：</strong>
论文主要探讨了原生VLM在广泛探索和推广中面临的两个核心问题：
*   原生VLM与模块化VLM之间的根本区别是什么，以及如何克服这些障碍？
*   如何使原生VLM的研究更易于访问和民主化，从而加速该领域的进展？</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
论文提出了构建原生VLM的指导原则，并推出了一个名为<strong>NEO</strong>的新型原生VLM家族，其关键创新包括：
*   <strong>原生VLM基元（Native VLM Primitive）：</strong> NEO设计了一个统一的视觉-语言基元，能够在一个模块中同时整合跨模态的编码、对齐和推理。这包括：
    *   灵活的位置编码方案，有效泛化到动态空间结构。
    *   多头原生注意力（MHNA），共同处理视觉-文本连接。
    *   原生旋转位置嵌入（Native-RoPE），具有模态特定频率，兼容预训练LLM权重并吸收原始VE的交互模式。
*   <strong>预缓冲区（Pre-Buffer）和后LLM（Post-LLM）架构：</strong> 为了高效地扩展视觉训练并确保像素-词语对齐的一致性，NEO将骨干网络划分为预缓冲区和后LLM层。这种设计在预训练阶段使预训练LLM能够引导视觉学习，并在后续阶段建立连贯的相关性。在训练后期，这种划分会消失，形成一个统一的架构。
*   <strong>端到端训练范式：</strong> NEO通过简化的端到端训练，在仅3.9亿图像-文本示例上，从头开始高效地发展视觉感知能力，同时缓解密集、单片模型内部的视觉-语言冲突。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>竞争性表现：</strong> 尽管训练数据和计算资源相对有限，NEO在2B和8B规模上均展现出高度竞争性的性能，在多个基准测试中与顶级的模块化VLM（如Qwen2-VL、InternVL2.5等）相媲美，甚至超越了许多使用更多训练资源的原生VLM。
*   <strong>高效的像素-词语对齐和推理：</strong> NEO通过其统一的基元设计和训练策略，能够从头开始将视觉输入与文本特征对齐，并支持复杂的视觉推理。
*   <strong>可重用组件和生态系统：</strong> NEO提供了丰富的可重用组件，促进了成本效益和可扩展的生态系统，降低了原生VLM开发的门槛。
*   <strong>对未来多模态系统的启示：</strong> 论文表明，下一代多模态系统可以源于原生、统一且本质上多模态的架构。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>数据和计算资源限制：</strong> NEO的性能仍受限于稀缺的训练数据和有限的计算资源，尤其是在知识密集型和OCR领域。
*   <strong>无法完全从头训练：</strong> 受限于当前的文本语料库和计算资源，NEO无法在不依赖现有LLM初始化的情况下完全从头训练一个完全原生的模型。这限制了缓解语言模态主导地位可能带来的潜在偏差的能力。
*   <strong>在某些任务上的不足：</strong> NEO在知识/OCR密集型任务（如MMMU、InfoVQA和TextVQA）上表现略逊一筹。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>持续投入和扩展：</strong> 持续投入大量资源，尤其是在预训练阶段，以充分释放NEO的性能潜力。
*   <strong>开放关键组件：</strong> 在中间开发阶段选择性地开放关键组件，以降低未来研究人员的后续训练成本，并吸引更多原生视觉-语言模型研究。
*   <strong>探索全谱模型能力：</strong> 扩展模型规模是推动实际应用的关键因素，NEO-2.2B的性能已接近同等容量的模块化VLM，表明0.6到80亿参数范围内的模型设计哲学已趋于成熟。
*   <strong>架构和应用升级：</strong> 将NEO视为一个视觉-语言智能的新范式，利用端到端训练和统一架构，消除手动施加的偏差和扩展复杂性。将NEO扩展到视频生成、长视频理解和具身AI等领域。
*   <strong>去中心化多模态训练：</strong> 探索从头开始进行多模态训练，以进一步揭示原生VLM架构的性能上限。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Hence, we launch NEO, a novel family of
native VLMs built from first principles, capable of rivaling top-tier modular
counterparts across diverse real-world scenarios.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.14979v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.14979v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.14965v1'></a></p>
<h2 id="changinggrounding-3d-visual-grounding-in-changing-scenes"><a href="https://arxiv.org/abs/2510.14965v1">ChangingGrounding: 3D Visual Grounding in Changing Scenes</a></h2>
<p><strong>Authors:</strong> Miao Hu, Zhiwei Huang, Tai Wang, Jiangmiao Pang, Dahua Lin, Nanning Zheng, Runsen Xu</p>
<p><strong>Published:</strong> 2025-10-16</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Real-world robots localize objects from natural-language instructions while
scenes around them keep changing. Yet most of the existing 3D visual grounding
(3DVG) method still assumes a reconstructed and up-to-date point cloud, an
assumption that forces costly re-scans and hinders deployment. We argue that
3DVG should be formulated as an active, memory-driven problem, and we introduce
ChangingGrounding, the first benchmark that explicitly measures how well an
agent can exploit past observations, explore only where needed, and still
deliver precise 3D boxes in changing scenes. To set a strong reference point,
we also propose Mem-ChangingGrounder, a zero-shot method for this task that
marries cross-modal retrieval with lightweight multi-view fusion: it identifies
the object type implied by the query, retrieves relevant memories to guide
actions, then explores the target efficiently in the scene, falls back when
previous operations are invalid, performs multi-view scanning of the target,
and projects the fused evidence from multi-view scans to get accurate object
bounding boxes. We evaluate different baselines on ChangingGrounding, and our
Mem-ChangingGrounder achieves the highest localization accuracy while greatly
reducing exploration cost. We hope this benchmark and method catalyze a shift
toward practical, memory-centric 3DVG research for real-world applications.
Project page: https://hm123450.github.io/CGB/ .</p>
<p><strong>Analysis:</strong></p>
<p>以下是Miao Hu等人撰写的论文“ChangingGrounding: 3D Visual Grounding in Changing Scenes”的全面摘要：</p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决现实世界机器人面临的一个核心挑战：如何在场景不断变化的环境中，根据自然语言指令准确有效地定位目标物体。现有的3D视觉定位（3DVG）方法大多假设场景是静态的，依赖于完整且最新的点云，这导致了昂贵的重复扫描和部署障碍。因此，研究问题是如何将3DVG重新定义为一个主动的、记忆驱动的问题，使机器人能够利用过去的观察、仅在需要时进行探索，并仍能在动态场景中提供精确的3D边界框。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>ChangingGrounding基准：</strong> 论文引入了首个专门用于衡量机器人在变化场景中3DVG性能的基准。该基准明确评估了代理利用过去观察、高效探索和精确3D定位的能力。
*   <strong>Mem-ChangingGrounder方法：</strong> 提出了一种零样本（zero-shot）方法来解决此任务，该方法结合了跨模态检索和轻量级多视图融合。其核心流程包括：
    *   <strong>查询分类：</strong> 识别查询中隐含的物体类型。
    *   <strong>记忆检索：</strong> 检索相关记忆以指导行动。
    *   <strong>高效探索：</strong> 在场景中高效探索目标，并在先前操作无效时进行回退。
    *   <strong>多视图扫描与融合：</strong> 对目标进行多视图扫描，并将融合的证据投影以获得准确的物体边界框。
*   <strong>行动策略（OSS和SRAS）：</strong> 引入了全向场景扫描器（Omnidirectional Scene Scanner, OSS）和空间关系感知扫描器（Spatial Relation Aware Scanner, SRAS）两种行动策略，用于在未知场景中探索和定位目标物体。
*   <strong>数据集构建：</strong> 基于3RScan数据集构建了一个新的ChangingGrounding数据集，包含空间关系描述、RGB-D图像、相机姿态和网格文件，以模拟物体移动和生成新观察。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>Mem-ChangingGrounder的优越性：</strong> 在ChangingGrounding基准测试中，Mem-ChangingGrounder在低分辨率和高分辨率设置下均实现了最高的定位精度（Acc@0.25分别为29.2%和36.8%），同时显著降低了探索成本（动作成本Ca和运动成本Cm）。
*   <strong>效率与准确性的平衡：</strong> 实验结果表明，该方法在准确性和效率之间取得了卓越的平衡，通过在移动前咨询记忆并执行有针对性的短动作，避免了长时间的探索循环。
*   <strong>基线比较：</strong> 与“漫游定位”（Wandering Grounding）、“中心旋转定位”（Central Rotation Grounding）和“仅记忆定位”（Memory-Only Grounding）等基线方法相比，Mem-ChangingGrounder在准确性和成本方面均表现出显著优势。
*   <strong>记忆和探索的重要性：</strong> 消融研究证实了记忆策略和回退机制的有效性，以及多视图投影对提高定位准确性的贡献。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>CGB基准的局限性：</strong> 当前数据集仅模拟目标及其周围环境的相对位置变化，未考虑光照变化、物体外观属性（颜色、材料、变形）或动态场景交互等关键因素。此外，缺乏“物体A在物体B前面”等绝对空间关系描述。
*   <strong>MCG方法的局限性：</strong>
    *   <strong>VLM能力依赖：</strong> MCG严重依赖底层视觉-语言模型（VLM）的能力，其性能受VLM能力和现实世界场景复杂性的影响。
    *   <strong>渲染图像的噪声：</strong> 渲染过程引入的噪声（如RGB图像中的伪影、深度图的不准确性）以及渲染图像与真实图像之间的固有差异，可能影响定位准确性。
    *   <strong>2D模型引入的噪声：</strong> MCG依赖2D物体检测器和分割网络，这些模型可能存在漏检、误报、边界框不精确和分割错误，这些缺陷会影响最终的定位准确性。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>提高VLM的鲁棒性：</strong> 开发更鲁棒的视觉-语言模型，以处理复杂的现实世界视觉信息并减少噪声影响。
*   <strong>增强多模态集成：</strong> 探索更好地集成多模态数据（视觉、语言和空间信息）以提高定位准确性的方法。
*   <strong>扩展基准多样性：</strong> 通过增加更多样化的场景（包括光照变化、物体外观和动态交互）来扩展CGB基准。
*   <strong>减少渲染数据中的噪声：</strong> 研究最小化渲染过程中引入的噪声，并弥合真实图像与渲染图像之间差距的方法。
*   <strong>推进2D到3D投影技术：</strong> 提高2D物体检测和分割模型的准确性和可靠性，以增强整体定位性能。</p>
<p>总而言之，这篇论文通过引入ChangingGrounding基准和Mem-ChangingGrounder方法，为动态场景中的3D视觉定位任务开辟了新方向。它强调了记忆和高效探索在实际机器人应用中的重要性，并为未来该领域的研究奠定了坚实基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We argue that
3DVG should be formulated as an active, memory-driven problem, and we introduce
ChangingGrounding, the first benchmark that explicitly measures how well an
agent can exploit past observations, explore only where needed, and still
deliver precise 3D boxes in changing scenes.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.14965v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.14965v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.14960v1'></a></p>
<h2 id="c4d-4d-made-from-3d-through-dual-correspondences"><a href="https://arxiv.org/abs/2510.14960v1">C4D: 4D Made from 3D through Dual Correspondences</a></h2>
<p><strong>Authors:</strong> Shizun Wang, Zhenxiang Jiang, Xingyi Yang, Xinchao Wang</p>
<p><strong>Published:</strong> 2025-10-16</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Recovering 4D from monocular video, which jointly estimates dynamic geometry
and camera poses, is an inevitably challenging problem. While recent
pointmap-based 3D reconstruction methods (e.g., DUSt3R) have made great
progress in reconstructing static scenes, directly applying them to dynamic
scenes leads to inaccurate results. This discrepancy arises because moving
objects violate multi-view geometric constraints, disrupting the
reconstruction. To address this, we introduce C4D, a framework that leverages
temporal Correspondences to extend existing 3D reconstruction formulation to
4D. Specifically, apart from predicting pointmaps, C4D captures two types of
correspondences: short-term optical flow and long-term point tracking. We train
a dynamic-aware point tracker that provides additional mobility information,
facilitating the estimation of motion masks to separate moving elements from
the static background, thus offering more reliable guidance for dynamic scenes.
Furthermore, we introduce a set of dynamic scene optimization objectives to
recover per-frame 3D geometry and camera parameters. Simultaneously, the
correspondences lift 2D trajectories into smooth 3D trajectories, enabling
fully integrated 4D reconstruction. Experiments show that our framework
achieves complete 4D recovery and demonstrates strong performance across
multiple downstream tasks, including depth estimation, camera pose estimation,
and point tracking. Project Page: https://littlepure2333.github.io/C4D</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于Shizun Wang, Zhenxiang Jiang, Xingyi Yang, Xinchao Wang撰写的论文“C4D: 4D Made from 3D through Dual Correspondences”的全面摘要。</p>
<p><strong>论文摘要：C4D: 通过双重对应从3D构建4D</strong></p>
<p>这篇论文《C4D: 4D Made from 3D through Dual Correspondences》提出了一种新颖的框架C4D，旨在解决从单目视频中恢复动态场景4D表示的挑战性问题。4D重建不仅涉及估计动态场景的几何形状，还包括相机姿态和3D点跟踪。</p>
<p><strong>1. 主要问题或研究问题</strong>
现有的基于点图（pointmap-based）的3D重建方法（如DUSt3R）在静态场景中取得了显著进展，但直接应用于动态场景时，由于移动物体违反了多视角几何约束，导致重建结果不准确。核心问题是如何在动态场景中实现准确、平滑且时间一致的4D重建，包括每帧3D几何、相机姿态和3D点轨迹。</p>
<p><strong>2. 关键创新或方法论贡献</strong>
C4D框架通过引入“双重对应”（Dual Correspondences）将现有的3D重建扩展到4D，其主要创新包括：</p>
<ul>
<li><strong>动态感知点跟踪器（DynPT）</strong>：C4D训练了一个能够预测点在世界坐标系中是否动态的跟踪器。这超越了传统2D点跟踪器仅预测位置和遮挡的能力，为区分相机运动和物体自身运动提供了关键信息。</li>
<li><strong>对应引导的运动掩码估计</strong>：利用短时光流（optical flow）和长时点跟踪（DynPT）两种对应，C4D能够生成可靠的运动掩码。这些掩码用于将动态元素从静态背景中分离出来，从而在静态区域进行更准确的相机参数估计和几何重建。</li>
<li><strong>对应辅助的动态场景优化目标</strong>：C4D引入了一系列新的优化目标，包括：<ul>
<li><strong>相机运动对齐（CMA）</strong>：确保估计的自我运动与静态区域的光流一致。</li>
<li><strong>相机轨迹平滑度（CTS）</strong>：通过惩罚连续帧之间相机旋转和平移的突然变化，强制相机运动平滑。</li>
<li><strong>点轨迹平滑度（PTS）</strong>：通过对稀疏3D点轨迹进行自适应加权的一维卷积平滑，然后通过线性混合位移（LBD）将其传播到所有点，确保3D点轨迹的时间平滑性。</li>
</ul>
</li>
<li><strong>完全集成4D重建</strong>：通过联合预测点图和上述双重对应，C4D将2D轨迹提升为平滑的3D轨迹，实现了每帧3D几何和相机参数的完全集成4D恢复。</li>
</ul>
<p><strong>3. 主要结果及其意义</strong>
实验结果表明，C4D框架在动态场景重建方面表现出色，并在多个下游任务中展示了强大的性能：</p>
<ul>
<li><strong>深度估计</strong>：C4D在Sintel、Bonn和KITTI数据集上实现了竞争性的深度估计性能，尤其在尺度不变对齐方面表现最佳。</li>
<li><strong>相机姿态估计</strong>：C4D在Sintel、TUM-dynamics和ScanNet数据集上显著提高了相机姿态估计的准确性，甚至优于一些专门的视觉里程计方法。</li>
<li><strong>点跟踪</strong>：尽管DynPT需要预测额外的“移动性”信息，但其在TAP-Vid和Kubric数据集上仍能与最先进的TAP方法保持竞争性性能，并准确预测点的动态状态。</li>
<li><strong>时间平滑性</strong>：C4D通过PTS目标显著改善了视频深度和3D点轨迹的时间平滑性，有效减少了现有方法中常见的闪烁伪影。</li>
<li><strong>运动掩码准确性</strong>：C4D生成的运动掩码比MonST3R等方法更准确和完整，尤其在复杂动态场景中。</li>
</ul>
<p>这些结果证明了C4D在处理动态场景时的有效性和鲁棒性，为单目视频4D重建领域树立了新的基准。</p>
<p><strong>4. 论文中提到的局限性</strong>
论文中没有明确提及C4D框架的显著局限性。然而，从方法论和实验设置中可以推断出一些潜在的限制：</p>
<ul>
<li><strong>计算成本</strong>：虽然论文提到通过稀疏场景图和滑动窗口策略来降低计算成本，但联合优化点图、光流、点跟踪和多个优化目标，可能仍然具有较高的计算复杂度，尤其是在处理超长视频时。</li>
<li><strong>合成数据依赖</strong>：DynPT的训练依赖于Kubric等合成数据集，这些数据集提供了地面真实移动性标签。尽管合成数据有助于控制变量，但其在真实世界复杂场景中的泛化能力可能仍需进一步验证。</li>
<li><strong>模型权重初始化</strong>：C4D利用了预训练的DUSt3R模型权重，这意味着其性能可能部分依赖于这些基础模型的质量和泛化能力。</li>
<li><strong>超参数敏感性</strong>：优化过程中涉及多个损失权重（WGA, WCMA, WCTS, WPTS）和一些超参数（如平滑因子λ、核大小k），这些参数的选择可能对最终性能有影响。</li>
</ul>
<p><strong>5. 潜在的未来研究方向</strong>
基于C4D的贡献和潜在局限性，未来的研究方向可能包括：</p>
<ul>
<li><strong>实时性能提升</strong>：进一步优化C4D的计算效率，使其能够实现更接近实时的4D重建，这对于机器人和增强现实等应用至关重要。</li>
<li><strong>更强的泛化能力</strong>：探索在更多样化、更具挑战性的真实世界动态场景数据集上训练和评估C4D，以提高其在未见场景中的泛化能力。</li>
<li><strong>自监督或弱监督学习</strong>：减少对地面真实移动性标签的依赖，开发更强大的自监督或弱监督方法来训练动态感知点跟踪器和运动掩码估计。</li>
<li><strong>集成语义信息</strong>：将语义分割或物体检测等高级语义信息集成到C4D框架中，以更好地理解场景中的动态元素，从而可能提高运动掩码和点轨迹的准确性。</li>
<li><strong>多模态输入</strong>：探索结合其他传感器数据（如IMU、激光雷达）来增强4D重建的鲁棒性和准确性，尤其是在光照不足或纹理稀疏的挑战性环境中。</li>
<li><strong>交互式4D重建</strong>：开发允许用户在4D重建过程中进行交互和修正的工具，以处理复杂或模糊的动态场景。</li>
</ul>
<p>总而言之，C4D为单目视频4D重建提供了一个全面且高性能的解决方案，通过巧妙地结合短时和长时对应，有效解决了动态场景中的挑战，并为未来的研究奠定了坚实的基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we introduce C4D, a framework that leverages
temporal Correspondences to extend existing 3D reconstruction formulation to
4D.</li>
<li>Furthermore, we introduce a set of dynamic scene optimization objectives to
recover per-frame 3D geometry and camera parameters.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.14960v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.14960v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.14958v1'></a></p>
<h2 id="mathcanvas-intrinsic-visual-chain-of-thought-for-multimodal-mathematical-reasoning"><a href="https://arxiv.org/abs/2510.14958v1">MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning</a></h2>
<p><strong>Authors:</strong> Weikang Shi, Aldrich Yu, Rongyao Fang, Houxing Ren, Ke Wang, Aojun Zhou, Changyao Tian, Xinyu Fu, Yuxuan Hu, Zimu Lu, Linjiang Huang, Si Liu, Rui Liu, Hongsheng Li</p>
<p><strong>Published:</strong> 2025-10-16</p>
<p><strong>Categories:</strong> cs.CV, cs.CL</p>
<p><strong>Abstract:</strong></p>
<p>While Large Language Models (LLMs) have excelled in textual reasoning, they
struggle with mathematical domains like geometry that intrinsically rely on
visual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often
limited by rigid external tools or fail to generate the high-fidelity,
strategically-timed diagrams necessary for complex problem-solving. To bridge
this gap, we introduce MathCanvas, a comprehensive framework designed to endow
unified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for
mathematics. Our approach consists of two phases. First, a Visual Manipulation
stage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M
caption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing
trajectories (MathCanvas-Edit), to master diagram generation and editing.
Second, a Strategic Visual-Aided Reasoning stage fine-tunes the model on
MathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual
reasoning paths, teaching it when and how to leverage visual aids. To
facilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging
benchmark with 3K problems that require models to produce interleaved
visual-textual solutions. Our model, BAGEL-Canvas, trained under this
framework, achieves an 86% relative improvement over strong LMM baselines on
MathCanvas-Bench, demonstrating excellent generalization to other public math
benchmarks. Our work provides a complete toolkit-framework, datasets, and
benchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project
Page: https://mathcanvas.github.io/</p>
<p><strong>Analysis:</strong></p>
<p>以下是Weikang Shi等人撰写的论文“MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning”的摘要：</p>
<p><strong>1. 主要问题或研究问题：</strong>
大型语言模型（LLMs）在文本推理方面表现出色，但在几何学等数学领域中，由于其本质上依赖视觉辅助，LLMs表现不佳。现有的视觉思维链（VCoT）方法通常受限于僵硬的外部工具，或无法生成高保真度、策略性适时的图表，从而难以解决复杂的数学问题。该研究旨在弥合这一鸿沟，赋予统一大型多模态模型（LMMs）内在的VCoT能力，以进行数学推理。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
该论文引入了MathCanvas，一个全面的框架，旨在赋予LMMs内在的VCoT能力。其方法分为两个阶段：
*   <strong>视觉操作（Visual Manipulation）阶段：</strong> 通过在一个包含15.2M对数据的新型语料库上预训练模型，掌握图表生成和编辑能力。该语料库包括10M的“标题-图表对”（MathCanvas-Imagen）和5.2M的“逐步编辑轨迹”（MathCanvas-Edit）。
*   <strong>策略性视觉辅助推理（Strategic Visual-Aided Reasoning）阶段：</strong> 在MathCanvas-Instruct数据集上对模型进行微调，该数据集是一个包含219K示例的交错视觉-文本推理路径数据集，旨在教授模型何时以及如何利用视觉辅助。
*   <strong>MathCanvas-Bench基准测试：</strong> 为了进行严格评估，引入了一个包含3K问题的挑战性基准测试，要求模型生成交错的视觉-文本解决方案。</p>
<p><strong>3. 主要结果及其意义：</strong>
该研究训练的模型BAGEL-Canvas，在该框架下，在MathCanvas-Bench上比强大的LMM基线取得了86%的相对改进，并展示了对其他公共数学基准的优秀泛化能力。这表明MathCanvas成功地解锁了LMMs中复杂、类人视觉辅助推理的能力。</p>
<p><strong>4. 论文中提及的局限性：</strong>
摘要中未明确提及论文的局限性，但从其强调“解锁复杂、类人视觉辅助推理”以及“对其他公共数学基准的优秀泛化能力”来看，可能暗示了现有模型在这些方面仍有提升空间，或者在某些特定数学领域（如微积分和向量）的改进相对较小（正文中提到微积分和向量领域的增益较小，可能超出当前视觉增强技术的范围）。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
该工作提供了一个完整的工具包、框架、数据集和基准测试，为LMMs中复杂、类人视觉辅助推理的未来研究奠定了坚实基础。未来的研究可以进一步探索如何优化模型在特定数学领域的表现，或扩展其在更广泛、更复杂的多模态推理任务中的应用。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To bridge
this gap, we introduce MathCanvas, a comprehensive framework designed to endow
unified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for
mathematics.</li>
<li>Our approach consists of two phases.</li>
<li>First, a Visual Manipulation
stage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M
caption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing
trajectories (MathCanvas-Edit), to master diagram generation and editing.</li>
<li>Second, a Strategic Visual-Aided Reasoning stage fine-tunes the model on
MathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual
reasoning paths, teaching it when and how to leverage visual aids.</li>
<li>To
facilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging
benchmark with 3K problems that require models to produce interleaved
visual-textual solutions.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.14958v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.14958v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.14954v1'></a></p>
<h2 id="omnimotion-multimodal-motion-generation-with-continuous-masked-autoregression"><a href="https://arxiv.org/abs/2510.14954v1">OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression</a></h2>
<p><strong>Authors:</strong> Zhe Li, Weihao Yuan, Weichao Shen, Siyu Zhu, Zilong Dong, Chang Xu</p>
<p><strong>Published:</strong> 2025-10-16</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Whole-body multi-modal human motion generation poses two primary challenges:
creating an effective motion generation mechanism and integrating various
modalities, such as text, speech, and music, into a cohesive framework. Unlike
previous methods that usually employ discrete masked modeling or autoregressive
modeling, we develop a continuous masked autoregressive motion transformer,
where a causal attention is performed considering the sequential nature within
the human motion. Within this transformer, we introduce a gated linear
attention and an RMSNorm module, which drive the transformer to pay attention
to the key actions and suppress the instability caused by either the abnormal
movements or the heterogeneous distributions within multi-modalities. To
further enhance both the motion generation and the multimodal generalization,
we employ the DiT structure to diffuse the conditions from the transformer
towards the targets. To fuse different modalities, AdaLN and cross-attention
are leveraged to inject the text, speech, and music signals. Experimental
results demonstrate that our framework outperforms previous methods across all
modalities, including text-to-motion, speech-to-gesture, and music-to-dance.
The code of our method will be made public.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Zhe Li等人撰写的论文“OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression”的全面摘要。</p>
<hr />
<h3 id="omnimotion">论文摘要：OmniMotion: 多模态连续掩码自回归运动生成</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决全身多模态人体运动生成领域的两大挑战：一是如何构建一个高效的运动生成机制，二是如何将文本、语音和音乐等多种模态有效地整合到一个统一的框架中。现有的方法通常专注于单一模态或采用离散的掩码或自回归建模，这限制了其泛化能力和生成质量。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
OmniMotion 提出了一个新颖的、统一的框架，用于从多种模态（文本、语音、音乐）生成全身人体运动。其核心创新包括：</p>
<ul>
<li><strong>连续掩码自回归运动Transformer (Continuous Masked Autoregressive Motion Transformer)：</strong> 与以往离散建模不同，该方法采用连续的掩码自回归Transformer。它通过因果注意力机制处理序列运动数据，以捕捉人体运动的顺序性，并预测被掩码的运动片段。</li>
<li><strong>门控线性注意力 (Gated Linear Attention) 和 RMSNorm 模块：</strong> 在Transformer内部，引入了门控线性注意力机制作为自适应特征选择器，使模型能够关注关键动作（如手势切换、大幅度运动），同时抑制不相关或冗余动作（如静止运动）。RMSNorm模块则用于处理多模态输入中异构分布带来的不稳定性，并缓解异常运动（如突然跳跃）引起的梯度不稳定性。</li>
<li><strong>DiT (Diffusion Transformer) 结构的应用：</strong> 为了进一步提升运动生成质量和多模态泛化能力，模型采用DiT结构，将Transformer生成的条件信息扩散到目标运动。在多模态学习阶段，DiT模块结构保持不变并被冻结，仅对掩码Transformer进行微调。</li>
<li><strong>多模态信号融合机制：</strong> 利用 AdaLN (Adaptive Layer Normalization) 和交叉注意力机制，将文本、语音和音乐信号有效地注入到Transformer中，实现多模态条件的融合。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
实验结果表明，OmniMotion 框架在所有模态（包括文本到运动、语音到手势和音乐到舞蹈）上均优于现有方法。
*   <strong>文本到运动生成：</strong> 在HumanML3D数据集上，模型在R-Precision (Top-1, 2, 3) 上分别提升了19.3%、13.5%和11.7%，FID分数提升了75.2%，表明其生成的运动具有卓越的保真度和与文本描述的高度对齐。
*   <strong>语音到手势生成：</strong> 在BEAT2数据集上，模型在手部和身体运动生成方面表现出良好的质量和多样性，并能更好地与第一人称语音的节奏对齐。
*   <strong>音乐到舞蹈生成：</strong> 在FineDance数据集上，模型在生成手部动作和身体运动方面略优于现有方法。
*   <strong>消融研究：</strong> 验证了因果注意力、DiT、门控线性机制、RMSNorm和交叉注意力模块对模型性能的积极贡献，尤其是在处理复杂多模态上下文时。</p>
<p>这些结果突显了OmniMotion在统一框架下处理多种模态输入、生成高质量全身人体运动的强大能力，为计算机视觉领域的运动生成任务树立了新的基准。</p>
<p><strong>4. 论文中提及的局限性：</strong>
论文指出，由于数据集的限制，运动生成模型的自然性和泛化能力仍然有限，尤其是在语音和音乐驱动的运动生成方面。这意味着模型可能在面对未见过或更复杂、更细致的语音/音乐输入时，其生成效果仍有提升空间。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
尽管论文未明确列出未来研究方向，但从其局限性可以推断出以下几个方向：
*   <strong>扩展数据集规模和多样性：</strong> 解决当前数据集限制，收集更大规模、更丰富多样（包含更多复杂动作、情感表达和风格）的多模态运动数据集，以进一步提升模型的自然性和泛化能力。
*   <strong>更精细化的多模态对齐：</strong> 探索更先进的机制，以实现语音、音乐与运动之间更精细、更准确的时间和语义对齐，尤其是在处理复杂节奏和情感表达时。
*   <strong>实时生成和交互：</strong> 优化模型效率，使其能够支持实时运动生成，并探索与用户进行交互式运动生成的方法。
*   <strong>个性化和风格化运动生成：</strong> 进一步研究如何根据用户的偏好或特定风格生成个性化和风格化的运动，例如通过学习不同舞者的风格或特定角色的动作特征。
*   <strong>结合物理约束：</strong> 引入物理约束或动力学模型，以生成更真实、更符合物理规律的运动，避免不自然的动作。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>Unlike
previous methods that usually employ discrete masked modeling or autoregressive
modeling, we develop a continuous masked autoregressive motion transformer,
where a causal attention is performed considering the sequential nature within
the human motion.</li>
<li>Within this transformer, we introduce a gated linear
attention and an RMSNorm module, which drive the transformer to pay attention
to the key actions and suppress the instability caused by either the abnormal
movements or the heterogeneous distributions within multi-modalities.</li>
<li>Experimental
results demonstrate that our framework outperforms previous methods across all
modalities, including text-to-motion, speech-to-gesture, and music-to-dance.</li>
<li>The code of our method will be made public.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.14954v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.14954v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.14945v1'></a></p>
<h2 id="3d-scene-prompting-for-scene-consistent-camera-controllable-video-generation"><a href="https://arxiv.org/abs/2510.14945v1">3D Scene Prompting for Scene-Consistent Camera-Controllable Video Generation</a></h2>
<p><strong>Authors:</strong> JoungBin Lee, Jaewoo Jung, Jisang Han, Takuya Narihira, Kazumi Fukuda, Junyoung Seo, Sunghwan Hong, Yuki Mitsufuji, Seungryong Kim</p>
<p><strong>Published:</strong> 2025-10-16</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We present 3DScenePrompt, a framework that generates the next video chunk
from arbitrary-length input while enabling precise camera control and
preserving scene consistency. Unlike methods conditioned on a single image or a
short clip, we employ dual spatio-temporal conditioning that reformulates
context-view referencing across the input video. Our approach conditions on
both temporally adjacent frames for motion continuity and spatially adjacent
content for scene consistency. However, when generating beyond temporal
boundaries, directly using spatially adjacent frames would incorrectly preserve
dynamic elements from the past. We address this by introducing a 3D scene
memory that represents exclusively the static geometry extracted from the
entire input video. To construct this memory, we leverage dynamic SLAM with our
newly introduced dynamic masking strategy that explicitly separates static
scene geometry from moving elements. The static scene representation can then
be projected to any target viewpoint, providing geometrically consistent warped
views that serve as strong 3D spatial prompts while allowing dynamic regions to
evolve naturally from temporal context. This enables our model to maintain
long-range spatial coherence and precise camera control without sacrificing
computational efficiency or motion realism. Extensive experiments demonstrate
that our framework significantly outperforms existing methods in scene
consistency, camera controllability, and generation quality. Project page :
https://cvlab-kaist.github.io/3DScenePrompt/</p>
<p><strong>Analysis:</strong></p>
<p>以下是论文“3D Scene Prompting for Scene-Consistent Camera-Controllable Video Generation”的全面摘要：</p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决场景一致、相机可控的视频生成任务。现有方法在处理任意长度的输入视频时，难以在保持长期空间一致性、精确相机控制和生成质量的同时，生成与输入视频场景一致的后续视频片段。具体来说，当生成超出时间边界的视频时，直接使用过去的空间相邻帧会导致动态元素（如移动物体）被错误地保留，从而破坏场景的自然演变。</p>
<p><strong>2. 关键创新或方法贡献：</strong>
*   <strong>双重时空条件作用（Dual Spatio-Temporal Conditioning）：</strong> 论文提出了一种新颖的双重时空条件作用策略，通过重新构建输入视频中的上下文视图引用，同时考虑时间相邻帧（用于运动连续性）和空间相邻内容（用于场景一致性）。
*   <strong>3D 场景记忆（3D Scene Memory）：</strong> 为了解决动态元素保留问题，论文引入了一个3D场景记忆，它专门表示从整个输入视频中提取的静态几何结构。这确保了空间条件作用只提供持久的静态场景结构，而动态内容可以从时间上下文中自然演变。
*   <strong>动态掩蔽策略（Dynamic Masking Strategy）：</strong> 为了构建3D场景记忆，论文利用动态SLAM，并引入了一种新的动态掩蔽策略，明确地将静态场景几何与移动元素分离。该策略通过像素级运动检测、反向跟踪聚合运动证据以及使用SAM2进行对象级掩蔽，确保只提取静态内容。
*   <strong>几何一致的扭曲视图作为空间提示：</strong> 静态场景表示可以投影到任何目标视点，生成几何一致的扭曲视图，作为强大的3D空间提示。这使得模型能够在不牺牲计算效率或运动真实感的情况下，保持长距离空间连贯性和精确相机控制。
*   <strong>基于预训练视频生成器的架构：</strong> 论文在强大的预训练视频生成器（如CogVideoX-I2V-5B）的基础上进行构建，通过重新设计内容引用方式，保留了其学习到的先验知识和训练效率。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>卓越的场景一致性：</strong> 3DScenePrompt在RealEstate10K和DynPose-100K数据集上，在PSNR、SSIM、LPIPS和MEt3R等所有指标上均显著优于现有方法（如DFoT）。尤其在MEt3R几何一致性误差上，下降了77%，表明其在多视图几何对齐方面的卓越性能。
*   <strong>精确的相机可控性：</strong> 论文方法在mRotErr、mTransErr和mCamMC等相机可控性指标上均优于MotionCtrl、CameraCtrl、FloVD和AC3D等基线方法，证明了其能够精确遵循给定的相机轨迹。
*   <strong>高质量的视频生成：</strong> 在FVD和VBench++（包括主体一致性、背景一致性、美学质量、成像质量、时间闪烁、运动平滑度和动态程度）等视频生成质量指标上，论文方法也取得了最佳性能。
*   <strong>计算效率和运动真实感：</strong> 通过选择性地检索最相关的帧（时空上），该框架实现了对任意长度视频的计算高效处理，同时保持了运动连续性和场景一致性。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   论文中没有明确提及当前方法的具体局限性。然而，通过与现有方法的比较，可以推断出，在处理长序列时，现有方法（如DFoT）由于内存限制，难以保持长期空间连贯性。3DScenePrompt通过其3D场景记忆和动态掩蔽策略解决了这一问题。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   论文明确指出，该方法为长篇视频合成应用开辟了新的可能性，这些应用中保持空间一致性和精确相机控制至关重要。这暗示了未来研究可以进一步探索该框架在电影制作、虚拟现实和合成数据生成等领域的应用。
*   进一步优化动态掩蔽策略，以处理更复杂或更细微的动态场景，例如快速变化的动态或半透明物体。
*   探索将3D场景记忆与更高级的视频扩散模型或生成架构相结合，以进一步提升生成质量和效率。
*   研究如何将用户交互（例如，通过文本提示或草图）更深入地整合到3D场景记忆的构建和利用中，以实现更灵活的创作控制。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present 3DScenePrompt, a framework that generates the next video chunk
from arbitrary-length input while enabling precise camera control and
preserving scene consistency.</li>
<li>Our approach conditions on
both temporally adjacent frames for motion continuity and spatially adjacent
content for scene consistency.</li>
<li>To construct this memory, we leverage dynamic SLAM with our
newly introduced dynamic masking strategy that explicitly separates static
scene geometry from moving elements.</li>
<li>Extensive experiments demonstrate
that our framework significantly outperforms existing methods in scene
consistency, camera controllability, and generation quality.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.14945v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.14945v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.14904v1'></a></p>
<h2 id="maskcaptioner-learning-to-jointly-segment-and-caption-object-trajectories-in-videos"><a href="https://arxiv.org/abs/2510.14904v1">MaskCaptioner : Learning to Jointly Segment and Caption Object Trajectories in Videos</a></h2>
<p><strong>Authors:</strong> Gabriel Fiastre, Antoine Yang, Cordelia Schmid</p>
<p><strong>Published:</strong> 2025-10-16</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Dense Video Object Captioning (DVOC) is the task of jointly detecting,
tracking, and captioning object trajectories in a video, requiring the ability
to understand spatio-temporal details and describe them in natural language.
Due to the complexity of the task and the high cost associated with manual
annotation, previous approaches resort to disjoint training strategies,
potentially leading to suboptimal performance. To circumvent this issue, we
propose to generate captions about spatio-temporally localized entities
leveraging a state-of-the-art VLM. By extending the LVIS and LV-VIS datasets
with our synthetic captions (LVISCap and LV-VISCap), we train MaskCaptioner, an
end-to-end model capable of jointly detecting, segmenting, tracking and
captioning object trajectories. Moreover, with pretraining on LVISCap and
LV-VISCap, MaskCaptioner achieves state-of-the-art DVOC results on three
existing benchmarks, VidSTG, VLN and BenSMOT. The datasets and code are
available at https://www.gabriel.fiastre.fr/maskcaptioner/.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“MaskCaptioner : Learning to Jointly Segment and Caption Object Trajectories in Videos”论文的全面摘要，内容将用中文呈现，并涵盖您要求的所有要点。</p>
<hr />
<h3 id="maskcaptioner">MaskCaptioner: 学习联合分割和描述视频中物体轨迹</h3>
<p><strong>1. 论文主要问题或研究问题</strong></p>
<p>该论文旨在解决<strong>密集视频物体描述 (Dense Video Object Captioning, DVOC)</strong> 任务中的核心挑战。DVOC要求模型能够联合检测、跟踪并用自然语言描述视频中所有物体的轨迹。这项任务的复杂性以及手动标注的高昂成本导致现有方法通常采用分离的训练策略，这可能导致次优性能。因此，论文的核心问题是如何实现端到端的DVOC训练，以克服对密集标注数据稀缺的依赖，并提高整体性能。</p>
<p><strong>2. 关键创新或方法论贡献</strong></p>
<ul>
<li><strong>VLM驱动的合成数据生成：</strong> 论文最主要的创新是提出了一种利用最先进的视觉语言模型（VLM，具体是Gemini 2.0 Flash）来生成物体级合成描述的方法。通过多模态提示策略，VLM能够为视频中空间局部化的物体生成详细的、以物体为中心的描述。</li>
<li><strong>扩展现有数据集：</strong> 论文将LVIS（图像分割）和LV-VIS（视频实例分割）数据集扩展为LVISCap和LV-VISCap，首次为DVOC任务提供了包含（掩码、边界框、类别、描述）标注的统一训练集。这些合成数据极大地弥补了DVOC任务所需密集标注的不足。</li>
<li><strong>端到端MaskCaptioner模型：</strong> 论文提出了MaskCaptioner，这是一个能够联合执行物体检测、分割、跟踪和描述的端到端模型。该架构基于Open-Vocabulary Video Instance Segmentation (OV-VIS) 模型OVFormer，并扩展了描述头，能够从视频片段级别的预测中聚合信息，生成轨迹级别的描述。</li>
<li><strong>统一训练策略：</strong> 通过使用生成的LVISCap和LV-VISCap数据集，MaskCaptioner能够进行端到端训练，避免了以往方法中分离训练策略的次优性。</li>
</ul>
<p><strong>3. 主要结果及其意义</strong></p>
<ul>
<li><strong>最先进的DVOC性能：</strong> MaskCaptioner在三个现有基准测试（VidSTG、VLN和BenSMOT）上取得了最先进的DVOC结果。这证明了其方法在联合检测、跟踪和描述物体轨迹方面的有效性。</li>
<li><strong>合成数据的重要性：</strong> 实验结果表明，LVISCap和LV-VISCap等合成数据集极大地提升了MaskCaptioner的DVOC性能。CapA（描述准确性）与训练描述数量呈对数相关，表明生成更多数据可能带来进一步改进。</li>
<li><strong>分割任务的扩展：</strong> 论文成功地将DVOC任务扩展到分割掩码，而不仅仅是边界框，这提供了更精细的物体定位和描述。</li>
<li><strong>鲁棒性：</strong> MaskCaptioner对视觉骨干网络的选择表现出鲁棒性。</li>
<li><strong>时间聚合的优势：</strong> 引入时间聚合模块显著提高了描述性能，通过整合来自多个视频片段的信息，丰富了对时间上扩展动作的描述。</li>
</ul>
<p><strong>4. 论文中提及的局限性</strong></p>
<ul>
<li><strong>定位和描述的改进空间：</strong> 尽管MaskCaptioner表现出色，但在定位和描述方面仍有改进空间，特别是对于小物体。</li>
<li><strong>描述的通用性：</strong> 自动生成的描述有时可能过于通用，或者在视频中混淆同一类的两个物体。</li>
<li><strong>复杂动作的限制：</strong> 当前基准测试中可观察到的动作复杂性有限，模型在处理更复杂的物体交互和多动作片段时可能面临挑战。</li>
<li><strong>识别错误：</strong> 在模糊上下文、模糊实例或稀有类别的情况下，MaskCaptioner可能无法正确识别所描述的物体，有时会导致错误的命名（例如，将“钳子”错误地识别为“刀”）。</li>
<li><strong>描述不一致：</strong> 在类似情况下，MaskCaptioner生成的描述在指代同一物体时可能不一致。</li>
<li><strong>检测/分割错误：</strong> 在复杂运动、外观变化或遮挡的情况下，MaskCaptioner有时会未能检测、分割或跟踪物体，导致缺少描述（例如，未能检测到苍鹭喙中的鱼）。</li>
</ul>
<p><strong>5. 潜在的未来研究方向</strong></p>
<ul>
<li><strong>更先进的自动描述技术：</strong> 未来的工作可以探索不同的自动描述技术，例如基于Ref-SAV（Yuan et al., 2025）的方法，该方法分多步生成描述，以区分外观和运动描述。</li>
<li><strong>构建更复杂的基准测试：</strong> 针对具有更复杂物体交互和多个动作片段的视频，构建新的基准测试，以推动DVOC任务的发展。</li>
<li><strong>提升小物体定位和描述：</strong> 进一步研究如何提高模型对视频中小物体的定位和描述能力。</li>
<li><strong>解决描述通用性和一致性问题：</strong> 探索方法以生成更具体、更一致的物体描述，避免混淆相似物体。</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To circumvent this issue, we
propose to generate captions about spatio-temporally localized entities
leveraging a state-of-the-art VLM.</li>
<li>Moreover, with pretraining on LVISCap and
LV-VISCap, MaskCaptioner achieves state-of-the-art DVOC results on three
existing benchmarks, VidSTG, VLN and BenSMOT.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.14904v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.14904v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.14876v1'></a></p>
<h2 id="badas-context-aware-collision-prediction-using-real-world-dashcam-data"><a href="https://arxiv.org/abs/2510.14876v1">BADAS: Context Aware Collision Prediction Using Real-World Dashcam Data</a></h2>
<p><strong>Authors:</strong> Roni Goldshmidt, Hamish Scott, Lorenzo Niccolini, Shizhan Zhu, Daniel Moura, Orly Zvitia</p>
<p><strong>Published:</strong> 2025-10-16</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Existing collision prediction methods often fail to distinguish between
ego-vehicle threats and random accidents not involving the ego vehicle, leading
to excessive false alerts in real-world deployment. We present BADAS, a family
of collision prediction models trained on Nexar's real-world dashcam collision
dataset -- the first benchmark designed explicitly for ego-centric evaluation.
We re-annotate major benchmarks to identify ego involvement, add consensus
alert-time labels, and synthesize negatives where needed, enabling fair AP/AUC
and temporal evaluation. BADAS uses a V-JEPA2 backbone trained end-to-end and
comes in two variants: BADAS-Open (trained on our 1.5k public videos) and
BADAS1.0 (trained on 40k proprietary videos). Across DAD, DADA-2000, DoTA, and
Nexar, BADAS achieves state-of-the-art AP/AUC and outperforms a
forward-collision ADAS baseline while producing more realistic time-to-accident
estimates. We release our BADAS-Open model weights and code, along with
re-annotations of all evaluation datasets to promote ego-centric collision
prediction research.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Roni Goldshmidt等人撰写的论文“BADAS: Context Aware Collision Prediction Using Real-World Dashcam Data”的全面摘要。</p>
<hr />
<h3 id="badas-context-aware-collision-prediction-using-real-world-dashcam-data_1">论文摘要：BADAS: Context Aware Collision Prediction Using Real-World Dashcam Data</h3>
<p><strong>1. 主要问题或研究问题：</strong>
现有的碰撞预测方法在实际部署中存在严重缺陷，即无法有效区分“自我车辆”（ego-vehicle）面临的威胁与不涉及自我车辆的随机事故。这导致了过多的虚假警报，降低了系统的实用性和用户接受度。论文旨在解决这一核心问题，即如何开发一种能够准确预测自我车辆相关碰撞，同时显著减少虚假警报的碰撞预测系统。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>以自我车辆为中心的碰撞预测范式：</strong> 论文重新定义了碰撞预测任务，使其专注于自我车辆的安全性。通过对DAD、DoTA和DADA-2000等主要基准数据集进行重新标注，明确区分了自我车辆是否卷入事故，发现现有数据集中40-92%的事故与自我车辆无关。这些重新标注的数据集（包括共识警报时间标签和合成的负样本）已公开，以促进以自我车辆为中心的碰撞预测研究。
*   <strong>V-JEPA2骨干网络的应用：</strong> BADAS模型采用V-JEPA2（一种先进的视频基础模型）作为骨干网络，并进行端到端训练。V-JEPA2在理解时间动态和视觉模式方面表现出色，特别适合预测性任务。
*   <strong>基于真实世界行车记录仪数据的训练：</strong> BADAS模型在Nexar的真实世界行车记录仪碰撞数据集上进行训练，该数据集是第一个专门为自我车辆评估设计的基准。该数据集包含自我车辆卷入的碰撞和险情事件，以及通过紧急操作成功避免危险情况的近碰撞事件，提供了丰富的训练信号。
*   <strong>标准化时间评估：</strong> 论文通过10位标注员的共识，建立了警报时间的连贯定义，并为所有测试集提供了精确的时间标注，解决了现有数据集中定义不一致和主观性问题。
*   <strong>数据增强和采样策略：</strong> 采用数据增强和正样本2倍过采样策略，显著提高了模型的性能和对真实世界变化的鲁棒性。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>最先进的性能：</strong> BADAS模型在DAD、DADA-2000、DoTA和Nexar等所有主要基准测试中均取得了最先进的AP/AUC性能。BADAS1.0（基于40k专有视频训练）在DAD数据集上达到了0.94 AP，远超基线方法的0.06 AP。在Nexar数据集上，BADAS-Open（基于1.5k公开视频训练）的AP达到0.86，显著优于学术基线（0.48-0.53）和商用FCW系统（0.58）。
*   <strong>更真实的事故发生时间估计：</strong> BADAS模型生成的平均事故发生时间（mTTA）估计值更符合人类预测能力（3-5秒），而基线方法则报告了不切实际的9-10秒预测。
*   <strong>鲁棒性和泛化能力：</strong> BADAS模型在不同数据集上保持了一致的性能，表明其具有强大的泛化能力。
*   <strong>数据规模效应：</strong> 随着训练数据量的增加（从1.5k到40k视频），Nexar验证AP呈对数级改进，证实了利用大规模真实世界数据能够持续提升性能。
*   <strong>定性分析：</strong> BADAS-Open的预测分数在碰撞临近时急剧上升，且预测稳定、自信，而基线方法则表现出不规律的模式和过早或不一致的警报。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>长尾性能下降：</strong> BADAS模型在少数类别（如行人、骑自行车者、摩托车和动物）上的召回率显著下降，表明模型在处理罕见、视觉多样且低频事件类型时存在泛化挑战。这是由于训练数据集中缺乏这些长尾事件所致。
*   <strong>数据量限制：</strong> 尽管BADAS1.0使用了40k专有视频，但论文指出数据规模的潜力尚未完全饱和，暗示仍有提升空间。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>扩展数据集：</strong> 进一步扩大数据集，以增强模型的泛化能力，尤其是在长尾类别方面。
*   <strong>改进平均警报时间（mTTA）：</strong> 优化模型以减少虚假警报，并提供更准确的警报时间。
*   <strong>解决长尾类别问题：</strong> 开发专门的策略来更好地评估和预测多样且罕见的驾驶场景中的长尾类别。
*   <strong>多级分类：</strong> 将碰撞预测模型从二元分类（碰撞/非碰撞）扩展到三级分类（正常、警告、警报），以实现更精细的风险评估和自适应决策。
*   <strong>整合到ADAS和自动驾驶系统：</strong> 进一步研究如何将BADAS的可靠、上下文感知碰撞预测能力整合到更广泛的ADAS和全自动驾驶系统中，以提高安全性。</p>
<hr />
<p>总而言之，BADAS论文通过引入以自我车辆为中心的范式、利用V-JEPA2骨干网络和Nexar真实世界行车记录仪数据，显著提升了碰撞预测的性能和实用性。其关键贡献在于重新标注了现有数据集、标准化了评估协议，并展示了在减少虚假警报和提供更准确警报时间方面的卓越能力。尽管在长尾事件处理上仍有挑战，但BADAS为未来更安全、更智能的驾驶辅助系统和自动驾驶技术奠定了坚实基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present BADAS, a family
of collision prediction models trained on Nexar's real-world dashcam collision
dataset -- the first benchmark designed explicitly for ego-centric evaluation.</li>
<li>Across DAD, DADA-2000, DoTA, and
Nexar, BADAS achieves state-of-the-art AP/AUC and outperforms a
forward-collision ADAS baseline while producing more realistic time-to-accident
estimates.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.14876v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.14876v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.14874v1'></a></p>
<h2 id="touch-text-guided-controllable-generation-of-free-form-hand-object-interactions"><a href="https://arxiv.org/abs/2510.14874v1">TOUCH: Text-guided Controllable Generation of Free-Form Hand-Object Interactions</a></h2>
<p><strong>Authors:</strong> Guangyi Han, Wei Zhai, Yuhang Yang, Yang Cao, Zheng-Jun Zha</p>
<p><strong>Published:</strong> 2025-10-16</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Hand-object interaction (HOI) is fundamental for humans to express intent.
Existing HOI generation research is predominantly confined to fixed grasping
patterns, where control is tied to physical priors such as force closure or
generic intent instructions, even when expressed through elaborate language.
Such an overly general conditioning imposes a strong inductive bias for stable
grasps, thus failing to capture the diversity of daily HOI. To address these
limitations, we introduce Free-Form HOI Generation, which aims to generate
controllable, diverse, and physically plausible HOI conditioned on fine-grained
intent, extending HOI from grasping to free-form interactions, like pushing,
poking, and rotating. To support this task, we construct WildO2, an in-the-wild
diverse 3D HOI dataset, which includes diverse HOI derived from internet
videos. Specifically, it contains 4.4k unique interactions across 92 intents
and 610 object categories, each with detailed semantic annotations. Building on
this dataset, we propose TOUCH, a three-stage framework centered on a
multi-level diffusion model that facilitates fine-grained semantic control to
generate versatile hand poses beyond grasping priors. This process leverages
explicit contact modeling for conditioning and is subsequently refined with
contact consistency and physical constraints to ensure realism. Comprehensive
experiments demonstrate our method's ability to generate controllable, diverse,
and physically plausible hand interactions representative of daily activities.
The project page is <script type="math/tex">\href{https://guangyid.github.io/hoi123touch}{here}</script>.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Guangyi Han等人撰写的论文“TOUCH: Text-guided Controllable Generation of Free-Form Hand-Object Interactions”的全面摘要。</p>
<hr />
<h3 id="touch">TOUCH: 文本引导的可控自由形式手物交互生成</h3>
<p><strong>1. 主要问题或研究问题：</strong>
现有的手物交互（HOI）生成研究主要局限于固定的抓取模式，其控制通常依赖于物理先验（如力闭合）或通用意图指令，即使通过复杂的语言表达，也未能捕捉到日常HOI的多样性。这种过度通用的条件设定导致模型偏向于生成稳定的抓取，从而限制了交互的多样性。该论文旨在解决这一限制，引入“自由形式HOI生成”任务，目标是生成可控、多样且物理上合理的手物交互，这些交互由细粒度意图条件化，并将HOI从抓取扩展到推、戳、旋转等自由形式交互。</p>
<p><strong>2. 关键创新或方法论贡献：</strong></p>
<ul>
<li><strong>自由形式HOI生成任务的引入：</strong> 论文首次提出了“自由形式HOI生成”任务，旨在打破以抓取为中心的限制，生成更广泛、更具语义表达力的日常交互，包括各种非抓取操作。</li>
<li><strong>WildO2数据集的构建：</strong> 为支持新任务，论文构建了一个名为WildO2的野外多样化3D HOI数据集。该数据集包含从互联网视频中提取的4.4k独特交互，涵盖92种意图和610个对象类别，并附有详细的语义标注。数据集的构建通过一个半自动化流程实现，该流程利用O2HOI（Object-only to Hand-Object Interaction）帧配对策略，解决了野外视频中手部遮挡导致的对象重建难题。</li>
<li><strong>TOUCH三阶段框架：</strong> 论文提出了一个名为TOUCH的三阶段框架，用于可控的自由形式HOI生成：<ul>
<li><strong>接触图预测（Contact Map Prediction）：</strong> 利用两个独立的CVAE模型，根据文本和对象几何信息，预测手和对象表面的二值接触图，为交互位置和姿态提供强烈的空间先验。</li>
<li><strong>多级条件扩散模型（Multi-Level Conditioned Diffusion）：</strong> 采用基于Transformer的去噪扩散概率模型（DDPM），通过注意力机制融合语义和几何信息。在早期扩散阶段，粗粒度意图和全局对象几何信息引导整体姿态；在后期阶段，细粒度文本和局部接触特征进一步细化细节动作，实现细粒度语义控制。</li>
<li><strong>物理约束细化（Physical Constraints Refinement）：</strong> 引入自监督循环一致性损失和物理约束，对生成的姿态进行优化，以确保交互的真实性和物理可行性，解决全局姿态漂移问题。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
全面的实验结果表明，TOUCH方法在生成可控、多样且物理上合理的手物交互方面表现出色，这些交互能代表日常活动。与现有基线方法（如ContactGen和Text2HOI）相比，TOUCH在接触准确性（P-IoU, P-F1）、物理合理性（MPVPE, PD, PV）、多样性（熵, 聚类大小）和语义一致性（P-FID, VLM辅助评估, 感知分数）等所有评估指标上均优于基线。消融研究进一步验证了接触引导和粗到细文本控制设计的有效性。WildO2数据集的构建为该领域未来的研究提供了关键资源。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>静态HOI快照：</strong> 当前框架主要关注静态HOI快照，这限制了其捕捉交互过程时间动态的能力。
*   <strong>数据集规模：</strong> 尽管WildO2数据集提供了快速扩展的潜力，但当前数据集的规模仍有增长空间。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>动态序列扩展：</strong> 未来工作计划将研究扩展到动态序列，利用大规模视频数据集和6-DoF对象姿态估计，从而建模完整的人-环境交互过程。
*   <strong>更精细的物理模拟：</strong> 进一步探索更精细的物理模拟，以提高交互的真实性和稳定性。
*   <strong>更广泛的交互类型：</strong> 持续扩展自由形式HOI的范围，涵盖更多复杂和多样的日常交互。</p>
<hr />
<p>这篇论文通过引入自由形式HOI生成任务和构建WildO2数据集，显著推动了手物交互生成领域的发展。其提出的TOUCH框架，通过多级扩散模型和物理约束细化，实现了对细粒度意图的可控生成，为AR/VR、机器人和具身AI等应用提供了新的可能性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these
limitations, we introduce Free-Form HOI Generation, which aims to generate
controllable, diverse, and physically plausible HOI conditioned on fine-grained
intent, extending HOI from grasping to free-form interactions, like pushing,
poking, and rotating.</li>
<li>Building on
this dataset, we propose TOUCH, a three-stage framework centered on a
multi-level diffusion model that facilitates fine-grained semantic control to
generate versatile hand poses beyond grasping priors.</li>
<li>Comprehensive
experiments demonstrate our method's ability to generate controllable, diverse,
and physically plausible hand interactions representative of daily activities.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.14874v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.14874v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-10-17 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
