<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2026-01-28 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-27
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2026-01-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-13
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2026-01-27/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2026-01-29/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2026-01-28">Arxiv Computer Vision Papers - 2026-01-28</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#duwatbench-bridging-language-and-visual-heritage-through-an-arabic-calligraphy-benchmark-for-multimodal-understanding" class="nav-link">DuwatBench: Bridging Language and Visual Heritage through an Arabic Calligraphy Benchmark for Multimodal Understanding</a>
                </li>
                <li class="nav-item">
                    <a href="#vggt-slam-20-real-time-dense-feed-forward-scene-reconstruction" class="nav-link">VGGT-SLAM 2.0: Real time Dense Feed-forward Scene Reconstruction</a>
                </li>
                <li class="nav-item">
                    <a href="#sonic-spectral-oriented-neural-invariant-convolutions" class="nav-link">SONIC: Spectral Oriented Neural Invariant Convolutions</a>
                </li>
                <li class="nav-item">
                    <a href="#sonic-spectral-oriented-neural-invariant-convolutions_1" class="nav-link">论文方法分析：SONIC (Spectral Oriented Neural Invariant Convolutions)</a>
                </li>
                <li class="nav-item">
                    <a href="#egohandicl-egocentric-3d-hand-reconstruction-with-in-context-learning" class="nav-link">EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning</a>
                </li>
                <li class="nav-item">
                    <a href="#egohandicl" class="nav-link">论文方法分析与总结：EgoHandICL</a>
                </li>
                <li class="nav-item">
                    <a href="#hexformer-hyperbolic-vision-transformer-with-exponential-map-aggregation" class="nav-link">HexFormer: Hyperbolic Vision Transformer with Exponential Map Aggregation</a>
                </li>
                <li class="nav-item">
                    <a href="#hexformer" class="nav-link">论文方法分析与总结：HexFormer</a>
                </li>
                <li class="nav-item">
                    <a href="#query-guided-spatial-temporal-frequency-interaction-for-music-audio-visual-question-answering" class="nav-link">Query-Guided Spatial-Temporal-Frequency Interaction for Music Audio-Visual Question Answering</a>
                </li>
                <li class="nav-item">
                    <a href="#_1" class="nav-link">论文方法分析与总结</a>
                </li>
                <li class="nav-item">
                    <a href="#youtu-vl-unleashing-visual-potential-via-unified-vision-language-supervision" class="nav-link">Youtu-VL: Unleashing Visual Potential via Unified Vision-Language Supervision</a>
                </li>
                <li class="nav-item">
                    <a href="#geodiff3d-self-supervised-3d-scene-generation-with-geometry-constrained-2d-diffusion-guidance" class="nav-link">GeoDiff3D: Self-Supervised 3D Scene Generation with Geometry-Constrained 2D Diffusion Guidance</a>
                </li>
                <li class="nav-item">
                    <a href="#waterclear-gs-optical-aware-gaussian-splatting-for-underwater-reconstruction-and-restoration" class="nav-link">WaterClear-GS: Optical-Aware Gaussian Splatting for Underwater Reconstruction and Restoration</a>
                </li>
                <li class="nav-item">
                    <a href="#benchmarking-multimodal-large-language-models-for-missing-modality-completion-in-product-catalogues" class="nav-link">Benchmarking Multimodal Large Language Models for Missing Modality Completion in Product Catalogues</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2026-01-28">Arxiv Computer Vision Papers - 2026-01-28</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，作为一名专注于计算机视觉和机器学习的研究助理，我将为您提供一份简明的 Arxiv 计算机视觉领域近期论文的执行摘要。</p>
<p><strong>执行摘要：2026年1月27日 Arxiv 计算机视觉论文精选</strong></p>
<p>本次报告涵盖了10篇于2026年1月27日发表在 Arxiv 上的计算机视觉领域论文，展现了该领域在多模态理解、三维场景重建、新型网络架构以及特定应用场景（如水下重建、产品目录）等方面的最新进展。</p>
<p><strong>1. 主要主题与趋势：</strong></p>
<ul>
<li><strong>多模态融合与理解的深化：</strong> 多个研究聚焦于整合视觉信息与语言、音频等其他模态，以实现更全面的理解和交互。特别是在特定领域（如阿拉伯书法、音乐问答、产品目录）的应用尤为突出。</li>
<li><strong>三维视觉的进步：</strong> 实时三维场景重建、 egocentric 手部三维重建以及基于扩散模型的通用三维场景生成是重要的研究方向。</li>
<li><strong>新型网络架构与表示：</strong> 探索了超双曲空间（Hyperbolic space）在视觉 Transformer 中的应用，以及用于不变性卷积的新型方法。</li>
<li><strong>特定场景的优化与应用：</strong> 针对水下环境的重建与修复，以及产品目录的缺失模态补全等实际应用场景提出了创新解决方案。</li>
</ul>
<p><strong>2. 亮点与创新：</strong></p>
<ul>
<li><strong>DuwatBench</strong> 和 <strong>Youtu-VL</strong> 均在多模态理解方面取得了显著进展，前者专注于阿拉伯书法这一特定文化遗产，后者则通过统一的视觉-语言监督来释放视觉潜力，显示了多模态研究的广度和深度。</li>
<li><strong>VGGT-SLAM 2.0</strong> 在实时稠密前馈场景重建方面展现了强大的性能，为机器人导航和增强现实等应用提供了关键技术支持。</li>
<li><strong>GeoDiff3D</strong> 提出的几何约束 2D 扩散引导，为自监督三维场景生成开辟了新的思路，尤其是在保证几何一致性方面具有重要意义。</li>
<li><strong>WaterClear-GS</strong> 针对水下环境的特殊性，提出了光学感知的 Gaussian Splatting 方法，解决了水下重建和修复的难题。</li>
</ul>
<p><strong>3. 新兴研究方向与技术：</strong></p>
<ul>
<li><strong>特定领域的多模态基准构建：</strong> 如 DuwatBench 所示，针对特定文化或语言的视觉-语言基准的构建将是未来多模态研究的重要方向。</li>
<li><strong>超双曲几何在视觉模型中的应用：</strong> HexFormer 的研究表明，超双曲空间可能为处理视觉数据的层级结构提供更有效的表示。</li>
<li><strong>扩散模型在三维生成中的几何约束：</strong> GeoDiff3D 展示了将几何先验知识融入扩散模型以提升三维生成质量和一致性的潜力。</li>
<li><strong>Egocentric 视角下的精细三维重建：</strong> EgoHandICL 的工作突出了在 egocentric 视角下进行高精度三维手部重建的重要性。</li>
<li><strong>缺失模态补全的通用化能力：</strong> Benchmarking Multimodal Large Language Models for Missing Modality Completion in Product Catalogues 指出了 LLM 在处理产品目录这类结构化数据时，在缺失模态补全方面的潜力。</li>
</ul>
<p><strong>4. 建议阅读全文的论文：</strong></p>
<p>基于其创新性、潜在影响力和对新兴方向的探索，以下论文值得深入阅读：</p>
<ul>
<li><strong>DuwatBench: Bridging Language and Visual Heritage through an Arabic Calligraphy Benchmark for Multimodal Understanding</strong> (对于多模态研究者，特别是对特定文化遗产感兴趣的团队)</li>
<li><strong>VGGT-SLAM 2.0: Real time Dense Feed-forward Scene Reconstruction</strong> (对于三维视觉、机器人和AR/VR领域的团队)</li>
<li><strong>GeoDiff3D: Self-Supervised 3D Scene Generation with Geometry-Constrained 2D Diffusion Guidance</strong> (对于三维生成、扩散模型和自监督学习的研究者)</li>
<li><strong>WaterClear-GS: Optical-Aware Gaussian Splatting for Underwater Reconstruction and Restoration</strong> (对于计算机视觉在特定恶劣环境下的应用研究者)</li>
<li><strong>HexFormer: Hyperbolic Vision Transformer with Exponential Map Aggregation</strong> (对于探索新型网络架构和表示学习的研究者)</li>
</ul>
<p>这份摘要旨在帮助您快速了解近期 Arxiv 计算机视觉领域的关键进展，并为进一步深入研究提供指引。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2601.19898v1">DuwatBench: Bridging Language and Visual Heritage through an Arabic Calligraphy Benchmark for Multimodal Understanding</a></li>
<li><a href="#2601.19887v1">VGGT-SLAM 2.0: Real time Dense Feed-forward Scene Reconstruction</a></li>
<li><a href="#2601.19884v1">SONIC: Spectral Oriented Neural Invariant Convolutions</a></li>
<li><a href="#2601.19850v1">EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning</a></li>
<li><a href="#2601.19849v1">HexFormer: Hyperbolic Vision Transformer with Exponential Map Aggregation</a></li>
<li><a href="#2601.19821v1">Query-Guided Spatial-Temporal-Frequency Interaction for Music Audio-Visual Question Answering</a></li>
<li><a href="#2601.19798v1">Youtu-VL: Unleashing Visual Potential via Unified Vision-Language Supervision</a></li>
<li><a href="#2601.19785v1">GeoDiff3D: Self-Supervised 3D Scene Generation with Geometry-Constrained 2D Diffusion Guidance</a></li>
<li><a href="#2601.19753v1">WaterClear-GS: Optical-Aware Gaussian Splatting for Underwater Reconstruction and Restoration</a></li>
<li><a href="#2601.19750v1">Benchmarking Multimodal Large Language Models for Missing Modality Completion in Product Catalogues</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2601.19898v1'></a></p>
<h2 id="duwatbench-bridging-language-and-visual-heritage-through-an-arabic-calligraphy-benchmark-for-multimodal-understanding"><a href="https://arxiv.org/abs/2601.19898v1">DuwatBench: Bridging Language and Visual Heritage through an Arabic Calligraphy Benchmark for Multimodal Understanding</a></h2>
<p><strong>Authors:</strong> Shubham Patle, Sara Ghaboura, Hania Tariq, Mohammad Usman Khan, Omkar Thawakar, Rao Muhammad Anwer, Salman Khan</p>
<p><strong>Published:</strong> 2026-01-27</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Arabic calligraphy represents one of the richest visual traditions of the Arabic language, blending linguistic meaning with artistic form. Although multimodal models have advanced across languages, their ability to process Arabic script, especially in artistic and stylized calligraphic forms, remains largely unexplored. To address this gap, we present DuwatBench, a benchmark of 1,272 curated samples containing about 1,475 unique words across six classical and modern calligraphic styles, each paired with sentence-level detection annotations. The dataset reflects real-world challenges in Arabic writing, such as complex stroke patterns, dense ligatures, and stylistic variations that often challenge standard text recognition systems. Using DuwatBench, we evaluated 13 leading Arabic and multilingual multimodal models and showed that while they perform well on clean text, they struggle with calligraphic variation, artistic distortions, and precise visual-text alignment. By publicly releasing DuwatBench and its annotations, we aim to advance culturally grounded multimodal research, foster fair inclusion of the Arabic language and visual heritage in AI systems, and support continued progress in this area. Our dataset (https://huggingface.co/datasets/MBZUAI/DuwatBench) and evaluation suit (https://github.com/mbzuai-oryx/DuwatBench) are publicly available.</p>
<p><strong>Analysis:</strong></p>
<p>好的，我将以一名AI领域高水平研究生的视角，深入分析您提供的论文方法部分，并遵循您提出的分析框架。请提供论文的具体内容，我将为您进行详细解读。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this gap, we present DuwatBench, a benchmark of 1,272 curated samples containing about 1,475 unique words across six classical and modern calligraphic styles, each paired with sentence-level detection annotations.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.19898v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.19898v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.19887v1'></a></p>
<h2 id="vggt-slam-20-real-time-dense-feed-forward-scene-reconstruction"><a href="https://arxiv.org/abs/2601.19887v1">VGGT-SLAM 2.0: Real time Dense Feed-forward Scene Reconstruction</a></h2>
<p><strong>Authors:</strong> Dominic Maggio, Luca Carlone</p>
<p><strong>Published:</strong> 2026-01-27</p>
<p><strong>Categories:</strong> cs.CV, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>We present VGGT-SLAM 2.0, a real time RGB feed-forward SLAM system which substantially improves upon VGGT-SLAM for incrementally aligning submaps created from VGGT. Firstly, we remove high-dimensional 15-degree-of-freedom drift and planar degeneracy from VGGT-SLAM by creating a new factor graph design while still addressing the reconstruction ambiguity of VGGT given unknown camera intrinsics. Secondly, by studying the attention layers of VGGT, we show that one of the layers is well suited to assist in image retrieval verification for free without additional training, which enables both rejecting false positive matches and allows for completing more loop closures. Finally, we conduct a suite of experiments which includes showing VGGT-SLAM 2.0 can easily be adapted for open-set object detection and demonstrating real time performance while running online onboard a ground robot using a Jetson Thor. We also test in environments ranging from cluttered indoor apartments and office scenes to a 4,200 square foot barn, and we also demonstrate VGGT-SLAM 2.0 achieves the highest accuracy on the TUM dataset with about 23 percent less pose error than VGGT-SLAM. Code will be released upon publication.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文标题：</strong> VGGT-SLAM 2.0: Real time Dense Feed-forward Scene Reconstruction
<strong>作者：</strong> Dominic Maggio, Luca Carlone
<strong>发表日期：</strong> 2026-01-27</p>
<hr />
<p><strong>1. 论文的主要贡献（2-3句话的简洁总结）</strong></p>
<p>VGGT-SLAM 2.0 提出了一种实时、密集、前馈的 RGB SLAM 系统，通过改进的因子图设计解决了 VGGT-SLAM 中的高维漂移和平面退化问题，同时处理了 VGGT 在未知相机内参下的重建歧义。该系统还利用 VGGT 的注意力层进行高效的图像检索验证，从而提升了回环检测能力，并在各种复杂环境中实现了实时性能和更高的重建精度。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<ul>
<li><strong>改进的因子图设计：</strong> 这是解决 VGGT-SLAM 中“高维 15 度自由度漂移”和“平面退化”问题的核心。通过重新设计因子图，作者能够更有效地约束和优化位姿估计，从而减少累积误差。</li>
<li><strong>利用 VGGT 注意力层进行图像检索验证：</strong> 这是该论文一个非常巧妙且具有成本效益的创新。作者发现 VGGT 的某个注意力层无需额外训练即可用于图像检索验证。这使得系统能够更可靠地识别和剔除错误的匹配点，并显著增强了回环检测（loop closure）的能力。这解决了 SLAM 系统中一个长期存在的挑战：如何在不显著增加计算负担的情况下提高匹配的鲁棒性。</li>
<li><strong>处理 VGGT 的重建歧义（未知相机内参）：</strong> VGGT 本身在处理未知相机内参时存在重建歧义。VGGT-SLAM 2.0 能够在这种情况下进行有效的重建，表明其在位姿估计和场景重建的联合优化方面取得了进展。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<ul>
<li><strong>提升实时稠密 SLAM 的性能和鲁棒性：</strong> VGGT-SLAM 2.0 在精度和实时性上都取得了显著提升，尤其是在复杂和开放集环境中。这为需要高精度、实时三维重建的应用提供了更强大的解决方案。</li>
<li><strong>降低回环检测的计算成本和提高准确性：</strong> 利用预训练模型的注意力层进行验证，是一种非常高效的策略。这种方法可以被推广到其他需要图像检索和验证的 SLAM 或视觉定位任务中，降低对额外训练数据的需求，并提高整体性能。</li>
<li><strong>推动前馈式（Feed-forward）SLAM 的发展：</strong> 该系统是“前馈式”的，意味着它能够以一种更直接、更少迭代的方式进行场景重建。这对于需要低延迟和高吞吐量的应用至关重要。</li>
<li><strong>为基于深度学习的 SLAM 提供新的思路：</strong> 该研究展示了如何有效地利用大型预训练模型（如 VGGT）的内部表示（注意力层）来解决 SLAM 中的特定问题，而无需对整个模型进行微调。这为未来结合大型预训练模型与传统 SLAM 框架提供了新的范式。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>机器人导航和自主驾驶：</strong> 实时、高精度的三维重建是机器人感知和导航的基础，尤其是在未知或动态环境中。</li>
<li><strong>增强现实（AR）和虚拟现实（VR）：</strong> 需要精确的场景理解和跟踪来提供沉浸式体验。</li>
<li><strong>三维重建和建模：</strong> 用于生成高保真度的环境模型，例如用于城市建模、建筑信息模型（BIM）等。</li>
<li><strong>物体识别和场景理解：</strong> 论文提到可以轻松适应开放集物体检测，表明其场景表示能力强大，可以作为更高级视觉任务的基础。</li>
<li><strong>无人机（UAV）测绘和检查：</strong> 在大范围或复杂地形中进行实时三维地图构建。</li>
<li><strong>工业自动化和检查：</strong> 在生产线或工厂环境中进行实时监控和质量控制。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<ul>
<li><strong>对 VGGT 的依赖性：</strong> 该系统在很大程度上依赖于 VGGT 的预训练模型。虽然利用其注意力层是创新的，但这意味着系统的性能和适用性可能受到 VGGT 模型本身特性（如训练数据、模型架构）的限制。如果 VGGT 在特定类型的场景或物体上表现不佳，VGGT-SLAM 2.0 也可能受到影响。</li>
<li><strong>“未知相机内参”的处理：</strong> 虽然论文声称解决了这个问题，但“重建歧义”的完全消除程度以及在极端情况下（例如，非常小的基线或高度退化的场景）的性能仍需进一步验证。</li>
<li><strong>“实时”的定义和硬件要求：</strong> 论文提到在 Jetson Thor 上实现了实时性能。然而，“实时”的定义（例如，帧率）以及具体的硬件配置和计算资源需求，在摘要中并未完全明确。在更低端或资源受限的硬件上，其实时性可能受到挑战。</li>
<li><strong>对“密集”重建的定义：</strong> 摘要中提到“密集”重建，但其密度（例如，点云密度、表面覆盖率）和质量在摘要中没有具体量化。</li>
<li><strong>代码发布时间：</strong> 代码将在发布后释放，这意味着在正式发布前，其可复现性和详细实现细节是不可见的。</li>
</ul>
<p>总而言之，VGGT-SLAM 2.0 是一项令人兴奋的研究，它通过巧妙地结合深度学习模型的内部表示与传统的 SLAM 框架，在实时稠密 SLAM 领域取得了显著的进步。其利用预训练模型注意力层的策略尤其具有启发性，为未来 SLAM 研究开辟了新的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present VGGT-SLAM 2.0, a real time RGB feed-forward SLAM system which substantially improves upon VGGT-SLAM for incrementally aligning submaps created from VGGT.</li>
<li>Firstly, we remove high-dimensional 15-degree-of-freedom drift and planar degeneracy from VGGT-SLAM by creating a new factor graph design while still addressing the reconstruction ambiguity of VGGT given unknown camera intrinsics.</li>
<li>Secondly, by studying the attention layers of VGGT, we show that one of the layers is well suited to assist in image retrieval verification for free without additional training, which enables both rejecting false positive matches and allows for completing more loop closures.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.19887v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.19887v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.19884v1'></a></p>
<h2 id="sonic-spectral-oriented-neural-invariant-convolutions"><a href="https://arxiv.org/abs/2601.19884v1">SONIC: Spectral Oriented Neural Invariant Convolutions</a></h2>
<p><strong>Authors:</strong> Gijs Joppe Moens, Regina Beets-Tan, Eduardo H. P. Pooch</p>
<p><strong>Published:</strong> 2026-01-27</p>
<p><strong>Categories:</strong> cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Convolutional Neural Networks (CNNs) rely on fixed-size kernels scanning local patches, which limits their ability to capture global context or long-range dependencies without very deep architectures. Vision Transformers (ViTs), in turn, provide global connectivity but lack spatial inductive bias, depend on explicit positional encodings, and remain tied to the initial patch size. Bridging these limitations requires a representation that is both structured and global. We introduce SONIC (Spectral Oriented Neural Invariant Convolutions), a continuous spectral parameterisation that models convolutional operators using a small set of shared, orientation-selective components. These components define smooth responses across the full frequency domain, yielding global receptive fields and filters that adapt naturally across resolutions. Across synthetic benchmarks, large-scale image classification, and 3D medical datasets, SONIC shows improved robustness to geometric transformations, noise, and resolution shifts, and matches or exceeds convolutional, attention-based, and prior spectral architectures with an order of magnitude fewer parameters. These results demonstrate that continuous, orientation-aware spectral parameterisations provide a principled and scalable alternative to conventional spatial and spectral operators.</p>
<p><strong>Analysis:</strong></p>
<p>好的，我将以AI领域高水平研究生的视角，深入分析这篇关于SONIC（Spectral Oriented Neural Invariant Convolutions）的论文，重点关注其方法创新、设计逻辑、优势与不足，并提供实用的指导。</p>
<hr />
<h2 id="sonic-spectral-oriented-neural-invariant-convolutions_1">论文方法分析：SONIC (Spectral Oriented Neural Invariant Convolutions)</h2>
<h3 id="1">1. 摘要翻译</h3>
<p><strong>SONIC：谱方向神经网络不变卷积</strong></p>
<p>卷积神经网络（CNNs）依赖于扫描局部块的固定大小卷积核，这限制了它们在没有非常深层架构的情况下捕捉全局上下文或长距离依赖关系的能力。而Vision Transformers（ViTs）则提供了全局连接性，但缺乏空间归纳偏置，依赖于显式的 positional encodings，并且仍然受限于初始的 patch 大小。弥合这些限制需要一种既结构化又全局的表示。我们引入了SONIC（Spectral Oriented Neural Invariant Convolutions），一种连续谱参数化方法，它使用一小组共享的、方向选择性组件来建模卷积算子。这些组件在整个频域中定义了平滑的响应，产生了全局感受野和自然适应各种分辨率的滤波器。在合成基准测试、大规模图像分类和3D医学数据集上，SONIC在几何变换、噪声和分辨率变化方面表现出更强的鲁棒性，并且参数量比卷积、基于注意力的方法和先前的谱架构少一个数量级，性能与之相当或更优。这些结果表明，连续的、方向感知的谱参数化为传统的空间和谱算子提供了一种原则性且可扩展的替代方案。</p>
<h3 id="2">2. 方法动机分析</h3>
<ul>
<li>
<p><strong>驱动力</strong>：</p>
<ul>
<li><strong>捕捉全局上下文和长距离依赖</strong>：传统CNNs的局部感受野限制了其理解图像整体的能力，需要非常深的层数才能达到全局感受野，这带来了计算和内存的负担。</li>
<li><strong>克服Vision Transformers的局限性</strong>：ViTs虽然提供了全局连接，但计算成本高昂（与图像大小的平方成正比），缺乏CNNs的空间归纳偏置，并且对输入patch大小敏感。</li>
<li><strong>实现分辨率不变性</strong>：现实世界中的视觉任务（如医学影像）经常面临分辨率变化，而现有方法在处理这些变化时鲁棒性不足。</li>
<li><strong>提高参数效率</strong>：在保持甚至超越现有方法性能的同时，显著减少模型参数量。</li>
</ul>
</li>
<li>
<p><strong>现有方法痛点</strong>：</p>
<ul>
<li><strong>CNNs</strong>：局部感受野，难以捕捉长距离依赖；对几何变换敏感。</li>
<li><strong>ViTs</strong>：计算成本高（二次方复杂度），缺乏空间归纳偏置，对patch大小敏感。</li>
<li><strong>现有谱方法（如GFNet, FNO）</strong>：虽然可以实现全局感受野，但通常是各向同性的或仅限于坐标轴方向，难以捕捉自然图像中的定向结构；一些方法（如GFNet, FNO）参数化的是离散的FFT网格，导致分辨率不具有真正的“不变性”。</li>
</ul>
</li>
<li>
<p><strong>研究假设</strong>：</p>
<ul>
<li>通过在<strong>频域</strong>中设计<strong>结构化</strong>、<strong>方向感知</strong>的<strong>连续谱算子</strong>，可以同时实现<strong>全局感受野</strong>、<strong>分辨率不变性</strong>和<strong>高参数效率</strong>。</li>
<li>将卷积算子分解为一系列<strong>共享的、可学习的定向模式</strong>，可以有效地建模复杂的空间结构，并提高模型的鲁棒性。</li>
</ul>
</li>
</ul>
<h3 id="3">3. 方法设计详解</h3>
<p><strong>流程总结</strong>：</p>
<p>SONIC的核心思想是将卷积算子在频域中进行参数化，并引入方向感知和分辨率不变性。其pipeline可以概括为：</p>
<ol>
<li><strong>输入与傅里叶变换</strong>：输入图像（或特征图）<script type="math/tex">x</script> 经过傅里叶变换（FFT）转换为频域表示 <script type="math/tex">\hat{x}</script>。</li>
<li><strong>谱算子建模（SONIC Block）</strong>：<ul>
<li><strong>模式分解 (Mode Decomposition)</strong>：SONIC将一个复杂的谱算子 <script type="math/tex">\hat{H}(\omega)</script> 分解为 <script type="math/tex">M</script> 个共享的、可学习的<strong>定向模式</strong> <script type="math/tex">T_m(\omega)</script> 的线性组合。</li>
<li><strong>谱混合 (Spectral Mixing)</strong>：每个模式 <script type="math/tex">T_m(\omega)</script> 独立地处理输入通道，并通过学习到的低秩矩阵 <script type="math/tex">B \in \mathbb{C}^{M \times C}</script> 和 <script type="math/tex">C \in \mathbb{C}^{K \times M}</script> 将其与输入通道和输出通道进行混合。具体来说，输出的谱响应 <script type="math/tex">\hat{H}_{k,c}(\omega)</script> 表示为：
    <script type="math/tex; mode=display"> \hat{H}_{k,c}(\omega) = \sum_{m=1}^{M} C_{km} T_m(\omega) B_{mc} </script>
    其中 <script type="math/tex">C_{km}</script> 和 <script type="math/tex">B_{mc}</script> 是学习到的混合权重，将输入通道 <script type="math/tex">c</script> 和输出通道 <script type="math/tex">k</script> 与模式 <script type="math/tex">m</script> 联系起来。</li>
<li><strong>定向模式 <script type="math/tex">T_m(\omega)</script> 的参数化</strong>：这是SONIC的关键创新点。每个模式 <script type="math/tex">T_m(\omega)</script> 都被设计成一个<strong>连续的、方向敏感的</strong>函数，其形式受到线性时不变（LTI）系统<strong>resolvent</strong>形式的启发：
    <script type="math/tex; mode=display"> T_m(\omega) = \frac{1}{i s_m (\omega \cdot v_m) - a_m + \tau_m \|(I - v_m v_m^T) \omega\|^2} </script>
    其中：<ul>
<li>
<script type="math/tex">v_m \in \mathbb{R}^D</script> 是一个单位向量，表示模式的<strong>方向</strong>。</li>
<li>
<script type="math/tex">s_m > 0</script> 是<strong>尺度</strong>参数，控制谱选择性。</li>
<li>
<script type="math/tex">a_m</script> 是复数参数，其<strong>实部</strong>控制阻尼（稳定性），<strong>虚部</strong>控制振荡行为。</li>
<li>
<script type="math/tex">\tau_m \ge 0</script> 是<strong>横向惩罚</strong>参数，控制垂直于 <script type="math/tex">v_m</script> 方向的衰减，增强方向选择性。</li>
<li>
<script type="math/tex">\omega</script> 是频率向量，<script type="math/tex">(\omega \cdot v_m)</script> 是 <script type="math/tex">\omega</script> 在 <script type="math/tex">v_m</script> 方向上的投影，<script type="math/tex">\|(I - v_m v_m^T) \omega\|^2</script> 是 <script type="math/tex">\omega</script> 垂直于 <script type="math/tex">v_m</script> 方向上的能量。
这种参数化使得每个模式都能捕捉特定方向上的频率特征，并能平滑地适应不同的分辨率。</li>
</ul>
</li>
</ul>
</li>
<li><strong>频域滤波</strong>：将输入频域表示 <script type="math/tex">\hat{x}</script> 与学习到的谱算子 <script type="math/tex">\hat{H}(\omega)</script> 进行逐点乘法：
    <script type="math/tex; mode=display"> \hat{y}(\omega) = \hat{H}(\omega) \hat{x}(\omega) </script>
</li>
<li><strong>逆傅里叶变换与残差连接</strong>：将滤波后的频域表示 <script type="math/tex">\hat{y}</script> 通过逆傅里叶变换（IFFT）回到空间域，得到输出 <script type="math/tex">y</script>。然后，将 <script type="math/tex">y</script> 与一个线性投影（用于通道映射）和残差连接（<script type="math/tex">W_s x^{(l)}</script>）结合，并通过一个非线性激活函数（如ReLU）得到下一层的输出 <script type="math/tex">x^{(l+1)}</script>。
    <script type="math/tex; mode=display"> x^{(l+1)} = \sigma(y^{(l)} + W_s x^{(l)}) </script>
</li>
</ol>
<p><strong>模型结构</strong>：</p>
<ul>
<li><strong>SONIC Block</strong>：这是论文的核心模块，它包含了上述的傅里叶变换、谱混合、定向模式参数化、频域滤波、逆傅里叶变换以及残差连接和非线性激活。</li>
<li><strong>多层SONIC网络</strong>：通过堆叠多个SONIC Block来构建深度网络，以学习更抽象的特征表示。</li>
<li><strong>参数化</strong>：<ul>
<li><strong>共享模式</strong>：<script type="math/tex">M</script> 个定向模式 <script type="math/tex">T_m(\omega)</script> 是共享的，每个模式都有其独立的参数 (<script type="math/tex">v_m, s_m, a_m, \tau_m</script>)。</li>
<li><strong>低秩混合</strong>：矩阵 <script type="math/tex">B</script> 和 <script type="math/tex">C</script> 是低秩的，用于在输入/输出通道和共享模式之间进行高效混合。</li>
<li><strong>端到端学习</strong>：所有参数（包括模式参数和混合矩阵）都通过反向传播进行端到端学习。</li>
</ul>
</li>
</ul>
<p><strong>算法解释</strong>：</p>
<ul>
<li><strong>定向模式 <script type="math/tex">T_m(\omega)</script> 的意义</strong>：<ul>
<li><strong>方向 <script type="math/tex">v_m</script></strong>：就像一个“指南针”，定义了该模式对哪个方向的频率分量最敏感。</li>
<li><strong>尺度 <script type="math/tex">s_m</script></strong>：控制该模式对沿 <script type="math/tex">v_m</script> 方向的频率变化的敏感度范围。小的 <script type="math/tex">s_m</script> 对应于平滑的低通滤波，大的 <script type="math/tex">s_m</script> 对应于更精细的带通滤波。</li>
<li><strong>阻尼 <script type="math/tex">Re(a_m)</script></strong>：确保系统的稳定性，并影响响应的衰减速度。</li>
<li><strong>振荡 <script type="math/tex">Im(a_m)</script></strong>：引入振荡特性，允许模型捕捉更复杂的周期性结构。</li>
<li><strong>横向惩罚 <script type="math/tex">\tau_m</script></strong>：抑制垂直于 <script type="math/tex">v_m</script> 方向的频率分量，使得滤波器的响应更聚焦于指定方向，避免各向同性扩散。</li>
</ul>
</li>
<li><strong>Resolvent 结构</strong>：借鉴了LTI系统的resolvent形式 <script type="math/tex">C(sI-A)^{-1}B</script>，这种形式本身就具有良好的谱特性和稳定性。SONIC通过将拉普拉斯变量 <script type="math/tex">s</script> 替换为与方向相关的频率项，并引入横向惩罚，将其推广到多维、定向的谱滤波器。</li>
<li><strong>低秩分解</strong>：将一个可能非常大的谱核矩阵分解为 <script type="math/tex">M</script> 个模式的组合，并通过低秩矩阵 <script type="math/tex">B, C</script> 进行混合，极大地减少了参数量，同时保持了表达能力。这类似于矩阵分解的思想，将全局的、复杂的交互分解为更简单的、共享的组件。</li>
</ul>
<h3 id="4">4. 方法对比分析</h3>
<ul>
<li>
<p><strong>本质区别</strong>：</p>
<ul>
<li><strong>与CNNs</strong>：SONIC在频域操作，实现全局感受野，而CNNs在空域操作，感受野受限于核大小。SONIC通过定向模式实现方向感知，而CNNs通常依赖于堆叠层来捕捉方向。</li>
<li><strong>与ViTs</strong>：SONIC在频域操作，避免了ViTs的二次方计算复杂度，并且通过定向模式引入了空间归纳偏置。SONIC的计算复杂度与通道数和模式数线性相关，与分辨率的二次方关系较弱。</li>
<li><strong>与现有谱方法（GFNet, FNO, S4ND等）</strong>：<ul>
<li><strong>方向感知</strong>：SONIC是<strong>方向感知</strong>的，而许多谱方法（如GFNet, FNO）是各向同性的或仅限于坐标轴方向。</li>
<li><strong>连续谱参数化</strong>：SONIC使用<strong>连续的、解析形式的定向模式</strong>来参数化谱算子，这保证了真正的<strong>分辨率不变性</strong>。而GFNet和FNO直接在离散的FFT网格上学习参数，其算子会随分辨率变化而变化。S4ND虽然是基于状态空间模型，但其多维形式通常也受限于坐标轴。</li>
<li><strong>结构化低秩分解</strong>：SONIC通过共享的、结构化的定向模式进行<strong>低秩分解</strong>，参数效率高。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>创新贡献</strong>：</p>
<ul>
<li><strong>谱方向感知</strong>：首次提出在频域中通过解析形式的定向模式来建模卷积算子，实现了对自然图像中定向结构的有效捕捉。</li>
<li><strong>真正的分辨率不变性</strong>：通过连续谱参数化，确保算子定义不依赖于采样网格，从而实现对不同分辨率的鲁棒性。</li>
<li><strong>高参数效率与全局感受野</strong>：通过低秩分解和共享模式，以远低于ViTs的参数量实现了全局感受野和强大的表达能力。</li>
<li><strong>统一的框架</strong>：提供了一个统一的框架，可以自然地处理多维信号，并与现有的谱方法（如S4ND）建立联系。</li>
</ul>
</li>
<li>
<p><strong>适用场景</strong>：</p>
<ul>
<li><strong>需要捕捉长距离依赖和全局上下文的任务</strong>：如图像分类、语义分割、目标检测等。</li>
<li><strong>对几何变换和分辨率变化敏感的任务</strong>：如医学影像分析（CT、MRI）、遥感影像处理、显微镜图像分析等。</li>
<li><strong>对模型参数量和计算效率有较高要求的场景</strong>：SONIC在参数效率和计算效率上具有显著优势。</li>
<li><strong>需要捕捉定向结构的任务</strong>：如纹理分析、边缘检测等。</li>
</ul>
</li>
</ul>
<h3 id="5">5. 实验分析</h3>
<ul>
<li>
<p><strong>验证方法</strong>：</p>
<ul>
<li><strong>SynthShape数据集</strong>：设计用于评估模型对几何变换（rescaling, rotation, translation, distortion, Gaussian noise）的鲁棒性。</li>
<li><strong>HalliGalli任务</strong>：一个空间推理任务，需要捕捉远距离的形状匹配关系，测试模型的长距离依赖建模能力。</li>
<li><strong>3D医学影像分割（KiTS, ACDC）</strong>：在真实世界的复杂高维数据上评估模型性能，特别是对长距离空间理解的需求。</li>
<li><strong>ImageNet-1K</strong>：在自然图像分类任务上评估模型性能，并特别测试了在不同输入分辨率下的鲁棒性。</li>
<li><strong>Prostate158和PROMIS数据集</strong>：用于外部验证，评估模型在不同扫描设备和协议下的泛化能力。</li>
</ul>
</li>
<li>
<p><strong>关键结果</strong>：</p>
<ul>
<li><strong>SynthShape</strong>：SONIC在各种几何变换下均表现出比ConvNet、ViT等方法更高的鲁棒性，并且在HalliGalli任务上是唯一能解决长距离依赖问题的模型。</li>
<li><strong>3D医学影像分割</strong>：SONIC在KiTS和ACDC数据集上取得了与最先进方法（如nnU-Net）相当甚至更好的性能，但参数量仅为其10%左右。</li>
<li><strong>ImageNet-1K</strong>：SONIC在ImageNet-50M上的ResNet-50变体中，在参数量和计算量都较低的情况下，取得了与强基线相当的准确率，并且在分辨率变化下表现出优异的鲁棒性（图3）。</li>
<li><strong>外部验证</strong>：SONIC在Prostate158和PROMIS数据集上，以更少的参数取得了比nnU-Net更好的检测性能。</li>
</ul>
</li>
<li>
<p><strong>优势场景</strong>：</p>
<ul>
<li><strong>鲁棒性</strong>：在SynthShape数据集上，SONIC在各种几何变换（尤其是rescaling, rotation, distortion）下表现出显著的优势。</li>
<li><strong>长距离依赖</strong>：HalliGalli任务证明了SONIC能够有效捕捉远距离的结构信息。</li>
<li><strong>参数效率</strong>：在3D医学影像分割和ImageNet任务中，SONIC以远低于其他方法的参数量取得了优异的性能。</li>
<li><strong>分辨率不变性</strong>：在ImageNet上的分辨率变化实验（图3）清晰地展示了SONIC在不同分辨率下性能下降最少。</li>
</ul>
</li>
<li>
<p><strong>局限性</strong>：</p>
<ul>
<li><strong>非线性激活在空间域</strong>：虽然核心计算在频域进行，但非线性激活函数必须在空间域应用，这导致需要进行多次FFT/IFFT操作，增加了计算开销。</li>
<li><strong>初始化稳定性</strong>：在某些情况下，SONIC块的初始化可能不稳定，这可能与不同数据集的物理尺度差异有关。</li>
<li><strong>精细局部结构捕捉</strong>：全局的频域表示可能限制了对非常精细局部结构的捕捉能力，作者建议未来可以探索混合谱-空间架构。</li>
<li><strong>计算开销</strong>：虽然比ViT的全局注意力高效，但FFT/IFFT操作仍然是计算瓶颈，尤其是在高分辨率下。</li>
</ul>
</li>
</ul>
<h3 id="6">6. 实用指南</h3>
<ul>
<li><strong>开源情况</strong>：论文提供了代码链接（https://github.com/GijsMoens/Sonic）。</li>
<li><strong>实现细节</strong>：<ul>
<li><strong>FFT/IFFT</strong>：论文使用了VkFFT库，这是一个高效的GPU FFT库。</li>
<li><strong>参数化</strong>：模式参数 (<script type="math/tex">v_m, s_m, a_m, \tau_m</script>) 的学习需要通过特定的重参数化技巧来保证约束（如 <script type="math/tex">s_m > 0</script>, <script type="math/tex">Re(a_m) < 0</script>）。</li>
<li><strong>混合矩阵</strong>：<script type="math/tex">B</script> 和 <script type="math/tex">C</script> 是复数矩阵，需要正确处理复数运算。</li>
<li><strong>归一化</strong>：在训练中使用了RMS谱增益归一化，以保持响应在不同分辨率下的尺度一致性。</li>
<li><strong>方向向量</strong>：方向向量 <script type="math/tex">v_m</script> 在使用前会进行归一化，以确保对像素间距不变。</li>
<li><strong>训练</strong>：使用了AdamW优化器，one-cycle学习率调度，并结合了交叉熵和Dice loss。</li>
</ul>
</li>
<li><strong>迁移可能</strong>：<ul>
<li><strong>通用性</strong>：SONIC作为一个模块，可以方便地集成到现有的CNN或Transformer架构中，替换原有的卷积层或注意力层。</li>
<li><strong>任务迁移</strong>：其在图像分类、分割、检测等任务上的成功表明了其广泛的适用性。</li>
<li><strong>多维数据</strong>：其设计本身就支持多维数据，因此可以轻松迁移到3D数据（如医学影像）或其他多维信号处理任务。</li>
<li><strong>超参数调整</strong>：关键超参数包括模式数量 <script type="math/tex">M</script> 和通道宽度 <script type="math/tex">K</script>。<script type="math/tex">M</script> 控制谱多样性，<script type="math/tex">K</script> 控制通道混合能力。根据任务和计算资源进行调整。</li>
</ul>
</li>
</ul>
<h3 id="7">7. 总结</h3>
<ul>
<li><strong>核心思想</strong>：<strong>频域定向模式低秩分解，实现分辨率不变的全局卷积。</strong></li>
<li><strong>速记版pipeline</strong>：<ol>
<li><strong>傅里叶变换</strong>：输入转到频域。</li>
<li><strong>定向模式滤波</strong>：用可学习的、方向敏感的模式在频域进行滤波。</li>
<li><strong>低秩混合</strong>：通过共享模式和低秩矩阵高效混合通道信息。</li>
<li><strong>逆傅里叶变换与残差</strong>：回到空域，结合残差连接。</li>
</ol>
</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce SONIC (Spectral Oriented Neural Invariant Convolutions), a continuous spectral parameterisation that models convolutional operators using a small set of shared, orientation-selective components.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.19884v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.19884v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.19850v1'></a></p>
<h2 id="egohandicl-egocentric-3d-hand-reconstruction-with-in-context-learning"><a href="https://arxiv.org/abs/2601.19850v1">EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning</a></h2>
<p><strong>Authors:</strong> Binzhu Xie, Shi Qiu, Sicheng Zhang, Yinqiao Wang, Hao Xu, Muzammal Naseer, Chi-Wing Fu, Pheng-Ann Heng</p>
<p><strong>Published:</strong> 2026-01-27</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignment, visual consistency, and robustness under challenging egocentric conditions. EgoHandICL introduces complementary exemplar retrieval guided by vision-language models (VLMs), an ICL-tailored tokenizer for multimodal context, and a masked autoencoder (MAE)-based architecture trained with hand-guided geometric and perceptual objectives. Experiments on ARCTIC and EgoExo4D show consistent gains over state-of-the-art methods. We also demonstrate real-world generalization and improve EgoVLM hand-object interaction reasoning by using reconstructed hands as visual prompts. Code and data: https://github.com/Nicous20/EgoHandICL</p>
<p><strong>Analysis:</strong></p>
<p>好的，我将以AI领域专业研究生的视角，深入分析这篇论文的方法部分，并遵循您提供的分析框架。</p>
<hr />
<h2 id="egohandicl">论文方法分析与总结：EgoHandICL</h2>
<h3 id="1_1">1. 摘要翻译</h3>
<p><strong>论文题目：</strong> EgoHandICL: 基于上下文学习的自顶向下3D手部重建</p>
<p><strong>摘要：</strong>
在自顶向下（egocentric）视觉中，由于深度模糊、自遮挡和复杂的手-物体交互，鲁棒的3D手部重建极具挑战性。现有方法试图通过扩大训练数据规模或引入辅助线索来缓解这些挑战，但往往难以有效处理未知的场景。本文提出了EgoHandICL，首个用于3D手部重建的上下文学习（ICL）框架，该框架能够实现强大的语义对齐、视觉一致性和在挑战性自顶向下条件下的鲁棒性。具体而言，我们开发了：(i) 由视觉语言模型（VLMs）引导的互补性示例检索策略；(ii) 一个集成了多模态上下文的ICL定制分词器；以及 (iii) 一个基于掩码自编码器（MAE）的架构，并使用3D手部引导的几何和感知目标进行训练。通过在ARCTIC和EgoExo4D基准上的全面实验，我们的EgoHandICL在最先进的3D手部重建方法上持续展现出显著的改进。我们还通过在真实世界的自顶向下场景中进行测试，并将其与EgoVLMs集成以增强其手-物体交互推理能力，进一步展示了EgoHandICL的适用性。我们的代码和数据可在以下网址获取：https://github.com/Nicous20/EgoHandICL。</p>
<h3 id="2_1">2. 方法动机分析</h3>
<ul>
<li>
<p><strong>驱动力</strong>：</p>
<ul>
<li><strong>自顶向下（Egocentric）场景的挑战性</strong>：自顶向下视角下的3D手部重建面临着严重的遮挡、视角变化、手部与物体复杂的交互以及深度模糊等问题，这些问题使得传统方法难以获得鲁棒和准确的结果。</li>
<li><strong>现有方法局限性</strong>：现有的3D手部重建方法，即使是针对自顶向下场景进行优化的，也常常依赖于额外的标注数据（如辅助线索），并且在处理极端遮挡和模糊交互时表现不佳。</li>
<li><strong>人类的类比</strong>：人类在处理不确定和模糊的视觉信息时，会自然地利用先验知识、多模态上下文和任务相关经验来做出判断。这与“上下文学习”（In-Context Learning, ICL）的核心思想高度契合。</li>
</ul>
</li>
<li>
<p><strong>现有方法痛点</strong>：</p>
<ul>
<li><strong>对额外标注的依赖</strong>：许多方法需要额外的、难以获取的标注数据。</li>
<li><strong>处理遮挡和交互的不足</strong>：在自顶向下场景中常见的严重遮挡和复杂手-物体交互问题，现有方法难以有效解决。</li>
<li><strong>泛化能力受限</strong>：模型在面对未见过或具有挑战性的场景时，鲁棒性和泛化能力不足。</li>
</ul>
</li>
<li>
<p><strong>研究假设</strong>：</p>
<ul>
<li><strong>ICL的潜力</strong>：通过在模型输入中提供少量相关的上下文示例（即“模板”），ICL能够引导模型在不更新模型参数的情况下，学习如何处理新的、具有挑战性的任务，这对于解决自顶向下3D手部重建中的不确定性问题至关重要。</li>
<li><strong>多模态信息融合</strong>：结合视觉、文本和结构化（MANO参数）信息，可以更全面地理解手部状态和上下文，从而提升重建精度。</li>
<li><strong>检索与推理的结合</strong>：通过智能地检索与查询图像相关的上下文示例，并利用这些示例进行推理，可以有效弥补自顶向下场景中信息的不完整性。</li>
</ul>
</li>
</ul>
<h3 id="3_1">3. 方法设计详解</h3>
<p><strong>流程总结：</strong></p>
<p>EgoHandICL 的核心流程可以概括为三个主要阶段：<strong>模板检索 (Template Retrieval)</strong>、<strong>上下文学习分词 (ICL Tokenization)</strong> 和 <strong>掩码自编码器训练与推理 (MAE-based Training &amp; Inference)</strong>。</p>
<p><strong>整体 Pipeline 示意图 (参考 Figure 2):</strong></p>
<ol>
<li>
<p><strong>模板检索 (Part A)</strong>:</p>
<ul>
<li><strong>输入</strong>：一张查询图像 (Query Image)。</li>
<li><strong>目标</strong>：为查询图像检索一个或多个最相关的模板图像 (Template Image)，这些模板图像将作为上下文示例。</li>
<li><strong>策略</strong>：<ul>
<li><strong>预定义视觉模板 (Pre-defined Visual Templates)</strong>：<ul>
<li>利用视觉语言模型 (VLM) 对查询图像进行分类，将其归入四种预定义的手部参与类型之一：左手、右手、双手、无手。</li>
<li>根据分类结果，从预定义的模板库中检索具有相同手部参与类型（例如，如果查询是右手，则检索右手模板）的图像。</li>
<li><strong>动机</strong>：确保检索到的模板在手部配置上与查询图像具有视觉一致性，覆盖了常见的自顶向下手部活动。</li>
</ul>
</li>
<li><strong>自适应文本模板 (Adaptive Textual Templates)</strong>：<ul>
<li><strong>描述性提示 (Description-style prompts)</strong>：向 VLM 提供查询图像，并要求其生成一个简洁的句子，描述图像中的可见手部及其与周围物体的交互（例如，“左手抓着一个杯子”）。</li>
<li><strong>推理式提示 (Reasoning-style prompts)</strong>：向 VLM 提供查询图像，并要求其生成一个更具推理性的描述，特别关注遮挡、交互细节以及可能影响3D重建的线索（例如，“右手正在用剪刀切割木盒，拇指和手指正在进行切割动作，这使得拇指区域的深度估计变得模糊”）。</li>
<li><strong>检索过程</strong>：利用 VLM 生成的文本描述，在模板库中搜索与这些描述在语义上最匹配的模板图像。</li>
<li><strong>动机</strong>：通过文本描述，可以更精细地捕捉查询图像的上下文信息，如特定的交互方式、遮挡情况等，从而检索到更具语义相关性的模板，尤其是在处理复杂或模糊场景时。</li>
</ul>
</li>
</ul>
</li>
<li><strong>输出</strong>：一个或多个模板图像 (Itpl)。</li>
</ul>
</li>
<li>
<p><strong>上下文学习分词 (Part B)</strong>:</p>
<ul>
<li><strong>输入</strong>：查询图像 (Iqry) 和检索到的模板图像 (Itpl)。</li>
<li><strong>目标</strong>：将多模态信息（图像、文本、3D手部结构）编码成统一的“ICL Tokens”，供 Transformer 模型使用。</li>
<li><strong>过程</strong>：<ul>
<li><strong>MANO 参数编码 (Structural Tokens Fm)</strong>：<ul>
<li>对于查询图像 Iqry 和模板图像 Itpl，首先使用一个预训练的手部重建模型（如 HaMeR 或 WiLoR）估计其粗糙的 MANO 参数 (Mqry, Mtpl)。</li>
<li>同时，获取这些图像对应的真实 MANO 参数 (Mqry, Mtpl)。</li>
<li>将这四组 MANO 参数（粗糙查询、真实查询、粗糙模板、真实模板）输入到一个 MANO 编码器 (MANO Encoder) 中。</li>
<li><strong>动机</strong>：MANO 参数编码器将3D手部结构信息（姿态、形状、全局方向）转化为结构化特征，这些特征保留了手部关节的运动和形状先验。</li>
</ul>
</li>
<li><strong>图像编码 (Image Tokens Fi)</strong>：<ul>
<li>使用预训练的 ViT (Vision Transformer) 编码器对查询图像 Iqry 和模板图像 Itpl 进行编码。</li>
<li><strong>动机</strong>：提取图像的外观和空间细节信息。</li>
</ul>
</li>
<li><strong>文本编码 (Text Tokens Ft)</strong>：<ul>
<li>将用于模板检索的 VLM 生成的文本描述，通过一个文本编码器进行编码。</li>
<li><strong>动机</strong>：将语义上下文信息融入到模型中。</li>
</ul>
</li>
<li><strong>多模态融合与分词</strong>：<ul>
<li>利用交叉注意力机制 (Cross-attention)，将上述三种模态（结构、图像、文本）的特征进行融合。</li>
<li>生成四组统一的 ICL Tokens：<ul>
<li><code>Ttpl_input</code>：模板图像的输入表示。</li>
<li><code>Ttpl_target</code>：模板图像的对应目标（真实 MANO 参数）。</li>
<li><code>Tqry_input</code>：查询图像的输入表示。</li>
<li><code>Tqry_target</code>：查询图像的对应目标（待预测的真实 MANO 参数）。</li>
</ul>
</li>
<li><strong>动机</strong>：将不同模态的信息整合成统一的 token 序列，使得 Transformer 模型能够在一个统一的表示空间中进行上下文学习。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>掩码自编码器训练与推理 (Part C)</strong>:</p>
<ul>
<li><strong>输入</strong>：生成的 ICL Tokens。</li>
<li><strong>目标</strong>：训练一个 Transformer 模型，使其能够根据上下文示例（模板 tokens）预测查询图像的 MANO 参数。</li>
<li><strong>模型结构</strong>：一个轻量级的 Transformer 编码器/解码器架构，借鉴了 Masked Autoencoders (MAE) 的思想。</li>
<li><strong>训练过程</strong>：<ul>
<li><strong>掩码策略</strong>：在训练时，随机且部分地掩盖模板目标 tokens (<code>Ttpl_target</code>) 和查询目标 tokens (<code>Tqry_target</code>)。</li>
<li><strong>目标</strong>：模型需要根据输入的查询输入 tokens (<code>Tqry_input</code>) 和掩码后的目标 tokens（包括模板的掩码目标和查询的掩码目标），来重建被掩盖的目标 tokens。</li>
<li><strong>动机</strong>：<ul>
<li><strong>模拟不完整信息</strong>：通过掩码，模拟真实推理场景中目标信息不可用的情况。</li>
<li><strong>强制上下文推理</strong>：迫使模型利用模板示例 (<code>Ttpl_input</code> -&gt; <code>Ttpl_target</code>) 和查询输入 (<code>Tqry_input</code>) 来推断被掩盖的查询目标 (<code>Tqry_target</code>)，从而实现上下文学习。</li>
<li><strong>MAE 思想</strong>：MAE 的掩码重建范式已被证明在学习鲁棒表示和处理不完整信息方面非常有效。</li>
</ul>
</li>
</ul>
</li>
<li><strong>推理过程</strong>：<ul>
<li><strong>输入</strong>：查询图像 Iqry，检索模板 Itpl，生成 ICL Tokens。</li>
<li><strong>掩码策略</strong>：在推理时，查询目标 tokens (<code>Tqry_target</code>) 被完全掩盖（不可用）。</li>
<li><strong>模型预测</strong>：Transformer 模型根据查询输入 tokens (<code>Tqry_input</code>) 和模板的 ICL Tokens (<code>Ttpl_input</code>, <code>Ttpl_target</code>)，来预测被完全掩盖的查询目标 tokens (<code>Tqry_target</code>)，即查询图像的 MANO 参数 (Mqry)。</li>
<li><strong>MANO 解码</strong>：将预测出的 MANO 参数 (Mqry) 输入到 MANO 解码器 (MANO Decoder) 中，生成最终的3D手部网格。</li>
</ul>
</li>
<li><strong>损失函数</strong>：<ul>
<li><strong>参数级损失 (Lmano)</strong>：直接衡量预测的 MANO 参数与真实 MANO 参数之间的差异（L2 距离）。</li>
<li><strong>顶点级损失 (Lv)</strong>：衡量预测的3D手部网格顶点与真实网格顶点之间的差异（L2 距离）。</li>
<li><strong>3D感知损失 (L3D)</strong>：引入一个预训练的3D特征编码器，对预测和真实的手部网格进行编码，然后计算编码后的特征之间的 L2 距离。此损失旨在强制模型学习更具语义一致性和鲁棒性的手部表示，尤其是在处理遮挡和模糊时。</li>
<li><strong>总损失</strong>：<code>L = λm * Lmano + λv * Lv + λ3D * L3D</code> (对于有 MANO ground truth 的数据集)。</li>
<li><strong>对于无 MANO ground truth 的数据集 (如 EgoExo4D)</strong>：使用3D关键点约束 <code>LJ</code>，总损失为 <code>L = λj * LJ + λ3D * L3D</code>。</li>
</ul>
</li>
</ul>
</li>
</ol>
<p><strong>模型结构：</strong></p>
<ul>
<li><strong>VLM (Vision-Language Model)</strong>: 用于模板检索，负责图像理解和文本生成。</li>
<li><strong>MANO Encoder/Decoder</strong>: 将 MANO 参数与3D手部网格进行相互转换。</li>
<li><strong>ViT Encoder</strong>: 用于提取图像特征。</li>
<li><strong>Text Encoder</strong>: 用于提取文本描述特征。</li>
<li><strong>Transformer Encoder/Decoder (MAE-based)</strong>: 核心的上下文学习模型，负责根据检索到的模板和查询图像的特征，预测查询图像的 MANO 参数。</li>
<li><strong>3D Feature Encoder</strong>: 用于计算 3D 感知损失。</li>
</ul>
<p><strong>算法解释：</strong></p>
<ul>
<li><strong>上下文学习 (ICL)</strong>：本质上是一种“少样本学习”范式，通过在输入中提供少量示例（prompting），让模型在推理时直接模仿这些示例的行为，而无需进行梯度更新。EgoHandICL 将其应用于3D手部重建，通过提供相关的3D手部姿态和形状示例来指导重建过程。</li>
<li><strong>掩码自编码器 (MAE)</strong>：一种自监督学习方法，通过随机掩盖输入的大部分区域，然后训练模型重建被掩盖的部分。这迫使模型学习全局上下文和依赖关系。EgoHandICL 将其应用于 ICL 框架，通过掩盖目标（MANO 参数），让模型学习如何利用上下文示例来预测目标。</li>
<li><strong>MANO 参数化</strong>：一种低维度的手部模型，通过一组姿态、形状和全局方向参数来表示3D手部。这为3D手部重建提供了一个统一的、可优化的表示空间，使得跨模态（图像、文本、3D）的对齐和学习成为可能。</li>
</ul>
<h3 id="4_1">4. 方法对比分析</h3>
<ul>
<li>
<p><strong>本质区别</strong>：</p>
<ul>
<li><strong>范式创新</strong>：EgoHandICL 是第一个将上下文学习（ICL）应用于3D手部重建的框架，特别是针对自顶向下场景。</li>
<li><strong>检索与推理的结合</strong>：它不依赖于固定的、预训练的端到端模型，而是通过动态检索上下文示例来适应不同的场景和挑战。</li>
<li><strong>多模态融合</strong>：它显式地融合了图像、文本和3D结构信息，以更全面地理解手部状态。</li>
<li><strong>MAE 架构的应用</strong>：将 MAE 的掩码重建思想应用于 ICL 框架，解决了 ICL 中推理时目标不可用的问题。</li>
</ul>
</li>
<li>
<p><strong>创新贡献</strong>：</p>
<ul>
<li><strong>首个 ICL 框架用于3D手部重建</strong>：开辟了利用 ICL 解决3D手部重建挑战的新方向。</li>
<li><strong>互补性模板检索策略</strong>：结合了视觉和文本信息，实现了更智能、更具适应性的模板检索。</li>
<li><strong>ICL Tokenizer</strong>：设计了统一的多模态 token 表示，有效融合了图像、文本和3D结构信息。</li>
<li><strong>MAE-驱动的 ICL 推理</strong>：解决了 ICL 在推理阶段目标不可用的问题，实现了鲁棒的上下文推理。</li>
<li><strong>在自顶向下场景下的显著性能提升</strong>：在 ARCTIC 和 EgoExo4D 等数据集上取得了 SOTA 性能。</li>
</ul>
</li>
<li>
<p><strong>适用场景</strong>：</p>
<ul>
<li><strong>自顶向下（Egocentric）3D手部重建</strong>：这是其主要设计目标和最佳应用场景，尤其是在存在遮挡、复杂交互和视角变化的情况下。</li>
<li><strong>需要适应新场景或领域的数据集</strong>：由于其 ICL 的特性，理论上可以快速适应新的、未见过的数据分布，只要能提供相关的上下文示例。</li>
<li><strong>需要精细化手-物体交互理解的任务</strong>：通过融合文本描述和3D手部信息，可以为下游任务（如 EgoVLM 的交互推理）提供更准确的手部视觉线索。</li>
</ul>
</li>
</ul>
<h3 id="5_1">5. 实验分析</h3>
<ul>
<li>
<p><strong>验证方法</strong>：</p>
<ul>
<li><strong>数据集</strong>：在 ARCTIC（高质量 MANO 参数标注）和 EgoExo4D（自顶向下视频，主要为关键点标注）两个数据集上进行评估。</li>
<li><strong>评估设置</strong>：<ul>
<li><strong>General Setting</strong>：在所有检测到的手部上进行评估。</li>
<li><strong>Bimanual Setting</strong>：仅在同时检测到双手的情况下进行评估，以更严格地衡量双手动作品质和空间一致性。</li>
</ul>
</li>
<li><strong>对比方法</strong>：与当时最先进的3D手部重建方法进行比较，包括 HaMeR, WiLoR, WildHand, HaWoR, POTTER, PCIE-EgoHandPose 等。</li>
<li><strong>消融实验</strong>：分析了不同组件（如模板检索策略、损失函数、掩码比例）的作用。</li>
<li><strong>定性分析</strong>：通过可视化重建结果，展示了方法在处理遮挡、交互等挑战性场景下的优势。</li>
<li><strong>EgoVLM 集成实验</strong>：评估了 EgoHandICL 输出作为视觉提示，如何提升 EgoVLM 的手-物体交互推理能力。</li>
</ul>
</li>
<li>
<p><strong>关键结果</strong>：</p>
<ul>
<li><strong>ARCTIC 数据集 (Table 1)</strong>：EgoHandICL 在 P-MPJPE 和 P-MPVPE 指标上均显著优于所有基线方法，尤其是在 Bimanual Setting 下，MRRPE 显著降低，表明其在双手动作品质和空间关系估计上表现出色。</li>
<li><strong>EgoExo4D 数据集 (Table 2)</strong>：EgoHandICL 在 MPJPE 和 P-MPJPE 指标上同样取得了 SOTA 性能，显著优于其他方法，尤其是在 MRRPE 指标上，大幅降低了误差。</li>
<li><strong>手部参与类型分析 (Table 3)</strong>：表明模型在训练时需要覆盖所有手部参与类型，以获得最佳的泛化能力。单独训练的模型在特定类型上表现最好，但泛化到其他类型时性能下降。</li>
<li><strong>不同提示词分析 (Table 4)</strong>：推理式提示词（Reas. Prompts）在处理遮挡和复杂交互时，比描述式提示词（Des. Prompts）能带来更大的性能提升。</li>
<li><strong>消融实验</strong>：<ul>
<li><strong>Backbone for Coarse MANO Prediction</strong>：表明 EgoHandICL 的性能提升主要归功于 ICL 范式本身，而非特定的粗糙 MANO 预测器。</li>
<li><strong>Mask Ratio for ICL Tokens</strong>：70% 的掩码比例在 ICL Tokens 上取得了最佳效果，表明在自顶向下场景下，更大的掩码比例能促使模型利用更强的上下文线索。</li>
<li><strong>3D Perceptual Loss</strong>：证明了 3D 感知损失对提升重建精度和鲁棒性有积极作用。</li>
</ul>
</li>
<li><strong>EgoVLM 集成 (Table 8)</strong>：将 EgoHandICL 的重建结果作为视觉提示输入 EgoVLM，显著提升了其在手-物体交互推理任务上的表现，尤其是在 verb 和 action 类别上。</li>
</ul>
</li>
<li>
<p><strong>优势场景</strong>：</p>
<ul>
<li><strong>处理遮挡和模糊</strong>：通过检索具有相似遮挡或交互模式的模板，模型能够更好地恢复被遮挡的手部区域。</li>
<li><strong>双手动作品质和空间关系</strong>：在 Bimanual Setting 下的显著优势表明，模型能够利用双手之间的相互约束来提高重建精度和空间一致性。</li>
<li><strong>复杂交互场景</strong>：通过推理式提示词引导 VLM 检索更具语义的模板，模型能更好地理解和重建复杂的手-物体交互。</li>
<li><strong>泛化能力</strong>：在不同数据集和场景下均表现出强大的性能，证明了 ICL 范式带来的泛化能力。</li>
</ul>
</li>
<li>
<p><strong>局限性</strong>：</p>
<ul>
<li><strong>计算开销</strong>：VLM 检索过程引入了额外的计算开销，限制了实时部署。</li>
<li><strong>模板库依赖</strong>：检索性能依赖于模板库的质量和多样性。如果模板库缺乏多样性，可能导致检索效果不佳。</li>
<li><strong>数据集标注限制</strong>：部分数据集（如 EgoExo4D）仅提供关键点标注，限制了 MANO 参数的精确监督，可能影响模型在这些数据集上的泛化能力。</li>
<li><strong>低质量检索的失败案例</strong>：当检索到的模板质量很低时（如图 F.1 所示），模型性能会显著下降。</li>
</ul>
</li>
</ul>
<h3 id="6_1">6. 实用指南</h3>
<ul>
<li><strong>开源情况</strong>：论文已开源代码和数据，链接为：https://github.com/Nicous20/EgoHandICL。</li>
<li><strong>实现细节</strong>：<ul>
<li><strong>VLM 选择</strong>：论文使用了 Qwen2.5-VL-72B-Instruct 作为 VLM。</li>
<li><strong>ViT Backbone</strong>：使用了与 WiLoR 相同的 ViT Backbone。</li>
<li><strong>Text Encoder</strong>：使用了 Qwen-7B。</li>
<li><strong>MANO Encoder/Decoder</strong>：使用 MLP 实现。</li>
<li><strong>Transformer</strong>：轻量级 Transformer 编码器。</li>
<li><strong>3D Feature Encoder</strong>：使用了 Uni3D-ti。</li>
<li><strong>训练参数</strong>：100 epochs，学习率 1e-4，AdamW 优化器。</li>
<li><strong>损失权重</strong>：<code>λm = 0.05</code>, <code>λv = 5.0</code>, <code>λj = 5.0</code>, <code>λ3D = 0.01</code>。</li>
<li><strong>Batch Size</strong>：64。</li>
<li><strong>硬件</strong>：单块 RTX 4090 GPU 进行训练，4 A100 GPU 用于数据预处理和检索。</li>
<li><strong>模板检索</strong>：训练时对每个样本进行三次模板检索以验证分类正确性，并进行随机采样以保持多样性。推理时使用相同的 VLM prompt 进行检索。</li>
<li><strong>掩码比例</strong>：70% 的掩码比例在 ICL Tokens 上表现最佳。</li>
</ul>
</li>
<li><strong>迁移可能</strong>：<ul>
<li><strong>迁移到其他自顶向下任务</strong>：该框架的核心思想（VLM 检索 + ICL + MAE）可以迁移到其他自顶向下视觉任务，如手部姿态跟踪、手-物体交互识别等，只需调整目标输出和损失函数。</li>
<li><strong>迁移到其他领域</strong>：如果能构建合适的模板库和检索策略，理论上也可以应用于非自顶向下场景，但其优势可能不如在自顶向下场景下明显。</li>
<li><strong>替换 VLM/Transformer</strong>：可以尝试使用更先进的 VLM 或 Transformer 模型来进一步提升性能。</li>
<li><strong>改进检索策略</strong>：探索更高效、更鲁棒的模板检索方法，如基于近似最近邻搜索或无检索的嵌入方法。</li>
</ul>
</li>
</ul>
<h3 id="7_1">7. 总结</h3>
<ul>
<li><strong>核心思想</strong>：利用上下文学习，通过检索相关示例来解决自顶向下3D手部重建的遮挡与交互难题。</li>
<li><strong>速记版pipeline</strong>：<ol>
<li><strong>看图找相似图</strong>：用 VLM 理解查询图，找到最像的模板图。</li>
<li><strong>提取多模态信息</strong>：将查询图、模板图及其对应的3D手部信息、文本描述，整合成统一的“上下文块”。</li>
<li><strong>学习预测</strong>：用类似“填空题”的方式，让模型根据上下文块预测查询图的3D手部信息。</li>
<li><strong>输出结果</strong>：将预测的3D手部信息转换为最终的3D模型。</li>
</ol>
</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignment, visual consistency, and robustness under challenging egocentric conditions.</li>
<li>Experiments on ARCTIC and EgoExo4D show consistent gains over state-of-the-art methods.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.19850v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.19850v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.19849v1'></a></p>
<h2 id="hexformer-hyperbolic-vision-transformer-with-exponential-map-aggregation"><a href="https://arxiv.org/abs/2601.19849v1">HexFormer: Hyperbolic Vision Transformer with Exponential Map Aggregation</a></h2>
<p><strong>Authors:</strong> Haya Alyoussef, Ahmad Bdeir, Diego Coello de Portugal Mecke, Tom Hanika, Niels Landwehr, Lars Schmidt-Thieme</p>
<p><strong>Published:</strong> 2026-01-27</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Data across modalities such as images, text, and graphs often contains hierarchical and relational structures, which are challenging to model within Euclidean geometry. Hyperbolic geometry provides a natural framework for representing such structures. Building on this property, this work introduces HexFormer, a hyperbolic vision transformer for image classification that incorporates exponential map aggregation within its attention mechanism. Two designs are explored: a hyperbolic ViT (HexFormer) and a hybrid variant (HexFormer-Hybrid) that combines a hyperbolic encoder with an Euclidean linear classification head. HexFormer incorporates a novel attention mechanism based on exponential map aggregation, which yields more accurate and stable aggregated representations than standard centroid based averaging, showing that simpler approaches retain competitive merit. Experiments across multiple datasets demonstrate consistent performance improvements over Euclidean baselines and prior hyperbolic ViTs, with the hybrid variant achieving the strongest overall results. Additionally, this study provides an analysis of gradient stability in hyperbolic transformers. The results reveal that hyperbolic models exhibit more stable gradients and reduced sensitivity to warmup strategies compared to Euclidean architectures, highlighting their robustness and efficiency in training. Overall, these findings indicate that hyperbolic geometry can enhance vision transformer architectures by improving gradient stability and accuracy. In addition, relatively simple mechanisms such as exponential map aggregation can provide strong practical benefits.</p>
<p><strong>Analysis:</strong></p>
<p>好的，我将以一名AI领域高水平研究生的视角，深入分析这篇关于“HexFormer: Hyperbolic Vision Transformer with Exponential Map Aggregation”的论文。我将重点关注其方法部分的创新点、设计逻辑、优势与不足，并提供实用的实现和迁移指南。</p>
<hr />
<h2 id="hexformer">论文方法分析与总结：HexFormer</h2>
<h3 id="1_2">1. 摘要翻译</h3>
<p><strong>论文题目：</strong> HexFormer: 双曲视觉 Transformer 结合指数映射聚合</p>
<p><strong>摘要：</strong>
数据在图像、文本和图等多种模态中经常包含难以在欧几里得几何中建模的层级和关系结构。双曲几何为表示这类结构提供了天然的框架。在此基础上，本文提出了 HexFormer，一种用于图像分类的双曲视觉 Transformer，它在注意力机制中引入了指数映射聚合。本文探索了两种设计：一种是纯双曲的 ViT (HexFormer)，另一种是混合变体 (HexFormer-Hybrid)，它结合了双曲编码器和一个欧几里得线性分类头。HexFormer 采用了一种基于指数映射聚合的新型注意力机制，该机制比标准的基于质心平均的方法能产生更准确、更稳定的聚合表示，表明更简单的方法也能保持竞争力。在多个数据集上的实验表明，与欧几里得基线和先前提出的双曲 ViT 相比，性能得到了一致的提升，其中混合变体取得了最好的整体结果。此外，本文还分析了双曲 Transformer 中的梯度稳定性。结果表明，与欧几里得架构相比，双曲模型表现出更稳定的梯度和对预热策略更低的敏感性，这突显了其训练的鲁棒性和效率。总的来说，这些发现表明双曲几何可以通过提高梯度稳定性和准确性来增强视觉 Transformer 架构。此外，相对简单的机制，如指数映射聚合，也能带来显著的实际效益。代码可在：https://github.com/HayaAlyoussef/HexFormer.git 获取。</p>
<h3 id="2_2">2. 方法动机分析</h3>
<ul>
<li>
<p><strong>驱动力</strong>：</p>
<ul>
<li><strong>数据内在结构</strong>：现实世界中的许多数据（如图像中的物体层级、文本中的语义关系、图中的节点连接）天然具有层级和关系结构，而传统的欧几里得几何空间难以有效捕捉这些结构。</li>
<li><strong>双曲几何的优势</strong>：双曲空间因其负曲率特性，能够以指数级的容量来表示层级结构，这使其成为建模这类数据的理想选择。</li>
<li><strong>Vision Transformer (ViT) 的局限性</strong>：现有的 ViT 模型主要基于欧几里得空间，这可能限制了它们在处理具有复杂层级结构的图像数据时的性能。</li>
</ul>
</li>
<li>
<p><strong>现有方法痛点</strong>：</p>
<ul>
<li><strong>欧几里得 ViT 的不足</strong>：在处理具有内在层级结构的图像时，欧几里得 ViT 可能无法充分利用数据的几何特性。</li>
<li><strong>现有双曲 ViT 的局限</strong>：<ul>
<li>一些方法仅将 ViT 的输出映射到双曲空间进行度量学习，并未完全整合到 Transformer 的核心模块中。</li>
<li>另一些方法（如 HVT, LViT）虽然将双曲操作整合到 Transformer 模块中，但可能仅限于部分组件，或者未深入研究训练动力学（如梯度稳定性）。</li>
<li>现有的双曲注意力机制中的聚合方法（如质心平均）在双曲空间中可能引入失真，尤其是在处理大值时。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>研究假设</strong>：</p>
<ul>
<li>将双曲几何的优势（特别是其表示层级结构的能力）融入 ViT 的核心架构，可以提升其在图像分类任务上的性能。</li>
<li>在双曲空间中采用更适合其几何特性的聚合方法（如指数映射聚合），可以克服现有方法的局限，实现更稳定、更准确的特征表示。</li>
<li>双曲 Transformer 在训练过程中可能表现出更好的梯度稳定性，从而降低对超参数（如预热策略）的敏感性。</li>
</ul>
</li>
</ul>
<h3 id="3_2">3. 方法设计详解</h3>
<p><strong>流程总结：</strong></p>
<p>HexFormer 的核心思想是将 Vision Transformer 的各个组件（如 Patch Embedding, Attention, MLP, Normalization, Classification Head）置于双曲空间（具体采用 Lorentz 模型）中进行计算，并引入一种新的指数映射聚合机制来改进注意力机制中的特征聚合方式。论文提出了两种主要变体：</p>
<ol>
<li><strong>HexFormer (纯双曲 ViT)</strong>：整个模型（编码器和分类头）都在双曲空间中操作。</li>
<li><strong>HexFormer-Hybrid (混合 ViT)</strong>：双曲编码器提取特征，但使用一个标准的欧几里得线性分类头进行最终分类。</li>
</ol>
<p><strong>详细流程：</strong></p>
<ol>
<li>
<p><strong>Patch Embedding 和 Positional Embedding (双曲化)</strong>：</p>
<ul>
<li><strong>Patch Embedding</strong>：输入图像被分割成 patches，然后通过一个 LorentzFC (Lorentz Fully Connected) 层将其投影到 Minkowski 空间，再通过一个 LorentzFC 层将其转换为双曲空间中的点。</li>
<li><strong>Positional Embedding</strong>：位置信息被添加到 patch 嵌入的空间分量中。</li>
<li><strong>Classification Token</strong>：一个特殊的分类 token 被添加到序列的开头，并初始化在双曲流形上。</li>
<li><strong>Lorentz Batch Normalization (LBN)</strong>：在每个 Transformer 块中应用 LBN，以确保特征保持在 Lorentz 流形上，从而维持双曲几何的约束。</li>
</ul>
</li>
<li>
<p><strong>Transformer Encoder (双曲化)</strong>：</p>
<ul>
<li><strong>Multi-Head Attention (MHA)</strong>：这是 HexFormer 的核心创新之一。<ul>
<li><strong>Query, Key, Value 投影</strong>：Q, K, V 通过 LorentzFC 层从双曲空间投影到切空间。</li>
<li><strong>Score Computation</strong>：注意力分数基于查询 (Q) 和键 (K) 之间的<strong>平方洛伦兹距离</strong>计算，而不是欧几里得点积。距离公式为 <script type="math/tex">d^2(x, y) = -2k - 2(x,y)_c</script>，其中 <script type="math/tex">k</script> 是曲率常数，<script type="math/tex">(x,y)_c</script> 是洛伦兹内积。</li>
<li><strong>Aggregation (Exponential Map Aggregation - ExpAgg)</strong>：<ul>
<li><strong>值 (Value) 映射到切空间</strong>：将 Value 向量从双曲流形映射到原点的切空间，使用对数映射：<script type="math/tex">V_{tan} = \log_0(V)</script>。</li>
<li><strong>切空间加权求和</strong>：使用 softmax 计算出的注意力权重（作为欧几里得标量）对切空间中的 Value 向量进行加权求和：<script type="math/tex">u = \sum \alpha_i V_{tan,i}</script>。</li>
<li><strong>映射回双曲空间</strong>：将加权求和的结果通过指数映射映射回双曲流形：<script type="math/tex">h_{agg} = \exp_0(u)</script>。</li>
<li><strong>动机</strong>：这种方法避免了在双曲空间中直接进行质心平均可能带来的失真和数值不稳定性问题，同时利用了指数映射将切空间的操作安全地映射回流形。</li>
</ul>
</li>
<li><strong>Scale 和 Temperature</strong>：引入一个可学习的温度参数 <script type="math/tex">\tau</script> 来调整 softmax 的尺度，以获得更好的性能。</li>
</ul>
</li>
<li><strong>Feed-Forward Network (FFN)</strong>：使用双曲的 LorentzFC 层（基于 HyboNet 的工作），确保其操作在双曲空间中。</li>
<li><strong>Normalization</strong>：使用双曲归一化层（如 LBN）。</li>
</ul>
</li>
<li>
<p><strong>Classification Head</strong>：</p>
<ul>
<li><strong>HexFormer</strong>：使用 Lorentz MLR (Multinomial Logistic Regression) 分类器，它将多项逻辑回归推广到双曲流形，通过测量嵌入与类定义超平面的双曲距离来计算 logits。</li>
<li><strong>HexFormer-Hybrid</strong>：使用标准的欧几里得线性分类器，作用于 CLS token 的欧几里得投影。</li>
</ul>
</li>
</ol>
<p><strong>关键公式/算法解释：</strong></p>
<ul>
<li><strong>Lorentz 模型</strong>：论文采用 Lorentz 模型来表示双曲空间 <script type="math/tex">L^n := \{x \in \mathbb{R}^{n+1} | (x,x)_c = 1/K, x_t > 0\}</script>，其中 <script type="math/tex">K < 0</script> 是负曲率。这种模型在深度学习中常用，因为它具有数值稳定性和闭式形式的距离、指数映射和对数映射。</li>
<li><strong>指数映射 (Exponential Map)</strong>：<script type="math/tex">\exp_x(z) = \cosh(\alpha) x + \sinh(\alpha) \frac{z}{\|z\|_c}</script>，其中 <script type="math/tex">\alpha = \sqrt{-K} \|z\|_c</script>。它将切空间中的向量 <script type="math/tex">z</script> 映射到流形上的点。</li>
<li><strong>对数映射 (Logarithmic Map)</strong>：<script type="math/tex">\log_x(y) = \frac{\cosh^{-1}(\beta)}{\sqrt{\beta^2 - 1}} (y - \beta x)</script>，其中 <script type="math/tex">\beta = K(x, y)_c</script>。它是指数映射的逆，将流形上的点 <script type="math/tex">y</script> 映射到点 <script type="math/tex">x</script> 的切空间。</li>
<li><strong>平方洛伦兹距离</strong>：<script type="math/tex">d^2(x, y) = -2k - 2(x,y)_c</script>。这是计算注意力分数的基础，它直接利用了双曲空间的几何特性。</li>
<li><strong>指数映射聚合 (ExpAgg)</strong>：<ol>
<li>将 Value 向量 <script type="math/tex">V</script> 映射到原点切空间：<script type="math/tex">V_{tan} = \log_0(V)</script>。</li>
<li>在切空间中进行加权求和：<script type="math/tex">u = \sum \alpha_i V_{tan,i}</script>。</li>
<li>将结果映射回流形：<script type="math/tex">h_{agg} = \exp_0(u)</script>。
这种方法确保了聚合操作在双曲几何上是有效的，并避免了直接在流形上进行质心平均可能带来的数值问题。</li>
</ol>
</li>
</ul>
<h3 id="4_2">4. 方法对比分析</h3>
<ul>
<li>
<p><strong>本质区别</strong>：</p>
<ul>
<li><strong>几何空间</strong>：HexFormer 完全或部分地将计算置于双曲空间（Lorentz 模型），而传统 ViT 在欧几里得空间。</li>
<li><strong>注意力聚合</strong>：HexFormer 引入了指数映射聚合 (ExpAgg)，而大多数欧几里得 ViT 使用点积注意力，双曲 ViT 的其他方法可能使用质心平均或直接在切空间进行聚合。ExpAgg 是在切空间进行加权求和后，再映射回流形，这是一种更精细的聚合方式。</li>
<li><strong>分类头</strong>：HexFormer 使用双曲分类器（Lorentz MLR），HexFormer-Hybrid 则结合了双曲编码器和欧几里得分类器。</li>
</ul>
</li>
<li>
<p><strong>创新贡献</strong>：</p>
<ul>
<li><strong>HexFormer 架构</strong>：首次将双曲几何（Lorentz 模型）和指数映射聚合机制完整地集成到 ViT 的注意力机制中，形成一个端到端的双曲 ViT。</li>
<li><strong>指数映射聚合 (ExpAgg)</strong>：提出了一种新的注意力聚合方法，解决了双曲空间中直接质心平均的数值不稳定性问题，并提高了准确性。</li>
<li><strong>HexFormer-Hybrid</strong>：提出了一种混合模型，结合了双曲编码器的强大表示能力和欧几里得分类器的简单高效性，取得了最佳性能。</li>
<li><strong>梯度稳定性分析</strong>：深入研究了双曲 ViT 的训练动力学，证明了其在梯度稳定性、对预热策略的鲁棒性方面优于欧几里得 ViT。</li>
</ul>
</li>
<li>
<p><strong>适用场景</strong>：</p>
<ul>
<li><strong>层级结构数据</strong>：特别适用于图像分类任务，当图像数据本身包含明显的层级或关系结构时（如 ImageNet 的类别层级），HexFormer 的优势会更明显。</li>
<li><strong>需要鲁棒训练的场景</strong>：双曲模型对超参数（如预热）的敏感性较低，适合需要简化超参数调优的场景。</li>
</ul>
</li>
</ul>
<h3 id="5_2">5. 实验分析</h3>
<ul>
<li>
<p><strong>验证方法</strong>：</p>
<ul>
<li><strong>数据集</strong>：在 CIFAR-10, CIFAR-100, Tiny-ImageNet 等数据集上进行了评估。</li>
<li><strong>模型对比</strong>：与 Euclidean ViT, HVT, LViT 等基线模型进行比较。</li>
<li><strong>模型变体</strong>：评估了 HexFormer 和 HexFormer-Hybrid 的性能。</li>
<li><strong>ViT 规模</strong>：在 Tiny, Small, Base 等不同 ViT 规模下进行了实验。</li>
<li><strong>训练策略</strong>：分析了预热 (warmup) 策略对模型性能和梯度稳定性的影响。</li>
<li><strong>聚合策略</strong>：对比了质心聚合 (Centroid) 和指数映射聚合 (ExpAgg) 的效果。</li>
</ul>
</li>
<li>
<p><strong>关键结果</strong>：</p>
<ul>
<li><strong>准确率提升</strong>：HexFormer 和 HexFormer-Hybrid 在所有数据集上都一致优于 Euclidean ViT。</li>
<li><strong>HexFormer-Hybrid 最佳</strong>：HexFormer-Hybrid 取得了最高的整体准确率。</li>
<li><strong>效率和参数量</strong>：即使使用更少的参数（如 Tiny-ViT 变体），HexFormer 也能达到更高的准确率，显示了其效率。</li>
<li><strong>梯度稳定性</strong>：双曲模型（HexFormer, HexFormer-Hybrid）在没有预热的情况下，梯度分布更稳定，对预热策略的依赖性更小。</li>
<li><strong>ExpAgg 优势</strong>：指数映射聚合 (ExpAgg) 比质心聚合 (Centroid) 提供了更好的准确性和数值稳定性，避免了训练中的 NaN/inf 问题。</li>
<li><strong>加速收敛</strong>：在短训练周期下，双曲模型表现出更高的准确率，表明其学习速度更快。</li>
</ul>
</li>
<li>
<p><strong>优势场景</strong>：</p>
<ul>
<li><strong>层级数据集</strong>：在 CIFAR-10, CIFAR-100, Tiny-ImageNet 等具有层级结构的图像数据集上表现最佳。</li>
<li><strong>无预热训练</strong>：在不使用预热策略时，双曲模型表现出更强的鲁棒性。</li>
</ul>
</li>
<li>
<p><strong>局限性</strong>：</p>
<ul>
<li><strong>计算开销</strong>：双曲模型通常比欧几里得模型需要更多的计算资源（如运行时、内存）。</li>
<li><strong>数值不稳定性</strong>：虽然 ExpAgg 缓解了问题，但双曲计算本身仍可能存在数值不稳定性，尤其是在处理极端值时。</li>
<li><strong>对弱层级数据效果不明显</strong>：在 MNIST, Fashion-MNIST, SVHN 等弱层级或平坦结构的数据集上，双曲模型的优势不明显，甚至可能略逊于欧几里得模型。</li>
</ul>
</li>
</ul>
<h3 id="6_2">6. 实用指南</h3>
<ul>
<li><strong>开源情况</strong>：论文提供了代码链接：https://github.com/HayaAlyoussef/HexFormer.git。</li>
<li><strong>实现细节</strong>：<ul>
<li><strong>Lorentz 模型</strong>：需要正确实现 Lorentz 模型的点积、距离、指数映射和对数映射。</li>
<li><strong>LorentzFC 层</strong>：这是实现双曲操作的关键，需要确保其正确性，特别是处理时间分量和空间分量。</li>
<li><strong>ExpAgg 机制</strong>：在注意力模块中，需要正确实现将 Value 映射到切空间，进行加权求和，再映射回流形的过程。</li>
<li><strong>LBN</strong>：Lorentz Batch Normalization 的实现也很重要，以保持数据在流形上。</li>
<li><strong>超参数</strong>：虽然双曲模型对预热不敏感，但学习率、权重衰减等仍需仔细调整。论文 Appendix A.1 提供了详细的超参数设置。</li>
<li><strong>曲率选择</strong>：论文中固定使用 <script type="math/tex">K=-1</script>，但也可以尝试其他值或学习曲率。</li>
</ul>
</li>
<li><strong>迁移可能</strong>：<ul>
<li><strong>其他视觉任务</strong>：该方法的核心思想（双曲化 ViT 架构和 ExpAgg 注意力）可以迁移到其他视觉任务，如目标检测、语义分割等，前提是这些任务的数据也具有层级或关系结构。</li>
<li><strong>其他模态</strong>：双曲几何在文本和图数据上也有广泛应用。可以将 HexFormer 的双曲化思想和 ExpAgg 机制迁移到处理文本或图的 Transformer 模型中，例如，用于图神经网络或序列模型。</li>
<li><strong>混合模型</strong>：HexFormer-Hybrid 的思路（双曲编码器 + 欧几里得分类器）是一种通用的混合策略，可以用于其他需要结合双曲表示和简单分类器的任务。</li>
</ul>
</li>
</ul>
<h3 id="7_2">7. 总结</h3>
<ul>
<li><strong>核心思想</strong>：双曲几何与指数映射聚合增强视觉 Transformer。</li>
<li><strong>速记版 pipeline</strong>：<ol>
<li><strong>双曲化输入</strong>：图像块和位置信息映射到双曲空间。</li>
<li><strong>双曲注意力</strong>：使用双曲距离计算分数，指数映射聚合值向量。</li>
<li><strong>双曲编码器</strong>：Transformer 层在双曲空间中运行。</li>
<li><strong>混合/双曲分类</strong>：使用双曲或欧几里得分类器输出结果。</li>
</ol>
</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>HexFormer incorporates a novel attention mechanism based on exponential map aggregation, which yields more accurate and stable aggregated representations than standard centroid based averaging, showing that simpler approaches retain competitive merit.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.19849v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.19849v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.19821v1'></a></p>
<h2 id="query-guided-spatial-temporal-frequency-interaction-for-music-audio-visual-question-answering"><a href="https://arxiv.org/abs/2601.19821v1">Query-Guided Spatial-Temporal-Frequency Interaction for Music Audio-Visual Question Answering</a></h2>
<p><strong>Authors:</strong> Kun Li, Michael Ying Yang, Sami Sebastian Brandt</p>
<p><strong>Published:</strong> 2026-01-27</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Audio--Visual Question Answering (AVQA) is a challenging multimodal task that requires jointly reasoning over audio, visual, and textual information in a given video to answer natural language questions. Inspired by recent advances in Video QA, many existing AVQA approaches primarily focus on visual information processing, leveraging pre-trained models to extract object-level and motion-level representations. However, in those methods, the audio input is primarily treated as complementary to video analysis, and the textual question information contributes minimally to audio--visual understanding, as it is typically integrated only in the final stages of reasoning. To address these limitations, we propose a novel Query-guided Spatial--Temporal--Frequency (QSTar) interaction method, which effectively incorporates question-guided clues and exploits the distinctive frequency-domain characteristics of audio signals, alongside spatial and temporal perception, to enhance audio--visual understanding. Furthermore, we introduce a Query Context Reasoning (QCR) block inspired by prompting, which guides the model to focus more precisely on semantically relevant audio and visual features. Extensive experiments conducted on several AVQA benchmarks demonstrate the effectiveness of our proposed method, achieving significant performance improvements over existing Audio QA, Visual QA, Video QA, and AVQA approaches. The code and pretrained models will be released after publication.</p>
<p><strong>Analysis:</strong></p>
<p>好的，我将以一名AI领域高水平研究生的视角，深入分析这篇论文的方法部分，并遵循您提供的分析框架。</p>
<hr />
<h2 id="_1">论文方法分析与总结</h2>
<h3 id="1_3">1. 摘要翻译</h3>
<p><strong>论文题目：</strong> 查询引导的空间-时间-频率交互用于音乐音频-视觉问答</p>
<p><strong>摘要翻译：</strong>
音频-视觉问答（AVQA）是一项挑战性的多模态任务，要求对给定视频中的音频、视觉和文本信息进行联合推理，以回答自然语言问题。受近期视频问答研究进展的启发，许多现有的AVQA方法主要关注视觉信息处理，利用预训练模型提取对象级和运动级表示。然而，在这些方法中，音频输入主要被视为视频分析的补充，而文本问题信息对音频-视觉理解的贡献很小，通常只在推理的最后阶段进行整合。为了解决这些局限性，我们提出了一种新颖的查询引导空间-时间-频率（QSTar）交互方法，该方法有效地整合了问题引导的线索，并利用音频信号独特的频域特征，以及空间和时间感知，来增强音频-视觉理解。此外，我们引入了一个受提示（prompting）启发的查询上下文推理（QCR）模块，该模块引导模型更精确地关注语义相关的音频和视觉特征。在几个AVQA基准上的广泛实验表明，我们提出的方法非常有效，在现有音频问答、视觉问答、视频问答和AVQA方法上取得了显著的性能提升。代码和预训练模型将在发布后公开。</p>
<h3 id="2_3">2. 方法动机分析</h3>
<ul>
<li>
<p><strong>驱动力</strong>：
    作者提出QSTar方法的核心驱动力在于解决现有音频-视觉问答（AVQA）方法在整合多模态信息，特别是音频信息方面存在的不足。具体来说，现有方法往往过度依赖视觉信息，将音频视为次要信息，并且在整合问题信息时不够精细，通常只在后期进行融合，导致信息利用不充分，推理能力受限。尤其是在音乐AVQA场景下，音频的频域特性（如音色、泛音）对于区分乐器和理解演奏至关重要，而现有方法未能充分挖掘这些信息。</p>
</li>
<li>
<p><strong>现有方法痛点</strong>：</p>
<ol>
<li><strong>视觉信息主导，音频信息被边缘化</strong>：现有AVQA方法主要依赖预训练模型提取视觉特征（对象级、运动级），音频信息仅作为补充，未被充分利用其独特价值。</li>
<li><strong>问题信息整合滞后且粗糙</strong>：问题信息通常在推理的最后阶段才通过简单的乘法等操作融入，限制了其对早期特征提取和对齐的指导作用。</li>
<li><strong>音频频域特性未被充分利用</strong>：在音乐场景中，乐器的音色、泛音等频域特征是区分乐器的关键，但现有方法（如基于VGGish的特征提取）未能充分挖掘这些信息。</li>
<li><strong>跨模态交互维度不足</strong>：现有方法主要关注空间和时间维度的交互，而忽略了音频的频率维度，这在区分具有相似时域或空间特征但频域特征不同的乐器时尤为重要。</li>
<li><strong>缺乏精细化的、问题导向的特征提取</strong>：模型倾向于提取全局或通用的音频视觉特征，而不是根据具体问题聚焦于最相关的线索。</li>
</ol>
</li>
<li>
<p><strong>研究假设</strong>：</p>
<ol>
<li>通过将问题信息（查询）从早期阶段就引入到音频和视觉特征的提取和交互过程中，可以实现更精细、更具针对性的多模态特征对齐。</li>
<li>充分利用音频的<strong>空间、时间、频率</strong>三个维度，并将其与视觉信息进行交互，能够更全面地理解音乐场景，尤其是在区分具有细微差别的乐器时。</li>
<li>通过引入一个专门的<strong>查询上下文推理（QCR）模块</strong>，利用提示（prompting）机制，可以进一步引导模型聚焦于问题相关的关键线索，从而提升推理的准确性。</li>
</ol>
</li>
</ul>
<h3 id="3_3">3. 方法设计详解</h3>
<p><strong>流程总结：</strong></p>
<p>QSTar方法的核心在于<strong>查询引导（Query-Guided）</strong>贯穿整个流程，并强调<strong>空间-时间-频率（Spatial-Temporal-Frequency）</strong>的跨模态交互。整体流程可以分解为以下几个主要模块：</p>
<ol>
<li>
<p><strong>输入表示 (Input Representation)</strong>：</p>
<ul>
<li><strong>视频分割</strong>：将输入视频分割成 <script type="math/tex">T</script> 个时长为1秒的非重叠音频和视觉片段。</li>
<li><strong>视觉特征提取</strong>：<ul>
<li>使用预训练的CLIP-ViT-L/14模型，对每个视觉片段提取<strong>帧级</strong>和<strong>块级</strong>（patch-level）特征。</li>
<li>块级特征通过Token Merging (ToMe)进行压缩，得到 <script type="math/tex">M'</script> 个块级token。</li>
<li>最终得到帧级特征 <script type="math/tex">F_v \in \mathbb{R}^{T \times D}</script> 和块级特征 <script type="math/tex">F_p \in \mathbb{R}^{T \times M' \times D}</script>。</li>
</ul>
</li>
<li><strong>音频特征提取</strong>：<ul>
<li>使用预训练的2D CNN VGGish模型（在AudioSet上预训练），提取每个音频片段的特征，得到 <script type="math/tex">F_a \in \mathbb{R}^{T \times D}</script>。</li>
</ul>
</li>
<li><strong>文本特征提取</strong>：<ul>
<li>使用预训练的CLIP文本编码器，对输入问题进行编码，得到<strong>句子级</strong>特征 <script type="math/tex">F_{sentence} \in \mathbb{R}^{D}</script> 和<strong>词级</strong>特征 <script type="math/tex">F_w \in \mathbb{R}^{N \times D}</script>（<script type="math/tex">N</script>为问题token数）。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>查询引导多模态相关性模块 (Query-Guided Multimodal Correlation, QGMC)</strong>：</p>
<ul>
<li><strong>动机</strong>：从早期阶段就利用问题信息来精炼音频和视觉特征，使其与问题语义对齐。</li>
<li><strong>设计</strong>：<ul>
<li><strong>自增强 (Self-Enhancing)</strong>：对视觉 (<script type="math/tex">F_v</script>)、音频 (<script type="math/tex">F_a</script>) 和词级文本 (<script type="math/tex">F_w</script>) 特征分别应用多头自注意力（SA）机制，增强模态内部的表示。</li>
<li><strong>跨模态捕捉 (Cross-modal Capturing)</strong>：使用自增强后的词级文本特征 (<script type="math/tex">SA(F_w)</script>) 作为查询（Query），将自增强后的视觉特征 (<script type="math/tex">SA(F_v)</script>) 和音频特征 (<script type="math/tex">SA(F_a)</script>) 作为键（Key）和值（Value），通过多头交叉注意力（CA）机制，捕捉与问题相关的视觉和音频信息。<ul>
<li>
<script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script> (这里论文中公式1写的是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，但根据上下文和图2，更可能是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但图2显示是文本作为Q，视觉和音频作为K,V，但公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，这里似乎有笔误，根据图2的流程，更可能是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，根据图2，更准确的理解是 <script type="math/tex">F_{qv} = CA(SA(F_w), SA(F_v), SA(F_a))</script>，即用文本作为Q，视觉和音频作为K,V。但论文公式1是 $F_{qv} = CA(SA(F_w),</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these limitations, we propose a novel Query-guided Spatial--Temporal--Frequency (QSTar) interaction method, which effectively incorporates question-guided clues and exploits the distinctive frequency-domain characteristics of audio signals, alongside spatial and temporal perception, to enhance audio--visual understanding.</li>
<li>Furthermore, we introduce a Query Context Reasoning (QCR) block inspired by prompting, which guides the model to focus more precisely on semantically relevant audio and visual features.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.19821v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.19821v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.19798v1'></a></p>
<h2 id="youtu-vl-unleashing-visual-potential-via-unified-vision-language-supervision"><a href="https://arxiv.org/abs/2601.19798v1">Youtu-VL: Unleashing Visual Potential via Unified Vision-Language Supervision</a></h2>
<p><strong>Authors:</strong> Zhixiang Wei, Yi Li, Zhehan Kan, Xinghua Jiang, Zuwei Long, Shifeng Liu, Hongze Shen, Wei Liu, Xiaoyu Tan, Haojia Lin, Yubo Zhu, Qianyu Li, Di Yin, Haoyu Cao, Weibo Gu, Xin Li, Yinsong Liu, Deqiang Jiang, Xing Sun, Yunsheng Wu, Mingkong Tang, Shuangyin Liu, Lexiang Tang, Haodong Lin, Junru Lu, Jiarui Qin, Lingfeng Qiao, Ruizhi Qiao, Bo Ke, Jianfeng He, Ke Li, Yangning Li, Yunhang Shen, Mengdan Zhang, Peixian Chen, Kun Yin, Bing Liu, Yunfei Wu, Huang Chen, Zhongpeng Cai, Xiaotian Li</p>
<p><strong>Published:</strong> 2026-01-27</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Despite the significant advancements represented by Vision-Language Models (VLMs), current architectures often exhibit limitations in retaining fine-grained visual information, leading to coarse-grained multimodal comprehension. We attribute this deficiency to a suboptimal training paradigm inherent in prevailing VLMs, which exhibits a text-dominant optimization bias by conceptualizing visual signals merely as passive conditional inputs rather than supervisory targets. To mitigate this, we introduce Youtu-VL, a framework leveraging the Vision-Language Unified Autoregressive Supervision (VLUAS) paradigm, which fundamentally shifts the optimization objective from <code>vision-as-input'' to</code>vision-as-target.'' By integrating visual tokens directly into the prediction stream, Youtu-VL applies unified autoregressive supervision to both visual details and linguistic content. Furthermore, we extend this paradigm to encompass vision-centric tasks, enabling a standard VLM to perform vision-centric tasks without task-specific additions. Extensive empirical evaluations demonstrate that Youtu-VL achieves competitive performance on both general multimodal tasks and vision-centric tasks, establishing a robust foundation for the development of comprehensive generalist visual agents.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文分析：Youtu-VL: Unleashing Visual Potential via Unified Vision-Language Supervision</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话的简洁总结）</strong></p>
<p>本论文提出了一种名为 Youtu-VL 的新框架，通过引入“视觉-语言统一自回归监督”（VLUAS）范式，解决了现有视觉语言模型（VLM）在细粒度视觉信息保留方面的不足。该范式将视觉信号视为预测目标而非仅仅是输入条件，实现了对视觉细节和语言内容的统一自回归监督，从而提升了多模态理解的精细度，并使通用 VLM 能够直接处理视觉中心任务。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>Youtu-VL 的核心创新在于其提出的 <strong>视觉-语言统一自回归监督（VLUAS）范式</strong>。其关键点在于：</p>
<ul>
<li><strong>“视觉即目标”（Vision-as-Target）的训练理念：</strong> 传统 VLM 通常将图像视为文本生成的条件输入，优化目标偏向于文本生成。VLUAS 则将视觉信息（以视觉 token 的形式）也纳入到模型的预测序列中，使其成为模型需要预测的目标之一。这意味着模型不仅要学会生成连贯的文本，还要学会“重构”或“预测”输入图像的视觉细节。</li>
<li><strong>统一的自回归监督：</strong> VLUAS 对视觉 token 和语言 token 应用相同的自回归预测机制。这使得模型能够学习到视觉和语言之间的深层、细粒度的关联，因为模型在预测下一个 token 时，需要同时考虑之前的视觉和语言上下文。</li>
<li><strong>通用 VLM 的视觉中心任务能力：</strong> 通过将视觉信息作为预测目标，Youtu-VL 使得一个标准的 VLM 能够直接处理原本需要专门设计的视觉中心任务（如图像描述、视觉问答等），而无需额外的任务特定模块。这体现了其“通用性”的潜力。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<p>Youtu-VL 的提出可能对计算机视觉和多模态学习领域产生深远影响：</p>
<ul>
<li><strong>提升 VLM 的细粒度理解能力：</strong> 解决了当前 VLM 在理解图像细节方面的短板，使得模型能够生成更准确、更丰富的图像描述，并能回答更具挑战性的视觉问题。</li>
<li><strong>推动通用视觉智能代理的发展：</strong> 通过使通用 VLM 能够直接处理视觉中心任务，Youtu-VL 为构建能够理解和操作视觉世界的通用人工智能代理奠定了基础。这可能加速多模态 AI 在各种应用中的落地。</li>
<li><strong>简化 VLM 的训练和部署：</strong> 统一的监督范式和无需任务特定模块的设计，有望简化 VLM 的训练流程和模型架构，降低开发成本。</li>
<li><strong>重新思考 VLM 的训练范式：</strong> VLUAS 范式挑战了“视觉作为被动输入”的传统观念，为未来 VLM 的设计提供了新的思路和方向。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>图像描述生成（Image Captioning）：</strong> 能够生成更具描述性、更准确的图像描述，捕捉图像中的细微之处。</li>
<li><strong>视觉问答（Visual Question Answering, VQA）：</strong> 能够理解更复杂的视觉场景，并回答需要深入分析图像细节的问题。</li>
<li><strong>视觉推理（Visual Reasoning）：</strong> 提升模型在理解图像之间关系、进行逻辑推理方面的能力。</li>
<li><strong>多模态检索（Multimodal Retrieval）：</strong> 提高文本到图像或图像到文本检索的准确性和相关性。</li>
<li><strong>视觉内容生成（Visual Content Generation）：</strong> 例如，更精细的图像编辑、风格迁移等，能够更好地理解和操纵视觉元素。</li>
<li><strong>机器人视觉（Robotics Vision）：</strong> 增强机器人对环境的理解能力，实现更精细的操作和交互。</li>
<li><strong>自动驾驶（Autonomous Driving）：</strong> 提升对复杂交通场景中物体细节的识别和理解能力。</li>
<li><strong>医疗影像分析（Medical Imaging Analysis）：</strong> 辅助医生进行更精细的病灶识别和诊断。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<p>尽管摘要描绘了 Youtu-VL 的强大潜力，但仍可推断出一些潜在的局限性：</p>
<ul>
<li><strong>计算成本和效率：</strong> 将视觉 token 纳入预测序列并进行自回归监督，可能会显著增加模型的计算复杂度和训练时间，尤其是在处理高分辨率图像时。摘要中提到“extensive empirical evaluations”，暗示了其在性能上的提升，但并未直接提及效率的改进。</li>
<li><strong>视觉 token 的表示和量化：</strong> 如何有效地将连续的视觉信息转化为离散的视觉 token，以及如何选择合适的视觉 token 数量和表示方式，是实现 VLUAS 范式成功的关键，也可能是一个挑战。摘要中提到“integrating visual tokens directly into the prediction stream”，但具体实现细节和效果未知。</li>
<li><strong>对大规模、多样化数据集的需求：</strong> 这种新的训练范式可能需要更大规模、更具多样性的视觉-语言配对数据集来充分发挥其潜力，以确保模型能够学习到广泛的视觉和语言知识。</li>
<li><strong>泛化到极端视觉任务的挑战：</strong> 虽然摘要提到“vision-centric tasks”，但对于一些高度专业化或对精度要求极高的视觉任务，仅凭 VLUAS 范式是否能达到 SOTA 的纯视觉模型性能，仍需进一步验证。</li>
<li><strong>“文本主导”偏见的完全消除：</strong> 尽管论文声称“fundamentally shifts the optimization objective”，但完全消除训练过程中可能存在的“文本主导”偏见是一个持续的挑战，尤其是在处理某些文本信息量远大于视觉信息的情况下。</li>
</ul>
<p>总而言之，Youtu-VL 提出的 VLUAS 范式是一个非常有前景的创新，它通过改变 VLM 的训练目标，有望显著提升模型对视觉信息的理解深度和广度，并为构建更通用的视觉智能代理开辟了新的道路。然而，其在计算效率、表示方法和对数据的需求等方面可能面临挑战，需要进一步的研究和验证。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To mitigate this, we introduce Youtu-VL, a framework leveraging the Vision-Language Unified Autoregressive Supervision (VLUAS) paradigm, which fundamentally shifts the optimization objective from <code>vision-as-input'' to</code>vision-as-target.'' By integrating visual tokens directly into the prediction stream, Youtu-VL applies unified autoregressive supervision to both visual details and linguistic content.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.19798v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.19798v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.19785v1'></a></p>
<h2 id="geodiff3d-self-supervised-3d-scene-generation-with-geometry-constrained-2d-diffusion-guidance"><a href="https://arxiv.org/abs/2601.19785v1">GeoDiff3D: Self-Supervised 3D Scene Generation with Geometry-Constrained 2D Diffusion Guidance</a></h2>
<p><strong>Authors:</strong> Haozhi Zhu, Miaomiao Zhao, Dingyao Liu, Runze Tian, Yan Zhang, Jie Guo, Fenggen Yu</p>
<p><strong>Published:</strong> 2026-01-27</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>3D scene generation is a core technology for gaming, film/VFX, and VR/AR. Growing demand for rapid iteration, high-fidelity detail, and accessible content creation has further increased interest in this area. Existing methods broadly follow two paradigms - indirect 2D-to-3D reconstruction and direct 3D generation - but both are limited by weak structural modeling and heavy reliance on large-scale ground-truth supervision, often producing structural artifacts, geometric inconsistencies, and degraded high-frequency details in complex scenes. We propose GeoDiff3D, an efficient self-supervised framework that uses coarse geometry as a structural anchor and a geometry-constrained 2D diffusion model to provide texture-rich reference images. Importantly, GeoDiff3D does not require strict multi-view consistency of the diffusion-generated references and remains robust to the resulting noisy, inconsistent guidance. We further introduce voxel-aligned 3D feature aggregation and dual self-supervision to maintain scene coherence and fine details while substantially reducing dependence on labeled data. GeoDiff3D also trains with low computational cost and enables fast, high-quality 3D scene generation. Extensive experiments on challenging scenes show improved generalization and generation quality over existing baselines, offering a practical solution for accessible and efficient 3D scene construction.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：GeoDiff3D: Self-Supervised 3D Scene Generation with Geometry-Constrained 2D Diffusion Guidance</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话的简洁总结）</strong></p>
<p>该论文提出了一种名为 GeoDiff3D 的高效自监督 3D 场景生成框架。其核心贡献在于利用粗糙几何作为结构锚点，并结合几何约束的 2D 扩散模型生成纹理丰富的参考图像，从而克服了现有方法在结构建模和监督数据依赖方面的局限性。GeoDiff3D 能够生成具有高保真细节和良好场景连贯性的 3D 内容，同时显著降低了对标注数据的需求和计算成本。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>GeoDiff3D 的关键创新在于其独特的方法论组合：</p>
<ul>
<li><strong>几何约束的 2D 扩散模型作为纹理引导：</strong> 这是最核心的创新点。不同于直接进行 3D 生成或依赖严格多视图一致性的方法，GeoDiff3D 利用 2D 扩散模型生成参考图像，并引入“几何约束”来指导扩散过程。这意味着扩散模型生成的图像虽然不要求严格的多视图一致性（这通常是生成 3D 内容的难点），但会受到粗糙几何信息的引导，从而在纹理生成时融入结构信息。这种方法巧妙地利用了 2D 扩散模型强大的纹理生成能力，同时通过几何约束来解决 3D 生成中的结构问题。</li>
<li><strong>粗糙几何作为结构锚点：</strong> 论文明确指出使用“粗糙几何作为结构锚点”。这表明该框架并不需要高精度的 3D 模型作为输入，而是利用相对简单的几何信息（例如，点云、体素网格的粗略表示，或者甚至是由其他方法生成的低保真几何）来约束整个场景的结构。这大大降低了对输入数据的要求，也使得框架更具通用性。</li>
<li><strong>体素对齐的 3D 特征聚合（Voxel-aligned 3D feature aggregation）：</strong> 这一技术旨在有效地将从 2D 参考图像中提取的特征与 3D 体素结构对齐。通过体素对齐，可以更精确地将纹理信息映射到 3D 空间中，从而在保持场景连贯性和细节的同时，实现高效的 3D 特征表示。</li>
<li><strong>双重自监督学习（Dual self-supervision）：</strong> 论文强调了“双重自监督”机制。这表明该框架通过设计多种自监督任务来学习 3D 表示和生成过程，从而减少对人工标注数据的依赖。这种方式对于处理复杂场景和大规模数据集尤为重要，因为获取高质量的 3D 标注数据成本高昂且困难。</li>
<li><strong>对扩散生成参考图像的噪声和不一致性鲁棒：</strong> 论文特别提到 GeoDiff3D 对扩散模型生成的参考图像的“噪声、不一致性”保持鲁棒。这进一步凸显了该方法设计的巧妙之处，它能够容忍 2D 扩散模型在生成过程中可能出现的非完美之处，并从中提取有用的信息。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<p>GeoDiff3D 的出现可能对 3D 场景生成领域产生显著影响：</p>
<ul>
<li><strong>降低 3D 内容创作门槛：</strong> 通过减少对大量标注数据的依赖和降低计算成本，GeoDiff3D 有望使更多开发者和艺术家能够轻松地创建高质量的 3D 内容，从而 democratize 3D 内容创作。</li>
<li><strong>提升生成质量和效率：</strong> 该框架在结构建模和细节表现上取得了突破，有望生成更逼真、更具细节的 3D 场景，同时实现更快的生成速度。</li>
<li><strong>推动自监督学习在 3D 生成中的应用：</strong> GeoDiff3D 的成功将进一步证明自监督学习在复杂 3D 生成任务中的潜力，鼓励更多研究者探索无监督或弱监督的 3D 生成方法。</li>
<li><strong>促进游戏、影视和 XR 行业的创新：</strong> 更高效、更高质量的 3D 场景生成能力将直接赋能游戏开发（快速迭代场景）、影视特效（降低制作成本和时间）以及 VR/AR 应用（构建沉浸式体验）。</li>
<li><strong>为多模态生成提供新思路：</strong> 将 2D 扩散模型的强大生成能力与 3D 几何约束相结合，为跨模态（2D 到 3D）的生成任务提供了新的视角和方法。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>游戏开发：</strong> 快速生成游戏场景、角色环境、道具等，加速游戏开发周期。</li>
<li><strong>影视特效 (VFX)：</strong> 制作电影、电视剧中的虚拟场景、背景，降低后期制作成本。</li>
<li><strong>虚拟现实 (VR) 和增强现实 (AR)：</strong> 构建逼真、沉浸式的虚拟环境和交互式 AR 体验。</li>
<li><strong>数字孪生：</strong> 生成现实世界场景的 3D 模型，用于模拟、分析和可视化。</li>
<li><strong>建筑可视化：</strong> 快速生成建筑设计方案的 3D 模型和渲染图。</li>
<li><strong>机器人导航和感知：</strong> 生成逼真的模拟环境，用于训练和测试机器人感知和导航算法。</li>
<li><strong>内容生成平台：</strong> 为用户提供易于使用的 3D 内容生成工具。</li>
<li><strong>3D 艺术和设计：</strong> 艺术家和设计师可以利用该技术快速实现创意构想。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<p>尽管摘要描绘了一个非常有前景的框架，但仍可以从摘要中推断出一些潜在的局限性：</p>
<ul>
<li><strong>“粗糙几何”的定义和获取：</strong> 摘要中提到使用“粗糙几何作为结构锚点”，但具体如何定义、获取和表示这种“粗糙几何”是关键。如果获取粗糙几何本身就具有挑战性，或者其质量对最终生成结果影响很大，那么这可能是一个限制。</li>
<li><strong>对扩散模型生成参考图像的鲁棒性上限：</strong> 虽然论文声称对噪声和不一致性鲁棒，但这种鲁棒性是有上限的。如果扩散模型生成的参考图像质量极差或与几何约束严重冲突，可能仍然会影响最终的 3D 生成效果。</li>
<li><strong>“自监督”的有效性：</strong> “双重自监督”的有效性取决于具体的自监督任务设计。如果设计的任务不够强大或无法捕捉所有必要的 3D 场景信息，那么对标注数据的依赖可能仍然存在，只是程度降低。</li>
<li><strong>复杂场景的细节表现：</strong> 摘要提到“复杂场景”，但对于极其复杂、精细的几何结构或纹理，该方法能否完全捕捉并生成高质量的细节仍需通过实验验证。例如，非常精细的织物纹理、复杂的机械结构等。</li>
<li><strong>计算成本的“低”是相对的：</strong> 摘要提到“低计算成本”和“快速生成”，但“低”是相对于现有方法而言的。对于某些资源受限的设备或实时应用，其计算需求可能仍然较高。</li>
<li><strong>“几何约束”的具体实现：</strong> 摘要没有详细说明“几何约束”是如何施加到 2D 扩散模型上的。这可能涉及到复杂的数学模型或损失函数设计，其实现难度和效果会影响整体框架的性能。</li>
</ul>
<p>总而言之，GeoDiff3D 是一项令人兴奋的研究，它通过巧妙地结合 2D 扩散模型的生成能力和几何约束，为解决 3D 场景生成中的关键挑战提供了新的思路。其自监督和高效的特性使其在实际应用中具有巨大的潜力。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose GeoDiff3D, an efficient self-supervised framework that uses coarse geometry as a structural anchor and a geometry-constrained 2D diffusion model to provide texture-rich reference images.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.19785v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.19785v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.19753v1'></a></p>
<h2 id="waterclear-gs-optical-aware-gaussian-splatting-for-underwater-reconstruction-and-restoration"><a href="https://arxiv.org/abs/2601.19753v1">WaterClear-GS: Optical-Aware Gaussian Splatting for Underwater Reconstruction and Restoration</a></h2>
<p><strong>Authors:</strong> Xinrui Zhang, Yufeng Wang, Shuangkang Fang, Zesheng Wang, Dacheng Qi, Wenrui Ding</p>
<p><strong>Published:</strong> 2026-01-27</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Underwater 3D reconstruction and appearance restoration are hindered by the complex optical properties of water, such as wavelength-dependent attenuation and scattering. Existing Neural Radiance Fields (NeRF)-based methods struggle with slow rendering speeds and suboptimal color restoration, while 3D Gaussian Splatting (3DGS) inherently lacks the capability to model complex volumetric scattering effects. To address these issues, we introduce WaterClear-GS, the first pure 3DGS-based framework that explicitly integrates underwater optical properties of local attenuation and scattering into Gaussian primitives, eliminating the need for an auxiliary medium network. Our method employs a dual-branch optimization strategy to ensure underwater photometric consistency while naturally recovering water-free appearances. This strategy is enhanced by depth-guided geometry regularization and perception-driven image loss, together with exposure constraints, spatially-adaptive regularization, and physically guided spectral regularization, which collectively enforce local 3D coherence and maintain natural visual perception. Experiments on standard benchmarks and our newly collected dataset demonstrate that WaterClear-GS achieves outstanding performance on both novel view synthesis (NVS) and underwater image restoration (UIR) tasks, while maintaining real-time rendering. The code will be available at https://buaaxrzhang.github.io/WaterClear-GS/.</p>
<p><strong>Analysis:</strong></p>
<p>作为一名计算机视觉和机器学习领域的专家，我对这篇论文摘要进行了深入分析。以下是我的评估：</p>
<p><strong>1. 论文的主要贡献（2-3句话的简洁总结）</strong></p>
<p>该论文提出了 WaterClear-GS，一个首创的、完全基于 3D 高斯溅射（3DGS）的框架，用于解决水下 3D 重建和外观恢复的挑战。它通过将水下光学特性（如衰减和散射）显式地集成到高斯基元中，克服了现有 NeRF 方法渲染速度慢和颜色恢复不佳的问题，以及传统 3DGS 无法处理复杂体积散射的局限性。WaterClear-GS 在新视角合成（NVS）和水下图像恢复（UIR）方面均取得了优异的性能，并实现了实时渲染。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>WaterClear-GS 的核心创新在于其<strong>将水下光学特性（局部衰减和散射）直接集成到 3DGS 的高斯基元中</strong>。这与以往方法需要额外的介质网络来模拟水下环境不同，实现了更高效和更直接的建模。具体来说，其方法论的关键点包括：</p>
<ul>
<li><strong>光学感知的高斯基元（Optical-Aware Gaussian Primitives）</strong>: 这是最核心的创新。将水下环境的光学特性（如波长依赖性衰减和散射）直接编码到每个高斯原子的属性中，使其能够自然地模拟光在水中的传播和交互。</li>
<li><strong>双分支优化策略（Dual-Branch Optimization Strategy）</strong>: 该策略旨在同时实现水下光度一致性（保证重建的准确性）和水下无关外观的恢复（即恢复物体在没有水影响下的真实外观）。这对于同时解决重建和恢复问题至关重要。</li>
<li><strong>多重正则化技术</strong>:<ul>
<li><strong>深度引导几何正则化（Depth-guided Geometry Regularization）</strong>: 利用深度信息来约束几何的准确性，提高重建的质量。</li>
<li><strong>感知驱动图像损失（Perception-driven Image Loss）</strong>: 引入感知损失，使恢复的图像在视觉上更自然、更符合人类的感知习惯。</li>
<li><strong>曝光约束（Exposure Constraints）</strong>: 确保恢复的图像具有合理的曝光度，避免过曝或欠曝。</li>
<li><strong>空间自适应正则化（Spatially-adaptive Regularization）</strong>: 根据图像的不同区域应用不同的正则化策略，以适应水下环境的复杂性。</li>
<li><strong>物理引导光谱正则化（Physically Guided Spectral Regularization）</strong>: 利用物理光学原理来约束光谱的恢复，提高颜色恢复的准确性。</li>
</ul>
</li>
</ul>
<p>这些技术共同作用，确保了局部 3D 的连贯性，并维持了自然视觉感知。</p>
<p><strong>3. 对该领域的潜在影响</strong></p>
<p>WaterClear-GS 的出现可能对以下方面产生重要影响：</p>
<ul>
<li><strong>推动 3DGS 在复杂环境下的应用</strong>: 3DGS 以其高效的渲染速度和高质量的重建能力在三维重建领域取得了巨大成功。WaterClear-GS 的工作表明，通过巧妙地集成物理光学模型，3DGS 可以被扩展到更具挑战性的场景，如水下环境。</li>
<li><strong>提升水下 3D 重建和图像恢复的性能</strong>: 该方法有望显著提高水下场景的重建精度和图像恢复的质量，为水下探测、科学研究、水下机器人导航等应用提供更可靠的技术支持。</li>
<li><strong>加速水下视觉研究</strong>: 通过提供一个高效且性能优越的框架，WaterClear-GS 可以加速水下视觉领域的研究进展，吸引更多研究者投入到相关问题的解决中。</li>
<li><strong>为其他受光学效应影响的场景提供借鉴</strong>: 该方法在处理水下光学衰减和散射的思路，也可能为处理其他受复杂光学效应影响的场景（如雾天、烟雾、特定材料的透射/反射等）提供新的思路和方法。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>水下机器人与自主导航</strong>: 精确的水下 3D 重建对于水下机器人的路径规划、避障和环境感知至关重要。</li>
<li><strong>水下考古与勘探</strong>: 能够清晰地重建水下遗迹和地质构造，有助于考古学家和地质学家进行研究和分析。</li>
<li><strong>水下生物学研究</strong>: 准确捕捉水下生物的形态和行为，有助于生物学家进行研究和监测。</li>
<li><strong>水下摄影与影视制作</strong>: 提高水下拍摄的图像质量，恢复真实色彩，为水下摄影和影视制作提供更好的素材。</li>
<li><strong>水下安全与监测</strong>: 用于水下基础设施的检查和维护，以及水下环境的监测。</li>
<li><strong>虚拟现实（VR）/增强现实（AR）在水下场景的应用</strong>: 创建更逼真、更具沉浸感的水下虚拟体验。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<p>尽管摘要展示了 WaterClear-GS 的强大性能，但仍可以推断出一些潜在的局限性：</p>
<ul>
<li><strong>对训练数据的依赖</strong>: 尽管摘要未明确提及，但任何基于学习的方法（即使是与物理模型结合）通常都需要大量的、高质量的训练数据来学习和优化模型参数。水下数据的采集可能仍然是一个挑战。</li>
<li><strong>计算复杂度</strong>: 尽管 3DGS 本身渲染速度快，但将复杂的物理光学模型集成到高斯基元中，以及多重正则化策略的应用，可能会增加训练和优化的计算复杂度，尽管其推理（渲染）速度可能仍然是实时的。</li>
<li><strong>对特定水质条件的泛化能力</strong>: 摘要提到“局部衰减和散射”，这可能意味着模型对不同水质（如浑浊度、藻类密度、盐度等）的适应性可能需要进一步验证。如果水质变化很大，可能需要针对不同水质进行模型调整或重新训练。</li>
<li><strong>对极端光学现象的处理</strong>: 对于非常极端的光学现象，例如强烈的反射、折射、或者非常复杂的散射模式，该方法是否能完全捕捉并准确恢复仍需进一步验证。</li>
<li><strong>“水下无关外观”的定义和恢复的完美程度</strong>: 恢复“水下无关外观”是一个具有挑战性的目标。摘要表明其“自然恢复”，但恢复的真实度和细节程度可能存在一定的限制，尤其是在物体表面存在复杂纹理或反射的情况下。</li>
</ul>
<p>总而言之，WaterClear-GS 是一项令人兴奋的研究，它巧妙地将 3DGS 的高效性与水下光学物理模型相结合，有望在水下 3D 重建和图像恢复领域带来突破。其核心创新在于将物理光学特性直接融入高斯基元，并辅以一系列精细的优化和正则化策略，这使其在技术上具有很高的趣味性和重要性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these issues, we introduce WaterClear-GS, the first pure 3DGS-based framework that explicitly integrates underwater optical properties of local attenuation and scattering into Gaussian primitives, eliminating the need for an auxiliary medium network.</li>
<li>Our method employs a dual-branch optimization strategy to ensure underwater photometric consistency while naturally recovering water-free appearances.</li>
<li>Experiments on standard benchmarks and our newly collected dataset demonstrate that WaterClear-GS achieves outstanding performance on both novel view synthesis (NVS) and underwater image restoration (UIR) tasks, while maintaining real-time rendering.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.19753v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.19753v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.19750v1'></a></p>
<h2 id="benchmarking-multimodal-large-language-models-for-missing-modality-completion-in-product-catalogues"><a href="https://arxiv.org/abs/2601.19750v1">Benchmarking Multimodal Large Language Models for Missing Modality Completion in Product Catalogues</a></h2>
<p><strong>Authors:</strong> Junchen Fu, Wenhao Deng, Kaiwen Zheng, Alexandros Karatzoglou, Ioannis Arapakis, Yu Ye, Yongxin Ni, Joemon M. Jose, Xuri Ge</p>
<p><strong>Published:</strong> 2026-01-27</p>
<p><strong>Categories:</strong> cs.MM, cs.CV, cs.IR</p>
<p><strong>Abstract:</strong></p>
<p>Missing-modality information on e-commerce platforms, such as absent product images or textual descriptions, often arises from annotation errors or incomplete metadata, impairing both product presentation and downstream applications such as recommendation systems. Motivated by the multimodal generative capabilities of recent Multimodal Large Language Models (MLLMs), this work investigates a fundamental yet underexplored question: can MLLMs generate missing modalities for products in e-commerce scenarios? We propose the Missing Modality Product Completion Benchmark (MMPCBench), which consists of two sub-benchmarks: a Content Quality Completion Benchmark and a Recommendation Benchmark.   We further evaluate six state-of-the-art MLLMs from the Qwen2.5-VL and Gemma-3 model families across nine real-world e-commerce categories, focusing on image-to-text and text-to-image completion tasks. Experimental results show that while MLLMs can capture high-level semantics, they struggle with fine-grained word-level and pixel- or patch-level alignment. In addition, performance varies substantially across product categories and model scales, and we observe no trivial correlation between model size and performance, in contrast to trends commonly reported in mainstream benchmarks. We also explore Group Relative Policy Optimization (GRPO) to better align MLLMs with this task. GRPO improves image-to-text completion but does not yield gains for text-to-image completion. Overall, these findings expose the limitations of current MLLMs in real-world cross-modal generation and represent an early step toward more effective missing-modality product completion.</p>
<p><strong>Analysis:</strong></p>
<p>好的，我将以AI领域高水平研究生的视角，深入分析您提供的论文方法部分，并遵循您提供的分析框架。请提供您希望我分析的论文内容（例如，论文的PDF文件或关键章节的文本）。</p>
<p>一旦您提供了论文内容，我将按照以下结构进行分析：</p>
<hr />
<h3 id="1_4">1. 摘要翻译</h3>
<h3 id="2_4">2. 方法动机分析</h3>
<ul>
<li><strong>驱动力</strong>：作者为什么提出这个方法？背后的核心动机是什么？</li>
<li><strong>现有方法痛点</strong>：具体指出当前方法的局限性和不足</li>
<li><strong>研究假设</strong>：用简洁语言概括论文的基本假设或核心直觉</li>
</ul>
<h3 id="3_4">3. 方法设计详解</h3>
<ul>
<li><strong>流程总结</strong>：提供清晰的方法pipeline，详细解释从输入到输出的每个步骤</li>
<li>必须讲清楚每一步的具体操作和技术细节</li>
<li>这是分析的核心部分，需要特别详尽</li>
<li><strong>模型结构</strong>：描述各模块功能与作用，以及它们如何协同工作</li>
<li><strong>算法解释</strong>：用通俗语言解释关键公式/算法的意义和作用</li>
</ul>
<h3 id="4_3">4. 方法对比分析</h3>
<ul>
<li><strong>本质区别</strong>：与现有主流方法的根本不同点</li>
<li><strong>创新贡献</strong>：明确指出方法的创新点及其贡献度</li>
<li><strong>适用场景</strong>：分析方法的适用范围和最佳应用场景</li>
</ul>
<h3 id="5_3">5. 实验分析</h3>
<ul>
<li><strong>验证方法</strong>：作者如何验证方法有效性？实验设计与设置</li>
<li><strong>关键结果</strong>：列出最具代表性的实验数据和结论</li>
<li><strong>优势场景</strong>：在哪些数据集或场景下表现最佳，提供具体证据</li>
<li><strong>局限性</strong>：指出方法的不足，如泛化能力、计算开销、数据依赖等</li>
</ul>
<h3 id="6_3">6. 实用指南</h3>
<ul>
<li><strong>开源情况</strong>：论文是否开源？实现/复现的关键步骤</li>
<li><strong>实现细节</strong>：需要注意的超参数、数据预处理、训练细节等</li>
<li><strong>迁移可能</strong>：该方法能否迁移到其他任务？如何迁移？</li>
</ul>
<h3 id="7_3">7. 总结</h3>
<ul>
<li><strong>核心思想</strong>：用一句话概括方法的核心思想（不超过20字）</li>
<li><strong>速记版pipeline</strong>：3-5个关键步骤，使用自明性语言，避免专业术语，直白表达内容，但避免流于表面的基础工作流</li>
</ul>
<hr />
<p>请您提供论文内容，我将立即开始分析。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose the Missing Modality Product Completion Benchmark (MMPCBench), which consists of two sub-benchmarks: a Content Quality Completion Benchmark and a Recommendation Benchmark.</li>
<li>We further evaluate six state-of-the-art MLLMs from the Qwen2.5-VL and Gemma-3 model families across nine real-world e-commerce categories, focusing on image-to-text and text-to-image completion tasks.</li>
<li>Experimental results show that while MLLMs can capture high-level semantics, they struggle with fine-grained word-level and pixel- or patch-level alignment.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.19750v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.19750v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2026-01-28 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
