<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-11-10 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-11-07/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-11-12/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-11-10">Arxiv Computer Vision Papers - 2025-11-10</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#timesearch-r-adaptive-temporal-search-for-long-form-video-understanding-via-self-verification-reinforcement-learning" class="nav-link">TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning</a>
                </li>
                <li class="nav-item">
                    <a href="#how-many-tokens-do-3d-point-cloud-transformer-architectures-really-need" class="nav-link">How Many Tokens Do 3D Point Cloud Transformer Architectures Really Need?</a>
                </li>
                <li class="nav-item">
                    <a href="#multi-modal-loop-closure-detection-with-foundation-models-in-severely-unstructured-environments" class="nav-link">Multi-modal Loop Closure Detection with Foundation Models in Severely Unstructured Environments</a>
                </li>
                <li class="nav-item">
                    <a href="#everydayvla-a-vision-language-action-model-for-affordable-robotic-manipulation" class="nav-link">EveryDayVLA: A Vision-Language-Action Model for Affordable Robotic Manipulation</a>
                </li>
                <li class="nav-item">
                    <a href="#preresq-r1-towards-fine-grained-rank-and-score-reinforcement-learning-for-visual-quality-assessment-via-preference-response-disentangled-policy-optimization" class="nav-link">PreResQ-R1: Towards Fine-Grained Rank-and-Score Reinforcement Learning for Visual Quality Assessment via Preference-Response Disentangled Policy Optimization</a>
                </li>
                <li class="nav-item">
                    <a href="#rethinking-metrics-and-diffusion-architecture-for-3d-point-cloud-generation" class="nav-link">Rethinking Metrics and Diffusion Architecture for 3D Point Cloud Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#livestar-live-streaming-assistant-for-real-world-online-video-understanding" class="nav-link">LiveStar: Live Streaming Assistant for Real-World Online Video Understanding</a>
                </li>
                <li class="nav-item">
                    <a href="#adpretrain-advancing-industrial-anomaly-detection-via-anomaly-representation-pretraining" class="nav-link">ADPretrain: Advancing Industrial Anomaly Detection via Anomaly Representation Pretraining</a>
                </li>
                <li class="nav-item">
                    <a href="#beyond-master-and-apprentice-grounding-foundation-models-for-symbiotic-interactive-learning-in-a-shared-latent-space" class="nav-link">Beyond Master and Apprentice: Grounding Foundation Models for Symbiotic Interactive Learning in a Shared Latent Space</a>
                </li>
                <li class="nav-item">
                    <a href="#another-brixel-in-the-wall-towards-cheaper-dense-features" class="nav-link">Another BRIXEL in the Wall: Towards Cheaper Dense Features</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-11-10">Arxiv Computer Vision Papers - 2025-11-10</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，这是一份为忙碌的研究人员准备的 Arxiv 计算机视觉领域日报执行摘要，涵盖了 2025 年 11 月 7 日发布的 10 篇论文。</p>
<hr />
<p><strong>Arxiv 计算机视觉日报执行摘要 (2025-11-07)</strong></p>
<p><strong>1. 主要主题与趋势概述：</strong></p>
<p>今天的论文集展示了计算机视觉领域几个关键且相互关联的趋势。<strong>多模态学习和基础模型</strong>的整合是核心，尤其是在机器人操作、视频理解和环境感知方面。<strong>效率和资源优化</strong>是另一个突出主题，体现在对更少数据、更轻量级模型或更经济特征表示的追求上。此外，<strong>3D 数据处理</strong>（点云生成、Transformer 架构优化）和<strong>视频理解</strong>（长视频、直播流、质量评估）仍然是活跃的研究领域，并开始与强化学习和自验证机制结合，以提高性能和鲁棒性。</p>
<p><strong>2. 显著或创新论文亮点：</strong></p>
<ul>
<li><strong>"EveryDayVLA: A Vision-Language-Action Model for Affordable Robotic Manipulation" (Samarth Chopra et al.)</strong>：这篇论文非常重要，因为它直接解决了机器人操作中成本和可访问性的痛点。通过提出一个“经济实惠”的视觉-语言-动作模型，它有望显著降低将先进机器人技术部署到现实世界场景的门槛，这对于实际应用具有巨大潜力。</li>
<li><strong>"TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning" (Junwen Pan et al.)</strong>：该工作通过引入自验证强化学习来解决长视频理解中的关键挑战。这种自适应的时间搜索方法有望显著提高处理复杂、长时间视频的效率和准确性，是视频分析领域的一个重要进步。</li>
<li><strong>"Multi-modal Loop Closure Detection with Foundation Models in Severely Unstructured Environments" (Laura Alejandra Encinar Gonzalez et al.)</strong>：这篇论文将基础模型应用于 SLAM 中的回环检测，特别是在非结构化环境中，展示了基础模型在复杂感知任务中的强大泛化能力，对自主导航和机器人技术具有重要意义。</li>
</ul>
<p><strong>3. 新兴研究方向或技术：</strong></p>
<ul>
<li><strong>强化学习与自验证机制的结合</strong>：在视频理解（TimeSearch-R）和视觉质量评估（PreResQ-R1）中，RL 被用于优化搜索策略和策略学习，并辅以自验证或偏好响应解耦，以提高学习效率和鲁棒性。</li>
<li><strong>基础模型在特定任务中的接地与优化</strong>：论文探讨了如何将大型基础模型有效地“接地”到特定应用（如机器人操作、回环检测），并探索了它们在共享潜在空间中进行交互式学习的潜力（Beyond Master and Apprentice）。</li>
<li><strong>效率驱动的架构和特征表示</strong>：对 3D 点云 Transformer 中 token 数量的重新思考（How Many Tokens...）以及对更便宜、更密集特征的追求（Another BRIXEL in the Wall）表明，研究人员正在积极寻找在保持性能的同时降低计算成本的方法。</li>
<li><strong>异常表示预训练 (Anomaly Representation Pretraining)</strong>：ADPretrain 提出了一种新颖的工业异常检测方法，通过预训练异常表示来提高检测性能，这可能为特定领域的预训练范式开辟新方向。</li>
</ul>
<p><strong>4. 最有价值的全文阅读建议：</strong></p>
<p>对于不同兴趣的研究人员，建议阅读以下论文：</p>
<ul>
<li><strong>机器人与具身智能方向：</strong><ul>
<li><strong>"EveryDayVLA: A Vision-Language-Action Model for Affordable Robotic Manipulation"</strong>：对于任何对机器人操作和实际部署感兴趣的人来说，这是必读的。</li>
<li><strong>"Multi-modal Loop Closure Detection with Foundation Models in Severely Unstructured Environments"</strong>：对 SLAM、自主导航和基础模型在感知中应用感兴趣的研究人员。</li>
</ul>
</li>
<li><strong>视频理解与分析方向：</strong><ul>
<li><strong>"TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning"</strong>：从事长视频分析、强化学习和视频内容理解的研究人员。</li>
<li><strong>"LiveStar: Live Streaming Assistant for Real-World Online Video Understanding"</strong>：对实时视频处理、直播流分析和实际应用感兴趣的研究人员。</li>
</ul>
</li>
<li><strong>3D 视觉与生成模型方向：</strong><ul>
<li><strong>"Rethinking Metrics and Diffusion Architecture for 3D Point Cloud Generation"</strong>：对 3D 生成模型、扩散模型和评估指标有兴趣的研究人员。</li>
<li><strong>"How Many Tokens Do 3D Point Cloud Transformer Architectures Really Need?"</strong>：对 3D Transformer 架构优化和效率感兴趣的研究人员。</li>
</ul>
</li>
<li><strong>通用机器学习与基础模型方向：</strong><ul>
<li><strong>"Beyond Master and Apprentice: Grounding Foundation Models for Symbiotic Interactive Learning in a Shared Latent Space"</strong>：对基础模型的交互式学习、接地问题和多模型协作感兴趣的研究人员。</li>
</ul>
</li>
</ul>
<hr />
<p>这份摘要旨在帮助您快速掌握今日 Arxiv 计算机视觉领域的关键进展。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2511.05489v1">TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning</a></li>
<li><a href="#2511.05449v1">How Many Tokens Do 3D Point Cloud Transformer Architectures Really Need?</a></li>
<li><a href="#2511.05404v1">Multi-modal Loop Closure Detection with Foundation Models in Severely Unstructured Environments</a></li>
<li><a href="#2511.05397v1">EveryDayVLA: A Vision-Language-Action Model for Affordable Robotic Manipulation</a></li>
<li><a href="#2511.05393v1">PreResQ-R1: Towards Fine-Grained Rank-and-Score Reinforcement Learning for Visual Quality Assessment via Preference-Response Disentangled Policy Optimization</a></li>
<li><a href="#2511.05308v1">Rethinking Metrics and Diffusion Architecture for 3D Point Cloud Generation</a></li>
<li><a href="#2511.05299v1">LiveStar: Live Streaming Assistant for Real-World Online Video Understanding</a></li>
<li><a href="#2511.05245v1">ADPretrain: Advancing Industrial Anomaly Detection via Anomaly Representation Pretraining</a></li>
<li><a href="#2511.05203v1">Beyond Master and Apprentice: Grounding Foundation Models for Symbiotic Interactive Learning in a Shared Latent Space</a></li>
<li><a href="#2511.05168v1">Another BRIXEL in the Wall: Towards Cheaper Dense Features</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2511.05489v1'></a></p>
<h2 id="timesearch-r-adaptive-temporal-search-for-long-form-video-understanding-via-self-verification-reinforcement-learning"><a href="https://arxiv.org/abs/2511.05489v1">TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Junwen Pan, Qizhe Zhang, Rui Zhang, Ming Lu, Xin Wan, Yuan Zhang, Chang Liu, Qi She</p>
<p><strong>Published:</strong> 2025-11-07</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Temporal search aims to identify a minimal set of relevant frames from tens
of thousands based on a given query, serving as a foundation for accurate
long-form video understanding. Existing works attempt to progressively narrow
the search space. However, these approaches typically rely on a hand-crafted
search process, lacking end-to-end optimization for learning optimal search
strategies. In this paper, we propose TimeSearch-R, which reformulates temporal
search as interleaved text-video thinking, seamlessly integrating searching
video clips into the reasoning process through reinforcement learning (RL).
However, applying RL training methods, such as Group Relative Policy
Optimization (GRPO), to video reasoning can result in unsupervised intermediate
search decisions. This leads to insufficient exploration of the video content
and inconsistent logical reasoning. To address these issues, we introduce GRPO
with Completeness Self-Verification (GRPO-CSV), which gathers searched video
frames from the interleaved reasoning process and utilizes the same policy
model to verify the adequacy of searched frames, thereby improving the
completeness of video reasoning. Additionally, we construct datasets
specifically designed for the SFT cold-start and RL training of GRPO-CSV,
filtering out samples with weak temporal dependencies to enhance task
difficulty and improve temporal search capabilities. Extensive experiments
demonstrate that TimeSearch-R achieves significant improvements on temporal
search benchmarks such as Haystack-LVBench and Haystack-Ego4D, as well as
long-form video understanding benchmarks like VideoMME and MLVU. Notably,
TimeSearch-R establishes a new state-of-the-art on LongVideoBench with 4.1%
improvement over the base model Qwen2.5-VL and 2.0% over the advanced video
reasoning model Video-R1. Our code is available at
https://github.com/Time-Search/TimeSearch-R.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning”论文的全面摘要，内容将涵盖您要求的所有方面：</p>
<p><strong>论文摘要：TimeSearch-R：通过自验证强化学习实现长视频理解的自适应时间搜索</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决长视频理解中的核心挑战：如何有效地从数万帧视频中识别出最相关的帧，以支持准确的视频问答和理解。现有方法通常依赖于手工设计的搜索策略，缺乏端到端优化，导致搜索效率低下、视频内容探索不足以及逻辑推理不一致。具体来说，强化学习（RL）方法，如Group Relative Policy Optimization (GRPO)，在视频推理中可能导致中间搜索决策缺乏监督，从而限制了模型的探索能力和推理的连贯性。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
TimeSearch-R引入了以下关键创新：
*   <strong>时间搜索的重新定义为文本-视频交错思考：</strong> 该框架将时间搜索任务重新定义为一个交错的文本-视频思考过程，通过强化学习（RL）将视频片段搜索无缝集成到推理过程中，从而使模型能够学习最优的搜索策略。
*   <strong>GRPO与完整性自验证（GRPO-CSV）：</strong> 为了解决现有RL方法中探索不足和逻辑推理不一致的问题，TimeSearch-R引入了GRPO-CSV。该机制从交错的推理过程中收集已搜索的视频帧，并利用相同的策略模型验证这些帧的充分性，从而提高视频推理的完整性。GRPO-CSV通过确保模型获取足够的视觉证据来解决探索不足问题，并通过重新回答问题来促进中间推理与最终答案之间的一致性。
*   <strong>高质量数据集构建：</strong> 论文构建了专门用于SFT冷启动和GRPO-CSV RL训练的数据集。通过两阶段过滤管道，过滤掉时间依赖性较弱的样本，以提高任务难度和时间搜索能力，确保模型学习正确的搜索过程。</p>
<p><strong>3. 主要结果及其意义：</strong>
TimeSearch-R在多个基准测试中取得了显著的性能提升：
*   <strong>时间搜索基准：</strong> 在Haystack-LVBench和Haystack-Ego4D等时间搜索基准上，TimeSearch-R实现了显著改进。例如，在Haystack-LVBench上，时间F1分数提高了5.6%，在Haystack-Ego4D上准确率提高了8.5%。
*   <strong>长视频理解基准：</strong> 在VideoMME和MLVU等长视频理解基准上，TimeSearch-R也表现出色。尤其值得注意的是，在LongVideoBench上，TimeSearch-R比基础模型Qwen2.5-VL提高了4.1%，比先进的视频推理模型Video-R1提高了2.0%，达到了新的SOTA水平。
*   <strong>训练方案和GRPO-CSV组件的消融研究：</strong> 实验证明SFT训练能够显著提升模型的搜索能力和帧完整性，而RL训练则进一步增强了视频推理性能和推理一致性。GRPO-CSV的引入显著提高了训练稳定性，并实现了最佳的QA性能。</p>
<p><strong>4. 论文中提及的局限性：</strong>
论文中没有明确列出当前TimeSearch-R的局限性。然而，从其改进动机和方法论来看，可以推断出以下几点是其试图克服的现有方法的局限：
*   <strong>现有方法依赖手工搜索：</strong> 传统方法依赖于手工设计的搜索过程，缺乏端到端优化，导致搜索策略次优。
*   <strong>RL训练中的探索不足和推理不一致：</strong> 原始GRPO等RL方法仅奖励最终输出，忽略中间搜索决策，导致视频内容探索不足和逻辑推理不一致。
*   <strong>数据集质量问题：</strong> 现有数据集中存在大量可通过语言偏差解决的“琐碎”样本，以及即使经过广泛搜索也无法解决的“噪声”样本，这阻碍了长视频推理的进展。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
论文指出TimeSearch-R的工作为未来研究提供了几个方向：
*   <strong>更自适应和交互式AI系统：</strong> 该方法将视频推理从静态帧采样转变为动态、交互式推理范式，这可能启发研究人员开发更具适应性和交互性的AI系统，并推广到其他多模态任务。
*   <strong>可扩展的弱监督过程奖励：</strong> 通过整合弱监督和强化学习（通过完整性自验证），该方法为训练复杂交互系统提供了一种可扩展的解决方案，无需细粒度的过程标签，从而可能降低标注成本并促进在不同领域的广泛应用。
*   <strong>提升视频可解释性和可解释性：</strong> TimeSearch-R引入的交错文本-视频推理轨迹提供了模型决策过程的透明洞察，这有助于构建更可解释的AI系统，提高视频领域中推理过程的信任度和可靠性。</p>
<p>总而言之，TimeSearch-R通过将时间搜索重新定义为RL驱动的文本-视频交错思考过程，并引入GRPO-CSV机制来解决探索不足和推理不一致的问题，为长视频理解领域带来了显著进步。其在多个基准测试上的SOTA表现，验证了其端到端学习策略的优越性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we propose TimeSearch-R, which reformulates temporal
search as interleaved text-video thinking, seamlessly integrating searching
video clips into the reasoning process through reinforcement learning (RL).</li>
<li>To address these issues, we introduce GRPO
with Completeness Self-Verification (GRPO-CSV), which gathers searched video
frames from the interleaved reasoning process and utilizes the same policy
model to verify the adequacy of searched frames, thereby improving the
completeness of video reasoning.</li>
<li>Notably,
TimeSearch-R establishes a new state-of-the-art on LongVideoBench with 4.1%
improvement over the base model Qwen2.5-VL and 2.0% over the advanced video
reasoning model Video-R1.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.05489v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.05489v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.05449v1'></a></p>
<h2 id="how-many-tokens-do-3d-point-cloud-transformer-architectures-really-need"><a href="https://arxiv.org/abs/2511.05449v1">How Many Tokens Do 3D Point Cloud Transformer Architectures Really Need?</a></h2>
<p><strong>Authors:</strong> Tuan Anh Tran, Duy M. H. Nguyen, Hoai-Chau Tran, Michael Barz, Khoa D. Doan, Roger Wattenhofer, Ngo Anh Vien, Mathias Niepert, Daniel Sonntag, Paul Swoboda</p>
<p><strong>Published:</strong> 2025-11-07</p>
<p><strong>Categories:</strong> cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Recent advances in 3D point cloud transformers have led to state-of-the-art
results in tasks such as semantic segmentation and reconstruction. However,
these models typically rely on dense token representations, incurring high
computational and memory costs during training and inference. In this work, we
present the finding that tokens are remarkably redundant, leading to
substantial inefficiency. We introduce gitmerge3D, a globally informed graph
token merging method that can reduce the token count by up to 90-95% while
maintaining competitive performance. This finding challenges the prevailing
assumption that more tokens inherently yield better performance and highlights
that many current models are over-tokenized and under-optimized for
scalability. We validate our method across multiple 3D vision tasks and show
consistent improvements in computational efficiency. This work is the first to
assess redundancy in large-scale 3D transformer models, providing insights into
the development of more efficient 3D foundation architectures. Our code and
checkpoints are publicly available at https://gitmerge3d.github.io</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Tuan Anh Tran等人撰写的论文“How Many Tokens Do 3D Point Cloud Transformer Architectures Really Need?”的全面摘要。</p>
<hr />
<p><strong>论文摘要：How Many Tokens Do 3D Point Cloud Transformer Architectures Really Need?</strong></p>
<p><strong>1. 解决的主要问题或研究问题：</strong>
该论文旨在解决3D点云Transformer架构中普遍存在的计算和内存效率低下问题。尽管这些模型在语义分割和重建等任务中取得了最先进的性能，但它们通常依赖于密集的token表示，导致训练和推理过程中产生高昂的计算和内存成本。核心研究问题是：3D点云Transformer架构是否真的需要如此多的token？以及如何有效地减少token数量以提高效率而不牺牲性能？</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>揭示token冗余性：</strong> 论文通过系统性研究发现，最先进的3D点云Transformer模型（特别是PTv3及其变体）存在显著的token冗余。研究表明，即使移除90-95%的token，模型性能也能保持竞争力，这挑战了“更多token带来更好性能”的普遍假设。
*   <strong>引入gitmerge3D：</strong> 提出了一种名为gitmerge3D的全局感知图token合并方法。该方法将局部几何结构和注意力显著性结合起来，以估计体素的重要性，从而实现激进的token削减。
*   <strong>自适应合并策略：</strong> gitmerge3D采用自适应合并策略，根据token的“能量分数”（反映token与所有patch质心之间的全局对齐程度）来指导合并决策。高能量token（与全局结构对齐较差，被认为信息量更大）被保留，而低能量token（与全局结构对齐较好，信息量较小）则被更积极地合并。
*   <strong>支持特征恢复：</strong> 与许多现有token合并方法不同，gitmerge3D设计了特征恢复机制，这对于需要细粒度预测的密集分割任务至关重要。
*   <strong>动态K参数：</strong> 引入了动态的K参数（每个patch中的bin数量），使其能根据合并率自然调整，从而在合并token的空间分布中保持平衡。</p>
<p><strong>3. 主要结果及其重要性：</strong>
*   <strong>显著的效率提升：</strong> gitmerge3D在PTv3上实现了FLOPs减少5.3倍（从107.5 GFLOPs降至19.9 GFLOPs），内存使用减少6.4倍（从10.12 GB降至1.6 GB），同时性能下降极小。在NuScenes数据集上，峰值内存使用减少超过85%，GFLOPS减少近70%，延迟降低约30%。
*   <strong>保持甚至超越基线性能：</strong> 即使在移除95%的token后，该方法仍能保持竞争性性能。通过仅用10%的原始训练周期进行微调，模型不仅完全恢复了基线性能，在ScanNet或S3DIS等某些数据集中甚至超越了基线。
*   <strong>跨任务验证：</strong> 该方法在3D语义分割（ScanNet、S3DIS、NuScenes）、3D重建（ShapeNet、ObjectVerse、GSO）和目标检测（Waymo）等多个3D视觉任务中得到了验证，均显示出计算效率的持续改进。
*   <strong>挑战传统假设：</strong> 论文结果有力地挑战了3D领域中密集token化对于Transformer性能至关重要的普遍假设，表明许多现有模型存在过度token化和可扩展性优化不足的问题。
*   <strong>传输熵分析：</strong> 传输熵分析表明，通过合并功能进行压缩导致的信息损失极小（传输率始终小于0.1），进一步支持了token表示中存在显著冗余的假设。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>合并率r的手动指定：</strong> 该方法的一个局限性是合并率r是手动指定的，而不是通过学习获得的。在FLOPs约束下自动优化r需要一个端到端框架，但这具有挑战性，因为排序和分组操作是不可微分的，需要梯度近似。
*   <strong>缺乏形式化框架：</strong> 缺乏一个形式化框架来量化和减少token表示中的冗余，这可能会进一步提高效率并为该方法提供更强的理论基础。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>学习合并率：</strong> 开发端到端可微分框架，以自动学习和优化合并率r，可能在FLOPs约束下进行。
*   <strong>冗余量化理论：</strong> 建立一个形式化的理论框架，用于量化和减少token表示中的冗余，从而为更高效的Transformer设计提供更强的理论基础。
*   <strong>更高效的3D基础架构：</strong> 论文的发现为开发更高效的3D基础架构提供了见解，鼓励未来研究探索轻量级和可扩展的3D点云Transformer架构。
*   <strong>探索其他token混合机制：</strong> 论文发现，有效的空间信息共享机制（如带有token洗牌的池化）可能与注意力层一样有效，这表明可以探索更高效的替代方案。</p>
<hr />
<p>这篇论文通过深入分析和创新的方法，为3D点云Transformer的效率优化开辟了新途径，对计算机视觉领域具有重要意义。它不仅揭示了现有模型的固有低效性，还提供了一个实用的解决方案，以实现更具可扩展性和计算效率的3D视觉系统。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Recent advances in 3D point cloud transformers have led to state-of-the-art
results in tasks such as semantic segmentation and reconstruction.</li>
<li>We introduce gitmerge3D, a globally informed graph
token merging method that can reduce the token count by up to 90-95% while
maintaining competitive performance.</li>
<li>We validate our method across multiple 3D vision tasks and show
consistent improvements in computational efficiency.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.05449v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.05449v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.05404v1'></a></p>
<h2 id="multi-modal-loop-closure-detection-with-foundation-models-in-severely-unstructured-environments"><a href="https://arxiv.org/abs/2511.05404v1">Multi-modal Loop Closure Detection with Foundation Models in Severely Unstructured Environments</a></h2>
<p><strong>Authors:</strong> Laura Alejandra Encinar Gonzalez, John Folkesson, Rudolph Triebel, Riccardo Giubilato</p>
<p><strong>Published:</strong> 2025-11-07</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, I.2.9; I.2.10</p>
<p><strong>Abstract:</strong></p>
<p>Robust loop closure detection is a critical component of Simultaneous
Localization and Mapping (SLAM) algorithms in GNSS-denied environments, such as
in the context of planetary exploration. In these settings, visual place
recognition often fails due to aliasing and weak textures, while LiDAR-based
methods suffer from sparsity and ambiguity. This paper presents MPRF, a
multimodal pipeline that leverages transformer-based foundation models for both
vision and LiDAR modalities to achieve robust loop closure in severely
unstructured environments. Unlike prior work limited to retrieval, MPRF
integrates a two-stage visual retrieval strategy with explicit 6-DoF pose
estimation, combining DINOv2 features with SALAD aggregation for efficient
candidate screening and SONATA-based LiDAR descriptors for geometric
verification. Experiments on the S3LI dataset and S3LI Vulcano dataset show
that MPRF outperforms state-of-the-art retrieval methods in precision while
enhancing pose estimation robustness in low-texture regions. By providing
interpretable correspondences suitable for SLAM back-ends, MPRF achieves a
favorable trade-off between accuracy, efficiency, and reliability,
demonstrating the potential of foundation models to unify place recognition and
pose estimation. Code and models will be released at github.com/DLR-RM/MPRF.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“Multi-modal Loop Closure Detection with Foundation Models in Severely Unstructured Environments”论文的全面摘要：</p>
<p><strong>论文题目：</strong> 基于基础模型在严重非结构化环境中进行多模态回环检测</p>
<p><strong>作者：</strong> Laura Alejandra Encinar Gonzalez, John Folkesson, Rudolph Triebel, Riccardo Giubilato</p>
<p><strong>1. 主要问题或研究问题：</strong>
该研究旨在解决在GNSS受限的严重非结构化环境中（例如行星探索）进行鲁棒回环检测的挑战。在这种环境中，传统的视觉地点识别方法常因混叠和弱纹理而失效，而基于LiDAR的方法则受限于稀疏性和模糊性。核心问题是如何开发一种能够有效结合视觉和LiDAR信息，实现高精度、高效率和高可靠性回环检测，并提供6自由度（6-DoF）位姿估计的系统。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
论文提出了MPRF（Multimodal Place Recognition leveraging Foundation models）多模态管道，其主要创新和贡献包括：
*   <strong>多模态基础模型融合：</strong> MPRF结合了基于Transformer的视觉（DINOv2）和LiDAR（SONATA）基础模型，利用它们在大规模数据上的预训练能力，在缺乏特定领域数据的行星状环境中实现强大的泛化能力。
*   <strong>两阶段视觉检索策略：</strong>
    *   <strong>高效全局筛选：</strong> 使用SALAD（Sinkhorn Algorithm for Locally Aggregated Descriptors）聚合DINOv2特征，进行高效的近似最近邻搜索，快速筛选出候选帧。
    *   <strong>多层补丁嵌入细化：</strong> 通过连接最后三个Transformer层的补丁嵌入并进行余弦相似度比较，进一步细化候选，以保留更丰富的空间线索并提高鲁棒性。
*   <strong>几何验证与6-DoF位姿估计：</strong> MPRF将几何验证集成到检索过程中，通过PnP+RANSAC和ICP从融合的视觉-LiDAR对应关系中估计显式的6-DoF相对位姿，弥合了地点识别和SLAM之间的鸿沟。
*   <strong>可解释的对应关系：</strong> 该方法通过显式匹配策略生成可解释的图像-LiDAR对应关系，这对于SLAM后端至关重要。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>S3LI数据集和S3LI Vulcano数据集上的卓越性能：</strong> 实验结果表明，MPRF在精度方面优于最先进的检索方法，并在低纹理区域增强了位姿估计的鲁棒性。
*   <strong>视觉基础模型在检索中的优势：</strong> DINOv2描述符在视觉检索中表现出色，尤其是在与SALAD聚合模块结合并进行微调后，实现了精度和效率的最佳平衡。
*   <strong>LiDAR几何信息在位姿估计中的关键作用：</strong> 尽管LiDAR数据对大规模检索的附加价值有限，但它在位姿估计中至关重要。MPRF通过融合SONATA的几何描述符和DINOv2嵌入，显著降低了偏航误差，并在低纹理区域提高了鲁棒性。
*   <strong>准确性、效率和可靠性的良好权衡：</strong> MPRF在保持高精度的同时，实现了相对较快的检索和位姿估计时间（Precision@1达到75.7%，检索时间低于500毫秒），并且通过阈值分析验证了位姿估计的可靠性。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>LiDAR-only方法在检索阶段的贡献有限：</strong> 在稀疏的行星状点云中，几何信息对大规模检索的改善不大。
*   <strong>回归式位姿估计器的局限性：</strong> 虽然回归式方法效率高（如Reloc3r），但它们通常输出5D位姿，不提供显式对应关系，限制了可解释性和下游验证。
*   <strong>当前运行时仍有优化空间：</strong> 尽管MPRF的运行时（每查询3.1秒）低于手工基线，但仍高于纯视觉方法，未来可以进一步优化。
*   <strong>未应用额外的阈值：</strong> 论文中未对内点计数或误差指标应用额外的阈值，这可能进一步提高鲁棒性。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>更快的位姿估计：</strong> 进一步优化算法以实现更快的位姿估计。
*   <strong>集成到多模态SLAM系统：</strong> 将MPRF更紧密地集成到完整的多模态SLAM系统中。
*   <strong>探索替代的重排序策略：</strong> 研究基于内点计数等替代策略，以进一步提高回环检测的鲁棒性。
*   <strong>SALAD的再训练：</strong> 尽管当前研究表明预训练的聚类空间更稳定，但未来仍可探索在目标域上对SALAD进行再训练以适应特定环境。</p>
<p>总而言之，MPRF通过创新性地结合视觉和LiDAR基础模型，为在挑战性非结构化环境中实现鲁榜回环检测和6-DoF位姿估计提供了一个统一的解决方案，为SLAM领域带来了显著进展。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Experiments on the S3LI dataset and S3LI Vulcano dataset show
that MPRF outperforms state-of-the-art retrieval methods in precision while
enhancing pose estimation robustness in low-texture regions.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.05404v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.05404v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.05397v1'></a></p>
<h2 id="everydayvla-a-vision-language-action-model-for-affordable-robotic-manipulation"><a href="https://arxiv.org/abs/2511.05397v1">EveryDayVLA: A Vision-Language-Action Model for Affordable Robotic Manipulation</a></h2>
<p><strong>Authors:</strong> Samarth Chopra, Alex McMoil, Ben Carnovale, Evan Sokolson, Rajkumar Kubendran, Samuel Dickerson</p>
<p><strong>Published:</strong> 2025-11-07</p>
<p><strong>Categories:</strong> cs.RO, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>While Vision-Language-Action (VLA) models map visual inputs and language
instructions directly to robot actions, they often rely on costly hardware and
struggle in novel or cluttered scenes. We introduce EverydayVLA, a 6-DOF
manipulator that can be assembled for under $300, capable of modest payloads
and workspace. A single unified model jointly outputs discrete and continuous
actions, and our adaptive-horizon ensemble monitors motion uncertainty to
trigger on-the-fly re-planning for safe, reliable operation. On LIBERO,
EverydayVLA matches state-of-the-art success rates, and in real-world tests it
outperforms prior methods by 49% in-distribution and 34.9% out-of-distribution.
By combining a state-of-the-art VLA with cost-effective hardware, EverydayVLA
democratizes access to a robotic foundation model and paves the way for
economical use in homes and research labs alike. Experiment videos and details:
https://everydayvla.github.io/</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Samarth Chopra等人撰写的论文“EveryDayVLA: A Vision-Language-Action Model for Affordable Robotic Manipulation”的全面摘要。</p>
<hr />
<h3 id="everydayvla-">论文摘要：EveryDayVLA: 一种经济实惠的机器人操作视觉-语言-动作模型</h3>
<p><strong>1. 主要问题或研究问题：</strong>
当前的视觉-语言-动作（VLA）模型在将视觉输入和语言指令直接映射到机器人动作方面表现出色，但它们通常依赖昂贵的硬件，并且在面对新颖或杂乱的场景时表现不佳。这限制了机器人基础模型的广泛应用和可及性。该研究旨在解决如何开发一种经济实惠、高性能且在复杂真实世界场景中具有鲁棒性的VLA系统。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
EveryDayVLA系统通过以下三个主要贡献解决了上述问题：</p>
<ul>
<li><strong>自适应视野集成器（AdaHorizon）的协同训练：</strong> 论文提出了一种新颖的协同训练方法，结合了基于L1回归的连续动作和自回归离散动作的联合训练。通过监测这两种动作预测之间的不一致性来估计模型不确定性，AdaHorizon动态调整动作视野，从而在实时约束下触发即时重新规划，确保安全可靠的操作。这种方法能够利用离散动作在高级语义推理方面的优势，以及连续动作在精细操作方面的精度。</li>
<li><strong>低成本、集成式6自由度机械臂：</strong> EveryDayVLA设计并构建了一个成本低于300美元的6自由度机械臂。该机械臂具有优于10毫米的重复性，并利用可访问的Arduino Uno breakout板和PCA9685 PWM驱动器进行12位PWM控制，大大降低了硬件成本，使其更易于家庭用户和研究实验室使用。</li>
<li><strong>自动化数据收集管道和公共数据集：</strong> 论文开发了一个简化的遥操作管道，用于收集包含语言指令、视频和末端执行器姿态的轨迹数据。研究发布了一个包含1200多个任务执行的公共数据集，以支持在多样化环境中进行可扩展的微调。</li>
</ul>
<p><strong>3. 主要结果及其重要性：</strong>
EveryDayVLA在多个方面展示了显著的性能提升：</p>
<ul>
<li><strong>LIBERO基准测试：</strong> 在LIBERO模拟基准测试中，EveryDayVLA达到了与最先进模型相当的成功率，在平均成功率方面位居第二。特别是在空间任务套件上，AdaHorizon集成器超越了所有基线，成功率提高了1.6%。</li>
<li><strong>真实世界性能：</strong> 在真实世界测试中，EveryDayVLA在分布内（in-distribution）任务中比现有方法平均提高了49%的成功率，在分布外（out-of-distribution）任务中平均提高了34.9%。这表明EveryDayVLA在处理新颖场景和条件方面的强大泛化能力和鲁棒性。</li>
<li><strong>推理效率：</strong> EveryDayVLA实现了高达108.4 Hz的推理速率，仅比OpenVLA-OFT增加了0.9毫秒的延迟，表明其在保持高性能的同时具有高效率。</li>
<li><strong>泛化和鲁棒性：</strong> 在对未见任务、环境和条件进行泛化和鲁棒性评估时，EveryDayVLA表现最佳，在静态和动态干扰物存在的情况下，性能下降最小。</li>
</ul>
<p>这些结果的重要性在于，EveryDayVLA通过结合最先进的VLA模型和经济高效的硬件，实现了机器人基础模型的民主化，为在家庭和研究实验室中经济地使用机器人铺平了道路。</p>
<p><strong>4. 论文中提及的局限性：</strong>
论文也坦诚地指出了EveryDayVLA的几个局限性：</p>
<ul>
<li><strong>硬件长期鲁棒性：</strong> 尚未确保机械臂的长期机械耐用性。</li>
<li><strong>精细操作限制：</strong> 由于伺服精度有限以及微调数据集中专家演示数量相对较少（与模拟数据相比），EveryDayVLA在执行精细操作时仍存在局限性。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
基于上述局限性，论文提出了以下未来研究方向：</p>
<ul>
<li><strong>提高机械臂的机械耐用性：</strong> 进一步改进硬件设计，以确保长期鲁棒性。</li>
<li><strong>使用更高精度的伺服电机：</strong> 采用更高精度的伺服电机来提升机器人的精细操作能力。</li>
<li><strong>收集更多专家轨迹：</strong> 收集更多高质量的专家轨迹，以巩固数据集，从而改进精细控制。</li>
</ul>
<hr />
<p>这份摘要旨在全面捕捉论文的核心内容，突出其在机器人VLA领域的技术贡献和实际意义。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>While Vision-Language-Action (VLA) models map visual inputs and language
instructions directly to robot actions, they often rely on costly hardware and
struggle in novel or cluttered scenes.</li>
<li>We introduce EverydayVLA, a 6-DOF
manipulator that can be assembled for under $300, capable of modest payloads
and workspace.</li>
<li>On LIBERO,
EverydayVLA matches state-of-the-art success rates, and in real-world tests it
outperforms prior methods by 49% in-distribution and 34.9% out-of-distribution.</li>
<li>By combining a state-of-the-art VLA with cost-effective hardware, EverydayVLA
democratizes access to a robotic foundation model and paves the way for
economical use in homes and research labs alike.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.05397v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.05397v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.05393v1'></a></p>
<h2 id="preresq-r1-towards-fine-grained-rank-and-score-reinforcement-learning-for-visual-quality-assessment-via-preference-response-disentangled-policy-optimization"><a href="https://arxiv.org/abs/2511.05393v1">PreResQ-R1: Towards Fine-Grained Rank-and-Score Reinforcement Learning for Visual Quality Assessment via Preference-Response Disentangled Policy Optimization</a></h2>
<p><strong>Authors:</strong> Zehui Feng, Tian Qiu, Tong Wu, Junxuan Li, Huayuan Xu, Ting Han</p>
<p><strong>Published:</strong> 2025-11-07</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Visual Quality Assessment (QA) seeks to predict human perceptual judgments of
visual fidelity. While recent multimodal large language models (MLLMs) show
promise in reasoning about image and video quality, existing approaches mainly
rely on supervised fine-tuning or rank-only objectives, resulting in shallow
reasoning, poor score calibration, and limited cross-domain generalization. We
propose PreResQ-R1, a Preference-Response Disentangled Reinforcement Learning
framework that unifies absolute score regression and relative ranking
consistency within a single reasoning-driven optimization scheme. Unlike prior
QA methods, PreResQ-R1 introduces a dual-branch reward formulation that
separately models intra-sample response coherence and inter-sample preference
alignment, optimized via Group Relative Policy Optimization (GRPO). This design
encourages fine-grained, stable, and interpretable chain-of-thought reasoning
about perceptual quality. To extend beyond static imagery, we further design a
global-temporal and local-spatial data flow strategy for Video Quality
Assessment. Remarkably, with reinforcement fine-tuning on only 6K images and
28K videos, PreResQ-R1 achieves state-of-the-art results across 10 IQA and 5
VQA benchmarks under both SRCC and PLCC metrics, surpassing by margins of 5.30%
and textbf2.15% in IQA task, respectively. Beyond quantitative gains, it
produces human-aligned reasoning traces that reveal the perceptual cues
underlying quality judgments. Code and model are available.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行详细分析。</p>
<hr />
<h3 id="preresq-r1">论文摘要分析：PreResQ-R1</h3>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>这篇论文提出了一种名为 PreResQ-R1 的新型强化学习框架，用于视觉质量评估（VQA）。它通过解耦偏好-响应的策略优化，将绝对分数回归和相对排序一致性统一起来，解决了现有方法在推理深度、分数校准和泛化能力上的不足。PreResQ-R1 在少量数据上进行强化微调后，在多个图像和视频质量评估基准测试中取得了最先进的性能，并能生成与人类判断一致的推理链。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<ul>
<li><strong>偏好-响应解耦的强化学习框架 (Preference-Response Disentangled Reinforcement Learning):</strong> 这是核心创新。它超越了传统的监督微调或仅基于排序的目标，通过强化学习来优化视觉质量评估。</li>
<li><strong>双分支奖励机制 (Dual-branch Reward Formulation):</strong> PreResQ-R1 引入了一个独特的奖励系统，该系统分别建模了：<ul>
<li><strong>样本内响应一致性 (Intra-sample response coherence):</strong> 确保模型对单个样本的质量预测是连贯和准确的（例如，与绝对分数回归相关）。</li>
<li><strong>样本间偏好对齐 (Inter-sample preference alignment):</strong> 确保模型在比较不同样本时，其相对排序与人类偏好一致（例如，与相对排序一致性相关）。</li>
</ul>
</li>
<li><strong>组相对策略优化 (Group Relative Policy Optimization - GRPO):</strong> 这种优化策略用于训练上述双分支奖励机制，旨在鼓励细粒度、稳定和可解释的思维链推理。</li>
<li><strong>全局-时间与局部-空间数据流策略 (Global-temporal and local-spatial data flow strategy):</strong> 针对视频质量评估（VQA）的特定设计，使其能够有效处理视频数据的时空特性。</li>
<li><strong>推理驱动的优化方案 (Reasoning-driven optimization scheme):</strong> 强调模型不仅给出分数，还能提供关于质量判断的“思维链”，增强了可解释性。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<ul>
<li><strong>提升视觉质量评估的准确性和鲁棒性:</strong> 在多个 IQA 和 VQA 基准测试中取得 SOTA 结果，表明其在预测人类感知质量方面的显著进步。</li>
<li><strong>增强模型的可解释性:</strong> 能够生成“人类对齐的推理轨迹”，揭示质量判断背后的感知线索，这对于理解模型决策和在实际应用中建立信任至关重要。</li>
<li><strong>推动多模态大模型在 VQA 领域的应用:</strong> 解决了现有 MLLMs 在 VQA 中存在的浅层推理、分数校准差和泛化能力有限的问题，为 MLLMs 在感知质量评估中的更深层次应用开辟了道路。</li>
<li><strong>数据效率的提升:</strong> 仅用 6K 图像和 28K 视频进行强化微调就能达到 SOTA 性能，这表明该方法在数据利用效率方面具有优势，对于高质量标注数据稀缺的领域尤其有价值。</li>
<li><strong>统一绝对分数和相对排序:</strong> 成功地将这两种重要的评估范式整合到一个框架中，为未来的 VQA 研究提供了一个新的范式。</li>
</ul>
<p><strong>4. 相关领域或应用</strong></p>
<ul>
<li><strong>图像/视频压缩与编码:</strong> 优化压缩算法以在给定比特率下最大化感知质量。</li>
<li><strong>图像/视频增强与修复:</strong> 评估不同算法对图像/视频质量的改善效果。</li>
<li><strong>内容生成与编辑:</strong> 评估 AI 生成内容（如 GANs、扩散模型）的视觉质量，并指导生成过程。</li>
<li><strong>多媒体内容推荐系统:</strong> 根据用户对视觉质量的偏好进行个性化推荐。</li>
<li><strong>自动驾驶与机器人视觉:</strong> 评估传感器数据或渲染图像的质量，确保系统决策的可靠性。</li>
<li><strong>医学影像分析:</strong> 评估医学图像的质量，辅助诊断。</li>
<li><strong>用户体验 (UX) 研究:</strong> 理解用户对视觉内容的感知和偏好。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<ul>
<li><strong>计算成本:</strong> 强化学习，特别是带有复杂奖励机制和策略优化的框架，通常计算成本较高，训练时间可能较长。摘要中未提及具体的训练资源需求。</li>
<li><strong>奖励函数设计的复杂性:</strong> 双分支奖励机制的设计和平衡可能需要精细的调优，以确保其有效性和稳定性。</li>
<li><strong>“思维链”的质量和泛化性:</strong> 尽管摘要提到生成了“人类对齐的推理轨迹”，但这些轨迹的深度、完整性和在各种复杂场景下的泛化能力仍需进一步验证。它们是否总是能捕捉到人类判断的所有细微之处？</li>
<li><strong>对基础 MLLM 的依赖:</strong> 摘要提到“现有方法主要依赖于监督微调或仅排序目标”，暗示 PreResQ-R1 可能是在 MLLMs 的基础上进行强化微调。因此，其性能可能部分受限于所使用的基础 MLLM 的能力。</li>
<li><strong>“细粒度”的定义和衡量:</strong> 摘要中提到“鼓励细粒度...推理”，但“细粒度”的具体定义和如何量化其效果并未详细说明。</li>
<li><strong>跨领域泛化能力的具体范围:</strong> 摘要提到解决了“有限的跨领域泛化”问题，但其在完全未见过的、与训练数据分布差异很大的领域中的表现如何，仍需进一步探讨。</li>
</ul>
<hr />
<p>总而言之，PreResQ-R1 提出了一种新颖且强大的强化学习方法，有望显著提升视觉质量评估的性能和可解释性，为该领域带来了重要的进展。其在数据效率和统一评估范式方面的优势也使其在实际应用中具有广阔前景。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Remarkably, with reinforcement fine-tuning on only 6K images and
28K videos, PreResQ-R1 achieves state-of-the-art results across 10 IQA and 5
VQA benchmarks under both SRCC and PLCC metrics, surpassing by margins of 5.30%
and textbf2.15% in IQA task, respectively.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.05393v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.05393v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.05308v1'></a></p>
<h2 id="rethinking-metrics-and-diffusion-architecture-for-3d-point-cloud-generation"><a href="https://arxiv.org/abs/2511.05308v1">Rethinking Metrics and Diffusion Architecture for 3D Point Cloud Generation</a></h2>
<p><strong>Authors:</strong> Matteo Bastico, David Ryckelynck, Laurent Corté, Yannick Tillier, Etienne Decencière</p>
<p><strong>Published:</strong> 2025-11-07</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>As 3D point clouds become a cornerstone of modern technology, the need for
sophisticated generative models and reliable evaluation metrics has grown
exponentially. In this work, we first expose that some commonly used metrics
for evaluating generated point clouds, particularly those based on Chamfer
Distance (CD), lack robustness against defects and fail to capture geometric
fidelity and local shape consistency when used as quality indicators. We
further show that introducing samples alignment prior to distance calculation
and replacing CD with Density-Aware Chamfer Distance (DCD) are simple yet
essential steps to ensure the consistency and robustness of point cloud
generative model evaluation metrics. While existing metrics primarily focus on
directly comparing 3D Euclidean coordinates, we present a novel metric, named
Surface Normal Concordance (SNC), which approximates surface similarity by
comparing estimated point normals. This new metric, when combined with
traditional ones, provides a more comprehensive evaluation of the quality of
generated samples. Finally, leveraging recent advancements in transformer-based
models for point cloud analysis, such as serialized patch attention , we
propose a new architecture for generating high-fidelity 3D structures, the
Diffusion Point Transformer. We perform extensive experiments and comparisons
on the ShapeNet dataset, showing that our model outperforms previous solutions,
particularly in terms of quality of generated point clouds, achieving new
state-of-the-art. Code available at
https://github.com/matteo-bastico/DiffusionPointTransformer.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行深入分析。</p>
<hr />
<p><strong>论文摘要分析：Rethinking Metrics and Diffusion Architecture for 3D Point Cloud Generation</strong></p>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>这篇论文主要贡献在于两方面：首先，它揭示了现有3D点云生成评估指标（特别是基于Chamfer Distance (CD) 的）的局限性，并提出了改进的评估方法，包括引入样本对齐和使用密度感知Chamfer Distance (DCD)，以及一种新的表面法线一致性 (SNC) 指标。其次，论文提出了一种名为 Diffusion Point Transformer 的新型扩散模型架构，用于生成高质量的3D点云，并在ShapeNet数据集上取得了最先进的性能。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<ul>
<li><strong>评估指标的创新与改进：</strong><ul>
<li><strong>揭示现有CD指标的缺陷：</strong> 指出CD在评估点云质量时对缺陷缺乏鲁棒性，且未能有效捕捉几何保真度和局部形状一致性。</li>
<li><strong>改进CD基准评估：</strong> 提出在距离计算前进行样本对齐，并用密度感知Chamfer Distance (DCD) 替代传统CD，以提高评估的一致性和鲁棒性。</li>
<li><strong>引入Surface Normal Concordance (SNC) 新指标：</strong> 这是一项全新的创新，通过比较估计的点法线来近似表面相似性，弥补了传统指标仅关注欧氏坐标的不足，提供了更全面的质量评估。</li>
</ul>
</li>
<li><strong>生成模型架构的创新：</strong><ul>
<li><strong>Diffusion Point Transformer：</strong> 结合了扩散模型和基于Transformer的点云分析技术（如序列化补丁注意力），设计了一种新的架构，旨在生成高保真度的3D结构。这是将Transformer的强大建模能力与扩散模型的生成优势相结合，以应对3D点云生成的复杂性。</li>
</ul>
</li>
</ul>
<p><strong>3. 对领域潜在影响</strong></p>
<ul>
<li><strong>推动3D点云生成评估的标准化和准确性：</strong> 论文对现有评估指标的批判性分析和提出的改进方案（DCD、SNC）将促使研究人员重新审视和采用更鲁棒、更全面的评估方法，从而更准确地衡量生成模型的性能。这将有助于避免因评估不当而导致的误导性结论。</li>
<li><strong>提升3D点云生成模型的性能上限：</strong> Diffusion Point Transformer 的提出及其在ShapeNet数据集上取得的最先进成果，为3D点云生成领域树立了新的标杆。它展示了结合Transformer和扩散模型在生成复杂3D结构方面的巨大潜力，可能会启发更多基于类似思想的后续研究。</li>
<li><strong>促进对几何细节和局部一致性的关注：</strong> SNC指标的引入强调了表面法线在评估3D形状质量中的重要性，这将鼓励未来的生成模型不仅关注点的位置分布，更要关注生成形状的几何细节和局部平滑性。</li>
</ul>
<p><strong>4. 相关领域或应用受益</strong></p>
<ul>
<li><strong>计算机图形学和动画：</strong> 自动生成高质量的3D模型，用于游戏开发、电影制作、虚拟现实/增强现实内容创建。</li>
<li><strong>机器人学和自主驾驶：</strong> 生成逼真的3D环境或物体，用于训练机器人感知系统、路径规划和模拟。</li>
<li><strong>工业设计和产品原型：</strong> 快速生成多种设计方案的3D模型，加速产品开发周期。</li>
<li><strong>医疗影像和生物医学：</strong> 生成或重建复杂的生物结构，用于疾病诊断、手术规划和医学研究。</li>
<li><strong>文化遗产数字化：</strong> 修复或重建受损的3D文物模型。</li>
<li><strong>3D打印和制造：</strong> 生成可直接用于3D打印的高质量模型。</li>
</ul>
<p><strong>5. 从摘要中可推断出的局限性</strong></p>
<ul>
<li><strong>计算成本：</strong> 扩散模型通常在训练和采样阶段计算成本较高，特别是对于高分辨率的3D点云。Transformer模型也以其高计算复杂度而闻名。摘要中未提及计算效率，这可能是一个潜在的挑战。</li>
<li><strong>泛化能力：</strong> 实验主要在ShapeNet数据集上进行。虽然ShapeNet是标准基准，但其物体类别和复杂性有限。模型在更复杂、多样化的真实世界场景或特定应用领域（如医学影像、建筑）中的泛化能力尚待验证。</li>
<li><strong>SNC指标的鲁棒性：</strong> SNC依赖于估计的点法线。点法线的估计本身就是一个具有挑战性的任务，尤其是在点云稀疏、噪声大或具有尖锐特征的区域。SNC对法线估计误差的敏感性如何，摘要中未详细说明。</li>
<li><strong>“序列化补丁注意力”的具体实现细节：</strong> 摘要提到了“serialized patch attention”，但未深入解释其在3D点云上下文中的具体工作原理以及如何与扩散模型结合，这可能需要读者查阅相关引用或论文正文。</li>
<li><strong>对不同类型缺陷的敏感性：</strong> 摘要提到现有CD指标对“缺陷”缺乏鲁棒性，但未具体说明是哪种类型的缺陷（例如，孔洞、噪声、不均匀采样、拓扑错误等）。新的指标和模型在处理这些特定缺陷方面的表现如何，仍需进一步探讨。</li>
</ul>
<hr />
<p>总而言之，这篇论文在3D点云生成领域做出了双重贡献：既提出了更可靠的评估工具，又开发了更强大的生成模型。这对于推动该领域的基础研究和实际应用都具有重要意义。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>While existing metrics primarily focus on
directly comparing 3D Euclidean coordinates, we present a novel metric, named
Surface Normal Concordance (SNC), which approximates surface similarity by
comparing estimated point normals.</li>
<li>This new metric, when combined with
traditional ones, provides a more comprehensive evaluation of the quality of
generated samples.</li>
<li>Finally, leveraging recent advancements in transformer-based
models for point cloud analysis, such as serialized patch attention , we
propose a new architecture for generating high-fidelity 3D structures, the
Diffusion Point Transformer.</li>
<li>We perform extensive experiments and comparisons
on the ShapeNet dataset, showing that our model outperforms previous solutions,
particularly in terms of quality of generated point clouds, achieving new
state-of-the-art.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.05308v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.05308v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.05299v1'></a></p>
<h2 id="livestar-live-streaming-assistant-for-real-world-online-video-understanding"><a href="https://arxiv.org/abs/2511.05299v1">LiveStar: Live Streaming Assistant for Real-World Online Video Understanding</a></h2>
<p><strong>Authors:</strong> Zhenyu Yang, Kairui Zhang, Yuhang Hu, Bing Wang, Shengsheng Qian, Bin Wen, Fan Yang, Tingting Gao, Weiming Dong, Changsheng Xu</p>
<p><strong>Published:</strong> 2025-11-07</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Despite significant progress in Video Large Language Models (Video-LLMs) for
offline video understanding, existing online Video-LLMs typically struggle to
simultaneously process continuous frame-by-frame inputs and determine optimal
response timing, often compromising real-time responsiveness and narrative
coherence. To address these limitations, we introduce LiveStar, a pioneering
live streaming assistant that achieves always-on proactive responses through
adaptive streaming decoding. Specifically, LiveStar incorporates: (1) a
training strategy enabling incremental video-language alignment for
variable-length video streams, preserving temporal consistency across
dynamically evolving frame sequences; (2) a response-silence decoding framework
that determines optimal proactive response timing via a single forward pass
verification; (3) memory-aware acceleration via peak-end memory compression for
online inference on 10+ minute videos, combined with streaming key-value cache
to achieve 1.53x faster inference. We also construct an OmniStar dataset, a
comprehensive dataset for training and benchmarking that encompasses 15 diverse
real-world scenarios and 5 evaluation tasks for online video understanding.
Extensive experiments across three benchmarks demonstrate LiveStar's
state-of-the-art performance, achieving an average 19.5% improvement in
semantic correctness with 18.1% reduced timing difference compared to existing
online Video-LLMs, while improving FPS by 12.0% across all five OmniStar tasks.
Our model and dataset can be accessed at https://github.com/yzy-bupt/LiveStar.</p>
<p><strong>Analysis:</strong></p>
<p>以下是Zhenyu Yang等人撰写的论文“LiveStar: Live Streaming Assistant for Real-World Online Video Understanding”的摘要：</p>
<p><strong>1. 主要问题或研究问题</strong>
尽管离线视频理解在视频大型语言模型（Video-LLMs）方面取得了显著进展，但现有的在线Video-LLMs在同时处理连续的逐帧输入和确定最佳响应时机方面面临挑战，这通常会损害实时响应能力和叙事连贯性。本研究旨在解决这些限制，开发一个能够实现始终在线主动响应的直播流媒体助手。</p>
<p><strong>2. 关键创新或方法论贡献</strong>
LiveStar引入了以下关键创新来解决上述问题：
*   <strong>增量视频-语言对齐训练策略（Streaming Causal Attention Masks, SCAM）</strong>：该策略通过构建交错的帧-字幕序列，在训练期间实现可变长度视频流的增量视频-语言对齐，确保动态演进帧序列中的时间一致性。
*   <strong>响应-静默解码框架（Streaming Verification Decoding, SVeD）</strong>：通过单次前向验证确定最佳主动响应时机，抑制冗余输出，并通过策略性静默保持叙事连续性。
*   <strong>内存感知加速</strong>：通过峰值-末端内存压缩（Peak-End Memory Compression）技术处理10分钟以上的视频，并结合流式键值缓存，实现1.53倍的推理速度提升。
*   <strong>OmniStar数据集</strong>：构建了一个全面的训练和基准测试数据集，涵盖15种不同的真实世界场景和5项在线视频理解评估任务。</p>
<p><strong>3. 主要结果及其意义</strong>
LiveStar在三个基准测试中展现了最先进的性能：
*   与现有在线Video-LLMs相比，语义正确性平均提高了19.5%。
*   时间差异减少了18.1%。
*   在所有五项OmniStar任务中，FPS提高了12.0%。
这些结果表明LiveStar在实时视频理解方面具有卓越的性能，能够提供更准确、及时和连贯的响应，显著优于现有方法。</p>
<p><strong>4. 论文中提到的局限性</strong>
*   <strong>视觉细节捕获限制</strong>：为了提高推理效率，LiveStar将每个视频帧压缩成16个视觉token。这种紧凑的表示虽然显著降低了计算成本，但不可避免地会损害模型捕获细粒度视觉细节的能力，可能在涉及细微运动变化或复杂场景动态的场景中遗漏关键线索。
*   <strong>多模态信息整合不足</strong>：当前版本的LiveStar仅支持视觉-文本模态，不包含音频信息。这一限制约束了模型在视频理解任务中充分利用多模态线索的能力。</p>
<p><strong>5. 潜在的未来研究方向</strong>
*   <strong>整合音频模态</strong>：未来的工作旨在将音频模态纳入模型，以实现更全面的多模态推理。
*   <strong>改进细粒度视觉理解</strong>：探索在保持推理效率的同时，提高模型捕获细粒度视觉细节的能力。
*   <strong>更广泛的应用场景</strong>：进一步探索LiveStar在直播平台、监控系统和电影工具等更多真实世界场景中的应用潜力。</p>
<p>LiveStar通过其创新的响应-静默范式和全面的基准测试，为在线视频理解领域开辟了新方向，有望刺激未来在复杂在线视频理解任务中开发更先进模型的研究。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these limitations, we introduce LiveStar, a pioneering
live streaming assistant that achieves always-on proactive responses through
adaptive streaming decoding.</li>
<li>Extensive experiments across three benchmarks demonstrate LiveStar's
state-of-the-art performance, achieving an average 19.5% improvement in
semantic correctness with 18.1% reduced timing difference compared to existing
online Video-LLMs, while improving FPS by 12.0% across all five OmniStar tasks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.05299v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.05299v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.05245v1'></a></p>
<h2 id="adpretrain-advancing-industrial-anomaly-detection-via-anomaly-representation-pretraining"><a href="https://arxiv.org/abs/2511.05245v1">ADPretrain: Advancing Industrial Anomaly Detection via Anomaly Representation Pretraining</a></h2>
<p><strong>Authors:</strong> Xincheng Yao, Yan Luo, Zefeng Qian, Chongyang Zhang</p>
<p><strong>Published:</strong> 2025-11-07</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>The current mainstream and state-of-the-art anomaly detection (AD) methods
are substantially established on pretrained feature networks yielded by
ImageNet pretraining. However, regardless of supervised or self-supervised
pretraining, the pretraining process on ImageNet does not match the goal of
anomaly detection (i.e., pretraining in natural images doesn't aim to
distinguish between normal and abnormal). Moreover, natural images and
industrial image data in AD scenarios typically have the distribution shift.
The two issues can cause ImageNet-pretrained features to be suboptimal for AD
tasks. To further promote the development of the AD field, pretrained
representations specially for AD tasks are eager and very valuable. To this
end, we propose a novel AD representation learning framework specially designed
for learning robust and discriminative pretrained representations for
industrial anomaly detection. Specifically, closely surrounding the goal of
anomaly detection (i.e., focus on discrepancies between normals and anomalies),
we propose angle- and norm-oriented contrastive losses to maximize the angle
size and norm difference between normal and abnormal features simultaneously.
To avoid the distribution shift from natural images to AD images, our
pretraining is performed on a large-scale AD dataset, RealIAD. To further
alleviate the potential shift between pretraining data and downstream AD
datasets, we learn the pretrained AD representations based on the
class-generalizable representation, residual features. For evaluation, based on
five embedding-based AD methods, we simply replace their original features with
our pretrained representations. Extensive experiments on five AD datasets and
five backbones consistently show the superiority of our pretrained features.
The code is available at https://github.com/xcyao00/ADPretrain.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Xincheng Yao等人撰写的论文“ADPretrain: Advancing Industrial Anomaly Detection via Anomaly Representation Pretraining”的全面摘要。</p>
<hr />
<h3 id="adpretrain">论文摘要：ADPretrain：通过异常表示预训练推进工业异常检测</h3>
<p><strong>1. 主要问题或研究问题：</strong>
当前主流的异常检测（AD）方法严重依赖于ImageNet预训练的特征网络。然而，这种预训练存在两个核心问题：
1. <strong>目标不匹配：</strong> ImageNet预训练（无论是监督还是自监督）旨在区分自然图像中的不同类别，而非区分正常与异常，这与异常检测的根本目标不符。
2. <strong>领域漂移：</strong> 自然图像与工业AD场景中的图像数据存在显著的分布差异。
这两个问题导致ImageNet预训练的特征对于AD任务而言并非最优，限制了AD领域的进一步发展。因此，论文旨在解决如何为AD任务学习专门的、鲁棒且具有判别力的预训练表示这一关键问题。</p>
<p><strong>2. 关键创新或方法贡献：</strong>
论文提出了一种新颖的AD表示学习框架，名为ADPretrain，其核心创新点包括：
*   <strong>异常表示预训练范式：</strong> 首次专门针对AD任务进行表示预训练，旨在学习能够有效区分正常和异常的特征。
*   <strong>角度-和范数-导向的对比损失：</strong> 为了最大化正常和异常特征之间的差异，论文设计了两种新型对比损失。
    *   <strong>角度-导向对比损失：</strong> 旨在最大化正常和异常特征之间的角度大小，通过将特征相对于正常特征中心进行偏移来计算余弦相似度，并确保负样本仅包含异常特征。
    *   <strong>范数-导向对比损失：</strong> 旨在最大化正常和异常特征之间的范数差异，通过将正常特征收缩到超球面内部，并将异常特征推到超球面外部，引入了伪Huber距离和动态边界。
*   <strong>大规模AD数据集RealIAD上的预训练：</strong> 为了避免自然图像到AD图像的分布漂移，预训练是在大规模工业AD数据集RealIAD上进行的，该数据集包含大量正常和异常图像。
*   <strong>基于残差特征的表示学习：</strong> 为了进一步缓解预训练数据与下游AD数据集之间的潜在漂移，论文基于类泛化残差特征学习预训练AD表示，这些残差特征通过匹配和减去正常参考特征获得，被认为具有领域不变性。
*   <strong>可学习的键/值注意力特征投影器：</strong> 论文设计了一个基于Transformer架构的特征投影器，其中用可学习的键/值注意力替代了传统的自注意力机制。通过将输入特征视为查询，可学习的参考表示作为键和值，并通过减法操作融合输入特征和注意力输出，以自适应地消除残差特征分布中的正常表示，进一步增加正常和异常残差特征之间的差异。</p>
<p><strong>3. 主要结果及其意义：</strong>
论文在五个AD数据集（MVTecAD、VisA、BTAD、MVTec3D、MPDD）和五种骨干网络（CLIP系列、DINOv2系列、ImageBind）上进行了广泛实验。
*   <strong>显著的性能提升：</strong> 将ADPretrain预训练的特征应用于五种基于嵌入的AD方法（PaDiM、PatchCore、CFLOW、GLASS、UniAD），结果一致表明其性能优于使用原始ImageNet预训练特征的方法。这证明了为AD任务学习专用预训练特征的有效性和价值。
*   <strong>更好的样本效率：</strong> 在仅使用10%正常样本进行训练的少样本AD场景中，ADPretrain的预训练特征带来了更显著的性能提升，表明其有助于提高下游AD模型的样本效率。
*   <strong>更强的鲁棒性：</strong> 在训练数据中加入噪声的情况下，ADPretrain的预训练特征表现出比原始特征更强的鲁棒性。
*   <strong>少样本异常检测能力：</strong> 论文提出的简单FeatureNorm基线（直接使用特征范数作为异常分数）在少样本AD设置下，性能可与主流FSAD方法（如KAG-Prompt）相媲美甚至更优，这突显了ADPretrain特征强大的表示能力。
*   <strong>直观的可视化：</strong> t-SNE可视化结果显示，通过ADPretrain预训练，正常特征更加紧凑，正常和异常特征之间分离更明显，证明了其学习到的AD表示更具判别力。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>骨干网络固定：</strong> 目前的方法固定了骨干网络，仅优化特征投影器。理想的AD特征提取网络应根据异常检测的特性专门设计骨干网络，并在预训练期间有效学习。
*   <strong>适用范围受限：</strong> 预训练特征目前只能集成到基于嵌入的AD方法中。对于不依赖预训练特征提取器的其他AD方法（例如基于扩散的方法），无法从中受益。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>全骨干网络预训练：</strong> 未来的工作可以探索设计和学习专门用于AD任务的完整骨干网络，使其能够同时考虑多个正常参考样本以增加正常上下文模式，并在网络编码过程中隐式体现正常和异常之间的对比。
*   <strong>更广泛的AD方法支持：</strong> 探索适用于更多AD方法的预训练框架，以扩大ADPretrain方法的应用范围。
*   <strong>更大规模、更高质量的AD数据集：</strong> 进一步强调AD领域需要更大规模、更高质量的数据集来支持全面的异常表示预训练。</p>
<hr />
<p>这篇论文通过提出一种新颖的AD表示预训练框架，为工业异常检测领域带来了重要突破。其核心思想是，通过专门设计的对比损失和大规模工业数据集上的预训练，可以学习到比通用ImageNet预训练特征更优越的、更具判别力和鲁棒性的异常表示。这不仅提升了现有AD方法的性能，也为少样本AD和未来AD研究开辟了新的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>The current mainstream and state-of-the-art anomaly detection (AD) methods
are substantially established on pretrained feature networks yielded by
ImageNet pretraining.</li>
<li>To this
end, we propose a novel AD representation learning framework specially designed
for learning robust and discriminative pretrained representations for
industrial anomaly detection.</li>
<li>Specifically, closely surrounding the goal of
anomaly detection (i.e., focus on discrepancies between normals and anomalies),
we propose angle- and norm-oriented contrastive losses to maximize the angle
size and norm difference between normal and abnormal features simultaneously.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.05245v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.05245v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.05203v1'></a></p>
<h2 id="beyond-master-and-apprentice-grounding-foundation-models-for-symbiotic-interactive-learning-in-a-shared-latent-space"><a href="https://arxiv.org/abs/2511.05203v1">Beyond Master and Apprentice: Grounding Foundation Models for Symbiotic Interactive Learning in a Shared Latent Space</a></h2>
<p><strong>Authors:</strong> Linus Nwankwo, Björn Ellensohn, Christian Rauch, Elmar Rueckert</p>
<p><strong>Published:</strong> 2025-11-07</p>
<p><strong>Categories:</strong> cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Today's autonomous agents can understand free-form natural language
instructions and execute long-horizon tasks in a manner akin to human-level
reasoning. These capabilities are mostly driven by large-scale pre-trained
foundation models (FMs). However, the approaches with which these models are
grounded for human-robot interaction (HRI) perpetuate a master-apprentice
model, where the apprentice (embodied agent) passively receives and executes
the master's (human's) commands without reciprocal learning. This reactive
interaction approach does not capture the co-adaptive dynamics inherent in
everyday multi-turn human-human interactions. To address this, we propose a
Symbiotic Interactive Learning (SIL) approach that enables both the master and
the apprentice to co-adapt through mutual, bidirectional interactions. We
formalised SIL as a co-adaptation process within a shared latent task space,
where the agent and human maintain joint belief states that evolve based on
interaction history. This enables the agent to move beyond reactive execution
to proactive clarification, adaptive suggestions, and shared plan refinement.
To realise these novel behaviours, we leveraged pre-trained FMs for spatial
perception and reasoning, alongside a lightweight latent encoder that grounds
the models' outputs into task-specific representations. Furthermore, to ensure
stability as the tasks evolve, we augment SIL with a memory architecture that
prevents the forgetting of learned task-space representations. We validate SIL
on both simulated and real-world embodied tasks, including instruction
following, information retrieval, query-oriented reasoning, and interactive
dialogues. Demos and resources are public
at:~\href{https://linusnep.github.io/SIL/}{https://linusnep.github.io/SIL/}.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“Beyond Master and Apprentice: Grounding Foundation Models for Symbiotic Interactive Learning in a Shared Latent Space”论文的全面摘要，用中文撰写，并突出其在计算机视觉和机器学习领域的新颖性和重要性。</p>
<hr />
<h3 id="_1">《超越师徒模式：在共享潜在空间中为共生交互学习奠定基础模型》论文摘要</h3>
<p><strong>1. 主要问题或研究问题：</strong>
当前人机交互（HRI）中，基于预训练基础模型（FMs）的自主智能体虽然能理解自然语言指令并执行复杂任务，但普遍采用“师徒模式”。在这种模式下，学徒（具身智能体）被动地接收并执行师傅（人类）的指令，缺乏相互学习和共同适应的能力。这种反应式交互无法捕捉日常多轮人机交互中固有的协同适应动态，导致交互肤浅、对歧义脆弱，且无法实现长期适应和共享心智模型。论文旨在解决这种单向接地问题，并提出一种能够实现人类和智能体之间持续相互适应的HRI框架。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>共生交互学习（SIL）框架：</strong> 论文提出了SIL方法，将人机交互重新构想为一个动态的、协同适应的过程。它使人类和智能体都能通过相互、双向的交互进行协同适应，超越了被动执行，实现了主动澄清、自适应建议和共享计划细化。
*   <strong>共享潜在任务空间中的协同适应：</strong> SIL被形式化为共享潜在任务空间中的协同适应过程，其中智能体和人类根据交互历史维护并演化联合信念状态。信念状态包括潜在任务嵌入、置信度标量、不确定性表示和时间记忆，人类信念状态还包含偏好模型。
*   <strong>双向影响机制：</strong> 引入了一种新颖的方法，通过学习转换来明确表示、测量和对齐人类和智能体的信念，从而实现双向影响机制。当信念对齐度低于阈值时，会触发澄清协议，以解决潜在的表征差异。
*   <strong>不确定性感知语言理解和解析：</strong> 采用多方面方法处理自然语言理解、不确定性量化和命令解析，结合基于集成模型的推理、语言特征分析和上下文感知解析，以确保用户输入的可靠解释。
*   <strong>持续学习架构和记忆机制：</strong> 开发了一种包含结构化情景记忆和语义记忆的持续学习架构，以保留知识并防止灾难性遗忘。情景记忆存储详细的交互数据，语义记忆将积累的交互整合为通用模式。通过弹性权重整合（EWC）机制，确保任务演化时的稳定性。
*   <strong>多模态感知和动作执行：</strong> 利用预训练的视觉-语言模型（如SAM和CLIP）进行零样本实例分割和开放词汇对象识别，将对象投影到3D坐标中，并结合卡尔曼滤波器进行姿态估计和跟踪，实现物理世界的视觉接地。</p>
<p><strong>3. 主要结果及其重要性：</strong>
*   <strong>显著的性能提升：</strong> 在模拟和真实世界的具身任务（包括指令遵循、信息检索、面向查询的推理和交互式对话）中，SIL在所有任务领域都显著优于单向基线和消融变体。平均任务完成率达到87-94%以上，比最佳消融变体提高了近20个百分点。
*   <strong>高效的交互和鲁棒性：</strong> SIL平均每任务澄清请求仅为0.46次，显示出更高的交互效率和鲁棒性。
*   <strong>信念对齐的快速收敛和持续维持：</strong> SIL展示了信念对齐的快速收敛，并能在整个交互过程中保持高对齐度（p ≈ 0.83），这直接证明了共享潜在空间中协同适应动态的有效性。
*   <strong>消融研究的重要性：</strong> 消融研究证实了协同适应机制、EWC、记忆、人类偏好建模和不确定性处理在SIL框架中的关键作用，其中协同适应机制的缺失导致了最显著的性能下降。</p>
<p><strong>4. 论文中提及的局限性：</strong>
论文中没有明确列出当前方法的局限性，但从其未来研究方向中可以推断出一些潜在的挑战，例如计算和可扩展性问题。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
未来的工作将解决扩展SIL时的计算和可扩展性挑战。</p>
<hr />
<p><strong>对计算机视觉领域的新颖性或重要性：</strong></p>
<p>这篇论文在计算机视觉和机器学习领域具有显著的新颖性和重要性，主要体现在以下几个方面：</p>
<ol>
<li><strong>突破传统HRI范式：</strong> 论文从根本上挑战了当前HRI中普遍存在的“师徒模式”，提出了一种更符合人类-人类交互的“共生”模式。这对于推动具身智能体从被动执行者向主动、智能的协作伙伴转变至关重要。</li>
<li><strong>共享潜在空间中的多模态融合：</strong> 通过将视觉感知（利用SAM和CLIP等先进的视觉-语言模型）与语言理解、信念状态和记忆机制在共享潜在任务空间中进行融合，SIL提供了一个统一的认知架构，这在多模态HRI领域是一个重要的进步。它不仅仅是简单地结合不同模态的信息，而是通过共享信念状态实现深层次的协同适应。</li>
<li><strong>不确定性感知与主动交互：</strong> 引入不确定性量化和主动澄清机制，使得智能体能够识别自身的理解局限性并主动寻求帮助，而不是被动地等待人类纠正。这对于在复杂、动态的真实世界环境中部署鲁棒的具身智能体至关重要。</li>
<li><strong>持续学习与记忆保障：</strong> 结合情景记忆、语义记忆和EWC等持续学习技术，解决了具身智能体在长期交互中面临的灾难性遗忘问题。这使得智能体能够积累经验、泛化知识，并随着时间的推移不断适应和个性化，这在实际应用中具有巨大的价值。</li>
<li><strong>对基础模型的有效利用与接地：</strong> 论文巧妙地利用了大型预训练基础模型（如LLM和VLM）的强大能力，并通过轻量级潜在编码器将其输出接地到任务特定的表示中，从而实现了高效且可控的具身智能体行为。这为如何将大型通用模型应用于特定具身任务提供了新的思路。</li>
</ol>
<p>总而言之，这篇论文为构建更智能、更具协作性、更像人类的具身智能体提供了一个全面的框架，其在共享潜在空间中的协同适应、不确定性感知和持续学习机制，为计算机视觉和机器学习领域在人机交互和具身智能体研究方面开辟了新的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we propose a
Symbiotic Interactive Learning (SIL) approach that enables both the master and
the apprentice to co-adapt through mutual, bidirectional interactions.</li>
<li>To realise these novel behaviours, we leveraged pre-trained FMs for spatial
perception and reasoning, alongside a lightweight latent encoder that grounds
the models' outputs into task-specific representations.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.05203v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.05203v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.05168v1'></a></p>
<h2 id="another-brixel-in-the-wall-towards-cheaper-dense-features"><a href="https://arxiv.org/abs/2511.05168v1">Another BRIXEL in the Wall: Towards Cheaper Dense Features</a></h2>
<p><strong>Authors:</strong> Alexander Lappe, Martin A. Giese</p>
<p><strong>Published:</strong> 2025-11-07</p>
<p><strong>Categories:</strong> cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Vision foundation models achieve strong performance on both global and
locally dense downstream tasks. Pretrained on large images, the recent DINOv3
model family is able to produce very fine-grained dense feature maps, enabling
state-of-the-art performance. However, computing these feature maps requires
the input image to be available at very high resolution, as well as large
amounts of compute due to the squared complexity of the transformer
architecture. To address these issues, we propose BRIXEL, a simple knowledge
distillation approach that has the student learn to reproduce its own feature
maps at higher resolution. Despite its simplicity, BRIXEL outperforms the
baseline DINOv3 models by large margins on downstream tasks when the resolution
is kept fixed. Moreover, it is able to produce feature maps that are very
similar to those of the teacher at a fraction of the computational cost. Code
and model weights are available at https://github.com/alexanderlappe/BRIXEL.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Alexander Lappe和Martin A. Giese撰写的论文“Another BRIXEL in the Wall: Towards Cheaper Dense Features”的全面摘要。</p>
<hr />
<p><strong>论文摘要：Another BRIXEL in the Wall: Towards Cheaper Dense Features</strong></p>
<p><strong>1. 主要问题或研究问题</strong>
该论文旨在解决视觉基础模型（特别是DINOv3模型家族）在生成高分辨率密集特征图时面临的计算效率问题。DINOv3模型在各种全局和局部密集下游任务中表现出色，能够生成非常精细的密集特征图。然而，由于Transformer架构的平方复杂度，计算这些特征图需要高分辨率的输入图像和大量的计算资源，这使得其在实际应用中成本高昂且受限。核心问题是如何在保持特征图质量的同时，显著降低生成这些高分辨率密集特征图的计算成本。</p>
<p><strong>2. 关键创新或方法论贡献</strong>
论文提出了名为<strong>BRIXEL</strong>（<strong>B</strong>rick in the Wall）的简单知识蒸馏方法，以解决上述问题。其主要创新点包括：
*   <strong>自蒸馏（Self-Distillation）框架：</strong> BRIXEL采用教师-学生（teacher-student）设置，其中教师和学生是共享冻结权重的相同网络。教师接收高分辨率输入图像，而学生接收降采样后的低分辨率图像。
*   <strong>Refiner网络和卷积头部：</strong> 学生网络连接到一个可训练的Refiner网络和一个卷积头部。Refiner网络旨在与低分辨率模型协同工作，以输出与高分辨率模型产生的密集特征图相同的特征图。
*   <strong>多重损失函数：</strong> 训练过程中，优化Refiner网络和头部的权重，使其输出模仿教师的输出。损失函数包括：
    *   <strong>L1损失：</strong> 衡量学生和教师输出特征图之间的像素级差异。
    *   <strong>边缘损失（Edge Loss）：</strong> 鼓励学生特征图匹配教师特征图的Sobel边缘检测器输出，以确保生成忠实的边界。
    *   <strong>频谱损失（Spectral Loss）：</strong> 鼓励学生和教师输出之间具有相似的高频分量，通过比较傅里叶变换后的频率谱实现。
*   <strong>计算效率提升：</strong> 通过避免Transformer架构的平方复杂度，BRIXEL能够在显著降低计算成本（FLOPs、运行时和内存）的情况下生成高分辨率特征图。</p>
<p><strong>3. 主要结果及其意义</strong>
*   <strong>性能超越基线：</strong> 在固定分辨率下，BRIXEL在各种下游任务（包括场景语义分割、单目深度估计和物体部分分割）上，其性能显著优于基线DINOv3模型。在42项模型比较中，BRIXEL在每一项上都超越了基线。
*   <strong>高分辨率特征图的忠实再现：</strong> BRIXEL能够生成与教师模型在高计算成本下产生的高分辨率特征图在视觉上几乎无法区分的特征图，即使输入图像分辨率较低。
*   <strong>显著的计算资源节省：</strong> BRIXEL在生成密集特征图时，在FLOPs、运行时和内存方面实现了巨大的节省。例如，使用Huge+模型可以在仅有4GB VRAM的笔记本电脑上生成特征图。这对于需要大量模型推理的下游应用具有重要意义。
*   <strong>泛化能力：</strong> 经过微调的BRIXEL模型在ADE20k数据集上，在128到512之间的所有图像尺寸下都优于DINOv3基线，表明该方法能够很好地泛化到训练图像尺寸之外的各种图像尺寸。
*   <strong>对其他骨干模型的适用性：</strong> 论文还展示了BRIXEL可以成功应用于其他基础模型（如SigLIP 2），尽管教师模型本身可能存在噪声特征图，BRIXEL学生仍能重建出合理的教师输出。</p>
<p><strong>4. 论文中提及的局限性</strong>
*   <strong>特征图噪声：</strong> 在应用于SigLIP 2等其他骨干模型时，BRIXEL学生会重现教师模型中存在的高频噪声，这可能源于图像尺寸之间不完美的泛化。
*   <strong>仅关注模型输出：</strong> 本项目主要研究模型输出的密集特征图。然而，一些工作表明中间特征对于空间任务也很有用。</p>
<p><strong>5. 潜在的未来研究方向</strong>
*   <strong>多阶段高分辨率特征重建：</strong> 未来的工作可以探索在多个阶段整合更高分辨率特征的重建。
*   <strong>改进高分辨率预训练：</strong> 论文建议未来的视觉基础模型迭代应包含一些高分辨率预训练，而不是为不同图像尺寸训练不同的模型。
*   <strong>优化噪声特征图：</strong> 针对其他骨干模型中出现的噪声特征图问题，可以研究如何改进BRIXEL的训练或架构，以减少或消除这种噪声。</p>
<hr />
<p>总而言之，这篇论文提出了一种简单而强大的知识蒸馏方法BRIXEL，它使得视觉基础模型能够在显著降低计算成本的同时，生成高质量、高分辨率的密集特征图。这不仅提升了现有模型在各种下游任务上的性能，也为资源受限环境下的实际应用提供了可行方案，并为未来视觉基础模型的训练策略提供了重要启示。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Pretrained on large images, the recent DINOv3
model family is able to produce very fine-grained dense feature maps, enabling
state-of-the-art performance.</li>
<li>To address these issues, we propose BRIXEL, a simple knowledge
distillation approach that has the student learn to reproduce its own feature
maps at higher resolution.</li>
<li>Despite its simplicity, BRIXEL outperforms the
baseline DINOv3 models by large margins on downstream tasks when the resolution
is kept fixed.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.05168v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.05168v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-11-10 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
