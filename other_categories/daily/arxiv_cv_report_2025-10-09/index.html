<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-10-09 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-21
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-10-08/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-10-10/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-10-09">Arxiv Computer Vision Papers - 2025-10-09</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#arxiv-2025-10-08" class="nav-link">Arxiv 计算机视觉每日报告执行摘要 (2025-10-08)</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#quantum-enhanced-computer-vision-going-beyond-classical-algorithms" class="nav-link">Quantum-enhanced Computer Vision: Going Beyond Classical Algorithms</a>
                </li>
                <li class="nav-item">
                    <a href="#vision-language-action-models-for-robotics-a-review-towards-real-world-applications" class="nav-link">Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications</a>
                </li>
                <li class="nav-item">
                    <a href="#diffusion-models-for-low-light-image-enhancement-a-multi-perspective-taxonomy-and-performance-analysis" class="nav-link">Diffusion Models for Low-Light Image Enhancement: A Multi-Perspective Taxonomy and Performance Analysis</a>
                </li>
                <li class="nav-item">
                    <a href="#matrix-mask-track-alignment-for-interaction-aware-video-generation" class="nav-link">MATRIX: Mask Track Alignment for Interaction-aware Video Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#genpilot-a-multi-agent-system-for-test-time-prompt-optimization-in-image-generation" class="nav-link">GenPilot: A Multi-Agent System for Test-Time Prompt Optimization in Image Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#tiger-tool-integrated-geometric-reasoning-in-vision-language-models-for-robotics" class="nav-link">TIGeR: Tool-Integrated Geometric Reasoning in Vision-Language Models for Robotics</a>
                </li>
                <li class="nav-item">
                    <a href="#are-we-using-the-right-benchmark-an-evaluation-framework-for-visual-token-compression-methods" class="nav-link">Are We Using the Right Benchmark: An Evaluation Framework for Visual Token Compression Methods</a>
                </li>
                <li class="nav-item">
                    <a href="#harp-next-high-speed-and-accurate-range-point-fusion-network-for-3d-lidar-semantic-segmentation" class="nav-link">HARP-NeXt: High-Speed and Accurate Range-Point Fusion Network for 3D LiDAR Semantic Segmentation</a>
                </li>
                <li class="nav-item">
                    <a href="#stylekeeper-prevent-content-leakage-using-negative-visual-query-guidance" class="nav-link">StyleKeeper: Prevent Content Leakage using Negative Visual Query Guidance</a>
                </li>
                <li class="nav-item">
                    <a href="#efficient-discriminative-joint-encoders-for-large-scale-vision-language-reranking" class="nav-link">Efficient Discriminative Joint Encoders for Large Scale Vision-Language Reranking</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-10-09">Arxiv Computer Vision Papers - 2025-10-09</h1>
<h2 id="executive-summary">Executive Summary</h2>
<h2 id="arxiv-2025-10-08">Arxiv 计算机视觉每日报告执行摘要 (2025-10-08)</h2>
<p><strong>概述：</strong></p>
<p>今天的 Arxiv 计算机视觉论文展现了该领域持续的活力和多元化发展。主要趋势集中在<strong>多模态学习（特别是视觉-语言-动作模型的融合）</strong>、<strong>生成模型（扩散模型及其应用）</strong>、<strong>机器人视觉与具身智能</strong>以及<strong>效率与性能优化</strong>。值得注意的是，有论文开始探索<strong>量子计算在计算机视觉中的潜力</strong>，预示着未来可能的新范式。</p>
<p><strong>主要主题和趋势：</strong></p>
<ol>
<li><strong>多模态学习与具身智能：</strong> 多篇论文（如2、6、10）强调了视觉、语言和动作的融合，特别是在机器人领域。这表明研究正从单一模态理解转向更全面的、能够与物理世界交互的智能系统。</li>
<li><strong>生成模型与扩散模型：</strong> 扩散模型继续是热门研究方向，不仅用于图像生成（如4、5），还扩展到低光照图像增强（3）和内容控制（9）。这表明扩散模型在解决各种视觉生成和增强任务中展现出强大的通用性。</li>
<li><strong>机器人视觉与几何推理：</strong> 机器人应用是另一个突出主题，涵盖了视觉-语言-动作模型（2）、工具集成几何推理（6）和3D语义分割（8），反映了为机器人提供更鲁棒、更智能感知能力的努力。</li>
<li><strong>效率与性能优化：</strong> 论文关注如何更有效地处理视觉数据，例如视觉 token 压缩（7）和大规模视觉-语言重排序的判别式联合编码器（10），这对于实际部署和大规模应用至关重要。</li>
<li><strong>量子计算的初步探索：</strong> 论文1首次将量子计算引入计算机视觉，尽管仍处于早期阶段，但预示着未来可能颠覆性的技术方向。</li>
</ol>
<p><strong>特别重要或创新的论文：</strong></p>
<ul>
<li><strong>1. "Quantum-enhanced Computer Vision: Going Beyond Classical Algorithms"：</strong> 这篇论文具有前瞻性，是该领域中较早探索量子计算与计算机视觉结合的尝试。虽然具体实现和优势仍需深入研究，但其概念上的创新性值得关注，可能为未来带来突破。</li>
<li><strong>2. "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications"：</strong> 作为一篇综述，它系统地梳理了视觉-语言-动作模型在机器人领域的进展，并展望了实际应用，对于理解该领域现状和未来方向具有重要指导意义。</li>
<li><strong>4. "MATRIX: Mask Track Alignment for Interaction-aware Video Generation"：</strong> 这篇论文在视频生成领域取得了进展，通过掩码轨迹对齐实现了交互感知的视频生成，对于生成更真实、更复杂的动态场景具有重要意义。</li>
<li><strong>6. "TIGeR: Tool-Integrated Geometric Reasoning in Vision-Language Models for Robotics"：</strong> 将工具使用和几何推理集成到视觉-语言模型中，为机器人提供了更高级别的操作能力，是具身智能领域的重要一步。</li>
</ul>
<p><strong>新兴研究方向或技术：</strong></p>
<ul>
<li><strong>量子计算机视觉：</strong> 尽管仍处于萌芽阶段，但量子计算与计算机视觉的结合（论文1）是一个潜在的颠覆性方向。</li>
<li><strong>多智能体系统在生成模型中的应用：</strong> GenPilot（论文5）利用多智能体系统进行提示词优化，为生成模型提供了更智能、更灵活的控制方式。</li>
<li><strong>具身智能中的高级推理：</strong> 工具集成几何推理（论文6）和视觉-语言-动作模型的结合（论文2）表明，研究正从感知转向更复杂的认知和操作推理。</li>
<li><strong>内容泄漏预防与控制：</strong> StyleKeeper（论文9）通过负面视觉查询指导来防止内容泄漏，这对于生成模型的安全性和可控性至关重要。</li>
</ul>
<p><strong>建议阅读的论文：</strong></p>
<p>对于希望快速了解领域前沿的忙碌研究人员，建议优先阅读以下论文：</p>
<ol>
<li><strong>"Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications" (Kento Kawaharazuka et al.)：</strong> 提供了一个全面的概览，是理解多模态机器人视觉最新进展的绝佳起点。</li>
<li><strong>"MATRIX: Mask Track Alignment for Interaction-aware Video Generation" (Siyoon Jin et al.)：</strong> 如果您对视频生成和复杂动态场景建模感兴趣，这篇论文提供了创新的解决方案。</li>
<li><strong>"TIGeR: Tool-Integrated Geometric Reasoning in Vision-Language Models for Robotics" (Yi Han et al.)：</strong> 对于具身智能和机器人操作感兴趣的研究人员，这篇论文展示了如何将高级推理能力融入机器人系统。</li>
<li><strong>"Quantum-enhanced Computer Vision: Going Beyond Classical Algorithms" (Natacha Kuete Meli et al.)：</strong> 如果您对未来技术趋势和潜在的范式转变感兴趣，这篇论文值得一读，尽管其影响可能需要更长时间才能显现。</li>
<li><strong>"Diffusion Models for Low-Light Image Enhancement: A Multi-Perspective Taxonomy and Performance Analysis" (Eashan Adhikarla, Yixin Liu, Brian D. Davison)：</strong> 如果您关注扩散模型在实际应用中的扩展，特别是图像增强领域，这篇综述和分析将很有价值。</li>
</ol>
<p>这些论文共同描绘了一个充满活力和快速发展的计算机视觉领域，其中多模态、生成模型和具身智能是当前和未来研究的关键驱动力。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2510.07317v1">Quantum-enhanced Computer Vision: Going Beyond Classical Algorithms</a></li>
<li><a href="#2510.07077v1">Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications</a></li>
<li><a href="#2510.05976v1">Diffusion Models for Low-Light Image Enhancement: A Multi-Perspective Taxonomy and Performance Analysis</a></li>
<li><a href="#2510.07310v1">MATRIX: Mask Track Alignment for Interaction-aware Video Generation</a></li>
<li><a href="#2510.07217v1">GenPilot: A Multi-Agent System for Test-Time Prompt Optimization in Image Generation</a></li>
<li><a href="#2510.07181v1">TIGeR: Tool-Integrated Geometric Reasoning in Vision-Language Models for Robotics</a></li>
<li><a href="#2510.07143v1">Are We Using the Right Benchmark: An Evaluation Framework for Visual Token Compression Methods</a></li>
<li><a href="#2510.06876v1">HARP-NeXt: High-Speed and Accurate Range-Point Fusion Network for 3D LiDAR Semantic Segmentation</a></li>
<li><a href="#2510.06827v1">StyleKeeper: Prevent Content Leakage using Negative Visual Query Guidance</a></li>
<li><a href="#2510.06820v1">Efficient Discriminative Joint Encoders for Large Scale Vision-Language Reranking</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2510.07317v1'></a></p>
<h2 id="quantum-enhanced-computer-vision-going-beyond-classical-algorithms"><a href="https://arxiv.org/abs/2510.07317v1">Quantum-enhanced Computer Vision: Going Beyond Classical Algorithms</a></h2>
<p><strong>Authors:</strong> Natacha Kuete Meli, Shuteng Wang, Marcel Seelbach Benkner, Michele Sasdelli, Tat-Jun Chin, Tolga Birdal, Michael Moeller, Vladislav Golyanik</p>
<p><strong>Published:</strong> 2025-10-08</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Quantum-enhanced Computer Vision (QeCV) is a new research field at the
intersection of computer vision, optimisation theory, machine learning and
quantum computing. It has high potential to transform how visual signals are
processed and interpreted with the help of quantum computing that leverages
quantum-mechanical effects in computations inaccessible to classical (i.e.
non-quantum) computers. In scenarios where existing non-quantum methods cannot
find a solution in a reasonable time or compute only approximate solutions,
quantum computers can provide, among others, advantages in terms of better time
scalability for multiple problem classes. Parametrised quantum circuits can
also become, in the long term, a considerable alternative to classical neural
networks in computer vision. However, specialised and fundamentally new
algorithms must be developed to enable compatibility with quantum hardware and
unveil the potential of quantum computational paradigms in computer vision.
This survey contributes to the existing literature on QeCV with a holistic
review of this research field. It is designed as a quantum computing reference
for the computer vision community, targeting computer vision students,
scientists and readers with related backgrounds who want to familiarise
themselves with QeCV. We provide a comprehensive introduction to QeCV, its
specifics, and methodologies for formulations compatible with quantum hardware
and QeCV methods, leveraging two main quantum computational paradigms, i.e.
gate-based quantum computing and quantum annealing. We elaborate on the
operational principles of quantum computers and the available tools to access,
program and simulate them in the context of QeCV. Finally, we review existing
quantum computing tools and learning materials and discuss aspects related to
publishing and reviewing QeCV papers, open challenges and potential social
implications.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行分析。</p>
<hr />
<p><strong>论文摘要分析：Quantum-enhanced Computer Vision: Going Beyond Classical Algorithms</strong></p>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>这篇论文是对量子增强计算机视觉（QeCV）领域的一项全面综述。它旨在为计算机视觉社区提供一个关于量子计算的参考指南，详细介绍了QeCV的基本概念、方法论以及如何将计算机视觉问题转化为量子硬件兼容的范式，并探讨了该领域的未来挑战和潜力。</p>
<p><strong>2. 关键创新或方法论方法</strong></p>
<p>这篇论文本身是一篇综述，其关键“创新”在于其<strong>全面性和整合性</strong>。它系统地梳理了QeCV这一新兴交叉领域，并提出了以下核心方法论视角：</p>
<ul>
<li><strong>范式兼容性：</strong> 强调开发与量子硬件兼容的全新算法的重要性，以充分利用量子计算的潜力。</li>
<li><strong>双重量子计算范式：</strong> 详细阐述了两种主要的量子计算范式——基于门（gate-based）的量子计算和量子退火（quantum annealing）——在QeCV中的应用方法。</li>
<li><strong>操作原理与工具：</strong> 提供了量子计算机的操作原理、访问、编程和模拟工具的介绍，旨在降低计算机视觉研究人员进入该领域的门槛。</li>
<li><strong>未来展望：</strong> 讨论了参数化量子电路作为经典神经网络的长期替代方案的可能性。</li>
</ul>
<p><strong>3. 对领域潜在影响</strong></p>
<p>这篇综述对计算机视觉领域具有深远的潜在影响：</p>
<ul>
<li><strong>知识普及与桥梁作用：</strong> 它将成为计算机视觉研究人员进入量子计算领域的关键参考资料，弥合了这两个复杂领域之间的知识鸿沟。</li>
<li><strong>催生新算法和研究方向：</strong> 通过系统介绍QeCV的潜力、挑战和现有工具，它有望激发计算机视觉社区开发出全新的、量子原生（quantum-native）的算法，解决经典方法难以处理的问题。</li>
<li><strong>推动QeCV发展：</strong> 作为一篇全面的综述，它将为QeCV领域的未来研究提供一个坚实的基础和路线图，加速该领域的发展。</li>
<li><strong>解决复杂问题：</strong> 强调量子计算在处理经典方法无法在合理时间内找到精确解的问题上的优势，预示着QeCV可能在某些极端复杂的视觉任务中取得突破。</li>
</ul>
<p><strong>4. 可能受益于这项研究的相关领域或应用</strong></p>
<ul>
<li><strong>大规模图像/视频分析：</strong> 对于需要处理海量数据、优化复杂模型或进行高维特征匹配的任务，量子计算的时间可伸缩性优势可能带来显著提升。</li>
<li><strong>医学影像分析：</strong> 在疾病诊断、药物发现等领域，需要处理高分辨率、多模态数据并进行精确模式识别，QeCV可能提供更高效、更准确的解决方案。</li>
<li><strong>自动驾驶与机器人视觉：</strong> 实时决策、复杂环境感知和路径规划中的优化问题，可能从量子优化算法中受益。</li>
<li><strong>计算摄影与图像重建：</strong> 涉及逆问题、去噪、超分辨率等任务，量子算法可能提供更优的解。</li>
<li><strong>模式识别与异常检测：</strong> 对于在复杂数据集中识别微弱模式或罕见异常，量子机器学习算法可能展现出独特优势。</li>
<li><strong>量子机器学习（QML）研究：</strong> QeCV是QML的一个重要应用分支，这篇综述将直接促进QML在视觉领域的应用和发展。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的任何局限性</strong></p>
<ul>
<li><strong>硬件成熟度：</strong> 摘要中提到“长期来看，参数化量子电路可以成为经典神经网络的重要替代方案”，这暗示了当前量子硬件的成熟度可能尚未达到能够大规模替代经典方法的程度。QeCV的实际应用仍受限于量子计算机的规模、稳定性、纠错能力等。</li>
<li><strong>算法开发难度：</strong> 摘要明确指出“必须开发专门的、根本性的新算法”，这表明将经典计算机视觉问题转化为量子硬件兼容的范式并非易事，需要深厚的跨学科知识和大量的研究投入。</li>
<li><strong>理论与实践差距：</strong> 作为一篇综述，它主要关注理论框架和潜力，实际的性能提升和应用效果仍需通过具体的实验和案例研究来验证。</li>
<li><strong>可访问性与学习曲线：</strong> 尽管旨在降低门槛，但量子计算本身固有的复杂性意味着计算机视觉研究人员仍需投入大量精力学习新的概念和工具。</li>
<li><strong>噪声与错误：</strong> 摘要中未提及当前量子硬件面临的噪声（noise）和错误（error）问题，这些是影响量子算法实际性能的关键因素。</li>
</ul>
<hr />
<p>总而言之，这篇综述论文在计算机视觉和量子计算的交叉领域具有开创性意义。它不仅为计算机视觉社区打开了一扇通往量子世界的大门，更重要的是，它系统地勾勒出QeCV的蓝图，指明了未来的研究方向和潜在的突破点，预示着计算机视觉领域可能迎来一场由量子计算驱动的范式转变。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Quantum-enhanced Computer Vision (QeCV) is a new research field at the
intersection of computer vision, optimisation theory, machine learning and
quantum computing.</li>
<li>However, specialised and fundamentally new
algorithms must be developed to enable compatibility with quantum hardware and
unveil the potential of quantum computational paradigms in computer vision.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.07317v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.07317v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.07077v1'></a></p>
<h2 id="vision-language-action-models-for-robotics-a-review-towards-real-world-applications"><a href="https://arxiv.org/abs/2510.07077v1">Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications</a></h2>
<p><strong>Authors:</strong> Kento Kawaharazuka, Jihoon Oh, Jun Yamada, Ingmar Posner, Yuke Zhu</p>
<p><strong>Published:</strong> 2025-10-08</p>
<p><strong>Categories:</strong> cs.RO, cs.AI, cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Amid growing efforts to leverage advances in large language models (LLMs) and
vision-language models (VLMs) for robotics, Vision-Language-Action (VLA) models
have recently gained significant attention. By unifying vision, language, and
action data at scale, which have traditionally been studied separately, VLA
models aim to learn policies that generalise across diverse tasks, objects,
embodiments, and environments. This generalisation capability is expected to
enable robots to solve novel downstream tasks with minimal or no additional
task-specific data, facilitating more flexible and scalable real-world
deployment. Unlike previous surveys that focus narrowly on action
representations or high-level model architectures, this work offers a
comprehensive, full-stack review, integrating both software and hardware
components of VLA systems. In particular, this paper provides a systematic
review of VLAs, covering their strategy and architectural transition,
architectures and building blocks, modality-specific processing techniques, and
learning paradigms. In addition, to support the deployment of VLAs in
real-world robotic applications, we also review commonly used robot platforms,
data collection strategies, publicly available datasets, data augmentation
methods, and evaluation benchmarks. Throughout this comprehensive survey, this
paper aims to offer practical guidance for the robotics community in applying
VLAs to real-world robotic systems. All references categorized by training
approach, evaluation method, modality, and dataset are available in the table
on our project website: https://vla-survey.github.io .</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Kento Kawaharazuka等人撰写的论文“Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications”的全面摘要。</p>
<hr />
<h3 id="vision-language-action-models-for-robotics-a-review-towards-real-world-applications_1">论文摘要：Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决机器人领域中一个核心挑战：如何开发能够跨不同任务、物体、机器人形态和环境进行泛化的机器人策略，以实现更灵活、可扩展的真实世界部署。具体来说，它关注于视觉-语言-动作（VLA）模型，这些模型通过统一视觉、语言和动作数据来学习通用策略，以克服传统机器人系统在泛化能力和数据效率方面的局限性。该综述的目的是提供一个全面的、全栈式的VLA系统回顾，并为机器人社区在实际应用VLA模型时提供实用指导。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
该论文本身是一篇综述，其主要贡献在于对VLA模型领域的系统性、全栈式分析和分类，而非提出新的模型或算法。其关键贡献包括：</p>
<ul>
<li><strong>全面的VLA模型分类：</strong> 论文系统地回顾了VLA模型的设计策略和架构演变，从早期的基于CNN的模型（如CLIPort）到基于Transformer的序列模型（如RT系列、VIMA），再到集成扩散模型和流匹配技术（如Octo、πo）的策略，以及最新的分层控制框架（如LAPA、πo.5、GROOT N1）。</li>
<li><strong>详细的架构和构建块分析：</strong> 论文深入探讨了VLA模型的核心架构组件，包括感知运动模型（Sensorimotor Model）、世界模型（World Model）和基于可供性模型（Affordance-Based Model）的七种变体，并详细介绍了它们如何处理视觉、语言和动作等模态。</li>
<li><strong>模态特定处理技术：</strong> 详细阐述了VLA模型如何处理各种输入模态，包括视觉特征提取（ResNet、ViT、VLM骨干）、语言分词和编码（T5、LLaMA、USE、CLIP文本编码器）以及动作表示（离散动作token、连续动作建模、潜在动作学习）。此外，还讨论了音频、触觉和3D信息（深度图像、多视角图像、体素表示、点云）的集成。</li>
<li><strong>训练策略和实现考量：</strong> 论文分类了VLA模型的训练方法，包括监督学习、自监督学习和强化学习，并讨论了预训练、后训练、梯度隔离、参数高效适应（如LoRA）以及实时推理（如RTC）等实用实现细节。</li>
<li><strong>数据生态系统综述：</strong> 论文全面回顾了数据收集策略（遥操作、代理设备、人类数据收集）、公开数据集（人类视频数据集、模拟数据集、真实机器人数据集）以及数据增强方法（视觉、语言、动作增强）。</li>
<li><strong>机器人平台和评估基准：</strong> 论文总结了VLA研究中常用的机器人平台（机械臂、手/夹持器、移动机器人、四足机器人、人形机器人）以及各种评估基准（如robosuite、ManiSkill、LIBERO、CALVIN、RLBench）。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
该综述揭示了VLA模型领域发展的几个关键洞察：</p>
<ul>
<li><strong>泛化能力的关键：</strong> 大规模数据集和预训练基础模型在实现VLA模型泛化方面发挥着至关重要的作用，使其能够将网络知识转移到机器人控制中。</li>
<li><strong>分层架构的兴起：</strong> 分层架构（将高级语言理解与低级运动执行分离）的出现，提高了模型在长周期、多步骤任务中的性能。</li>
<li><strong>多模态输入的重要性：</strong> 除了视觉和语言，集成其他模态（如音频、触觉、3D信息）对于增强机器人的感知和交互能力越来越重要。</li>
<li><strong>真实世界部署的挑战：</strong> 尽管取得了进展，但模拟到真实世界的迁移和机器人形态泛化仍然是重大挑战，限制了VLA模型在非结构化真实世界环境中的广泛部署。</li>
<li><strong>实用指导：</strong> 论文为从业者提供了具体的建议，例如优先选择多样化高质量数据集、通过生成方法实现连续动作生成、在预训练期间进行梯度隔离、采用轻量级适应方法、整合世界模型或潜在动作学习，以及通过多任务学习增强动作生成表示。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong>
该论文在讨论VLA模型时也指出了现有方法的局限性：</p>
<ul>
<li><strong>数据稀缺性：</strong> 缺乏大规模、多样化且标注良好的视觉、语言和动作数据，特别是高质量的机器人演示数据收集成本高昂且难以扩展。</li>
<li><strong>机器人形态迁移：</strong> 机器人形态的多样性（关节配置、传感器类型、运动空间）使得跨不同机器人形态迁移策略成为重大挑战。将人类运动数据映射到机器人可执行动作也非易事。</li>
<li><strong>计算和训练成本：</strong> VLA模型训练计算需求巨大，尤其是在处理长时序序列、高分辨率图像或额外模态时。推理时的延迟和内存使用也限制了在资源受限机器人平台上的部署。</li>
<li><strong>泛化能力不足：</strong> 早期模型在统一不同模态和有效扩展方面面临挑战，泛化能力仍受限于预定义任务集。</li>
<li><strong>缺乏中间推理：</strong> 典型的VLA模型缺乏中间推理能力，难以处理复杂、长周期的任务。</li>
<li><strong>RL的局限性：</strong> 强化学习（RL）虽然能提高鲁棒性，但样本效率低，且在真实世界探索中存在安全风险，限制了其在VLA模型中的广泛应用。</li>
<li><strong>多模态数据收集的挑战：</strong> 收集大规模的音频、触觉和3D点云数据仍然是一个重大挑战，缺乏标准化传感器配置进一步复杂化了多模态数据的集成。</li>
<li><strong>评估的不足：</strong> VLA模型的评估指标定义不明确，尤其是在真实世界环境中。缺乏统计学上严格的评估，使得难以确定哪种方法最有效。</li>
<li><strong>安全问题：</strong> 在非结构化环境中部署VLA模型存在显著安全挑战，现有系统缺乏检测和避免意外人类存在的机制，增加了碰撞风险。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
论文指出了VLA模型未来研究的几个关键方向：</p>
<ul>
<li><strong>多模态数据集成：</strong> 解决大规模多模态数据（特别是触觉和3D信息）收集的挑战，并标准化传感器配置以实现可扩展的VLA系统。</li>
<li><strong>推理能力增强：</strong> 开发能够保留相关信息、选择性关注关键信息、支持时间抽象和基于记忆检索的推理机制，以解决长周期、多步骤任务。</li>
<li><strong>持续学习：</strong> 使VLA模型能够在线或持续学习，适应新情况，克服灾难性遗忘，并安全有效地在真实世界中运行。</li>
<li><strong>强化学习与世界模型结合：</strong> 通过在学习到的世界模型中进行RL微调，实现更安全、样本高效的RL，并结合模拟到真实（sim-to-real）技术。</li>
<li><strong>安全性和可靠性：</strong> 整合VLA与基于模型的控制方法，提供预测性推理，以提高在安全关键情况下的安全性。</li>
<li><strong>故障检测与恢复：</strong> 开发能够检测故障、适应性地重新规划和恢复的机制，以提高VLA系统在真实世界部署中的鲁棒性和可靠性。</li>
<li><strong>标准化评估：</strong> 建立统计学上严格的评估协议和基准，以公平、可靠地比较不同VLA模型的性能。</li>
<li><strong>实际应用：</strong> 推动VLA模型在医疗保健、辅助技术、工业自动化和自动驾驶等领域的实际部署，使其达到人类水平的性能和可靠性。</li>
</ul>
<hr />
<p>这篇综述为机器人领域的VLA模型研究提供了一个全面的路线图，不仅总结了现有进展，还明确指出了未来的挑战和机遇。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This generalisation capability is expected to
enable robots to solve novel downstream tasks with minimal or no additional
task-specific data, facilitating more flexible and scalable real-world
deployment.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.07077v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.07077v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.05976v1'></a></p>
<h2 id="diffusion-models-for-low-light-image-enhancement-a-multi-perspective-taxonomy-and-performance-analysis"><a href="https://arxiv.org/abs/2510.05976v1">Diffusion Models for Low-Light Image Enhancement: A Multi-Perspective Taxonomy and Performance Analysis</a></h2>
<p><strong>Authors:</strong> Eashan Adhikarla, Yixin Liu, Brian D. Davison</p>
<p><strong>Published:</strong> 2025-10-07</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Low-light image enhancement (LLIE) is vital for safety-critical applications
such as surveillance, autonomous navigation, and medical imaging, where
visibility degradation can impair downstream task performance. Recently,
diffusion models have emerged as a promising generative paradigm for LLIE due
to their capacity to model complex image distributions via iterative denoising.
This survey provides an up-to-date critical analysis of diffusion models for
LLIE, distinctively featuring an in-depth comparative performance evaluation
against Generative Adversarial Network and Transformer-based state-of-the-art
methods, a thorough examination of practical deployment challenges, and a
forward-looking perspective on the role of emerging paradigms like foundation
models. We propose a multi-perspective taxonomy encompassing six categories:
Intrinsic Decomposition, Spectral &amp; Latent, Accelerated, Guided, Multimodal,
and Autonomous; that map enhancement methods across physical priors,
conditioning schemes, and computational efficiency. Our taxonomy is grounded in
a hybrid view of both the model mechanism and the conditioning signals. We
evaluate qualitative failure modes, benchmark inconsistencies, and trade-offs
between interpretability, generalization, and inference efficiency. We also
discuss real-world deployment constraints (e.g., memory, energy use) and
ethical considerations. This survey aims to guide the next generation of
diffusion-based LLIE research by highlighting trends and surfacing open
research questions, including novel conditioning, real-time adaptation, and the
potential of foundation models.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于Eashan Adhikarla, Yixin Liu, Brian D. Davison撰写的论文“Diffusion Models for Low-Light Image Enhancement: A Multi-Perspective Taxonomy and Performance Analysis”的全面摘要，旨在突出其对计算机视觉领域的新颖性和重要性。</p>
<p><strong>论文摘要：扩散模型在低光图像增强中的多视角分类与性能分析</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决低光图像增强（LLIE）这一关键问题，它在监控、自动驾驶和医学成像等安全关键应用中至关重要，因为可见度下降会严重影响下游任务的性能。近年来，扩散模型因其通过迭代去噪建模复杂图像分布的能力，已成为LLIE领域一个有前景的生成范式。本综述的核心研究问题是：如何对当前扩散模型在LLIE中的应用进行系统性、批判性的分析，包括其性能、挑战、局限性以及未来发展方向，并将其与GAN和Transformer等现有最先进方法进行比较。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
该论文的主要贡献在于提出了一个全面的、多视角的扩散模型LLIE方法分类法，涵盖六个主要类别：
*   <strong>内在分解（Intrinsic Decomposition）：</strong> 基于Retinex理论或物理驱动模型，将图像分解为反射率和光照等分量进行增强。
*   <strong>频谱与潜在空间（Spectral &amp; Latent）：</strong> 在频域或学习到的潜在空间中进行扩散，以提高处理效率和鲁棒性。
*   <strong>加速（Accelerated）：</strong> 通过采样步数减少、蒸馏和知识迁移等技术，显著降低推理时间，使扩散模型适用于实时应用。
*   <strong>引导（Guided）：</strong> 引入外部信号（如区域掩码、用户指令、曝光参数）来控制增强过程，实现空间自适应和语义驱动的增强。
*   <strong>多模态（Multimodal）：</strong> 针对特定下游任务（如OCR、目标检测）进行增强，或融合来自事件相机、红外等多种传感模态的信息。
*   <strong>自主（Autonomous）：</strong> 利用自监督学习、零样本适应和无监督域对齐，无需配对训练数据即可进行增强，提高泛化能力。</p>
<p>该分类法融合了模型机制和条件信号的混合视角，为理解LLIE中扩散模型的多样化方法提供了清晰的框架。</p>
<p><strong>3. 主要结果及其重要性：</strong>
论文通过对扩散模型与GAN和Transformer等最先进方法的深入比较分析，揭示了以下主要趋势和结果：
*   <strong>性能优势：</strong> 扩散模型在感知质量方面表现出新兴的主导地位，尤其在生成逼真细节和纹理方面优于早期方法。它们在极端光照条件下提供更稳定的训练和更可靠的结果，并能有效缓解模式崩溃问题。
*   <strong>效率与保真度权衡：</strong> 扩散模型虽然质量高，但计算成本高（高FLOPs、多参数、推理慢）。加速技术（如潜在扩散、蒸馏）旨在缓解这一问题，但可能以牺牲部分增强质量或稳定性为代价。
*   <strong>范式转变：</strong> LLIE的焦点正从单一的图像恢复任务转向更细致、可控和上下文感知的过程。引导式和多模态扩散框架通过整合语义线索、用户指令和传感器融合，实现了任务感知型增强。
*   <strong>泛化能力：</strong> 自主扩散模型通过减少对配对数据的依赖，提高了对未见光照条件的泛化能力和鲁棒性。</p>
<p><strong>4. 论文中提及的局限性：</strong>
论文也指出了扩散模型在LLIE中面临的挑战和局限性：
*   <strong>计算开销和推理延迟：</strong> 扩散模型的迭代去噪过程导致高计算成本和推理延迟，使其难以实时部署。
*   <strong>泛化性和鲁棒性：</strong> 模型在面对与训练数据显著不同的域外（OOD）输入时，性能可能下降，尤其是在极端黑暗、非均匀光照和特定传感器噪声模式下。
*   <strong>数据依赖性和稀缺性：</strong> 监督式扩散模型严重依赖大规模、高质量、多样化的配对训练数据，但此类数据获取困难且成本高昂。
*   <strong>感知质量、保真度与效率的权衡：</strong> 在这些相互冲突的目标之间找到最佳平衡仍然是一个挑战，传统指标（PSNR/SSIM）可能无法完全反映人类感知质量。
*   <strong>可解释性（XAI）：</strong> 扩散模型通常是“黑箱”模型，难以理解其决策过程，这阻碍了在安全关键应用中的信任和调试。
*   <strong>伦理考量：</strong> 存在偏见放大、恶意使用、增强质量公平性、环境影响以及“现实扭曲”的风险，即生成看似合理但虚假的细节。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
论文展望了未来研究的几个关键方向：
*   <strong>利用基础模型：</strong> 有效地将现有强大的图像生成扩散模型（如Stable Diffusion）适应于LLIE任务，包括专门的微调技术和新颖的提示策略。
*   <strong>多模态基础模型：</strong> 探索能够处理和整合文本、音频或其他传感器信息与视觉数据的多模态基础模型，以实现更具上下文感知和可控性的LLIE。
*   <strong>实时和设备端LLIE：</strong> 持续研究先进的加速技术（模型压缩、蒸馏、高效网络架构）和硬件协同设计，以实现实时和设备端部署。
*   <strong>有原则的无监督、自监督和零样本学习：</strong> 开发更复杂的无监督方法，以更好地建模真实世界的低光降级，并实现域泛化和解耦表示学习。
*   <strong>增强可控性和可解释性：</strong> 实现更细粒度的语义控制（超越简单的文本提示或掩码），并开发针对生成模型的XAI技术，以提高信任和调试能力。</p>
<p>总而言之，这篇综述为扩散模型在LLIE领域的最新进展提供了一个全面的路线图，不仅系统地分析了现有方法，还明确指出了当前面临的挑战和未来研究的机遇，特别是通过混合策略、基础模型和对伦理考量的关注来推动该领域的发展。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This survey provides an up-to-date critical analysis of diffusion models for
LLIE, distinctively featuring an in-depth comparative performance evaluation
against Generative Adversarial Network and Transformer-based state-of-the-art
methods, a thorough examination of practical deployment challenges, and a
forward-looking perspective on the role of emerging paradigms like foundation
models.</li>
<li>We propose a multi-perspective taxonomy encompassing six categories:
Intrinsic Decomposition, Spectral &amp; Latent, Accelerated, Guided, Multimodal,
and Autonomous; that map enhancement methods across physical priors,
conditioning schemes, and computational efficiency.</li>
<li>This survey aims to guide the next generation of
diffusion-based LLIE research by highlighting trends and surfacing open
research questions, including novel conditioning, real-time adaptation, and the
potential of foundation models.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.05976v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.05976v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.07310v1'></a></p>
<h2 id="matrix-mask-track-alignment-for-interaction-aware-video-generation"><a href="https://arxiv.org/abs/2510.07310v1">MATRIX: Mask Track Alignment for Interaction-aware Video Generation</a></h2>
<p><strong>Authors:</strong> Siyoon Jin, Seongchan Kim, Dahyun Chung, Jaeho Lee, Hyunwook Choi, Jisu Nam, Jiyoung Kim, Seungryong Kim</p>
<p><strong>Published:</strong> 2025-10-08</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Video DiTs have advanced video generation, yet they still struggle to model
multi-instance or subject-object interactions. This raises a key question: How
do these models internally represent interactions? To answer this, we curate
MATRIX-11K, a video dataset with interaction-aware captions and multi-instance
mask tracks. Using this dataset, we conduct a systematic analysis that
formalizes two perspectives of video DiTs: semantic grounding, via
video-to-text attention, which evaluates whether noun and verb tokens capture
instances and their relations; and semantic propagation, via video-to-video
attention, which assesses whether instance bindings persist across frames. We
find both effects concentrate in a small subset of interaction-dominant layers.
Motivated by this, we introduce MATRIX, a simple and effective regularization
that aligns attention in specific layers of video DiTs with multi-instance mask
tracks from the MATRIX-11K dataset, enhancing both grounding and propagation.
We further propose InterGenEval, an evaluation protocol for interaction-aware
video generation. In experiments, MATRIX improves both interaction fidelity and
semantic alignment while reducing drift and hallucination. Extensive ablations
validate our design choices. Codes and weights will be released.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行深入分析。</p>
<hr />
<p><strong>论文摘要分析：MATRIX: Mask Track Alignment for Interaction-aware Video Generation</strong></p>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>这篇论文的核心贡献在于深入分析了视频DiTs（Diffusion Transformers）内部如何处理多实例和主体-客体交互，并在此基础上提出了一种名为MATRIX的正则化方法。MATRIX通过将视频DiTs特定层中的注意力与多实例掩码轨迹对齐，显著提升了交互感知视频生成的质量，解决了现有模型在建模复杂交互方面的不足。</p>
<p><strong>2. 关键创新或方法学方法</strong></p>
<p>该论文的关键创新和方法学方法体现在以下几个方面：</p>
<ul>
<li><strong>问题诊断与形式化分析：</strong> 论文首先识别出视频DiTs在处理多实例和交互方面的局限性。为了回答“模型内部如何表示交互”这一关键问题，作者提出了两种形式化的分析视角：<ul>
<li><strong>语义接地 (Semantic Grounding)：</strong> 通过视频到文本的注意力（video-to-text attention）来评估名词和动词token是否能准确捕捉实例及其关系。</li>
<li><strong>语义传播 (Semantic Propagation)：</strong> 通过视频到视频的注意力（video-to-video attention）来评估实例绑定（instance bindings）在帧间是否能持续保持。</li>
</ul>
</li>
<li><strong>数据集创新：</strong> 作者构建了 <strong>MATRIX-11K</strong> 数据集，这是一个包含交互感知字幕和多实例掩码轨迹的视频数据集。这个数据集对于进行上述形式化分析和后续的正则化方法至关重要。</li>
<li><strong>发现交互主导层：</strong> 通过对MATRIX-11K数据集的分析，论文发现语义接地和语义传播这两种效应都集中在视频DiTs中一小部分“交互主导层”（interaction-dominant layers）中。这一发现为后续的正则化方法提供了关键的指导。</li>
<li><strong>MATRIX正则化方法：</strong> 基于上述发现，论文提出了 <strong>MATRIX</strong>，一种简单而有效的正则化方法。它通过将视频DiTs特定层（即交互主导层）中的注意力与MATRIX-11K数据集中的多实例掩码轨迹进行对齐。这种对齐旨在增强模型的语义接地和传播能力。</li>
<li><strong>评估协议创新：</strong> 论文还提出了 <strong>InterGenEval</strong>，一个专门用于评估交互感知视频生成的协议，这对于该领域未来的研究具有重要意义。</li>
</ul>
<p><strong>3. 对领域潜在影响</strong></p>
<ul>
<li><strong>推动交互感知视频生成：</strong> MATRIX方法直接解决了当前视频生成模型在处理复杂交互方面的痛点，有望显著提升生成视频的真实感和语义一致性，特别是在涉及多个主体和复杂动作的场景中。</li>
<li><strong>深入理解DiTs内部机制：</strong> 对语义接地和语义传播的系统分析，以及发现“交互主导层”的洞察，为理解视频DiTs如何处理时空信息和语义关系提供了宝贵的见解，可能启发未来模型架构的设计。</li>
<li><strong>标准化评估：</strong> InterGenEval协议的提出，为交互感知视频生成提供了一个统一、客观的评估标准，有助于推动该领域的公平比较和快速发展。</li>
<li><strong>数据集贡献：</strong> MATRIX-11K数据集的发布，将为研究人员提供一个高质量的资源，用于训练和评估更先进的交互感知视频生成模型。</li>
</ul>
<p><strong>4. 相关领域或应用受益</strong></p>
<ul>
<li><strong>视频内容创作：</strong> 电影制作、动画、游戏开发等领域可以利用更强大的交互感知视频生成技术，自动生成更复杂、更逼真的场景和角色交互。</li>
<li><strong>虚拟现实/增强现实 (VR/AR)：</strong> 生成具有高度交互性的虚拟环境，提升用户沉浸感。</li>
<li><strong>机器人学和具身智能：</strong> 帮助机器人理解和预测复杂的人机或物物交互，生成更合理的行为序列。</li>
<li><strong>视频编辑和合成：</strong> 自动填充或修改视频中的交互，简化后期制作流程。</li>
<li><strong>多模态学习：</strong> 促进视频、文本和掩码轨迹等多模态数据之间的更深层次融合和理解。</li>
<li><strong>可控视频生成：</strong> 允许用户通过文本或掩码轨迹更精细地控制生成视频中的交互行为。</li>
</ul>
<p><strong>5. 从摘要中可推断出的局限性</strong></p>
<ul>
<li><strong>数据集规模：</strong> 摘要中提到“MATRIX-11K”，虽然11K对于视频数据集来说不算小，但与一些大规模图像或文本数据集相比，其规模可能仍有局限性，这可能会影响模型在更广泛、更多样化场景下的泛化能力。</li>
<li><strong>“简单而有效”的含义：</strong> 摘要称MATRIX是一种“简单而有效”的正则化方法。虽然“简单”通常是优点，但有时也可能意味着其理论深度或复杂性不如某些更复杂的架构创新。具体实现细节（例如，如何精确对齐注意力，损失函数设计）需要查阅正文才能了解其潜在的复杂性。</li>
<li><strong>计算成本：</strong> 视频DiTs本身通常计算成本较高，而引入多实例掩码轨迹的对齐正则化，可能会进一步增加训练的计算资源需求和时间。摘要中未提及这方面的考量。</li>
<li><strong>交互复杂性：</strong> 摘要中强调“多实例或主体-客体交互”。虽然这是一个重要的进步，但交互的类型和复杂程度是无限的。MATRIX是否能处理所有类型的复杂交互（例如，隐式交互、长时程交互、多方交互等）仍需进一步验证。</li>
<li><strong>泛化到其他DiT架构：</strong> 摘要中提到“video DiTs”，但没有明确说明MATRIX是否能无缝地应用于所有现有的或未来的DiT架构，或者是否需要针对特定架构进行调整。</li>
<li><strong>“减少漂移和幻觉”的具体程度：</strong> 摘要提到MATRIX“减少漂移和幻觉”，但没有量化这些改进的程度。这需要通过实验结果来具体评估。</li>
</ul>
<hr />
<p>总的来说，这篇论文在视频生成领域迈出了重要一步，通过对模型内部机制的深入分析和创新的正则化方法，显著提升了交互感知视频生成的质量。其提出的数据集和评估协议也为该领域的未来研究奠定了基础，具有重要的理论和实践意义。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Motivated by this, we introduce MATRIX, a simple and effective regularization
that aligns attention in specific layers of video DiTs with multi-instance mask
tracks from the MATRIX-11K dataset, enhancing both grounding and propagation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.07310v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.07310v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.07217v1'></a></p>
<h2 id="genpilot-a-multi-agent-system-for-test-time-prompt-optimization-in-image-generation"><a href="https://arxiv.org/abs/2510.07217v1">GenPilot: A Multi-Agent System for Test-Time Prompt Optimization in Image Generation</a></h2>
<p><strong>Authors:</strong> Wen Ye, Zhaocheng Liu, Yuwei Gui, Tingyu Yuan, Yunyue Su, Bowen Fang, Chaoyang Zhao, Qiang Liu, Liang Wang</p>
<p><strong>Published:</strong> 2025-10-08</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Text-to-image synthesis has made remarkable progress, yet accurately
interpreting complex and lengthy prompts remains challenging, often resulting
in semantic inconsistencies and missing details. Existing solutions, such as
fine-tuning, are model-specific and require training, while prior automatic
prompt optimization (APO) approaches typically lack systematic error analysis
and refinement strategies, resulting in limited reliability and effectiveness.
Meanwhile, test-time scaling methods operate on fixed prompts and on noise or
sample numbers, limiting their interpretability and adaptability. To solve
these, we introduce a flexible and efficient test-time prompt optimization
strategy that operates directly on the input text. We propose a plug-and-play
multi-agent system called GenPilot, integrating error analysis,
clustering-based adaptive exploration, fine-grained verification, and a memory
module for iterative optimization. Our approach is model-agnostic,
interpretable, and well-suited for handling long and complex prompts.
Simultaneously, we summarize the common patterns of errors and the refinement
strategy, offering more experience and encouraging further exploration.
Experiments on DPG-bench and Geneval with improvements of up to 16.9% and 5.7%
demonstrate the strong capability of our methods in enhancing the text and
image consistency and structural coherence of generated images, revealing the
effectiveness of our test-time prompt optimization strategy. The code is
available at https://github.com/27yw/GenPilot.</p>
<p><strong>Analysis:</strong></p>
<p>以下是Wen Ye等人撰写的论文“GenPilot: A Multi-Agent System for Test-Time Prompt Optimization in Image Generation”的摘要：</p>
<p><strong>1. 主要问题或研究问题</strong>
尽管文本到图像（T2I）合成技术取得了显著进展，但现有模型在准确解释复杂和冗长的提示时仍面临挑战。这常常导致生成的图像存在语义不一致和细节缺失。传统的解决方案（如微调）通常是模型特定的且需要训练，而现有的自动提示优化（APO）方法缺乏系统的错误分析和细化策略，导致可靠性和有效性有限。此外，测试时缩放方法通常在固定提示或噪声/样本数量上操作，限制了其可解释性和适应性。该研究旨在解决这些问题，开发一种灵活高效的测试时提示优化策略，直接作用于输入文本。</p>
<p><strong>2. 关键创新或方法论贡献</strong>
本文提出了GenPilot，一个即插即用的多智能体系统，用于测试时提示优化。其核心创新包括：
*   <strong>多智能体系统架构：</strong> GenPilot将提示优化视为一个搜索问题，通过迭代优化过程实现动态和可解释的提示细化。该系统是模型无关的，无需额外训练即可应用于各种T2I模型。
*   <strong>错误分析与映射：</strong> 系统首先将初始提示分解为“元句子”，并利用视觉问答（VQA）和图像描述（captioning）进行并行错误检测。一个错误整合智能体将不一致性汇总为错误列表，并映射回特定的提示片段。
*   <strong>测试时提示优化：</strong> 包含一个细化智能体，根据原始提示、图像、错误分析和映射等元数据生成候选提示。这些候选提示通过多模态大语言模型（MLLM）评分器和评分策略进行评估。
*   <strong>聚类与记忆模块：</strong> GenPilot对候选提示进行聚类，并选择最优聚类进行采样和图像生成。记忆模块通过视觉和文本反馈进行迭代更新，以支持持续优化。
*   <strong>错误模式与细化策略总结：</strong> 论文总结了常见的错误模式及其对应的细化策略，为未来的研究提供了实践资源。</p>
<p><strong>3. 主要结果及其意义</strong>
GenPilot在DPG-bench和GenEval数据集上的实验结果表明，其方法在提高文本与图像一致性和生成图像的结构连贯性方面具有显著优势。在DPG-bench上，性能提升高达16.9%，在GenEval上提升高达5.7%。这些结果证明了GenPilot在处理复杂和冗长提示方面的强大能力，能够有效排除不必要的物体，并准确处理属性绑定、复杂构图、空间推理和非现实描述等挑战性任务。GenPilot在不同模型上均表现出一致的性能提升，包括DALL-E 3、FLUX.1 schnell、Sana-1.0 1.6B和Stable Diffusion系列，突显了其鲁棒性和泛化能力。</p>
<p><strong>4. 论文中提及的局限性</strong>
尽管GenPilot在多种场景下表现出色，但仍存在一些挑战：
*   <strong>计算时间：</strong> 尽管该框架避免了模型微调，但在推理过程中引入了额外的计算时间，这对于延迟敏感的应用可能是一个非小问题。
*   <strong>MLLM性能依赖：</strong> GenPilot的性能受其所使用的多模态大语言模型（MLLM）智能体能力的影响。如果用户使用能力较弱的MLLM，可能导致性能下降。</p>
<p><strong>5. 潜在的未来研究方向</strong>
论文鼓励进一步探索错误模式和细化策略，并提供了35种常见错误模式及其对应的细化策略作为实践资源。未来的研究可以进一步优化计算效率，减少推理时间，并探索如何使GenPilot在不同MLLM能力水平下保持高性能。此外，对提示可控性和优化策略的深入研究也是一个有前景的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To solve
these, we introduce a flexible and efficient test-time prompt optimization
strategy that operates directly on the input text.</li>
<li>We propose a plug-and-play
multi-agent system called GenPilot, integrating error analysis,
clustering-based adaptive exploration, fine-grained verification, and a memory
module for iterative optimization.</li>
<li>Our approach is model-agnostic,
interpretable, and well-suited for handling long and complex prompts.</li>
<li>Experiments on DPG-bench and Geneval with improvements of up to 16.9% and 5.7%
demonstrate the strong capability of our methods in enhancing the text and
image consistency and structural coherence of generated images, revealing the
effectiveness of our test-time prompt optimization strategy.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.07217v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.07217v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.07181v1'></a></p>
<h2 id="tiger-tool-integrated-geometric-reasoning-in-vision-language-models-for-robotics"><a href="https://arxiv.org/abs/2510.07181v1">TIGeR: Tool-Integrated Geometric Reasoning in Vision-Language Models for Robotics</a></h2>
<p><strong>Authors:</strong> Yi Han, Cheng Chi, Enshen Zhou, Shanyu Rong, Jingkun An, Pengwei Wang, Zhongyuan Wang, Lu Sheng, Shanghang Zhang</p>
<p><strong>Published:</strong> 2025-10-08</p>
<p><strong>Categories:</strong> cs.RO, cs.AI, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Vision-Language Models (VLMs) have shown remarkable capabilities in spatial
reasoning, yet they remain fundamentally limited to qualitative precision and
lack the computational precision required for real-world robotics. Current
approaches fail to leverage metric cues from depth sensors and camera
calibration, instead reducing geometric problems to pattern recognition tasks
that cannot deliver the centimeter-level accuracy essential for robotic
manipulation. We present TIGeR (Tool-Integrated Geometric Reasoning), a novel
framework that transforms VLMs from perceptual estimators to geometric
computers by enabling them to generate and execute precise geometric
computations through external tools. Rather than attempting to internalize
complex geometric operations within neural networks, TIGeR empowers models to
recognize geometric reasoning requirements, synthesize appropriate
computational code, and invoke specialized libraries for exact calculations. To
support this paradigm, we introduce TIGeR-300K, a comprehensive
tool-invocation-oriented dataset covering point transformations, pose
estimation, trajectory generation, and spatial compatibility verification,
complete with tool invocation sequences and intermediate computations. Through
a two-stage training pipeline combining supervised fine-tuning (SFT) and
reinforcement fine-tuning (RFT) with our proposed hierarchical reward design,
TIGeR achieves SOTA performance on geometric reasoning benchmarks while
demonstrating centimeter-level precision in real-world robotic manipulation
tasks.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“TIGeR: Tool-Integrated Geometric Reasoning in Vision-Language Models for Robotics”论文的全面摘要：</p>
<p><strong>论文摘要：TIGeR：机器人领域中工具集成几何推理的视觉-语言模型</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
尽管视觉-语言模型（VLMs）在空间推理方面表现出显著能力，但它们在定性精度上仍存在根本性限制，并且缺乏现实世界机器人所需的计算精度。现有方法未能充分利用深度传感器和相机校准提供的度量线索，而是将几何问题简化为模式识别任务，无法达到机器人操作所需的厘米级精度。这阻碍了具身VLM支持精确的机器人操作和规划。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
该论文提出了TIGeR（工具集成几何推理）框架，旨在将VLM从感知估计器转变为几何计算器。其核心创新在于：
*   <strong>外部工具集成：</strong> TIGeR使模型能够识别几何推理需求，生成适当的计算代码，并调用外部工具库进行精确计算，而不是试图在神经网络内部实现复杂的几何运算。这通过与深度传感器、相机内参和几何库的集成，实现了超越近似的精确计算。
*   <strong>TIGeR-300K数据集：</strong> 为支持这一范式，作者构建了一个包含30万个样本的综合性、面向工具调用的数据集。该数据集涵盖点变换、姿态估计、轨迹生成和空间兼容性验证，并包含完整的工具调用序列和中间计算。
*   <strong>两阶段训练流程：</strong> 采用监督微调（SFT）和强化微调（RFT）相结合的两阶段训练流程。SFT用于灌输工具使用能力，RFT则通过提出的分层奖励设计来提高精度和任务完成度。分层奖励包括格式奖励、工具调用奖励、参数内容奖励、代码生成奖励和答案奖励，为工具集成推理提供了细粒度的监督。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>最先进的性能：</strong> TIGeR在几何推理基准测试中取得了最先进的性能。
*   <strong>厘米级精度：</strong> 在现实世界机器人操作任务中，TIGeR展示了厘米级的精度，解决了现有VLM在定量空间推理方面的局限性。
*   <strong>准确的空间定位、统一的跨视角推理和可解释性/适应性：</strong> TIGeR通过利用现有3D基础模型实现精确的3D空间定位，通过3D几何重建模型实现跨视角的统一数值推理，并通过显式调用工具和暴露中间步骤提供高可解释性和适应性。</p>
<p><strong>4. 论文中提到的局限性：</strong>
论文中没有明确提及TIGeR框架的局限性。然而，从其方法论和背景来看，潜在的局限性可能包括：
*   <strong>工具库的完备性：</strong> TIGeR的性能依赖于外部工具库的质量和覆盖范围。如果某些几何计算没有高质量的工具，或者工具的集成不够完善，可能会影响其性能。
*   <strong>计算开销：</strong> 生成代码和调用外部工具可能比纯粹的神经网络推理带来更高的计算开销和延迟。
*   <strong>泛化能力：</strong> 尽管数据集设计旨在促进泛化，但模型在面对与训练数据显著不同的全新、复杂几何任务时，其泛化能力仍需进一步验证。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
论文中没有明确列出未来研究方向，但根据其贡献和潜在局限性，可以推断出以下方向：
*   <strong>扩展工具库：</strong> 进一步扩展和优化TIGeR的工具库，以涵盖更广泛的几何计算和更复杂的机器人操作任务。
*   <strong>提高效率和实时性：</strong> 优化代码生成和工具执行的效率，以满足实时机器人操作的需求。
*   <strong>更强的泛化能力：</strong> 探索如何进一步提高TIGeR在面对未知或高度复杂几何场景时的泛化能力，例如通过更先进的少样本学习或零样本学习方法。
*   <strong>多模态融合：</strong> 探索更深层次的多模态信息融合，例如将触觉或力觉传感器数据集成到几何推理中，以支持更精细的机器人交互。
*   <strong>自主学习和适应：</strong> 研究如何使TIGeR能够自主学习新的几何概念和工具使用策略，以适应不断变化的机器人环境和任务。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present TIGeR (Tool-Integrated Geometric Reasoning), a novel
framework that transforms VLMs from perceptual estimators to geometric
computers by enabling them to generate and execute precise geometric
computations through external tools.</li>
<li>To
support this paradigm, we introduce TIGeR-300K, a comprehensive
tool-invocation-oriented dataset covering point transformations, pose
estimation, trajectory generation, and spatial compatibility verification,
complete with tool invocation sequences and intermediate computations.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.07181v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.07181v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.07143v1'></a></p>
<h2 id="are-we-using-the-right-benchmark-an-evaluation-framework-for-visual-token-compression-methods"><a href="https://arxiv.org/abs/2510.07143v1">Are We Using the Right Benchmark: An Evaluation Framework for Visual Token Compression Methods</a></h2>
<p><strong>Authors:</strong> Chenfei Liao, Wensong Wang, Zichen Wen, Xu Zheng, Yiyu Wang, Haocong He, Yuanhuiyi Lyu, Lutao Jiang, Xin Zou, Yuqian Fu, Bin Ren, Linfeng Zhang, Xuming Hu</p>
<p><strong>Published:</strong> 2025-10-08</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent endeavors to accelerate inference in Multimodal Large Language Models
(MLLMs) have primarily focused on visual token compression. The effectiveness
of these methods is typically assessed by measuring the accuracy drop on
established benchmarks, comparing model performance before and after
compression. However, these benchmarks are originally designed to assess the
perception and reasoning capabilities of MLLMs, rather than to evaluate
compression techniques. As a result, directly applying them to visual token
compression introduces a task mismatch. Strikingly, our investigation reveals
that simple image downsampling consistently outperforms many advanced
compression methods across multiple widely used benchmarks. Through extensive
experiments, we make the following observations: (i) Current benchmarks are
noisy for the visual token compression task. (ii) Down-sampling is able to
serve as a data filter to evaluate the difficulty of samples in the visual
token compression task. Motivated by these findings, we introduce VTC-Bench, an
evaluation framework that incorporates a data filtering mechanism to denoise
existing benchmarks, thereby enabling fairer and more accurate assessment of
visual token compression methods. All data and code are available at
https://github.com/Chenfei-Liao/VTC-Bench.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Chenfei Liao等人撰写的论文“Are We Using the Right Benchmark: An Evaluation Framework for Visual Token Compression Methods”的全面摘要。</p>
<hr />
<h3 id="are-we-using-the-right-benchmark-an-evaluation-framework-for-visual-token-compression-methods_1">论文摘要：Are We Using the Right Benchmark: An Evaluation Framework for Visual Token Compression Methods</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决多模态大型语言模型（MLLMs）中视觉token压缩方法评估的现有问题。当前，评估这些方法有效性的基准测试主要关注MLLMs的感知和推理能力，而非专门设计用于评估压缩技术。这种任务不匹配导致了一个令人惊讶的现象：简单的图像下采样方法在多个广泛使用的基准测试中，其性能始终优于许多先进的压缩方法。这引发了对现有评估框架是否能公平准确地衡量视觉token压缩方法真实性能的质疑。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
为了解决现有基准测试中存在的“简单性偏差”和噪声问题，论文提出了一个名为 <strong>VTC-Bench</strong> 的新型评估框架。VTC-Bench的核心创新在于引入了一个<strong>数据过滤机制</strong>，利用下采样作为判别器来区分基准测试样本的难度。具体步骤包括：
*   <strong>推理与压缩：</strong> 对给定样本和目标压缩比，同时运行下采样基线（作为过滤器）和先进的视觉token压缩方法进行推理。
*   <strong>分组：</strong> 首先剔除原始Qwen2-VL模型无法正确回答的样本。然后，根据下采样方法的性能将剩余样本分为两组：Group A（“困难样本”，下采样方法无法正确回答）和Group B（“简单样本”，下采样方法可以正确回答）。
*   <strong>结果聚合：</strong> 对“困难样本”组中的压缩方法性能进行统计分析，以获得更公平、准确的评估指标。</p>
<p>通过这种方式，VTC-Bench能够从现有基准测试中识别出真正具有挑战性的、与视觉token压缩任务相关的样本，从而去噪并提供更有效的评估。</p>
<p><strong>3. 主要结果及其意义：</strong>
论文通过广泛实验得出了以下关键发现：
*   <strong>现有基准测试对视觉token压缩任务存在噪声：</strong> 简单的图像下采样方法在许多现有基准测试中表现优异，这表明这些基准测试包含大量可以通过低分辨率全局信息解决的“简单”样本，未能充分反映视觉token压缩的真实挑战。
*   <strong>下采样可作为数据过滤器：</strong> 下采样方法能够有效区分样本的难度。在VTC-Bench过滤出的“困难样本”组中，先进的视觉token压缩方法显著超越了下采样基线，证明了它们在需要精细视觉理解的任务中的真正价值。
*   <strong>VTC-Bench提升了评估的公平性和有效性：</strong> 通过聚焦于“困难样本”，VTC-Bench消除了与视觉token压缩任务无关的数据噪声，并显著扩大了不同压缩方法之间的性能差距，从而更清晰地揭示了方法的优劣。</p>
<p>这些结果的意义在于，它们揭示了当前视觉token压缩领域评估标准的缺陷，并为未来设计更有效、更具挑战性的基准测试提供了明确的指导原则。</p>
<p><strong>4. 论文中提及的局限性：</strong>
论文提到了VTC-Bench的两个主要局限性：
*   <strong>对下采样作为过滤器的依赖：</strong> 如果下采样本身在某些任务上表现不佳，可能会导致“困难样本”的数量不足，从而影响评估的全面性。
*   <strong>未考虑模型差异：</strong> 不同的MLLMs对图像分辨率和视觉细节的敏感度不同，这可能会影响样本分组的通用性。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
论文鼓励社区进一步讨论“如何正确评估高效的MLLMs”，这暗示了以下潜在的未来研究方向：
*   开发更鲁棒、更通用的数据过滤机制，以减少对单一过滤方法的依赖。
*   探索考虑不同MLLMs模型特性和敏感度的评估框架，以提高评估结果的泛化能力。
*   设计专门针对视觉token压缩任务的全新基准测试，从一开始就避免“简单性偏差”，并包含更多需要精细视觉理解的挑战性样本。
*   研究如何将VTC-Bench的理念扩展到其他MLLM加速或效率优化任务的评估中。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>Strikingly, our investigation reveals
that simple image downsampling consistently outperforms many advanced
compression methods across multiple widely used benchmarks.</li>
<li>Motivated by these findings, we introduce VTC-Bench, an
evaluation framework that incorporates a data filtering mechanism to denoise
existing benchmarks, thereby enabling fairer and more accurate assessment of
visual token compression methods.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.07143v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.07143v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.06876v1'></a></p>
<h2 id="harp-next-high-speed-and-accurate-range-point-fusion-network-for-3d-lidar-semantic-segmentation"><a href="https://arxiv.org/abs/2510.06876v1">HARP-NeXt: High-Speed and Accurate Range-Point Fusion Network for 3D LiDAR Semantic Segmentation</a></h2>
<p><strong>Authors:</strong> Samir Abou Haidar, Alexandre Chariot, Mehdi Darouich, Cyril Joly, Jean-Emmanuel Deschaud</p>
<p><strong>Published:</strong> 2025-10-08</p>
<p><strong>Categories:</strong> cs.CV, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>LiDAR semantic segmentation is crucial for autonomous vehicles and mobile
robots, requiring high accuracy and real-time processing, especially on
resource-constrained embedded systems. Previous state-of-the-art methods often
face a trade-off between accuracy and speed. Point-based and sparse
convolution-based methods are accurate but slow due to the complexity of
neighbor searching and 3D convolutions. Projection-based methods are faster but
lose critical geometric information during the 2D projection. Additionally,
many recent methods rely on test-time augmentation (TTA) to improve
performance, which further slows the inference. Moreover, the pre-processing
phase across all methods increases execution time and is demanding on embedded
platforms. Therefore, we introduce HARP-NeXt, a high-speed and accurate LiDAR
semantic segmentation network. We first propose a novel pre-processing
methodology that significantly reduces computational overhead. Then, we design
the Conv-SE-NeXt feature extraction block to efficiently capture
representations without deep layer stacking per network stage. We also employ a
multi-scale range-point fusion backbone that leverages information at multiple
abstraction levels to preserve essential geometric details, thereby enhancing
accuracy. Experiments on the nuScenes and SemanticKITTI benchmarks show that
HARP-NeXt achieves a superior speed-accuracy trade-off compared to all
state-of-the-art methods, and, without relying on ensemble models or TTA, is
comparable to the top-ranked PTv3, while running 24<script type="math/tex">\times</script> faster. The code is
available at https://github.com/SamirAbouHaidar/HARP-NeXt</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Samir Abou Haidar等人撰写的论文“HARP-NeXt: High-Speed and Accurate Range-Point Fusion Network for 3D LiDAR Semantic Segmentation”的全面摘要。</p>
<hr />
<h3 id="harp-next-lidar">HARP-NeXt: 高速高精度LiDAR点云语义分割网络摘要</h3>
<p><strong>1. 主要问题或研究问题：</strong>
LiDAR语义分割对于自动驾驶和移动机器人至关重要，但现有方法在精度和实时处理速度之间存在固有的权衡。特别是，在资源受限的嵌入式系统上，点云和稀疏卷积方法虽然准确但速度慢，而基于投影的方法虽然快但会丢失关键几何信息。此外，许多方法依赖于测试时增强（TTA）或复杂的预处理阶段，进一步增加了推理时间。因此，研究的核心问题是：如何在保证高精度的同时，显著提高LiDAR语义分割的分割速度，使其能够在嵌入式平台上实时运行。</p>
<p><strong>2. 关键创新或方法贡献：</strong>
HARP-NeXt通过以下关键创新解决了上述问题：</p>
<ul>
<li><strong>新型预处理方法：</strong> 提出了一种创新的预处理流程，将大部分计算密集型操作（如球面投影）转移到GPU上并行执行，显著减少了CPU负载和数据传输瓶颈，从而大幅降低了预处理阶段的计算开销。</li>
<li><strong>Conv-SE-NeXt特征提取模块：</strong> 设计了一种名为Conv-SE-NeXt的新型特征提取模块。该模块结合了深度可分离卷积（显著减少参数量和计算量）和Squeeze-and-Excitation（SE）机制（自适应地重新校准特征响应），以高效捕获表示，而无需堆叠多层，从而在每个网络阶段保持轻量和高效。</li>
<li><strong>多尺度范围-点融合骨干网络：</strong> 采用了一个多尺度融合骨干网络，通过高效的映射（Pt2Px和Px2Pt）在2D范围图像像素特征和3D点特征之间进行信息融合。这种融合策略利用了多个抽象级别的信息，保留了关键的几何细节，增强了模型的学习能力和准确性。</li>
<li><strong>轻量级融合策略：</strong> 采用了一种轻量级的融合策略，通过矢量化索引和稀疏张量操作，在3D点和2D范围图像之间映射特征，在保留丰富的空间上下文的同时不牺牲效率。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
HARP-NeXt在nuScenes和SemanticKITTI基准测试上取得了显著的性能：</p>
<ul>
<li><strong>卓越的速度-精度权衡：</strong> 在nuScenes数据集上，HARP-NeXt实现了77.1%的mIoU，优于所有基于投影、稀疏卷积和融合的方法，并与排名第一的PTv3（78.4%）相当，但速度快了24倍（在RTX4090上为10毫秒，在Jetson AGX Orin上为71毫秒）。</li>
<li><strong>资源受限平台上的高效性：</strong> 在Jetson AGX Orin等嵌入式系统上，HARP-NeXt展现出卓越的实时性能，其总运行时（预处理+推理）远低于其他方法，同时保持了较低的参数量（5.4M）和计算复杂度。</li>
<li><strong>无需TTA或集成模型：</strong> HARP-NeXt在不依赖测试时增强（TTA）或集成模型的情况下，实现了与最先进方法相当的性能，这进一步证明了其固有的高效性和鲁棒性。</li>
<li><strong>定性结果：</strong> 定性分析表明，HARP-NeXt在关键类别（如行人、汽车、人行道）上的错误分类显著少于其他方法，尤其是在复杂场景下。</li>
</ul>
<p>这些结果表明HARP-NeXt在LiDAR语义分割领域取得了重大进展，为自动驾驶和移动机器人等需要实时、高精度感知的应用提供了切实可行的解决方案。</p>
<p><strong>4. 论文中提及的局限性：</strong>
论文中没有明确提及HARP-NeXt自身的局限性。然而，它间接指出了现有方法的局限性，这些局限性促成了HARP-NeXt的开发：
*   <strong>现有方法的权衡：</strong> 大多数现有方法在精度和速度之间存在权衡，无法同时满足高精度和实时处理的需求。
*   <strong>预处理开销：</strong> 现有方法中的预处理阶段通常计算量大且耗时，尤其是在嵌入式平台上。
*   <strong>几何信息丢失：</strong> 基于投影的方法在2D投影过程中会丢失关键的3D几何信息。
*   <strong>TTA的额外开销：</strong> 许多方法依赖TTA来提高性能，但这会进一步减慢推理速度。
*   <strong>稀疏点云的挑战：</strong> 稀疏户外点云缺乏足够的局部相似性，使得某些方法（如Superpoint Transformer）效果不佳。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
论文中没有明确提出未来的研究方向，但从其贡献和现有方法的局限性来看，可以推断出以下潜在方向：
*   <strong>更广泛的硬件平台部署：</strong> 进一步优化HARP-NeXt以适应更广泛的资源受限硬件平台，探索其在不同计算能力和功耗限制下的性能。
*   <strong>自适应模型：</strong> 研究如何使HARP-NeXt能够自适应地调整其架构或参数，以应对不同LiDAR传感器类型、点云密度或环境条件。
*   <strong>多模态融合的探索：</strong> 虽然HARP-NeXt专注于LiDAR数据，但可以探索将其与相机图像或其他传感器数据进行更深层次的融合，以进一步提高在复杂场景下的鲁棒性和准确性。
*   <strong>实时在线学习：</strong> 研究如何将在线学习或增量学习机制集成到HARP-NeXt中，使其能够在新环境中持续学习和适应，而无需重新训练整个模型。
*   <strong>不确定性量化：</strong> 进一步探索和改进HARP-NeXt的不确定性量化能力，为自动驾驶等安全关键应用提供更可靠的预测。
*   <strong>轻量化架构搜索：</strong> 结合神经架构搜索（NAS）技术，自动探索更轻量、更高效的HARP-NeXt变体，以进一步优化其在嵌入式系统上的性能。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>Previous state-of-the-art methods often
face a trade-off between accuracy and speed.</li>
<li>Therefore, we introduce HARP-NeXt, a high-speed and accurate LiDAR
semantic segmentation network.</li>
<li>We first propose a novel pre-processing
methodology that significantly reduces computational overhead.</li>
<li>Experiments on the nuScenes and SemanticKITTI benchmarks show that
HARP-NeXt achieves a superior speed-accuracy trade-off compared to all
state-of-the-art methods, and, without relying on ensemble models or TTA, is
comparable to the top-ranked PTv3, while running 24<script type="math/tex">\times</script> faster.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.06876v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.06876v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.06827v1'></a></p>
<h2 id="stylekeeper-prevent-content-leakage-using-negative-visual-query-guidance"><a href="https://arxiv.org/abs/2510.06827v1">StyleKeeper: Prevent Content Leakage using Negative Visual Query Guidance</a></h2>
<p><strong>Authors:</strong> Jaeseok Jeong, Junho Kim, Gayoung Lee, Yunjey Choi, Youngjung Uh</p>
<p><strong>Published:</strong> 2025-10-08</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>In the domain of text-to-image generation, diffusion models have emerged as
powerful tools. Recently, studies on visual prompting, where images are used as
prompts, have enabled more precise control over style and content. However,
existing methods often suffer from content leakage, where undesired elements of
the visual style prompt are transferred along with the intended style. To
address this issue, we 1) extend classifier-free guidance (CFG) to utilize
swapping self-attention and propose 2) negative visual query guidance (NVQG) to
reduce the transfer of unwanted contents. NVQG employs negative score by
intentionally simulating content leakage scenarios that swap queries instead of
key and values of self-attention layers from visual style prompts. This simple
yet effective method significantly reduces content leakage. Furthermore, we
provide careful solutions for using a real image as visual style prompts.
Through extensive evaluation across various styles and text prompts, our method
demonstrates superiority over existing approaches, reflecting the style of the
references, and ensuring that resulting images match the text prompts. Our code
is available \href{https://github.com/naver-ai/StyleKeeper}{here}.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行分析。</p>
<hr />
<p><strong>论文摘要分析：StyleKeeper: Prevent Content Leakage using Negative Visual Query Guidance</strong></p>
<p><strong>1. 论文主要贡献的简洁总结 (A concise summary of the paper's main contribution)</strong></p>
<p>这篇论文提出了一种名为“负视觉查询引导 (NVQG)”的新方法，旨在解决文本到图像生成中视觉风格提示的“内容泄露”问题。通过扩展分类器无关引导 (CFG) 并利用交换自注意力机制，NVQG 有效地减少了不希望的风格提示内容转移，同时保持了对目标风格的忠实反映和文本提示的匹配。</p>
<p><strong>2. 关键创新或方法论 (The key innovation or methodological approach)</strong></p>
<p>该论文的关键创新在于：
*   <strong>扩展分类器无关引导 (CFG) 以利用交换自注意力 (swapping self-attention)：</strong> 这是其方法的基础，允许更精细地控制风格和内容的分离。
*   <strong>负视觉查询引导 (NVQG)：</strong> 这是核心创新。NVQG 通过“故意模拟内容泄露场景”来生成负分数。具体来说，它不是交换自注意力层中的键 (key) 和值 (value)，而是交换查询 (queries)。这种机制旨在明确地学习和抑制那些与风格提示中不希望的内容相关的特征，从而在生成过程中主动避免这些内容的转移。
*   <strong>为使用真实图像作为视觉风格提示提供解决方案：</strong> 这表明该方法不仅限于合成数据，而是具有实际应用潜力。</p>
<p><strong>3. 对该领域的潜在影响 (Potential impact on the field)</strong></p>
<ul>
<li><strong>提高文本到图像生成的可控性：</strong> 解决内容泄露是视觉提示领域的一个重要挑战。StyleKeeper 的成功将显著提高用户在使用图像作为风格提示时对生成结果的精确控制能力，减少意外和不希望的元素。</li>
<li><strong>促进更自然的视觉提示交互：</strong> 允许用户更自信地使用真实图像作为风格参考，而无需担心图像中无关背景或物体会污染生成结果，从而使视觉提示更加直观和实用。</li>
<li><strong>为扩散模型中的引导机制提供新思路：</strong> NVQG 这种通过模拟“负面”场景来生成负分数以抑制特定特征的方法，可能会启发其他扩散模型中更复杂的引导策略，用于解决其他类型的生成控制问题。</li>
<li><strong>推动个性化和定制化内容生成：</strong> 更好的风格控制意味着用户可以更精确地将特定艺术风格、纹理或视觉特征应用到他们的生成内容中，这对于创意产业、设计和个性化媒体生成具有巨大价值。</li>
</ul>
<p><strong>4. 可能受益于这项研究的相关领域或应用 (Related areas or applications that might benefit from this research)</strong></p>
<ul>
<li><strong>数字艺术和设计：</strong> 艺术家和设计师可以使用参考图像来精确地将特定风格（如印象派、赛博朋克、水彩画等）应用到他们的文本描述中，而无需担心参考图像中的具体内容被复制。</li>
<li><strong>时尚和产品设计：</strong> 设计师可以利用特定材质或纹理的图像作为风格提示，生成具有这些视觉特征的新产品设计，同时确保产品本身符合文本描述。</li>
<li><strong>虚拟现实 (VR) 和游戏开发：</strong> 快速生成具有特定艺术风格或环境氛围的资产和场景，减少手动纹理和风格化的工作量。</li>
<li><strong>个性化内容创作：</strong> 用户可以根据自己的喜好，将特定风格应用到照片或视频的生成中，例如将自拍照转换为漫画风格，同时保持人物特征不变。</li>
<li><strong>图像编辑和风格迁移：</strong> 虽然主要针对生成，但其核心思想可能启发更高级的风格迁移算法，使其在保留内容的同时，更精确地应用目标风格。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的任何局限性 (Any limitations that can be inferred from the abstract)</strong></p>
<ul>
<li><strong>“简单但有效”的局限性：</strong> 摘要中提到“This simple yet effective method”，虽然强调了其效率，但也可能暗示其在处理极其复杂或微妙的内容泄露场景时，可能不如更复杂的、基于深度语义理解的方法那样鲁棒。</li>
<li><strong>计算成本：</strong> 扩展 CFG 和引入 NVQG 可能会增加推理时的计算成本。虽然摘要没有明确说明，但任何额外的引导步骤通常都会带来计算开销。</li>
<li><strong>“负视觉查询”的定义和泛化能力：</strong> NVQG 依赖于“故意模拟内容泄露场景”。这种模拟的有效性在多大程度上能够覆盖所有潜在的内容泄露类型？如果泄露的模式非常规或难以模拟，该方法的性能可能会受到影响。</li>
<li><strong>风格与内容的模糊边界：</strong> 在某些情况下，风格和内容之间的界限可能非常模糊。例如，如果一个风格本身就包含特定的图案或纹理，那么区分“不希望的内容”和“期望的风格元素”可能会变得困难。摘要没有详细说明如何处理这种边界模糊的情况。</li>
<li><strong>对特定扩散模型架构的依赖：</strong> 摘要提到了“自注意力层”，这表明该方法可能与基于 Transformer 的扩散模型（如 Stable Diffusion）紧密相关。它是否能直接应用于所有类型的扩散模型（例如，基于 U-Net 的模型）可能需要进一步验证。</li>
</ul>
<hr />
<p>总的来说，StyleKeeper 提出了一种新颖且有前景的方法来解决文本到图像生成中视觉提示的关键挑战——内容泄露。其通过负视觉查询引导来主动抑制不希望的内容转移的思路，为该领域带来了重要的贡献，并有望显著提升扩散模型在风格控制方面的实用性和用户体验。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Through extensive evaluation across various styles and text prompts, our method
demonstrates superiority over existing approaches, reflecting the style of the
references, and ensuring that resulting images match the text prompts.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.06827v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.06827v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.06820v1'></a></p>
<h2 id="efficient-discriminative-joint-encoders-for-large-scale-vision-language-reranking"><a href="https://arxiv.org/abs/2510.06820v1">Efficient Discriminative Joint Encoders for Large Scale Vision-Language Reranking</a></h2>
<p><strong>Authors:</strong> Mitchell Keren Taraday, Shahaf Wagner, Chaim Baskin</p>
<p><strong>Published:</strong> 2025-10-08</p>
<p><strong>Categories:</strong> cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Multimodal retrieval still leans on embedding-based models like CLIP for fast
vector search over pre-computed image embeddings. Yet, unlike text retrieval,
where joint-encoder rerankers are standard, comparable vision--language
rerankers are largely absent. We find that seminal joint encoders such as BLIP
are severely bottlenecked by an expensive visual feature-extraction stage,
preventing practical deployment at scale. Motivated by this bottleneck, we
introduce EDJE, an Efficient Discriminative Joint Encoder that precomputes
vision tokens offline and compresses them via a lightweight attention-based
adapter, so online inference runs only a compact joint encoder over a small set
of visual tokens plus the text. EDJE preserves strong retrieval performance
while drastically reducing storage and online compute, enabling high-throughput
inference. Specifically, EDJE processes 50k image--text pairs/second while
requiring 49kB of disk storage per image, matching prior art on Flickr
(zero-shot) and COCO (fine-tuned) retrieval. The implementation and checkpoints
will be made publicly available shortly.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Mitchell Keren Taraday, Shahaf Wagner, Chaim Baskin撰写的论文“Efficient Discriminative Joint Encoders for Large Scale Vision-Language Reranking”的全面摘要。</p>
<hr />
<h3 id="-">论文摘要：高效判别式联合编码器用于大规模视觉-语言重排序</h3>
<p><strong>1. 主要问题或研究问题：</strong>
当前多模态检索主要依赖于基于嵌入的模型（如CLIP）进行快速向量搜索，但与文本检索中联合编码器重排序器已成为标准不同，视觉-语言重排序器在大规模实际部署中仍普遍缺失。现有联合编码器（如BLIP）因其昂贵的视觉特征提取阶段而受到严重瓶颈，导致在线推理速度慢，无法在大规模应用中实用。因此，论文旨在解决的核心问题是：如何在实现大规模检索所需效率的同时，利用联合建模的优势来显著提升跨模态检索性能？</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
论文引入了<strong>EDJE (Efficient Discriminative Joint Encoder)</strong>，一个高效的判别式联合编码器，其核心创新在于：
*   <strong>视觉特征预计算与离线存储：</strong> EDJE将昂贵的视觉特征提取阶段从在线推理转移到离线预处理，图像的视觉token被预先计算并存储在磁盘上。这显著消除了在线推理的瓶颈。
*   <strong>轻量级token压缩适配器：</strong> 为了解决存储所有视觉token的巨大成本问题，EDJE提出了一种基于注意力的轻量级适配器。该适配器将长序列的视觉token压缩成一组紧凑且富有表达力的token（例如，从576个token压缩到64个），从而大幅减少了每图像的存储需求和在线推理时联合编码器需要处理的token数量。
*   <strong>紧凑型联合编码器：</strong> 在线推理时，EDJE仅运行一个紧凑的联合编码器，处理少量压缩的视觉token和文本token，以生成重排序分数。这使得在线推理速度极快。
*   <strong>多任务训练策略：</strong> EDJE采用多阶段训练协议，包括预训练和微调。预训练目标包括图像-文本匹配、掩码语言建模和文本嵌入恢复，以增强模型的判别能力和跨模态对齐。
*   <strong>局部到压缩的知识蒸馏：</strong> 为了进一步提升压缩模型的性能，论文还引入了logit层面的知识蒸馏，利用局部适配器模型作为教师模型，将判别能力从局部模型转移到压缩模型。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>显著的效率提升：</strong> EDJE在保持强大检索性能的同时，大幅降低了存储和在线计算成本，实现了高吞吐量推理。具体而言，EDJE每秒可处理5万对图像-文本，每张图像仅需49KB的磁盘存储。
*   <strong>与现有技术媲美的检索性能：</strong> EDJE在Flickr30k（零样本）和COCO（微调）检索任务上，其性能与现有最先进的联合编码器（如BLIP）相当或超越。例如，在Flickr30k零样本检索中，EDJE在Recall@1指标上比原始CLIP模型提升高达15%。
*   <strong>模块化和鲁棒性：</strong> EDJE作为可插入的重排序器，能够一致地提升各种基于嵌入模型的零样本检索性能，无论底层嵌入模型如何。它对重排序池大小和压缩token数量的变化表现出鲁棒性。
*   <strong>存储成本效益：</strong> 即使在未压缩形式下，EDJE的存储成本也相对较低，而token压缩变体则以极低的存储成本（例如，64个token仅需49KB）保持了大部分检索精度。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>概念验证性质：</strong> 论文将自身定位为概念验证，旨在启发后续工作，这意味着当前版本可能未涵盖所有潜在的应用场景。
*   <strong>未涵盖多语言-多模态检索：</strong> 论文未涉及多语言-多模态检索，这是一个日益受到关注的领域。
*   <strong>未涵盖其他模态：</strong> 论文主要关注视觉-语言模态，未扩展到音频或视频等其他模态。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>扩展到多语言和多模态检索：</strong> 将EDJE的框架应用于多语言和包含音频、视频等其他模态的检索任务。
*   <strong>进一步探索联合编码器：</strong> 论文认为联合编码器领域仍有大量未被充分探索的潜力，投入更多精力改进它们将有益于零样本分类和过滤大型配对数据集等多种应用。
*   <strong>优化token压缩策略：</strong> 进一步研究更先进的token压缩技术，以在性能和存储之间找到更优的平衡。</p>
<hr />
<p>总而言之，这篇论文通过引入EDJE，成功地解决了大规模视觉-语言重排序中效率与性能的矛盾。其核心思想是将昂贵的视觉特征提取离线化，并通过轻量级适配器压缩视觉token，使得在线推理能够以极高的效率运行，同时保持甚至超越现有最先进模型的检索性能。EDJE的模块化设计和显著的效率提升使其成为未来大规模多模态检索系统的重要基石。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Specifically, EDJE processes 50k image--text pairs/second while
requiring 49kB of disk storage per image, matching prior art on Flickr
(zero-shot) and COCO (fine-tuned) retrieval.</li>
<li>The implementation and checkpoints
will be made publicly available shortly.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.06820v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.06820v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-10-09 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
