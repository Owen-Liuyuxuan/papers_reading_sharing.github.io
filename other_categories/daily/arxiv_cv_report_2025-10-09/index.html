<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-10-09 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DEâ«¶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Birdâs-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">è®ºæéè¯»</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-Î±
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-10-08/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-10-10/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-10-09">Arxiv Computer Vision Papers - 2025-10-09</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#arxiv-2025-10-08" class="nav-link">Arxiv è®¡ç®æºè§è§æ¯æ¥æ¥åæ§è¡æè¦ (2025-10-08)</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#quantum-enhanced-computer-vision-going-beyond-classical-algorithms" class="nav-link">Quantum-enhanced Computer Vision: Going Beyond Classical Algorithms</a>
                </li>
                <li class="nav-item">
                    <a href="#vision-language-action-models-for-robotics-a-review-towards-real-world-applications" class="nav-link">Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications</a>
                </li>
                <li class="nav-item">
                    <a href="#diffusion-models-for-low-light-image-enhancement-a-multi-perspective-taxonomy-and-performance-analysis" class="nav-link">Diffusion Models for Low-Light Image Enhancement: A Multi-Perspective Taxonomy and Performance Analysis</a>
                </li>
                <li class="nav-item">
                    <a href="#matrix-mask-track-alignment-for-interaction-aware-video-generation" class="nav-link">MATRIX: Mask Track Alignment for Interaction-aware Video Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#genpilot-a-multi-agent-system-for-test-time-prompt-optimization-in-image-generation" class="nav-link">GenPilot: A Multi-Agent System for Test-Time Prompt Optimization in Image Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#tiger-tool-integrated-geometric-reasoning-in-vision-language-models-for-robotics" class="nav-link">TIGeR: Tool-Integrated Geometric Reasoning in Vision-Language Models for Robotics</a>
                </li>
                <li class="nav-item">
                    <a href="#are-we-using-the-right-benchmark-an-evaluation-framework-for-visual-token-compression-methods" class="nav-link">Are We Using the Right Benchmark: An Evaluation Framework for Visual Token Compression Methods</a>
                </li>
                <li class="nav-item">
                    <a href="#harp-next-high-speed-and-accurate-range-point-fusion-network-for-3d-lidar-semantic-segmentation" class="nav-link">HARP-NeXt: High-Speed and Accurate Range-Point Fusion Network for 3D LiDAR Semantic Segmentation</a>
                </li>
                <li class="nav-item">
                    <a href="#stylekeeper-prevent-content-leakage-using-negative-visual-query-guidance" class="nav-link">StyleKeeper: Prevent Content Leakage using Negative Visual Query Guidance</a>
                </li>
                <li class="nav-item">
                    <a href="#efficient-discriminative-joint-encoders-for-large-scale-vision-language-reranking" class="nav-link">Efficient Discriminative Joint Encoders for Large Scale Vision-Language Reranking</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-10-09">Arxiv Computer Vision Papers - 2025-10-09</h1>
<h2 id="executive-summary">Executive Summary</h2>
<h2 id="arxiv-2025-10-08">Arxiv è®¡ç®æºè§è§æ¯æ¥æ¥åæ§è¡æè¦ (2025-10-08)</h2>
<p><strong>æ¦è¿°ï¼</strong></p>
<p>ä»å¤©ç Arxiv è®¡ç®æºè§è§è®ºæå±ç°äºè¯¥é¢åæç»­çæ´»ååå¤åååå±ãä¸»è¦è¶å¿éä¸­å¨<strong>å¤æ¨¡æå­¦ä¹ ï¼ç¹å«æ¯è§è§-è¯­è¨-å¨ä½æ¨¡åçèåï¼</strong>ã<strong>çææ¨¡åï¼æ©æ£æ¨¡ååå¶åºç¨ï¼</strong>ã<strong>æºå¨äººè§è§ä¸å·èº«æºè½</strong>ä»¥å<strong>æçä¸æ§è½ä¼å</strong>ãå¼å¾æ³¨æçæ¯ï¼æè®ºæå¼å§æ¢ç´¢<strong>éå­è®¡ç®å¨è®¡ç®æºè§è§ä¸­çæ½å</strong>ï¼é¢ç¤ºçæªæ¥å¯è½çæ°èå¼ã</p>
<p><strong>ä¸»è¦ä¸»é¢åè¶å¿ï¼</strong></p>
<ol>
<li><strong>å¤æ¨¡æå­¦ä¹ ä¸å·èº«æºè½ï¼</strong> å¤ç¯è®ºæï¼å¦2ã6ã10ï¼å¼ºè°äºè§è§ãè¯­è¨åå¨ä½çèåï¼ç¹å«æ¯å¨æºå¨äººé¢åãè¿è¡¨æç ç©¶æ­£ä»åä¸æ¨¡æçè§£è½¬åæ´å¨é¢çãè½å¤ä¸ç©çä¸çäº¤äºçæºè½ç³»ç»ã</li>
<li><strong>çææ¨¡åä¸æ©æ£æ¨¡åï¼</strong> æ©æ£æ¨¡åç»§ç»­æ¯ç­é¨ç ç©¶æ¹åï¼ä¸ä»ç¨äºå¾åçæï¼å¦4ã5ï¼ï¼è¿æ©å±å°ä½åç§å¾åå¢å¼ºï¼3ï¼ååå®¹æ§å¶ï¼9ï¼ãè¿è¡¨ææ©æ£æ¨¡åå¨è§£å³åç§è§è§çæåå¢å¼ºä»»å¡ä¸­å±ç°åºå¼ºå¤§çéç¨æ§ã</li>
<li><strong>æºå¨äººè§è§ä¸å ä½æ¨çï¼</strong> æºå¨äººåºç¨æ¯å¦ä¸ä¸ªçªåºä¸»é¢ï¼æ¶µçäºè§è§-è¯­è¨-å¨ä½æ¨¡åï¼2ï¼ãå·¥å·éæå ä½æ¨çï¼6ï¼å3Dè¯­ä¹åå²ï¼8ï¼ï¼åæ äºä¸ºæºå¨äººæä¾æ´é²æ£ãæ´æºè½æç¥è½åçåªåã</li>
<li><strong>æçä¸æ§è½ä¼åï¼</strong> è®ºæå³æ³¨å¦ä½æ´ææå°å¤çè§è§æ°æ®ï¼ä¾å¦è§è§ token åç¼©ï¼7ï¼åå¤§è§æ¨¡è§è§-è¯­è¨éæåºçå¤å«å¼èåç¼ç å¨ï¼10ï¼ï¼è¿å¯¹äºå®éé¨ç½²åå¤§è§æ¨¡åºç¨è³å³éè¦ã</li>
<li><strong>éå­è®¡ç®çåæ­¥æ¢ç´¢ï¼</strong> è®ºæ1é¦æ¬¡å°éå­è®¡ç®å¼å¥è®¡ç®æºè§è§ï¼å°½ç®¡ä»å¤äºæ©æé¶æ®µï¼ä½é¢ç¤ºçæªæ¥å¯è½é¢ è¦æ§çææ¯æ¹åã</li>
</ol>
<p><strong>ç¹å«éè¦æåæ°çè®ºæï¼</strong></p>
<ul>
<li><strong>1. "Quantum-enhanced Computer Vision: Going Beyond Classical Algorithms"ï¼</strong> è¿ç¯è®ºæå·æåç»æ§ï¼æ¯è¯¥é¢åä¸­è¾æ©æ¢ç´¢éå­è®¡ç®ä¸è®¡ç®æºè§è§ç»åçå°è¯ãè½ç¶å·ä½å®ç°åä¼å¿ä»éæ·±å¥ç ç©¶ï¼ä½å¶æ¦å¿µä¸çåæ°æ§å¼å¾å³æ³¨ï¼å¯è½ä¸ºæªæ¥å¸¦æ¥çªç ´ã</li>
<li><strong>2. "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications"ï¼</strong> ä½ä¸ºä¸ç¯ç»¼è¿°ï¼å®ç³»ç»å°æ¢³çäºè§è§-è¯­è¨-å¨ä½æ¨¡åå¨æºå¨äººé¢åçè¿å±ï¼å¹¶å±æäºå®éåºç¨ï¼å¯¹äºçè§£è¯¥é¢åç°ç¶åæªæ¥æ¹åå·æéè¦æå¯¼æä¹ã</li>
<li><strong>4. "MATRIX: Mask Track Alignment for Interaction-aware Video Generation"ï¼</strong> è¿ç¯è®ºæå¨è§é¢çæé¢ååå¾äºè¿å±ï¼éè¿æ©ç è½¨è¿¹å¯¹é½å®ç°äºäº¤äºæç¥çè§é¢çæï¼å¯¹äºçææ´çå®ãæ´å¤æçå¨æåºæ¯å·æéè¦æä¹ã</li>
<li><strong>6. "TIGeR: Tool-Integrated Geometric Reasoning in Vision-Language Models for Robotics"ï¼</strong> å°å·¥å·ä½¿ç¨åå ä½æ¨çéæå°è§è§-è¯­è¨æ¨¡åä¸­ï¼ä¸ºæºå¨äººæä¾äºæ´é«çº§å«çæä½è½åï¼æ¯å·èº«æºè½é¢åçéè¦ä¸æ­¥ã</li>
</ul>
<p><strong>æ°å´ç ç©¶æ¹åæææ¯ï¼</strong></p>
<ul>
<li><strong>éå­è®¡ç®æºè§è§ï¼</strong> å°½ç®¡ä»å¤äºèè½é¶æ®µï¼ä½éå­è®¡ç®ä¸è®¡ç®æºè§è§çç»åï¼è®ºæ1ï¼æ¯ä¸ä¸ªæ½å¨çé¢ è¦æ§æ¹åã</li>
<li><strong>å¤æºè½ä½ç³»ç»å¨çææ¨¡åä¸­çåºç¨ï¼</strong> GenPilotï¼è®ºæ5ï¼å©ç¨å¤æºè½ä½ç³»ç»è¿è¡æç¤ºè¯ä¼åï¼ä¸ºçææ¨¡åæä¾äºæ´æºè½ãæ´çµæ´»çæ§å¶æ¹å¼ã</li>
<li><strong>å·èº«æºè½ä¸­çé«çº§æ¨çï¼</strong> å·¥å·éæå ä½æ¨çï¼è®ºæ6ï¼åè§è§-è¯­è¨-å¨ä½æ¨¡åçç»åï¼è®ºæ2ï¼è¡¨æï¼ç ç©¶æ­£ä»æç¥è½¬åæ´å¤æçè®¤ç¥åæä½æ¨çã</li>
<li><strong>åå®¹æ³æ¼é¢é²ä¸æ§å¶ï¼</strong> StyleKeeperï¼è®ºæ9ï¼éè¿è´é¢è§è§æ¥è¯¢æå¯¼æ¥é²æ­¢åå®¹æ³æ¼ï¼è¿å¯¹äºçææ¨¡åçå®å¨æ§åå¯æ§æ§è³å³éè¦ã</li>
</ul>
<p><strong>å»ºè®®éè¯»çè®ºæï¼</strong></p>
<p>å¯¹äºå¸æå¿«éäºè§£é¢ååæ²¿çå¿ç¢ç ç©¶äººåï¼å»ºè®®ä¼åéè¯»ä»¥ä¸è®ºæï¼</p>
<ol>
<li><strong>"Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications" (Kento Kawaharazuka et al.)ï¼</strong> æä¾äºä¸ä¸ªå¨é¢çæ¦è§ï¼æ¯çè§£å¤æ¨¡ææºå¨äººè§è§ææ°è¿å±çç»ä½³èµ·ç¹ã</li>
<li><strong>"MATRIX: Mask Track Alignment for Interaction-aware Video Generation" (Siyoon Jin et al.)ï¼</strong> å¦ææ¨å¯¹è§é¢çæåå¤æå¨æåºæ¯å»ºæ¨¡æå´è¶£ï¼è¿ç¯è®ºææä¾äºåæ°çè§£å³æ¹æ¡ã</li>
<li><strong>"TIGeR: Tool-Integrated Geometric Reasoning in Vision-Language Models for Robotics" (Yi Han et al.)ï¼</strong> å¯¹äºå·èº«æºè½åæºå¨äººæä½æå´è¶£çç ç©¶äººåï¼è¿ç¯è®ºæå±ç¤ºäºå¦ä½å°é«çº§æ¨çè½åèå¥æºå¨äººç³»ç»ã</li>
<li><strong>"Quantum-enhanced Computer Vision: Going Beyond Classical Algorithms" (Natacha Kuete Meli et al.)ï¼</strong> å¦ææ¨å¯¹æªæ¥ææ¯è¶å¿åæ½å¨çèå¼è½¬åæå´è¶£ï¼è¿ç¯è®ºæå¼å¾ä¸è¯»ï¼å°½ç®¡å¶å½±åå¯è½éè¦æ´é¿æ¶é´æè½æ¾ç°ã</li>
<li><strong>"Diffusion Models for Low-Light Image Enhancement: A Multi-Perspective Taxonomy and Performance Analysis" (Eashan Adhikarla, Yixin Liu, Brian D. Davison)ï¼</strong> å¦ææ¨å³æ³¨æ©æ£æ¨¡åå¨å®éåºç¨ä¸­çæ©å±ï¼ç¹å«æ¯å¾åå¢å¼ºé¢åï¼è¿ç¯ç»¼è¿°ååæå°å¾æä»·å¼ã</li>
</ol>
<p>è¿äºè®ºæå±åæç»äºä¸ä¸ªåæ»¡æ´»ååå¿«éåå±çè®¡ç®æºè§è§é¢åï¼å¶ä¸­å¤æ¨¡æãçææ¨¡ååå·èº«æºè½æ¯å½ååæªæ¥ç ç©¶çå³é®é©±å¨åã</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2510.07317v1">Quantum-enhanced Computer Vision: Going Beyond Classical Algorithms</a></li>
<li><a href="#2510.07077v1">Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications</a></li>
<li><a href="#2510.05976v1">Diffusion Models for Low-Light Image Enhancement: A Multi-Perspective Taxonomy and Performance Analysis</a></li>
<li><a href="#2510.07310v1">MATRIX: Mask Track Alignment for Interaction-aware Video Generation</a></li>
<li><a href="#2510.07217v1">GenPilot: A Multi-Agent System for Test-Time Prompt Optimization in Image Generation</a></li>
<li><a href="#2510.07181v1">TIGeR: Tool-Integrated Geometric Reasoning in Vision-Language Models for Robotics</a></li>
<li><a href="#2510.07143v1">Are We Using the Right Benchmark: An Evaluation Framework for Visual Token Compression Methods</a></li>
<li><a href="#2510.06876v1">HARP-NeXt: High-Speed and Accurate Range-Point Fusion Network for 3D LiDAR Semantic Segmentation</a></li>
<li><a href="#2510.06827v1">StyleKeeper: Prevent Content Leakage using Negative Visual Query Guidance</a></li>
<li><a href="#2510.06820v1">Efficient Discriminative Joint Encoders for Large Scale Vision-Language Reranking</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2510.07317v1'></a></p>
<h2 id="quantum-enhanced-computer-vision-going-beyond-classical-algorithms"><a href="https://arxiv.org/abs/2510.07317v1">Quantum-enhanced Computer Vision: Going Beyond Classical Algorithms</a></h2>
<p><strong>Authors:</strong> Natacha Kuete Meli, Shuteng Wang, Marcel Seelbach Benkner, Michele Sasdelli, Tat-Jun Chin, Tolga Birdal, Michael Moeller, Vladislav Golyanik</p>
<p><strong>Published:</strong> 2025-10-08</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Quantum-enhanced Computer Vision (QeCV) is a new research field at the
intersection of computer vision, optimisation theory, machine learning and
quantum computing. It has high potential to transform how visual signals are
processed and interpreted with the help of quantum computing that leverages
quantum-mechanical effects in computations inaccessible to classical (i.e.
non-quantum) computers. In scenarios where existing non-quantum methods cannot
find a solution in a reasonable time or compute only approximate solutions,
quantum computers can provide, among others, advantages in terms of better time
scalability for multiple problem classes. Parametrised quantum circuits can
also become, in the long term, a considerable alternative to classical neural
networks in computer vision. However, specialised and fundamentally new
algorithms must be developed to enable compatibility with quantum hardware and
unveil the potential of quantum computational paradigms in computer vision.
This survey contributes to the existing literature on QeCV with a holistic
review of this research field. It is designed as a quantum computing reference
for the computer vision community, targeting computer vision students,
scientists and readers with related backgrounds who want to familiarise
themselves with QeCV. We provide a comprehensive introduction to QeCV, its
specifics, and methodologies for formulations compatible with quantum hardware
and QeCV methods, leveraging two main quantum computational paradigms, i.e.
gate-based quantum computing and quantum annealing. We elaborate on the
operational principles of quantum computers and the available tools to access,
program and simulate them in the context of QeCV. Finally, we review existing
quantum computing tools and learning materials and discuss aspects related to
publishing and reviewing QeCV papers, open challenges and potential social
implications.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°å¯¹è¿ç¯è®ºææè¦è¿è¡åæã</p>
<hr />
<p><strong>è®ºææè¦åæï¼Quantum-enhanced Computer Vision: Going Beyond Classical Algorithms</strong></p>
<p><strong>1. è®ºæä¸»è¦è´¡ç®çç®æ´æ»ç» (2-3 å¥è¯)</strong></p>
<p>è¿ç¯è®ºææ¯å¯¹éå­å¢å¼ºè®¡ç®æºè§è§ï¼QeCVï¼é¢åçä¸é¡¹å¨é¢ç»¼è¿°ãå®æ¨å¨ä¸ºè®¡ç®æºè§è§ç¤¾åºæä¾ä¸ä¸ªå³äºéå­è®¡ç®çåèæåï¼è¯¦ç»ä»ç»äºQeCVçåºæ¬æ¦å¿µãæ¹æ³è®ºä»¥åå¦ä½å°è®¡ç®æºè§è§é®é¢è½¬åä¸ºéå­ç¡¬ä»¶å¼å®¹çèå¼ï¼å¹¶æ¢è®¨äºè¯¥é¢åçæªæ¥ææåæ½åã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºæ¹æ³</strong></p>
<p>è¿ç¯è®ºææ¬èº«æ¯ä¸ç¯ç»¼è¿°ï¼å¶å³é®âåæ°âå¨äºå¶<strong>å¨é¢æ§åæ´åæ§</strong>ãå®ç³»ç»å°æ¢³çäºQeCVè¿ä¸æ°å´äº¤åé¢åï¼å¹¶æåºäºä»¥ä¸æ ¸å¿æ¹æ³è®ºè§è§ï¼</p>
<ul>
<li><strong>èå¼å¼å®¹æ§ï¼</strong> å¼ºè°å¼åä¸éå­ç¡¬ä»¶å¼å®¹çå¨æ°ç®æ³çéè¦æ§ï¼ä»¥ååå©ç¨éå­è®¡ç®çæ½åã</li>
<li><strong>åééå­è®¡ç®èå¼ï¼</strong> è¯¦ç»éè¿°äºä¸¤ç§ä¸»è¦çéå­è®¡ç®èå¼ââåºäºé¨ï¼gate-basedï¼çéå­è®¡ç®åéå­éç«ï¼quantum annealingï¼ââå¨QeCVä¸­çåºç¨æ¹æ³ã</li>
<li><strong>æä½åçä¸å·¥å·ï¼</strong> æä¾äºéå­è®¡ç®æºçæä½åçãè®¿é®ãç¼ç¨åæ¨¡æå·¥å·çä»ç»ï¼æ¨å¨éä½è®¡ç®æºè§è§ç ç©¶äººåè¿å¥è¯¥é¢åçé¨æ§ã</li>
<li><strong>æªæ¥å±æï¼</strong> è®¨è®ºäºåæ°åéå­çµè·¯ä½ä¸ºç»å¸ç¥ç»ç½ç»çé¿ææ¿ä»£æ¹æ¡çå¯è½æ§ã</li>
</ul>
<p><strong>3. å¯¹é¢åæ½å¨å½±å</strong></p>
<p>è¿ç¯ç»¼è¿°å¯¹è®¡ç®æºè§è§é¢åå·ææ·±è¿çæ½å¨å½±åï¼</p>
<ul>
<li><strong>ç¥è¯æ®åä¸æ¡¥æ¢ä½ç¨ï¼</strong> å®å°æä¸ºè®¡ç®æºè§è§ç ç©¶äººåè¿å¥éå­è®¡ç®é¢åçå³é®åèèµæï¼å¼¥åäºè¿ä¸¤ä¸ªå¤æé¢åä¹é´çç¥è¯é¸¿æ²ã</li>
<li><strong>å¬çæ°ç®æ³åç ç©¶æ¹åï¼</strong> éè¿ç³»ç»ä»ç»QeCVçæ½åãææåç°æå·¥å·ï¼å®æææ¿åè®¡ç®æºè§è§ç¤¾åºå¼ååºå¨æ°çãéå­åçï¼quantum-nativeï¼çç®æ³ï¼è§£å³ç»å¸æ¹æ³é¾ä»¥å¤ççé®é¢ã</li>
<li><strong>æ¨å¨QeCVåå±ï¼</strong> ä½ä¸ºä¸ç¯å¨é¢çç»¼è¿°ï¼å®å°ä¸ºQeCVé¢åçæªæ¥ç ç©¶æä¾ä¸ä¸ªåå®çåºç¡åè·¯çº¿å¾ï¼å éè¯¥é¢åçåå±ã</li>
<li><strong>è§£å³å¤æé®é¢ï¼</strong> å¼ºè°éå­è®¡ç®å¨å¤çç»å¸æ¹æ³æ æ³å¨åçæ¶é´åæ¾å°ç²¾ç¡®è§£çé®é¢ä¸çä¼å¿ï¼é¢ç¤ºçQeCVå¯è½å¨æäºæç«¯å¤æçè§è§ä»»å¡ä¸­åå¾çªç ´ã</li>
</ul>
<p><strong>4. å¯è½åçäºè¿é¡¹ç ç©¶çç¸å³é¢åæåºç¨</strong></p>
<ul>
<li><strong>å¤§è§æ¨¡å¾å/è§é¢åæï¼</strong> å¯¹äºéè¦å¤çæµ·éæ°æ®ãä¼åå¤ææ¨¡åæè¿è¡é«ç»´ç¹å¾å¹éçä»»å¡ï¼éå­è®¡ç®çæ¶é´å¯ä¼¸ç¼©æ§ä¼å¿å¯è½å¸¦æ¥æ¾èæåã</li>
<li><strong>å»å­¦å½±ååæï¼</strong> å¨ç¾çè¯æ­ãè¯ç©åç°ç­é¢åï¼éè¦å¤çé«åè¾¨çãå¤æ¨¡ææ°æ®å¹¶è¿è¡ç²¾ç¡®æ¨¡å¼è¯å«ï¼QeCVå¯è½æä¾æ´é«æãæ´åç¡®çè§£å³æ¹æ¡ã</li>
<li><strong>èªå¨é©¾é©¶ä¸æºå¨äººè§è§ï¼</strong> å®æ¶å³ç­ãå¤æç¯å¢æç¥åè·¯å¾è§åä¸­çä¼åé®é¢ï¼å¯è½ä»éå­ä¼åç®æ³ä¸­åçã</li>
<li><strong>è®¡ç®æå½±ä¸å¾åéå»ºï¼</strong> æ¶åéé®é¢ãå»åªãè¶åè¾¨çç­ä»»å¡ï¼éå­ç®æ³å¯è½æä¾æ´ä¼çè§£ã</li>
<li><strong>æ¨¡å¼è¯å«ä¸å¼å¸¸æ£æµï¼</strong> å¯¹äºå¨å¤ææ°æ®éä¸­è¯å«å¾®å¼±æ¨¡å¼æç½è§å¼å¸¸ï¼éå­æºå¨å­¦ä¹ ç®æ³å¯è½å±ç°åºç¬ç¹ä¼å¿ã</li>
<li><strong>éå­æºå¨å­¦ä¹ ï¼QMLï¼ç ç©¶ï¼</strong> QeCVæ¯QMLçä¸ä¸ªéè¦åºç¨åæ¯ï¼è¿ç¯ç»¼è¿°å°ç´æ¥ä¿è¿QMLå¨è§è§é¢åçåºç¨ååå±ã</li>
</ul>
<p><strong>5. ä»æè¦ä¸­å¯ä»¥æ¨æ­åºçä»»ä½å±éæ§</strong></p>
<ul>
<li><strong>ç¡¬ä»¶æçåº¦ï¼</strong> æè¦ä¸­æå°âé¿ææ¥çï¼åæ°åéå­çµè·¯å¯ä»¥æä¸ºç»å¸ç¥ç»ç½ç»çéè¦æ¿ä»£æ¹æ¡âï¼è¿æç¤ºäºå½åéå­ç¡¬ä»¶çæçåº¦å¯è½å°æªè¾¾å°è½å¤å¤§è§æ¨¡æ¿ä»£ç»å¸æ¹æ³çç¨åº¦ãQeCVçå®éåºç¨ä»åéäºéå­è®¡ç®æºçè§æ¨¡ãç¨³å®æ§ãçº éè½åç­ã</li>
<li><strong>ç®æ³å¼åé¾åº¦ï¼</strong> æè¦æç¡®æåºâå¿é¡»å¼åä¸é¨çãæ ¹æ¬æ§çæ°ç®æ³âï¼è¿è¡¨æå°ç»å¸è®¡ç®æºè§è§é®é¢è½¬åä¸ºéå­ç¡¬ä»¶å¼å®¹çèå¼å¹¶éæäºï¼éè¦æ·±åçè·¨å­¦ç§ç¥è¯åå¤§éçç ç©¶æå¥ã</li>
<li><strong>çè®ºä¸å®è·µå·®è·ï¼</strong> ä½ä¸ºä¸ç¯ç»¼è¿°ï¼å®ä¸»è¦å³æ³¨çè®ºæ¡æ¶åæ½åï¼å®éçæ§è½æåååºç¨ææä»ééè¿å·ä½çå®éªåæ¡ä¾ç ç©¶æ¥éªè¯ã</li>
<li><strong>å¯è®¿é®æ§ä¸å­¦ä¹ æ²çº¿ï¼</strong> å°½ç®¡æ¨å¨éä½é¨æ§ï¼ä½éå­è®¡ç®æ¬èº«åºæçå¤ææ§æå³çè®¡ç®æºè§è§ç ç©¶äººåä»éæå¥å¤§éç²¾åå­¦ä¹ æ°çæ¦å¿µåå·¥å·ã</li>
<li><strong>åªå£°ä¸éè¯¯ï¼</strong> æè¦ä¸­æªæåå½åéå­ç¡¬ä»¶é¢ä¸´çåªå£°ï¼noiseï¼åéè¯¯ï¼errorï¼é®é¢ï¼è¿äºæ¯å½±åéå­ç®æ³å®éæ§è½çå³é®å ç´ ã</li>
</ul>
<hr />
<p>æ»èè¨ä¹ï¼è¿ç¯ç»¼è¿°è®ºæå¨è®¡ç®æºè§è§åéå­è®¡ç®çäº¤åé¢åå·æå¼åæ§æä¹ãå®ä¸ä»ä¸ºè®¡ç®æºè§è§ç¤¾åºæå¼äºä¸æéå¾éå­ä¸ççå¤§é¨ï¼æ´éè¦çæ¯ï¼å®ç³»ç»å°å¾ååºQeCVçèå¾ï¼ææäºæªæ¥çç ç©¶æ¹ååæ½å¨ççªç ´ç¹ï¼é¢ç¤ºçè®¡ç®æºè§è§é¢åå¯è½è¿æ¥ä¸åºç±éå­è®¡ç®é©±å¨çèå¼è½¬åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Quantum-enhanced Computer Vision (QeCV) is a new research field at the
intersection of computer vision, optimisation theory, machine learning and
quantum computing.</li>
<li>However, specialised and fundamentally new
algorithms must be developed to enable compatibility with quantum hardware and
unveil the potential of quantum computational paradigms in computer vision.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.07317v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.07317v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.07077v1'></a></p>
<h2 id="vision-language-action-models-for-robotics-a-review-towards-real-world-applications"><a href="https://arxiv.org/abs/2510.07077v1">Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications</a></h2>
<p><strong>Authors:</strong> Kento Kawaharazuka, Jihoon Oh, Jun Yamada, Ingmar Posner, Yuke Zhu</p>
<p><strong>Published:</strong> 2025-10-08</p>
<p><strong>Categories:</strong> cs.RO, cs.AI, cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Amid growing efforts to leverage advances in large language models (LLMs) and
vision-language models (VLMs) for robotics, Vision-Language-Action (VLA) models
have recently gained significant attention. By unifying vision, language, and
action data at scale, which have traditionally been studied separately, VLA
models aim to learn policies that generalise across diverse tasks, objects,
embodiments, and environments. This generalisation capability is expected to
enable robots to solve novel downstream tasks with minimal or no additional
task-specific data, facilitating more flexible and scalable real-world
deployment. Unlike previous surveys that focus narrowly on action
representations or high-level model architectures, this work offers a
comprehensive, full-stack review, integrating both software and hardware
components of VLA systems. In particular, this paper provides a systematic
review of VLAs, covering their strategy and architectural transition,
architectures and building blocks, modality-specific processing techniques, and
learning paradigms. In addition, to support the deployment of VLAs in
real-world robotic applications, we also review commonly used robot platforms,
data collection strategies, publicly available datasets, data augmentation
methods, and evaluation benchmarks. Throughout this comprehensive survey, this
paper aims to offer practical guidance for the robotics community in applying
VLAs to real-world robotic systems. All references categorized by training
approach, evaluation method, modality, and dataset are available in the table
on our project website: https://vla-survey.github.io .</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Kento Kawaharazukaç­äººæ°åçè®ºæâVision-Language-Action Models for Robotics: A Review Towards Real-World Applicationsâçå¨é¢æè¦ã</p>
<hr />
<h3 id="vision-language-action-models-for-robotics-a-review-towards-real-world-applications_1">è®ºææè¦ï¼Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³æºå¨äººé¢åä¸­ä¸ä¸ªæ ¸å¿ææï¼å¦ä½å¼åè½å¤è·¨ä¸åä»»å¡ãç©ä½ãæºå¨äººå½¢æåç¯å¢è¿è¡æ³åçæºå¨äººç­ç¥ï¼ä»¥å®ç°æ´çµæ´»ãå¯æ©å±ççå®ä¸çé¨ç½²ãå·ä½æ¥è¯´ï¼å®å³æ³¨äºè§è§-è¯­è¨-å¨ä½ï¼VLAï¼æ¨¡åï¼è¿äºæ¨¡åéè¿ç»ä¸è§è§ãè¯­è¨åå¨ä½æ°æ®æ¥å­¦ä¹ éç¨ç­ç¥ï¼ä»¥åæä¼ ç»æºå¨äººç³»ç»å¨æ³åè½ååæ°æ®æçæ¹é¢çå±éæ§ãè¯¥ç»¼è¿°çç®çæ¯æä¾ä¸ä¸ªå¨é¢çãå¨æ å¼çVLAç³»ç»åé¡¾ï¼å¹¶ä¸ºæºå¨äººç¤¾åºå¨å®éåºç¨VLAæ¨¡åæ¶æä¾å®ç¨æå¯¼ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥è®ºææ¬èº«æ¯ä¸ç¯ç»¼è¿°ï¼å¶ä¸»è¦è´¡ç®å¨äºå¯¹VLAæ¨¡åé¢åçç³»ç»æ§ãå¨æ å¼åæååç±»ï¼èéæåºæ°çæ¨¡åæç®æ³ãå¶å³é®è´¡ç®åæ¬ï¼</p>
<ul>
<li><strong>å¨é¢çVLAæ¨¡ååç±»ï¼</strong> è®ºæç³»ç»å°åé¡¾äºVLAæ¨¡åçè®¾è®¡ç­ç¥åæ¶ææ¼åï¼ä»æ©æçåºäºCNNçæ¨¡åï¼å¦CLIPortï¼å°åºäºTransformerçåºåæ¨¡åï¼å¦RTç³»åãVIMAï¼ï¼åå°éææ©æ£æ¨¡ååæµå¹éææ¯ï¼å¦OctoãÏoï¼çç­ç¥ï¼ä»¥åææ°çåå±æ§å¶æ¡æ¶ï¼å¦LAPAãÏo.5ãGROOT N1ï¼ã</li>
<li><strong>è¯¦ç»çæ¶æåæå»ºååæï¼</strong> è®ºææ·±å¥æ¢è®¨äºVLAæ¨¡åçæ ¸å¿æ¶æç»ä»¶ï¼åæ¬æç¥è¿å¨æ¨¡åï¼Sensorimotor Modelï¼ãä¸çæ¨¡åï¼World Modelï¼ååºäºå¯ä¾æ§æ¨¡åï¼Affordance-Based Modelï¼çä¸ç§åä½ï¼å¹¶è¯¦ç»ä»ç»äºå®ä»¬å¦ä½å¤çè§è§ãè¯­è¨åå¨ä½ç­æ¨¡æã</li>
<li><strong>æ¨¡æç¹å®å¤çææ¯ï¼</strong> è¯¦ç»éè¿°äºVLAæ¨¡åå¦ä½å¤çåç§è¾å¥æ¨¡æï¼åæ¬è§è§ç¹å¾æåï¼ResNetãViTãVLMéª¨å¹²ï¼ãè¯­è¨åè¯åç¼ç ï¼T5ãLLaMAãUSEãCLIPææ¬ç¼ç å¨ï¼ä»¥åå¨ä½è¡¨ç¤ºï¼ç¦»æ£å¨ä½tokenãè¿ç»­å¨ä½å»ºæ¨¡ãæ½å¨å¨ä½å­¦ä¹ ï¼ãæ­¤å¤ï¼è¿è®¨è®ºäºé³é¢ãè§¦è§å3Dä¿¡æ¯ï¼æ·±åº¦å¾åãå¤è§è§å¾åãä½ç´ è¡¨ç¤ºãç¹äºï¼çéæã</li>
<li><strong>è®­ç»ç­ç¥åå®ç°èéï¼</strong> è®ºæåç±»äºVLAæ¨¡åçè®­ç»æ¹æ³ï¼åæ¬çç£å­¦ä¹ ãèªçç£å­¦ä¹ åå¼ºåå­¦ä¹ ï¼å¹¶è®¨è®ºäºé¢è®­ç»ãåè®­ç»ãæ¢¯åº¦éç¦»ãåæ°é«æéåºï¼å¦LoRAï¼ä»¥åå®æ¶æ¨çï¼å¦RTCï¼ç­å®ç¨å®ç°ç»èã</li>
<li><strong>æ°æ®çæç³»ç»ç»¼è¿°ï¼</strong> è®ºæå¨é¢åé¡¾äºæ°æ®æ¶éç­ç¥ï¼é¥æä½ãä»£çè®¾å¤ãäººç±»æ°æ®æ¶éï¼ãå¬å¼æ°æ®éï¼äººç±»è§é¢æ°æ®éãæ¨¡ææ°æ®éãçå®æºå¨äººæ°æ®éï¼ä»¥åæ°æ®å¢å¼ºæ¹æ³ï¼è§è§ãè¯­è¨ãå¨ä½å¢å¼ºï¼ã</li>
<li><strong>æºå¨äººå¹³å°åè¯ä¼°åºåï¼</strong> è®ºææ»ç»äºVLAç ç©¶ä¸­å¸¸ç¨çæºå¨äººå¹³å°ï¼æºæ¢°èãæ/å¤¹æå¨ãç§»å¨æºå¨äººãåè¶³æºå¨äººãäººå½¢æºå¨äººï¼ä»¥ååç§è¯ä¼°åºåï¼å¦robosuiteãManiSkillãLIBEROãCALVINãRLBenchï¼ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
è¯¥ç»¼è¿°æ­ç¤ºäºVLAæ¨¡åé¢ååå±çå ä¸ªå³é®æ´å¯ï¼</p>
<ul>
<li><strong>æ³åè½åçå³é®ï¼</strong> å¤§è§æ¨¡æ°æ®éåé¢è®­ç»åºç¡æ¨¡åå¨å®ç°VLAæ¨¡åæ³åæ¹é¢åæ¥çè³å³éè¦çä½ç¨ï¼ä½¿å¶è½å¤å°ç½ç»ç¥è¯è½¬ç§»å°æºå¨äººæ§å¶ä¸­ã</li>
<li><strong>åå±æ¶æçå´èµ·ï¼</strong> åå±æ¶æï¼å°é«çº§è¯­è¨çè§£ä¸ä½çº§è¿å¨æ§è¡åç¦»ï¼çåºç°ï¼æé«äºæ¨¡åå¨é¿å¨æãå¤æ­¥éª¤ä»»å¡ä¸­çæ§è½ã</li>
<li><strong>å¤æ¨¡æè¾å¥çéè¦æ§ï¼</strong> é¤äºè§è§åè¯­è¨ï¼éæå¶ä»æ¨¡æï¼å¦é³é¢ãè§¦è§ã3Dä¿¡æ¯ï¼å¯¹äºå¢å¼ºæºå¨äººçæç¥åäº¤äºè½åè¶æ¥è¶éè¦ã</li>
<li><strong>çå®ä¸çé¨ç½²çææï¼</strong> å°½ç®¡åå¾äºè¿å±ï¼ä½æ¨¡æå°çå®ä¸ççè¿ç§»åæºå¨äººå½¢ææ³åä»ç¶æ¯éå¤§ææï¼éå¶äºVLAæ¨¡åå¨éç»æåçå®ä¸çç¯å¢ä¸­çå¹¿æ³é¨ç½²ã</li>
<li><strong>å®ç¨æå¯¼ï¼</strong> è®ºæä¸ºä»ä¸èæä¾äºå·ä½çå»ºè®®ï¼ä¾å¦ä¼åéæ©å¤æ ·åé«è´¨éæ°æ®éãéè¿çææ¹æ³å®ç°è¿ç»­å¨ä½çæãå¨é¢è®­ç»æé´è¿è¡æ¢¯åº¦éç¦»ãéç¨è½»éçº§éåºæ¹æ³ãæ´åä¸çæ¨¡åææ½å¨å¨ä½å­¦ä¹ ï¼ä»¥åéè¿å¤ä»»å¡å­¦ä¹ å¢å¼ºå¨ä½çæè¡¨ç¤ºã</li>
</ul>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
è¯¥è®ºæå¨è®¨è®ºVLAæ¨¡åæ¶ä¹æåºäºç°ææ¹æ³çå±éæ§ï¼</p>
<ul>
<li><strong>æ°æ®ç¨ç¼ºæ§ï¼</strong> ç¼ºä¹å¤§è§æ¨¡ãå¤æ ·åä¸æ æ³¨è¯å¥½çè§è§ãè¯­è¨åå¨ä½æ°æ®ï¼ç¹å«æ¯é«è´¨éçæºå¨äººæ¼ç¤ºæ°æ®æ¶éææ¬é«æä¸é¾ä»¥æ©å±ã</li>
<li><strong>æºå¨äººå½¢æè¿ç§»ï¼</strong> æºå¨äººå½¢æçå¤æ ·æ§ï¼å³èéç½®ãä¼ æå¨ç±»åãè¿å¨ç©ºé´ï¼ä½¿å¾è·¨ä¸åæºå¨äººå½¢æè¿ç§»ç­ç¥æä¸ºéå¤§ææãå°äººç±»è¿å¨æ°æ®æ å°å°æºå¨äººå¯æ§è¡å¨ä½ä¹éæäºã</li>
<li><strong>è®¡ç®åè®­ç»ææ¬ï¼</strong> VLAæ¨¡åè®­ç»è®¡ç®éæ±å·¨å¤§ï¼å°¤å¶æ¯å¨å¤çé¿æ¶åºåºåãé«åè¾¨çå¾åæé¢å¤æ¨¡ææ¶ãæ¨çæ¶çå»¶è¿ååå­ä½¿ç¨ä¹éå¶äºå¨èµæºåéæºå¨äººå¹³å°ä¸çé¨ç½²ã</li>
<li><strong>æ³åè½åä¸è¶³ï¼</strong> æ©ææ¨¡åå¨ç»ä¸ä¸åæ¨¡æåæææ©å±æ¹é¢é¢ä¸´ææï¼æ³åè½åä»åéäºé¢å®ä¹ä»»å¡éã</li>
<li><strong>ç¼ºä¹ä¸­é´æ¨çï¼</strong> å¸åçVLAæ¨¡åç¼ºä¹ä¸­é´æ¨çè½åï¼é¾ä»¥å¤çå¤æãé¿å¨æçä»»å¡ã</li>
<li><strong>RLçå±éæ§ï¼</strong> å¼ºåå­¦ä¹ ï¼RLï¼è½ç¶è½æé«é²æ£æ§ï¼ä½æ ·æ¬æçä½ï¼ä¸å¨çå®ä¸çæ¢ç´¢ä¸­å­å¨å®å¨é£é©ï¼éå¶äºå¶å¨VLAæ¨¡åä¸­çå¹¿æ³åºç¨ã</li>
<li><strong>å¤æ¨¡ææ°æ®æ¶éçææï¼</strong> æ¶éå¤§è§æ¨¡çé³é¢ãè§¦è§å3Dç¹äºæ°æ®ä»ç¶æ¯ä¸ä¸ªéå¤§ææï¼ç¼ºä¹æ ååä¼ æå¨éç½®è¿ä¸æ­¥å¤æåäºå¤æ¨¡ææ°æ®çéæã</li>
<li><strong>è¯ä¼°çä¸è¶³ï¼</strong> VLAæ¨¡åçè¯ä¼°ææ å®ä¹ä¸æç¡®ï¼å°¤å¶æ¯å¨çå®ä¸çç¯å¢ä¸­ãç¼ºä¹ç»è®¡å­¦ä¸ä¸¥æ ¼çè¯ä¼°ï¼ä½¿å¾é¾ä»¥ç¡®å®åªç§æ¹æ³æææã</li>
<li><strong>å®å¨é®é¢ï¼</strong> å¨éç»æåç¯å¢ä¸­é¨ç½²VLAæ¨¡åå­å¨æ¾èå®å¨ææï¼ç°æç³»ç»ç¼ºä¹æ£æµåé¿åæå¤äººç±»å­å¨çæºå¶ï¼å¢å äºç¢°æé£é©ã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºææåºäºVLAæ¨¡åæªæ¥ç ç©¶çå ä¸ªå³é®æ¹åï¼</p>
<ul>
<li><strong>å¤æ¨¡ææ°æ®éæï¼</strong> è§£å³å¤§è§æ¨¡å¤æ¨¡ææ°æ®ï¼ç¹å«æ¯è§¦è§å3Dä¿¡æ¯ï¼æ¶éçææï¼å¹¶æ ååä¼ æå¨éç½®ä»¥å®ç°å¯æ©å±çVLAç³»ç»ã</li>
<li><strong>æ¨çè½åå¢å¼ºï¼</strong> å¼åè½å¤ä¿çç¸å³ä¿¡æ¯ãéæ©æ§å³æ³¨å³é®ä¿¡æ¯ãæ¯ææ¶é´æ½è±¡ååºäºè®°å¿æ£ç´¢çæ¨çæºå¶ï¼ä»¥è§£å³é¿å¨æãå¤æ­¥éª¤ä»»å¡ã</li>
<li><strong>æç»­å­¦ä¹ ï¼</strong> ä½¿VLAæ¨¡åè½å¤å¨çº¿ææç»­å­¦ä¹ ï¼éåºæ°æåµï¼åæç¾é¾æ§éå¿ï¼å¹¶å®å¨ææå°å¨çå®ä¸çä¸­è¿è¡ã</li>
<li><strong>å¼ºåå­¦ä¹ ä¸ä¸çæ¨¡åç»åï¼</strong> éè¿å¨å­¦ä¹ å°çä¸çæ¨¡åä¸­è¿è¡RLå¾®è°ï¼å®ç°æ´å®å¨ãæ ·æ¬é«æçRLï¼å¹¶ç»åæ¨¡æå°çå®ï¼sim-to-realï¼ææ¯ã</li>
<li><strong>å®å¨æ§åå¯é æ§ï¼</strong> æ´åVLAä¸åºäºæ¨¡åçæ§å¶æ¹æ³ï¼æä¾é¢æµæ§æ¨çï¼ä»¥æé«å¨å®å¨å³é®æåµä¸çå®å¨æ§ã</li>
<li><strong>æéæ£æµä¸æ¢å¤ï¼</strong> å¼åè½å¤æ£æµæéãéåºæ§å°éæ°è§ååæ¢å¤çæºå¶ï¼ä»¥æé«VLAç³»ç»å¨çå®ä¸çé¨ç½²ä¸­çé²æ£æ§åå¯é æ§ã</li>
<li><strong>æ ååè¯ä¼°ï¼</strong> å»ºç«ç»è®¡å­¦ä¸ä¸¥æ ¼çè¯ä¼°åè®®ååºåï¼ä»¥å¬å¹³ãå¯é å°æ¯è¾ä¸åVLAæ¨¡åçæ§è½ã</li>
<li><strong>å®éåºç¨ï¼</strong> æ¨å¨VLAæ¨¡åå¨å»çä¿å¥ãè¾å©ææ¯ãå·¥ä¸èªå¨ååèªå¨é©¾é©¶ç­é¢åçå®éé¨ç½²ï¼ä½¿å¶è¾¾å°äººç±»æ°´å¹³çæ§è½åå¯é æ§ã</li>
</ul>
<hr />
<p>è¿ç¯ç»¼è¿°ä¸ºæºå¨äººé¢åçVLAæ¨¡åç ç©¶æä¾äºä¸ä¸ªå¨é¢çè·¯çº¿å¾ï¼ä¸ä»æ»ç»äºç°æè¿å±ï¼è¿æç¡®æåºäºæªæ¥çææåæºéã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This generalisation capability is expected to
enable robots to solve novel downstream tasks with minimal or no additional
task-specific data, facilitating more flexible and scalable real-world
deployment.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.07077v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.07077v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.05976v1'></a></p>
<h2 id="diffusion-models-for-low-light-image-enhancement-a-multi-perspective-taxonomy-and-performance-analysis"><a href="https://arxiv.org/abs/2510.05976v1">Diffusion Models for Low-Light Image Enhancement: A Multi-Perspective Taxonomy and Performance Analysis</a></h2>
<p><strong>Authors:</strong> Eashan Adhikarla, Yixin Liu, Brian D. Davison</p>
<p><strong>Published:</strong> 2025-10-07</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Low-light image enhancement (LLIE) is vital for safety-critical applications
such as surveillance, autonomous navigation, and medical imaging, where
visibility degradation can impair downstream task performance. Recently,
diffusion models have emerged as a promising generative paradigm for LLIE due
to their capacity to model complex image distributions via iterative denoising.
This survey provides an up-to-date critical analysis of diffusion models for
LLIE, distinctively featuring an in-depth comparative performance evaluation
against Generative Adversarial Network and Transformer-based state-of-the-art
methods, a thorough examination of practical deployment challenges, and a
forward-looking perspective on the role of emerging paradigms like foundation
models. We propose a multi-perspective taxonomy encompassing six categories:
Intrinsic Decomposition, Spectral &amp; Latent, Accelerated, Guided, Multimodal,
and Autonomous; that map enhancement methods across physical priors,
conditioning schemes, and computational efficiency. Our taxonomy is grounded in
a hybrid view of both the model mechanism and the conditioning signals. We
evaluate qualitative failure modes, benchmark inconsistencies, and trade-offs
between interpretability, generalization, and inference efficiency. We also
discuss real-world deployment constraints (e.g., memory, energy use) and
ethical considerations. This survey aims to guide the next generation of
diffusion-based LLIE research by highlighting trends and surfacing open
research questions, including novel conditioning, real-time adaptation, and the
potential of foundation models.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºEashan Adhikarla, Yixin Liu, Brian D. Davisonæ°åçè®ºæâDiffusion Models for Low-Light Image Enhancement: A Multi-Perspective Taxonomy and Performance Analysisâçå¨é¢æè¦ï¼æ¨å¨çªåºå¶å¯¹è®¡ç®æºè§è§é¢åçæ°é¢æ§åéè¦æ§ã</p>
<p><strong>è®ºææè¦ï¼æ©æ£æ¨¡åå¨ä½åå¾åå¢å¼ºä¸­çå¤è§è§åç±»ä¸æ§è½åæ</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³ä½åå¾åå¢å¼ºï¼LLIEï¼è¿ä¸å³é®é®é¢ï¼å®å¨çæ§ãèªå¨é©¾é©¶åå»å­¦æåç­å®å¨å³é®åºç¨ä¸­è³å³éè¦ï¼å ä¸ºå¯è§åº¦ä¸éä¼ä¸¥éå½±åä¸æ¸¸ä»»å¡çæ§è½ãè¿å¹´æ¥ï¼æ©æ£æ¨¡åå å¶éè¿è¿­ä»£å»åªå»ºæ¨¡å¤æå¾ååå¸çè½åï¼å·²æä¸ºLLIEé¢åä¸ä¸ªæåæ¯ççæèå¼ãæ¬ç»¼è¿°çæ ¸å¿ç ç©¶é®é¢æ¯ï¼å¦ä½å¯¹å½åæ©æ£æ¨¡åå¨LLIEä¸­çåºç¨è¿è¡ç³»ç»æ§ãæ¹å¤æ§çåæï¼åæ¬å¶æ§è½ãææãå±éæ§ä»¥åæªæ¥åå±æ¹åï¼å¹¶å°å¶ä¸GANåTransformerç­ç°ææåè¿æ¹æ³è¿è¡æ¯è¾ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥è®ºæçä¸»è¦è´¡ç®å¨äºæåºäºä¸ä¸ªå¨é¢çãå¤è§è§çæ©æ£æ¨¡åLLIEæ¹æ³åç±»æ³ï¼æ¶µçå­ä¸ªä¸»è¦ç±»å«ï¼
*   <strong>åå¨åè§£ï¼Intrinsic Decompositionï¼ï¼</strong> åºäºRetinexçè®ºæç©çé©±å¨æ¨¡åï¼å°å¾ååè§£ä¸ºåå°çååç§ç­åéè¿è¡å¢å¼ºã
*   <strong>é¢è°±ä¸æ½å¨ç©ºé´ï¼Spectral &amp; Latentï¼ï¼</strong> å¨é¢åæå­¦ä¹ å°çæ½å¨ç©ºé´ä¸­è¿è¡æ©æ£ï¼ä»¥æé«å¤çæçåé²æ£æ§ã
*   <strong>å éï¼Acceleratedï¼ï¼</strong> éè¿éæ ·æ­¥æ°åå°ãè¸é¦åç¥è¯è¿ç§»ç­ææ¯ï¼æ¾èéä½æ¨çæ¶é´ï¼ä½¿æ©æ£æ¨¡åéç¨äºå®æ¶åºç¨ã
*   <strong>å¼å¯¼ï¼Guidedï¼ï¼</strong> å¼å¥å¤é¨ä¿¡å·ï¼å¦åºåæ©ç ãç¨æ·æä»¤ãæååæ°ï¼æ¥æ§å¶å¢å¼ºè¿ç¨ï¼å®ç°ç©ºé´èªéåºåè¯­ä¹é©±å¨çå¢å¼ºã
*   <strong>å¤æ¨¡æï¼Multimodalï¼ï¼</strong> éå¯¹ç¹å®ä¸æ¸¸ä»»å¡ï¼å¦OCRãç®æ æ£æµï¼è¿è¡å¢å¼ºï¼æèåæ¥èªäºä»¶ç¸æºãçº¢å¤ç­å¤ç§ä¼ ææ¨¡æçä¿¡æ¯ã
*   <strong>èªä¸»ï¼Autonomousï¼ï¼</strong> å©ç¨èªçç£å­¦ä¹ ãé¶æ ·æ¬éåºåæ çç£åå¯¹é½ï¼æ ééå¯¹è®­ç»æ°æ®å³å¯è¿è¡å¢å¼ºï¼æé«æ³åè½åã</p>
<p>è¯¥åç±»æ³èåäºæ¨¡åæºå¶åæ¡ä»¶ä¿¡å·çæ··åè§è§ï¼ä¸ºçè§£LLIEä¸­æ©æ£æ¨¡åçå¤æ ·åæ¹æ³æä¾äºæ¸æ°çæ¡æ¶ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶éè¦æ§ï¼</strong>
è®ºæéè¿å¯¹æ©æ£æ¨¡åä¸GANåTransformerç­æåè¿æ¹æ³çæ·±å¥æ¯è¾åæï¼æ­ç¤ºäºä»¥ä¸ä¸»è¦è¶å¿åç»æï¼
*   <strong>æ§è½ä¼å¿ï¼</strong> æ©æ£æ¨¡åå¨æç¥è´¨éæ¹é¢è¡¨ç°åºæ°å´çä¸»å¯¼å°ä½ï¼å°¤å¶å¨çæé¼çç»èåçº¹çæ¹é¢ä¼äºæ©ææ¹æ³ãå®ä»¬å¨æç«¯åç§æ¡ä»¶ä¸æä¾æ´ç¨³å®çè®­ç»åæ´å¯é çç»æï¼å¹¶è½ææç¼è§£æ¨¡å¼å´©æºé®é¢ã
*   <strong>æçä¸ä¿çåº¦æè¡¡ï¼</strong> æ©æ£æ¨¡åè½ç¶è´¨éé«ï¼ä½è®¡ç®ææ¬é«ï¼é«FLOPsãå¤åæ°ãæ¨çæ¢ï¼ãå éææ¯ï¼å¦æ½å¨æ©æ£ãè¸é¦ï¼æ¨å¨ç¼è§£è¿ä¸é®é¢ï¼ä½å¯è½ä»¥çºç²é¨åå¢å¼ºè´¨éæç¨³å®æ§ä¸ºä»£ä»·ã
*   <strong>èå¼è½¬åï¼</strong> LLIEçç¦ç¹æ­£ä»åä¸çå¾åæ¢å¤ä»»å¡è½¬åæ´ç»è´ãå¯æ§åä¸ä¸ææç¥çè¿ç¨ãå¼å¯¼å¼åå¤æ¨¡ææ©æ£æ¡æ¶éè¿æ´åè¯­ä¹çº¿ç´¢ãç¨æ·æä»¤åä¼ æå¨èåï¼å®ç°äºä»»å¡æç¥åå¢å¼ºã
*   <strong>æ³åè½åï¼</strong> èªä¸»æ©æ£æ¨¡åéè¿åå°å¯¹éå¯¹æ°æ®çä¾èµï¼æé«äºå¯¹æªè§åç§æ¡ä»¶çæ³åè½ååé²æ£æ§ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¹æåºäºæ©æ£æ¨¡åå¨LLIEä¸­é¢ä¸´çææåå±éæ§ï¼
*   <strong>è®¡ç®å¼éåæ¨çå»¶è¿ï¼</strong> æ©æ£æ¨¡åçè¿­ä»£å»åªè¿ç¨å¯¼è´é«è®¡ç®ææ¬åæ¨çå»¶è¿ï¼ä½¿å¶é¾ä»¥å®æ¶é¨ç½²ã
*   <strong>æ³åæ§åé²æ£æ§ï¼</strong> æ¨¡åå¨é¢å¯¹ä¸è®­ç»æ°æ®æ¾èä¸åçåå¤ï¼OODï¼è¾å¥æ¶ï¼æ§è½å¯è½ä¸éï¼å°¤å¶æ¯å¨æç«¯é»æãéåååç§åç¹å®ä¼ æå¨åªå£°æ¨¡å¼ä¸ã
*   <strong>æ°æ®ä¾èµæ§åç¨ç¼ºæ§ï¼</strong> çç£å¼æ©æ£æ¨¡åä¸¥éä¾èµå¤§è§æ¨¡ãé«è´¨éãå¤æ ·åçéå¯¹è®­ç»æ°æ®ï¼ä½æ­¤ç±»æ°æ®è·åå°é¾ä¸ææ¬é«æã
*   <strong>æç¥è´¨éãä¿çåº¦ä¸æççæè¡¡ï¼</strong> å¨è¿äºç¸äºå²çªçç®æ ä¹é´æ¾å°æä½³å¹³è¡¡ä»ç¶æ¯ä¸ä¸ªææï¼ä¼ ç»ææ ï¼PSNR/SSIMï¼å¯è½æ æ³å®å¨åæ äººç±»æç¥è´¨éã
*   <strong>å¯è§£éæ§ï¼XAIï¼ï¼</strong> æ©æ£æ¨¡åéå¸¸æ¯âé»ç®±âæ¨¡åï¼é¾ä»¥çè§£å¶å³ç­è¿ç¨ï¼è¿é»ç¢äºå¨å®å¨å³é®åºç¨ä¸­çä¿¡ä»»åè°è¯ã
*   <strong>ä¼¦çèéï¼</strong> å­å¨åè§æ¾å¤§ãæ¶æä½¿ç¨ãå¢å¼ºè´¨éå¬å¹³æ§ãç¯å¢å½±åä»¥åâç°å®æ­æ²âçé£é©ï¼å³çæçä¼¼åçä½èåçç»èã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºæå±æäºæªæ¥ç ç©¶çå ä¸ªå³é®æ¹åï¼
*   <strong>å©ç¨åºç¡æ¨¡åï¼</strong> ææå°å°ç°æå¼ºå¤§çå¾åçææ©æ£æ¨¡åï¼å¦Stable Diffusionï¼éåºäºLLIEä»»å¡ï¼åæ¬ä¸é¨çå¾®è°ææ¯åæ°é¢çæç¤ºç­ç¥ã
*   <strong>å¤æ¨¡æåºç¡æ¨¡åï¼</strong> æ¢ç´¢è½å¤å¤çåæ´åææ¬ãé³é¢æå¶ä»ä¼ æå¨ä¿¡æ¯ä¸è§è§æ°æ®çå¤æ¨¡æåºç¡æ¨¡åï¼ä»¥å®ç°æ´å·ä¸ä¸ææç¥åå¯æ§æ§çLLIEã
*   <strong>å®æ¶åè®¾å¤ç«¯LLIEï¼</strong> æç»­ç ç©¶åè¿çå éææ¯ï¼æ¨¡ååç¼©ãè¸é¦ãé«æç½ç»æ¶æï¼åç¡¬ä»¶ååè®¾è®¡ï¼ä»¥å®ç°å®æ¶åè®¾å¤ç«¯é¨ç½²ã
*   <strong>æååçæ çç£ãèªçç£åé¶æ ·æ¬å­¦ä¹ ï¼</strong> å¼åæ´å¤æçæ çç£æ¹æ³ï¼ä»¥æ´å¥½å°å»ºæ¨¡çå®ä¸ççä½åéçº§ï¼å¹¶å®ç°åæ³ååè§£è¦è¡¨ç¤ºå­¦ä¹ ã
*   <strong>å¢å¼ºå¯æ§æ§åå¯è§£éæ§ï¼</strong> å®ç°æ´ç»ç²åº¦çè¯­ä¹æ§å¶ï¼è¶è¶ç®åçææ¬æç¤ºææ©ç ï¼ï¼å¹¶å¼åéå¯¹çææ¨¡åçXAIææ¯ï¼ä»¥æé«ä¿¡ä»»åè°è¯è½åã</p>
<p>æ»èè¨ä¹ï¼è¿ç¯ç»¼è¿°ä¸ºæ©æ£æ¨¡åå¨LLIEé¢åçææ°è¿å±æä¾äºä¸ä¸ªå¨é¢çè·¯çº¿å¾ï¼ä¸ä»ç³»ç»å°åæäºç°ææ¹æ³ï¼è¿æç¡®æåºäºå½åé¢ä¸´çææåæªæ¥ç ç©¶çæºéï¼ç¹å«æ¯éè¿æ··åç­ç¥ãåºç¡æ¨¡ååå¯¹ä¼¦çèéçå³æ³¨æ¥æ¨å¨è¯¥é¢åçåå±ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This survey provides an up-to-date critical analysis of diffusion models for
LLIE, distinctively featuring an in-depth comparative performance evaluation
against Generative Adversarial Network and Transformer-based state-of-the-art
methods, a thorough examination of practical deployment challenges, and a
forward-looking perspective on the role of emerging paradigms like foundation
models.</li>
<li>We propose a multi-perspective taxonomy encompassing six categories:
Intrinsic Decomposition, Spectral &amp; Latent, Accelerated, Guided, Multimodal,
and Autonomous; that map enhancement methods across physical priors,
conditioning schemes, and computational efficiency.</li>
<li>This survey aims to guide the next generation of
diffusion-based LLIE research by highlighting trends and surfacing open
research questions, including novel conditioning, real-time adaptation, and the
potential of foundation models.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.05976v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.05976v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.07310v1'></a></p>
<h2 id="matrix-mask-track-alignment-for-interaction-aware-video-generation"><a href="https://arxiv.org/abs/2510.07310v1">MATRIX: Mask Track Alignment for Interaction-aware Video Generation</a></h2>
<p><strong>Authors:</strong> Siyoon Jin, Seongchan Kim, Dahyun Chung, Jaeho Lee, Hyunwook Choi, Jisu Nam, Jiyoung Kim, Seungryong Kim</p>
<p><strong>Published:</strong> 2025-10-08</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Video DiTs have advanced video generation, yet they still struggle to model
multi-instance or subject-object interactions. This raises a key question: How
do these models internally represent interactions? To answer this, we curate
MATRIX-11K, a video dataset with interaction-aware captions and multi-instance
mask tracks. Using this dataset, we conduct a systematic analysis that
formalizes two perspectives of video DiTs: semantic grounding, via
video-to-text attention, which evaluates whether noun and verb tokens capture
instances and their relations; and semantic propagation, via video-to-video
attention, which assesses whether instance bindings persist across frames. We
find both effects concentrate in a small subset of interaction-dominant layers.
Motivated by this, we introduce MATRIX, a simple and effective regularization
that aligns attention in specific layers of video DiTs with multi-instance mask
tracks from the MATRIX-11K dataset, enhancing both grounding and propagation.
We further propose InterGenEval, an evaluation protocol for interaction-aware
video generation. In experiments, MATRIX improves both interaction fidelity and
semantic alignment while reducing drift and hallucination. Extensive ablations
validate our design choices. Codes and weights will be released.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°å¯¹è¿ç¯è®ºææè¦è¿è¡æ·±å¥åæã</p>
<hr />
<p><strong>è®ºææè¦åæï¼MATRIX: Mask Track Alignment for Interaction-aware Video Generation</strong></p>
<p><strong>1. è®ºæä¸»è¦è´¡ç®çç®æ´æ»ç» (2-3 å¥è¯)</strong></p>
<p>è¿ç¯è®ºæçæ ¸å¿è´¡ç®å¨äºæ·±å¥åæäºè§é¢DiTsï¼Diffusion Transformersï¼åé¨å¦ä½å¤çå¤å®ä¾åä¸»ä½-å®¢ä½äº¤äºï¼å¹¶å¨æ­¤åºç¡ä¸æåºäºä¸ç§åä¸ºMATRIXçæ­£ååæ¹æ³ãMATRIXéè¿å°è§é¢DiTsç¹å®å±ä¸­çæ³¨æåä¸å¤å®ä¾æ©ç è½¨è¿¹å¯¹é½ï¼æ¾èæåäºäº¤äºæç¥è§é¢çæçè´¨éï¼è§£å³äºç°ææ¨¡åå¨å»ºæ¨¡å¤æäº¤äºæ¹é¢çä¸è¶³ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³å­¦æ¹æ³</strong></p>
<p>è¯¥è®ºæçå³é®åæ°åæ¹æ³å­¦æ¹æ³ä½ç°å¨ä»¥ä¸å ä¸ªæ¹é¢ï¼</p>
<ul>
<li><strong>é®é¢è¯æ­ä¸å½¢å¼ååæï¼</strong> è®ºæé¦åè¯å«åºè§é¢DiTså¨å¤çå¤å®ä¾åäº¤äºæ¹é¢çå±éæ§ãä¸ºäºåç­âæ¨¡ååé¨å¦ä½è¡¨ç¤ºäº¤äºâè¿ä¸å³é®é®é¢ï¼ä½èæåºäºä¸¤ç§å½¢å¼åçåæè§è§ï¼<ul>
<li><strong>è¯­ä¹æ¥å° (Semantic Grounding)ï¼</strong> éè¿è§é¢å°ææ¬çæ³¨æåï¼video-to-text attentionï¼æ¥è¯ä¼°åè¯åå¨è¯tokenæ¯å¦è½åç¡®ææå®ä¾åå¶å³ç³»ã</li>
<li><strong>è¯­ä¹ä¼ æ­ (Semantic Propagation)ï¼</strong> éè¿è§é¢å°è§é¢çæ³¨æåï¼video-to-video attentionï¼æ¥è¯ä¼°å®ä¾ç»å®ï¼instance bindingsï¼å¨å¸§é´æ¯å¦è½æç»­ä¿æã</li>
</ul>
</li>
<li><strong>æ°æ®éåæ°ï¼</strong> ä½èæå»ºäº <strong>MATRIX-11K</strong> æ°æ®éï¼è¿æ¯ä¸ä¸ªåå«äº¤äºæç¥å­å¹åå¤å®ä¾æ©ç è½¨è¿¹çè§é¢æ°æ®éãè¿ä¸ªæ°æ®éå¯¹äºè¿è¡ä¸è¿°å½¢å¼ååæååç»­çæ­£ååæ¹æ³è³å³éè¦ã</li>
<li><strong>åç°äº¤äºä¸»å¯¼å±ï¼</strong> éè¿å¯¹MATRIX-11Kæ°æ®éçåæï¼è®ºæåç°è¯­ä¹æ¥å°åè¯­ä¹ä¼ æ­è¿ä¸¤ç§æåºé½éä¸­å¨è§é¢DiTsä¸­ä¸å°é¨åâäº¤äºä¸»å¯¼å±âï¼interaction-dominant layersï¼ä¸­ãè¿ä¸åç°ä¸ºåç»­çæ­£ååæ¹æ³æä¾äºå³é®çæå¯¼ã</li>
<li><strong>MATRIXæ­£ååæ¹æ³ï¼</strong> åºäºä¸è¿°åç°ï¼è®ºææåºäº <strong>MATRIX</strong>ï¼ä¸ç§ç®åèææçæ­£ååæ¹æ³ãå®éè¿å°è§é¢DiTsç¹å®å±ï¼å³äº¤äºä¸»å¯¼å±ï¼ä¸­çæ³¨æåä¸MATRIX-11Kæ°æ®éä¸­çå¤å®ä¾æ©ç è½¨è¿¹è¿è¡å¯¹é½ãè¿ç§å¯¹é½æ¨å¨å¢å¼ºæ¨¡åçè¯­ä¹æ¥å°åä¼ æ­è½åã</li>
<li><strong>è¯ä¼°åè®®åæ°ï¼</strong> è®ºæè¿æåºäº <strong>InterGenEval</strong>ï¼ä¸ä¸ªä¸é¨ç¨äºè¯ä¼°äº¤äºæç¥è§é¢çæçåè®®ï¼è¿å¯¹äºè¯¥é¢åæªæ¥çç ç©¶å·æéè¦æä¹ã</li>
</ul>
<p><strong>3. å¯¹é¢åæ½å¨å½±å</strong></p>
<ul>
<li><strong>æ¨å¨äº¤äºæç¥è§é¢çæï¼</strong> MATRIXæ¹æ³ç´æ¥è§£å³äºå½åè§é¢çææ¨¡åå¨å¤çå¤æäº¤äºæ¹é¢ççç¹ï¼æææ¾èæåçæè§é¢ççå®æåè¯­ä¹ä¸è´æ§ï¼ç¹å«æ¯å¨æ¶åå¤ä¸ªä¸»ä½åå¤æå¨ä½çåºæ¯ä¸­ã</li>
<li><strong>æ·±å¥çè§£DiTsåé¨æºå¶ï¼</strong> å¯¹è¯­ä¹æ¥å°åè¯­ä¹ä¼ æ­çç³»ç»åæï¼ä»¥ååç°âäº¤äºä¸»å¯¼å±âçæ´å¯ï¼ä¸ºçè§£è§é¢DiTså¦ä½å¤çæ¶ç©ºä¿¡æ¯åè¯­ä¹å³ç³»æä¾äºå®è´µçè§è§£ï¼å¯è½å¯åæªæ¥æ¨¡åæ¶æçè®¾è®¡ã</li>
<li><strong>æ ååè¯ä¼°ï¼</strong> InterGenEvalåè®®çæåºï¼ä¸ºäº¤äºæç¥è§é¢çææä¾äºä¸ä¸ªç»ä¸ãå®¢è§çè¯ä¼°æ åï¼æå©äºæ¨å¨è¯¥é¢åçå¬å¹³æ¯è¾åå¿«éåå±ã</li>
<li><strong>æ°æ®éè´¡ç®ï¼</strong> MATRIX-11Kæ°æ®éçåå¸ï¼å°ä¸ºç ç©¶äººåæä¾ä¸ä¸ªé«è´¨éçèµæºï¼ç¨äºè®­ç»åè¯ä¼°æ´åè¿çäº¤äºæç¥è§é¢çææ¨¡åã</li>
</ul>
<p><strong>4. ç¸å³é¢åæåºç¨åç</strong></p>
<ul>
<li><strong>è§é¢åå®¹åä½ï¼</strong> çµå½±å¶ä½ãå¨ç»ãæ¸¸æå¼åç­é¢åå¯ä»¥å©ç¨æ´å¼ºå¤§çäº¤äºæç¥è§é¢çæææ¯ï¼èªå¨çææ´å¤æãæ´é¼ççåºæ¯åè§è²äº¤äºã</li>
<li><strong>èæç°å®/å¢å¼ºç°å® (VR/AR)ï¼</strong> çæå·æé«åº¦äº¤äºæ§çèæç¯å¢ï¼æåç¨æ·æ²æµ¸æã</li>
<li><strong>æºå¨äººå­¦åå·èº«æºè½ï¼</strong> å¸®å©æºå¨äººçè§£åé¢æµå¤æçäººæºæç©ç©äº¤äºï¼çææ´åççè¡ä¸ºåºåã</li>
<li><strong>è§é¢ç¼è¾ååæï¼</strong> èªå¨å¡«åæä¿®æ¹è§é¢ä¸­çäº¤äºï¼ç®ååæå¶ä½æµç¨ã</li>
<li><strong>å¤æ¨¡æå­¦ä¹ ï¼</strong> ä¿è¿è§é¢ãææ¬åæ©ç è½¨è¿¹ç­å¤æ¨¡ææ°æ®ä¹é´çæ´æ·±å±æ¬¡èååçè§£ã</li>
<li><strong>å¯æ§è§é¢çæï¼</strong> åè®¸ç¨æ·éè¿ææ¬ææ©ç è½¨è¿¹æ´ç²¾ç»å°æ§å¶çæè§é¢ä¸­çäº¤äºè¡ä¸ºã</li>
</ul>
<p><strong>5. ä»æè¦ä¸­å¯æ¨æ­åºçå±éæ§</strong></p>
<ul>
<li><strong>æ°æ®éè§æ¨¡ï¼</strong> æè¦ä¸­æå°âMATRIX-11Kâï¼è½ç¶11Kå¯¹äºè§é¢æ°æ®éæ¥è¯´ä¸ç®å°ï¼ä½ä¸ä¸äºå¤§è§æ¨¡å¾åæææ¬æ°æ®éç¸æ¯ï¼å¶è§æ¨¡å¯è½ä»æå±éæ§ï¼è¿å¯è½ä¼å½±åæ¨¡åå¨æ´å¹¿æ³ãæ´å¤æ ·ååºæ¯ä¸çæ³åè½åã</li>
<li><strong>âç®åèææâçå«ä¹ï¼</strong> æè¦ç§°MATRIXæ¯ä¸ç§âç®åèææâçæ­£ååæ¹æ³ãè½ç¶âç®åâéå¸¸æ¯ä¼ç¹ï¼ä½ææ¶ä¹å¯è½æå³çå¶çè®ºæ·±åº¦æå¤ææ§ä¸å¦æäºæ´å¤æçæ¶æåæ°ãå·ä½å®ç°ç»èï¼ä¾å¦ï¼å¦ä½ç²¾ç¡®å¯¹é½æ³¨æåï¼æå¤±å½æ°è®¾è®¡ï¼éè¦æ¥éæ­£ææè½äºè§£å¶æ½å¨çå¤ææ§ã</li>
<li><strong>è®¡ç®ææ¬ï¼</strong> è§é¢DiTsæ¬èº«éå¸¸è®¡ç®ææ¬è¾é«ï¼èå¼å¥å¤å®ä¾æ©ç è½¨è¿¹çå¯¹é½æ­£ååï¼å¯è½ä¼è¿ä¸æ­¥å¢å è®­ç»çè®¡ç®èµæºéæ±åæ¶é´ãæè¦ä¸­æªæåè¿æ¹é¢çèéã</li>
<li><strong>äº¤äºå¤ææ§ï¼</strong> æè¦ä¸­å¼ºè°âå¤å®ä¾æä¸»ä½-å®¢ä½äº¤äºâãè½ç¶è¿æ¯ä¸ä¸ªéè¦çè¿æ­¥ï¼ä½äº¤äºçç±»ååå¤æç¨åº¦æ¯æ éçãMATRIXæ¯å¦è½å¤çææç±»åçå¤æäº¤äºï¼ä¾å¦ï¼éå¼äº¤äºãé¿æ¶ç¨äº¤äºãå¤æ¹äº¤äºç­ï¼ä»éè¿ä¸æ­¥éªè¯ã</li>
<li><strong>æ³åå°å¶ä»DiTæ¶æï¼</strong> æè¦ä¸­æå°âvideo DiTsâï¼ä½æ²¡ææç¡®è¯´æMATRIXæ¯å¦è½æ ç¼å°åºç¨äºææç°æçææªæ¥çDiTæ¶æï¼æèæ¯å¦éè¦éå¯¹ç¹å®æ¶æè¿è¡è°æ´ã</li>
<li><strong>âåå°æ¼ç§»åå¹»è§âçå·ä½ç¨åº¦ï¼</strong> æè¦æå°MATRIXâåå°æ¼ç§»åå¹»è§âï¼ä½æ²¡æéåè¿äºæ¹è¿çç¨åº¦ãè¿éè¦éè¿å®éªç»ææ¥å·ä½è¯ä¼°ã</li>
</ul>
<hr />
<p>æ»çæ¥è¯´ï¼è¿ç¯è®ºæå¨è§é¢çæé¢åè¿åºäºéè¦ä¸æ­¥ï¼éè¿å¯¹æ¨¡ååé¨æºå¶çæ·±å¥åæååæ°çæ­£ååæ¹æ³ï¼æ¾èæåäºäº¤äºæç¥è§é¢çæçè´¨éãå¶æåºçæ°æ®éåè¯ä¼°åè®®ä¹ä¸ºè¯¥é¢åçæªæ¥ç ç©¶å¥ å®äºåºç¡ï¼å·æéè¦ççè®ºåå®è·µæä¹ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Motivated by this, we introduce MATRIX, a simple and effective regularization
that aligns attention in specific layers of video DiTs with multi-instance mask
tracks from the MATRIX-11K dataset, enhancing both grounding and propagation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.07310v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.07310v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.07217v1'></a></p>
<h2 id="genpilot-a-multi-agent-system-for-test-time-prompt-optimization-in-image-generation"><a href="https://arxiv.org/abs/2510.07217v1">GenPilot: A Multi-Agent System for Test-Time Prompt Optimization in Image Generation</a></h2>
<p><strong>Authors:</strong> Wen Ye, Zhaocheng Liu, Yuwei Gui, Tingyu Yuan, Yunyue Su, Bowen Fang, Chaoyang Zhao, Qiang Liu, Liang Wang</p>
<p><strong>Published:</strong> 2025-10-08</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Text-to-image synthesis has made remarkable progress, yet accurately
interpreting complex and lengthy prompts remains challenging, often resulting
in semantic inconsistencies and missing details. Existing solutions, such as
fine-tuning, are model-specific and require training, while prior automatic
prompt optimization (APO) approaches typically lack systematic error analysis
and refinement strategies, resulting in limited reliability and effectiveness.
Meanwhile, test-time scaling methods operate on fixed prompts and on noise or
sample numbers, limiting their interpretability and adaptability. To solve
these, we introduce a flexible and efficient test-time prompt optimization
strategy that operates directly on the input text. We propose a plug-and-play
multi-agent system called GenPilot, integrating error analysis,
clustering-based adaptive exploration, fine-grained verification, and a memory
module for iterative optimization. Our approach is model-agnostic,
interpretable, and well-suited for handling long and complex prompts.
Simultaneously, we summarize the common patterns of errors and the refinement
strategy, offering more experience and encouraging further exploration.
Experiments on DPG-bench and Geneval with improvements of up to 16.9% and 5.7%
demonstrate the strong capability of our methods in enhancing the text and
image consistency and structural coherence of generated images, revealing the
effectiveness of our test-time prompt optimization strategy. The code is
available at https://github.com/27yw/GenPilot.</p>
<p><strong>Analysis:</strong></p>
<p>ä»¥ä¸æ¯Wen Yeç­äººæ°åçè®ºæâGenPilot: A Multi-Agent System for Test-Time Prompt Optimization in Image Generationâçæè¦ï¼</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢</strong>
å°½ç®¡ææ¬å°å¾åï¼T2Iï¼åæææ¯åå¾äºæ¾èè¿å±ï¼ä½ç°ææ¨¡åå¨åç¡®è§£éå¤æååé¿çæç¤ºæ¶ä»é¢ä¸´ææãè¿å¸¸å¸¸å¯¼è´çæçå¾åå­å¨è¯­ä¹ä¸ä¸è´åç»èç¼ºå¤±ãä¼ ç»çè§£å³æ¹æ¡ï¼å¦å¾®è°ï¼éå¸¸æ¯æ¨¡åç¹å®çä¸éè¦è®­ç»ï¼èç°æçèªå¨æç¤ºä¼åï¼APOï¼æ¹æ³ç¼ºä¹ç³»ç»çéè¯¯åæåç»åç­ç¥ï¼å¯¼è´å¯é æ§åæææ§æéãæ­¤å¤ï¼æµè¯æ¶ç¼©æ¾æ¹æ³éå¸¸å¨åºå®æç¤ºæåªå£°/æ ·æ¬æ°éä¸æä½ï¼éå¶äºå¶å¯è§£éæ§åéåºæ§ãè¯¥ç ç©¶æ¨å¨è§£å³è¿äºé®é¢ï¼å¼åä¸ç§çµæ´»é«æçæµè¯æ¶æç¤ºä¼åç­ç¥ï¼ç´æ¥ä½ç¨äºè¾å¥ææ¬ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®</strong>
æ¬ææåºäºGenPilotï¼ä¸ä¸ªå³æå³ç¨çå¤æºè½ä½ç³»ç»ï¼ç¨äºæµè¯æ¶æç¤ºä¼åãå¶æ ¸å¿åæ°åæ¬ï¼
*   <strong>å¤æºè½ä½ç³»ç»æ¶æï¼</strong> GenPilotå°æç¤ºä¼åè§ä¸ºä¸ä¸ªæç´¢é®é¢ï¼éè¿è¿­ä»£ä¼åè¿ç¨å®ç°å¨æåå¯è§£éçæç¤ºç»åãè¯¥ç³»ç»æ¯æ¨¡åæ å³çï¼æ éé¢å¤è®­ç»å³å¯åºç¨äºåç§T2Iæ¨¡åã
*   <strong>éè¯¯åæä¸æ å°ï¼</strong> ç³»ç»é¦åå°åå§æç¤ºåè§£ä¸ºâåå¥å­âï¼å¹¶å©ç¨è§è§é®ç­ï¼VQAï¼åå¾åæè¿°ï¼captioningï¼è¿è¡å¹¶è¡éè¯¯æ£æµãä¸ä¸ªéè¯¯æ´åæºè½ä½å°ä¸ä¸è´æ§æ±æ»ä¸ºéè¯¯åè¡¨ï¼å¹¶æ å°åç¹å®çæç¤ºçæ®µã
*   <strong>æµè¯æ¶æç¤ºä¼åï¼</strong> åå«ä¸ä¸ªç»åæºè½ä½ï¼æ ¹æ®åå§æç¤ºãå¾åãéè¯¯åæåæ å°ç­åæ°æ®çæåéæç¤ºãè¿äºåéæç¤ºéè¿å¤æ¨¡æå¤§è¯­è¨æ¨¡åï¼MLLMï¼è¯åå¨åè¯åç­ç¥è¿è¡è¯ä¼°ã
*   <strong>èç±»ä¸è®°å¿æ¨¡åï¼</strong> GenPilotå¯¹åéæç¤ºè¿è¡èç±»ï¼å¹¶éæ©æä¼èç±»è¿è¡éæ ·åå¾åçæãè®°å¿æ¨¡åéè¿è§è§åææ¬åé¦è¿è¡è¿­ä»£æ´æ°ï¼ä»¥æ¯ææç»­ä¼åã
*   <strong>éè¯¯æ¨¡å¼ä¸ç»åç­ç¥æ»ç»ï¼</strong> è®ºææ»ç»äºå¸¸è§çéè¯¯æ¨¡å¼åå¶å¯¹åºçç»åç­ç¥ï¼ä¸ºæªæ¥çç ç©¶æä¾äºå®è·µèµæºã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹</strong>
GenPilotå¨DPG-benchåGenEvalæ°æ®éä¸çå®éªç»æè¡¨æï¼å¶æ¹æ³å¨æé«ææ¬ä¸å¾åä¸è´æ§åçæå¾åçç»æè¿è´¯æ§æ¹é¢å·ææ¾èä¼å¿ãå¨DPG-benchä¸ï¼æ§è½æåé«è¾¾16.9%ï¼å¨GenEvalä¸æåé«è¾¾5.7%ãè¿äºç»æè¯æäºGenPilotå¨å¤çå¤æååé¿æç¤ºæ¹é¢çå¼ºå¤§è½åï¼è½å¤æææé¤ä¸å¿è¦çç©ä½ï¼å¹¶åç¡®å¤çå±æ§ç»å®ãå¤ææå¾ãç©ºé´æ¨çåéç°å®æè¿°ç­æææ§ä»»å¡ãGenPilotå¨ä¸åæ¨¡åä¸åè¡¨ç°åºä¸è´çæ§è½æåï¼åæ¬DALL-E 3ãFLUX.1 schnellãSana-1.0 1.6BåStable Diffusionç³»åï¼çªæ¾äºå¶é²æ£æ§åæ³åè½åã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§</strong>
å°½ç®¡GenPilotå¨å¤ç§åºæ¯ä¸è¡¨ç°åºè²ï¼ä½ä»å­å¨ä¸äºææï¼
*   <strong>è®¡ç®æ¶é´ï¼</strong> å°½ç®¡è¯¥æ¡æ¶é¿åäºæ¨¡åå¾®è°ï¼ä½å¨æ¨çè¿ç¨ä¸­å¼å¥äºé¢å¤çè®¡ç®æ¶é´ï¼è¿å¯¹äºå»¶è¿ææçåºç¨å¯è½æ¯ä¸ä¸ªéå°é®é¢ã
*   <strong>MLLMæ§è½ä¾èµï¼</strong> GenPilotçæ§è½åå¶æä½¿ç¨çå¤æ¨¡æå¤§è¯­è¨æ¨¡åï¼MLLMï¼æºè½ä½è½åçå½±åãå¦æç¨æ·ä½¿ç¨è½åè¾å¼±çMLLMï¼å¯è½å¯¼è´æ§è½ä¸éã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å</strong>
è®ºæé¼å±è¿ä¸æ­¥æ¢ç´¢éè¯¯æ¨¡å¼åç»åç­ç¥ï¼å¹¶æä¾äº35ç§å¸¸è§éè¯¯æ¨¡å¼åå¶å¯¹åºçç»åç­ç¥ä½ä¸ºå®è·µèµæºãæªæ¥çç ç©¶å¯ä»¥è¿ä¸æ­¥ä¼åè®¡ç®æçï¼åå°æ¨çæ¶é´ï¼å¹¶æ¢ç´¢å¦ä½ä½¿GenPilotå¨ä¸åMLLMè½åæ°´å¹³ä¸ä¿æé«æ§è½ãæ­¤å¤ï¼å¯¹æç¤ºå¯æ§æ§åä¼åç­ç¥çæ·±å¥ç ç©¶ä¹æ¯ä¸ä¸ªæåæ¯çæ¹åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To solve
these, we introduce a flexible and efficient test-time prompt optimization
strategy that operates directly on the input text.</li>
<li>We propose a plug-and-play
multi-agent system called GenPilot, integrating error analysis,
clustering-based adaptive exploration, fine-grained verification, and a memory
module for iterative optimization.</li>
<li>Our approach is model-agnostic,
interpretable, and well-suited for handling long and complex prompts.</li>
<li>Experiments on DPG-bench and Geneval with improvements of up to 16.9% and 5.7%
demonstrate the strong capability of our methods in enhancing the text and
image consistency and structural coherence of generated images, revealing the
effectiveness of our test-time prompt optimization strategy.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.07217v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.07217v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.07181v1'></a></p>
<h2 id="tiger-tool-integrated-geometric-reasoning-in-vision-language-models-for-robotics"><a href="https://arxiv.org/abs/2510.07181v1">TIGeR: Tool-Integrated Geometric Reasoning in Vision-Language Models for Robotics</a></h2>
<p><strong>Authors:</strong> Yi Han, Cheng Chi, Enshen Zhou, Shanyu Rong, Jingkun An, Pengwei Wang, Zhongyuan Wang, Lu Sheng, Shanghang Zhang</p>
<p><strong>Published:</strong> 2025-10-08</p>
<p><strong>Categories:</strong> cs.RO, cs.AI, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Vision-Language Models (VLMs) have shown remarkable capabilities in spatial
reasoning, yet they remain fundamentally limited to qualitative precision and
lack the computational precision required for real-world robotics. Current
approaches fail to leverage metric cues from depth sensors and camera
calibration, instead reducing geometric problems to pattern recognition tasks
that cannot deliver the centimeter-level accuracy essential for robotic
manipulation. We present TIGeR (Tool-Integrated Geometric Reasoning), a novel
framework that transforms VLMs from perceptual estimators to geometric
computers by enabling them to generate and execute precise geometric
computations through external tools. Rather than attempting to internalize
complex geometric operations within neural networks, TIGeR empowers models to
recognize geometric reasoning requirements, synthesize appropriate
computational code, and invoke specialized libraries for exact calculations. To
support this paradigm, we introduce TIGeR-300K, a comprehensive
tool-invocation-oriented dataset covering point transformations, pose
estimation, trajectory generation, and spatial compatibility verification,
complete with tool invocation sequences and intermediate computations. Through
a two-stage training pipeline combining supervised fine-tuning (SFT) and
reinforcement fine-tuning (RFT) with our proposed hierarchical reward design,
TIGeR achieves SOTA performance on geometric reasoning benchmarks while
demonstrating centimeter-level precision in real-world robotic manipulation
tasks.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºâTIGeR: Tool-Integrated Geometric Reasoning in Vision-Language Models for Roboticsâè®ºæçå¨é¢æè¦ï¼</p>
<p><strong>è®ºææè¦ï¼TIGeRï¼æºå¨äººé¢åä¸­å·¥å·éæå ä½æ¨ççè§è§-è¯­è¨æ¨¡å</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
å°½ç®¡è§è§-è¯­è¨æ¨¡åï¼VLMsï¼å¨ç©ºé´æ¨çæ¹é¢è¡¨ç°åºæ¾èè½åï¼ä½å®ä»¬å¨å®æ§ç²¾åº¦ä¸ä»å­å¨æ ¹æ¬æ§éå¶ï¼å¹¶ä¸ç¼ºä¹ç°å®ä¸çæºå¨äººæéçè®¡ç®ç²¾åº¦ãç°ææ¹æ³æªè½ååå©ç¨æ·±åº¦ä¼ æå¨åç¸æºæ ¡åæä¾çåº¦éçº¿ç´¢ï¼èæ¯å°å ä½é®é¢ç®åä¸ºæ¨¡å¼è¯å«ä»»å¡ï¼æ æ³è¾¾å°æºå¨äººæä½æéçåç±³çº§ç²¾åº¦ãè¿é»ç¢äºå·èº«VLMæ¯æç²¾ç¡®çæºå¨äººæä½åè§åã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥è®ºææåºäºTIGeRï¼å·¥å·éæå ä½æ¨çï¼æ¡æ¶ï¼æ¨å¨å°VLMä»æç¥ä¼°è®¡å¨è½¬åä¸ºå ä½è®¡ç®å¨ãå¶æ ¸å¿åæ°å¨äºï¼
*   <strong>å¤é¨å·¥å·éæï¼</strong> TIGeRä½¿æ¨¡åè½å¤è¯å«å ä½æ¨çéæ±ï¼çæéå½çè®¡ç®ä»£ç ï¼å¹¶è°ç¨å¤é¨å·¥å·åºè¿è¡ç²¾ç¡®è®¡ç®ï¼èä¸æ¯è¯å¾å¨ç¥ç»ç½ç»åé¨å®ç°å¤æçå ä½è¿ç®ãè¿éè¿ä¸æ·±åº¦ä¼ æå¨ãç¸æºåååå ä½åºçéæï¼å®ç°äºè¶è¶è¿ä¼¼çç²¾ç¡®è®¡ç®ã
*   <strong>TIGeR-300Kæ°æ®éï¼</strong> ä¸ºæ¯æè¿ä¸èå¼ï¼ä½èæå»ºäºä¸ä¸ªåå«30ä¸ä¸ªæ ·æ¬çç»¼åæ§ãé¢åå·¥å·è°ç¨çæ°æ®éãè¯¥æ°æ®éæ¶µçç¹åæ¢ãå§¿æä¼°è®¡ãè½¨è¿¹çæåç©ºé´å¼å®¹æ§éªè¯ï¼å¹¶åå«å®æ´çå·¥å·è°ç¨åºååä¸­é´è®¡ç®ã
*   <strong>ä¸¤é¶æ®µè®­ç»æµç¨ï¼</strong> éç¨çç£å¾®è°ï¼SFTï¼åå¼ºåå¾®è°ï¼RFTï¼ç¸ç»åçä¸¤é¶æ®µè®­ç»æµç¨ãSFTç¨äºçè¾å·¥å·ä½¿ç¨è½åï¼RFTåéè¿æåºçåå±å¥å±è®¾è®¡æ¥æé«ç²¾åº¦åä»»å¡å®æåº¦ãåå±å¥å±åæ¬æ ¼å¼å¥å±ãå·¥å·è°ç¨å¥å±ãåæ°åå®¹å¥å±ãä»£ç çæå¥å±åç­æ¡å¥å±ï¼ä¸ºå·¥å·éææ¨çæä¾äºç»ç²åº¦ççç£ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æåè¿çæ§è½ï¼</strong> TIGeRå¨å ä½æ¨çåºåæµè¯ä¸­åå¾äºæåè¿çæ§è½ã
*   <strong>åç±³çº§ç²¾åº¦ï¼</strong> å¨ç°å®ä¸çæºå¨äººæä½ä»»å¡ä¸­ï¼TIGeRå±ç¤ºäºåç±³çº§çç²¾åº¦ï¼è§£å³äºç°æVLMå¨å®éç©ºé´æ¨çæ¹é¢çå±éæ§ã
*   <strong>åç¡®çç©ºé´å®ä½ãç»ä¸çè·¨è§è§æ¨çåå¯è§£éæ§/éåºæ§ï¼</strong> TIGeRéè¿å©ç¨ç°æ3Dåºç¡æ¨¡åå®ç°ç²¾ç¡®ç3Dç©ºé´å®ä½ï¼éè¿3Då ä½éå»ºæ¨¡åå®ç°è·¨è§è§çç»ä¸æ°å¼æ¨çï¼å¹¶éè¿æ¾å¼è°ç¨å·¥å·åæ´é²ä¸­é´æ­¥éª¤æä¾é«å¯è§£éæ§åéåºæ§ã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
è®ºæä¸­æ²¡ææç¡®æåTIGeRæ¡æ¶çå±éæ§ãç¶èï¼ä»å¶æ¹æ³è®ºåèæ¯æ¥çï¼æ½å¨çå±éæ§å¯è½åæ¬ï¼
*   <strong>å·¥å·åºçå®å¤æ§ï¼</strong> TIGeRçæ§è½ä¾èµäºå¤é¨å·¥å·åºçè´¨éåè¦çèå´ãå¦ææäºå ä½è®¡ç®æ²¡æé«è´¨éçå·¥å·ï¼æèå·¥å·çéæä¸å¤å®åï¼å¯è½ä¼å½±åå¶æ§è½ã
*   <strong>è®¡ç®å¼éï¼</strong> çæä»£ç åè°ç¨å¤é¨å·¥å·å¯è½æ¯çº¯ç²¹çç¥ç»ç½ç»æ¨çå¸¦æ¥æ´é«çè®¡ç®å¼éåå»¶è¿ã
*   <strong>æ³åè½åï¼</strong> å°½ç®¡æ°æ®éè®¾è®¡æ¨å¨ä¿è¿æ³åï¼ä½æ¨¡åå¨é¢å¯¹ä¸è®­ç»æ°æ®æ¾èä¸åçå¨æ°ãå¤æå ä½ä»»å¡æ¶ï¼å¶æ³åè½åä»éè¿ä¸æ­¥éªè¯ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºæä¸­æ²¡ææç¡®ååºæªæ¥ç ç©¶æ¹åï¼ä½æ ¹æ®å¶è´¡ç®åæ½å¨å±éæ§ï¼å¯ä»¥æ¨æ­åºä»¥ä¸æ¹åï¼
*   <strong>æ©å±å·¥å·åºï¼</strong> è¿ä¸æ­¥æ©å±åä¼åTIGeRçå·¥å·åºï¼ä»¥æ¶µçæ´å¹¿æ³çå ä½è®¡ç®åæ´å¤æçæºå¨äººæä½ä»»å¡ã
*   <strong>æé«æçåå®æ¶æ§ï¼</strong> ä¼åä»£ç çæåå·¥å·æ§è¡çæçï¼ä»¥æ»¡è¶³å®æ¶æºå¨äººæä½çéæ±ã
*   <strong>æ´å¼ºçæ³åè½åï¼</strong> æ¢ç´¢å¦ä½è¿ä¸æ­¥æé«TIGeRå¨é¢å¯¹æªç¥æé«åº¦å¤æå ä½åºæ¯æ¶çæ³åè½åï¼ä¾å¦éè¿æ´åè¿çå°æ ·æ¬å­¦ä¹ æé¶æ ·æ¬å­¦ä¹ æ¹æ³ã
*   <strong>å¤æ¨¡æèåï¼</strong> æ¢ç´¢æ´æ·±å±æ¬¡çå¤æ¨¡æä¿¡æ¯èåï¼ä¾å¦å°è§¦è§æåè§ä¼ æå¨æ°æ®éæå°å ä½æ¨çä¸­ï¼ä»¥æ¯ææ´ç²¾ç»çæºå¨äººäº¤äºã
*   <strong>èªä¸»å­¦ä¹ åéåºï¼</strong> ç ç©¶å¦ä½ä½¿TIGeRè½å¤èªä¸»å­¦ä¹ æ°çå ä½æ¦å¿µåå·¥å·ä½¿ç¨ç­ç¥ï¼ä»¥éåºä¸æ­ååçæºå¨äººç¯å¢åä»»å¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present TIGeR (Tool-Integrated Geometric Reasoning), a novel
framework that transforms VLMs from perceptual estimators to geometric
computers by enabling them to generate and execute precise geometric
computations through external tools.</li>
<li>To
support this paradigm, we introduce TIGeR-300K, a comprehensive
tool-invocation-oriented dataset covering point transformations, pose
estimation, trajectory generation, and spatial compatibility verification,
complete with tool invocation sequences and intermediate computations.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.07181v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.07181v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.07143v1'></a></p>
<h2 id="are-we-using-the-right-benchmark-an-evaluation-framework-for-visual-token-compression-methods"><a href="https://arxiv.org/abs/2510.07143v1">Are We Using the Right Benchmark: An Evaluation Framework for Visual Token Compression Methods</a></h2>
<p><strong>Authors:</strong> Chenfei Liao, Wensong Wang, Zichen Wen, Xu Zheng, Yiyu Wang, Haocong He, Yuanhuiyi Lyu, Lutao Jiang, Xin Zou, Yuqian Fu, Bin Ren, Linfeng Zhang, Xuming Hu</p>
<p><strong>Published:</strong> 2025-10-08</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent endeavors to accelerate inference in Multimodal Large Language Models
(MLLMs) have primarily focused on visual token compression. The effectiveness
of these methods is typically assessed by measuring the accuracy drop on
established benchmarks, comparing model performance before and after
compression. However, these benchmarks are originally designed to assess the
perception and reasoning capabilities of MLLMs, rather than to evaluate
compression techniques. As a result, directly applying them to visual token
compression introduces a task mismatch. Strikingly, our investigation reveals
that simple image downsampling consistently outperforms many advanced
compression methods across multiple widely used benchmarks. Through extensive
experiments, we make the following observations: (i) Current benchmarks are
noisy for the visual token compression task. (ii) Down-sampling is able to
serve as a data filter to evaluate the difficulty of samples in the visual
token compression task. Motivated by these findings, we introduce VTC-Bench, an
evaluation framework that incorporates a data filtering mechanism to denoise
existing benchmarks, thereby enabling fairer and more accurate assessment of
visual token compression methods. All data and code are available at
https://github.com/Chenfei-Liao/VTC-Bench.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Chenfei Liaoç­äººæ°åçè®ºæâAre We Using the Right Benchmark: An Evaluation Framework for Visual Token Compression Methodsâçå¨é¢æè¦ã</p>
<hr />
<h3 id="are-we-using-the-right-benchmark-an-evaluation-framework-for-visual-token-compression-methods_1">è®ºææè¦ï¼Are We Using the Right Benchmark: An Evaluation Framework for Visual Token Compression Methods</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³å¤æ¨¡æå¤§åè¯­è¨æ¨¡åï¼MLLMsï¼ä¸­è§è§tokenåç¼©æ¹æ³è¯ä¼°çç°æé®é¢ãå½åï¼è¯ä¼°è¿äºæ¹æ³æææ§çåºåæµè¯ä¸»è¦å³æ³¨MLLMsçæç¥åæ¨çè½åï¼èéä¸é¨è®¾è®¡ç¨äºè¯ä¼°åç¼©ææ¯ãè¿ç§ä»»å¡ä¸å¹éå¯¼è´äºä¸ä¸ªä»¤äººæè®¶çç°è±¡ï¼ç®åçå¾åä¸éæ ·æ¹æ³å¨å¤ä¸ªå¹¿æ³ä½¿ç¨çåºåæµè¯ä¸­ï¼å¶æ§è½å§ç»ä¼äºè®¸å¤åè¿çåç¼©æ¹æ³ãè¿å¼åäºå¯¹ç°æè¯ä¼°æ¡æ¶æ¯å¦è½å¬å¹³åç¡®å°è¡¡éè§è§tokenåç¼©æ¹æ³çå®æ§è½çè´¨çã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ä¸ºäºè§£å³ç°æåºåæµè¯ä¸­å­å¨çâç®åæ§åå·®âååªå£°é®é¢ï¼è®ºææåºäºä¸ä¸ªåä¸º <strong>VTC-Bench</strong> çæ°åè¯ä¼°æ¡æ¶ãVTC-Benchçæ ¸å¿åæ°å¨äºå¼å¥äºä¸ä¸ª<strong>æ°æ®è¿æ»¤æºå¶</strong>ï¼å©ç¨ä¸éæ ·ä½ä¸ºå¤å«å¨æ¥åºååºåæµè¯æ ·æ¬çé¾åº¦ãå·ä½æ­¥éª¤åæ¬ï¼
*   <strong>æ¨çä¸åç¼©ï¼</strong> å¯¹ç»å®æ ·æ¬åç®æ åç¼©æ¯ï¼åæ¶è¿è¡ä¸éæ ·åºçº¿ï¼ä½ä¸ºè¿æ»¤å¨ï¼ååè¿çè§è§tokenåç¼©æ¹æ³è¿è¡æ¨çã
*   <strong>åç»ï¼</strong> é¦ååé¤åå§Qwen2-VLæ¨¡åæ æ³æ­£ç¡®åç­çæ ·æ¬ãç¶åï¼æ ¹æ®ä¸éæ ·æ¹æ³çæ§è½å°å©ä½æ ·æ¬åä¸ºä¸¤ç»ï¼Group Aï¼âå°é¾æ ·æ¬âï¼ä¸éæ ·æ¹æ³æ æ³æ­£ç¡®åç­ï¼åGroup Bï¼âç®åæ ·æ¬âï¼ä¸éæ ·æ¹æ³å¯ä»¥æ­£ç¡®åç­ï¼ã
*   <strong>ç»æèåï¼</strong> å¯¹âå°é¾æ ·æ¬âç»ä¸­çåç¼©æ¹æ³æ§è½è¿è¡ç»è®¡åæï¼ä»¥è·å¾æ´å¬å¹³ãåç¡®çè¯ä¼°ææ ã</p>
<p>éè¿è¿ç§æ¹å¼ï¼VTC-Benchè½å¤ä»ç°æåºåæµè¯ä¸­è¯å«åºçæ­£å·ææææ§çãä¸è§è§tokenåç¼©ä»»å¡ç¸å³çæ ·æ¬ï¼ä»èå»åªå¹¶æä¾æ´ææçè¯ä¼°ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
è®ºæéè¿å¹¿æ³å®éªå¾åºäºä»¥ä¸å³é®åç°ï¼
*   <strong>ç°æåºåæµè¯å¯¹è§è§tokenåç¼©ä»»å¡å­å¨åªå£°ï¼</strong> ç®åçå¾åä¸éæ ·æ¹æ³å¨è®¸å¤ç°æåºåæµè¯ä¸­è¡¨ç°ä¼å¼ï¼è¿è¡¨æè¿äºåºåæµè¯åå«å¤§éå¯ä»¥éè¿ä½åè¾¨çå¨å±ä¿¡æ¯è§£å³çâç®åâæ ·æ¬ï¼æªè½åååæ è§è§tokenåç¼©ççå®ææã
*   <strong>ä¸éæ ·å¯ä½ä¸ºæ°æ®è¿æ»¤å¨ï¼</strong> ä¸éæ ·æ¹æ³è½å¤ææåºåæ ·æ¬çé¾åº¦ãå¨VTC-Benchè¿æ»¤åºçâå°é¾æ ·æ¬âç»ä¸­ï¼åè¿çè§è§tokenåç¼©æ¹æ³æ¾èè¶è¶äºä¸éæ ·åºçº¿ï¼è¯æäºå®ä»¬å¨éè¦ç²¾ç»è§è§çè§£çä»»å¡ä¸­ççæ­£ä»·å¼ã
*   <strong>VTC-Benchæåäºè¯ä¼°çå¬å¹³æ§åæææ§ï¼</strong> éè¿èç¦äºâå°é¾æ ·æ¬âï¼VTC-Benchæ¶é¤äºä¸è§è§tokenåç¼©ä»»å¡æ å³çæ°æ®åªå£°ï¼å¹¶æ¾èæ©å¤§äºä¸ååç¼©æ¹æ³ä¹é´çæ§è½å·®è·ï¼ä»èæ´æ¸æ°å°æ­ç¤ºäºæ¹æ³çä¼å£ã</p>
<p>è¿äºç»æçæä¹å¨äºï¼å®ä»¬æ­ç¤ºäºå½åè§è§tokenåç¼©é¢åè¯ä¼°æ åçç¼ºé·ï¼å¹¶ä¸ºæªæ¥è®¾è®¡æ´ææãæ´å·æææ§çåºåæµè¯æä¾äºæç¡®çæå¯¼ååã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºææå°äºVTC-Benchçä¸¤ä¸ªä¸»è¦å±éæ§ï¼
*   <strong>å¯¹ä¸éæ ·ä½ä¸ºè¿æ»¤å¨çä¾èµï¼</strong> å¦æä¸éæ ·æ¬èº«å¨æäºä»»å¡ä¸è¡¨ç°ä¸ä½³ï¼å¯è½ä¼å¯¼è´âå°é¾æ ·æ¬âçæ°éä¸è¶³ï¼ä»èå½±åè¯ä¼°çå¨é¢æ§ã
*   <strong>æªèèæ¨¡åå·®å¼ï¼</strong> ä¸åçMLLMså¯¹å¾ååè¾¨çåè§è§ç»èçææåº¦ä¸åï¼è¿å¯è½ä¼å½±åæ ·æ¬åç»çéç¨æ§ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºæé¼å±ç¤¾åºè¿ä¸æ­¥è®¨è®ºâå¦ä½æ­£ç¡®è¯ä¼°é«æçMLLMsâï¼è¿æç¤ºäºä»¥ä¸æ½å¨çæªæ¥ç ç©¶æ¹åï¼
*   å¼åæ´é²æ£ãæ´éç¨çæ°æ®è¿æ»¤æºå¶ï¼ä»¥åå°å¯¹åä¸è¿æ»¤æ¹æ³çä¾èµã
*   æ¢ç´¢èèä¸åMLLMsæ¨¡åç¹æ§åææåº¦çè¯ä¼°æ¡æ¶ï¼ä»¥æé«è¯ä¼°ç»æçæ³åè½åã
*   è®¾è®¡ä¸é¨éå¯¹è§è§tokenåç¼©ä»»å¡çå¨æ°åºåæµè¯ï¼ä»ä¸å¼å§å°±é¿åâç®åæ§åå·®âï¼å¹¶åå«æ´å¤éè¦ç²¾ç»è§è§çè§£çæææ§æ ·æ¬ã
*   ç ç©¶å¦ä½å°VTC-Benchççå¿µæ©å±å°å¶ä»MLLMå éææçä¼åä»»å¡çè¯ä¼°ä¸­ã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>Strikingly, our investigation reveals
that simple image downsampling consistently outperforms many advanced
compression methods across multiple widely used benchmarks.</li>
<li>Motivated by these findings, we introduce VTC-Bench, an
evaluation framework that incorporates a data filtering mechanism to denoise
existing benchmarks, thereby enabling fairer and more accurate assessment of
visual token compression methods.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.07143v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.07143v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.06876v1'></a></p>
<h2 id="harp-next-high-speed-and-accurate-range-point-fusion-network-for-3d-lidar-semantic-segmentation"><a href="https://arxiv.org/abs/2510.06876v1">HARP-NeXt: High-Speed and Accurate Range-Point Fusion Network for 3D LiDAR Semantic Segmentation</a></h2>
<p><strong>Authors:</strong> Samir Abou Haidar, Alexandre Chariot, Mehdi Darouich, Cyril Joly, Jean-Emmanuel Deschaud</p>
<p><strong>Published:</strong> 2025-10-08</p>
<p><strong>Categories:</strong> cs.CV, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>LiDAR semantic segmentation is crucial for autonomous vehicles and mobile
robots, requiring high accuracy and real-time processing, especially on
resource-constrained embedded systems. Previous state-of-the-art methods often
face a trade-off between accuracy and speed. Point-based and sparse
convolution-based methods are accurate but slow due to the complexity of
neighbor searching and 3D convolutions. Projection-based methods are faster but
lose critical geometric information during the 2D projection. Additionally,
many recent methods rely on test-time augmentation (TTA) to improve
performance, which further slows the inference. Moreover, the pre-processing
phase across all methods increases execution time and is demanding on embedded
platforms. Therefore, we introduce HARP-NeXt, a high-speed and accurate LiDAR
semantic segmentation network. We first propose a novel pre-processing
methodology that significantly reduces computational overhead. Then, we design
the Conv-SE-NeXt feature extraction block to efficiently capture
representations without deep layer stacking per network stage. We also employ a
multi-scale range-point fusion backbone that leverages information at multiple
abstraction levels to preserve essential geometric details, thereby enhancing
accuracy. Experiments on the nuScenes and SemanticKITTI benchmarks show that
HARP-NeXt achieves a superior speed-accuracy trade-off compared to all
state-of-the-art methods, and, without relying on ensemble models or TTA, is
comparable to the top-ranked PTv3, while running 24<script type="math/tex">\times</script> faster. The code is
available at https://github.com/SamirAbouHaidar/HARP-NeXt</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Samir Abou Haidarç­äººæ°åçè®ºæâHARP-NeXt: High-Speed and Accurate Range-Point Fusion Network for 3D LiDAR Semantic Segmentationâçå¨é¢æè¦ã</p>
<hr />
<h3 id="harp-next-lidar">HARP-NeXt: é«éé«ç²¾åº¦LiDARç¹äºè¯­ä¹åå²ç½ç»æè¦</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
LiDARè¯­ä¹åå²å¯¹äºèªå¨é©¾é©¶åç§»å¨æºå¨äººè³å³éè¦ï¼ä½ç°ææ¹æ³å¨ç²¾åº¦åå®æ¶å¤çéåº¦ä¹é´å­å¨åºæçæè¡¡ãç¹å«æ¯ï¼å¨èµæºåéçåµå¥å¼ç³»ç»ä¸ï¼ç¹äºåç¨çå·ç§¯æ¹æ³è½ç¶åç¡®ä½éåº¦æ¢ï¼èåºäºæå½±çæ¹æ³è½ç¶å¿«ä½ä¼ä¸¢å¤±å³é®å ä½ä¿¡æ¯ãæ­¤å¤ï¼è®¸å¤æ¹æ³ä¾èµäºæµè¯æ¶å¢å¼ºï¼TTAï¼æå¤æçé¢å¤çé¶æ®µï¼è¿ä¸æ­¥å¢å äºæ¨çæ¶é´ãå æ­¤ï¼ç ç©¶çæ ¸å¿é®é¢æ¯ï¼å¦ä½å¨ä¿è¯é«ç²¾åº¦çåæ¶ï¼æ¾èæé«LiDARè¯­ä¹åå²çåå²éåº¦ï¼ä½¿å¶è½å¤å¨åµå¥å¼å¹³å°ä¸å®æ¶è¿è¡ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è´¡ç®ï¼</strong>
HARP-NeXtéè¿ä»¥ä¸å³é®åæ°è§£å³äºä¸è¿°é®é¢ï¼</p>
<ul>
<li><strong>æ°åé¢å¤çæ¹æ³ï¼</strong> æåºäºä¸ç§åæ°çé¢å¤çæµç¨ï¼å°å¤§é¨åè®¡ç®å¯éåæä½ï¼å¦çé¢æå½±ï¼è½¬ç§»å°GPUä¸å¹¶è¡æ§è¡ï¼æ¾èåå°äºCPUè´è½½åæ°æ®ä¼ è¾ç¶é¢ï¼ä»èå¤§å¹éä½äºé¢å¤çé¶æ®µçè®¡ç®å¼éã</li>
<li><strong>Conv-SE-NeXtç¹å¾æåæ¨¡åï¼</strong> è®¾è®¡äºä¸ç§åä¸ºConv-SE-NeXtçæ°åç¹å¾æåæ¨¡åãè¯¥æ¨¡åç»åäºæ·±åº¦å¯åç¦»å·ç§¯ï¼æ¾èåå°åæ°éåè®¡ç®éï¼åSqueeze-and-Excitationï¼SEï¼æºå¶ï¼èªéåºå°éæ°æ ¡åç¹å¾ååºï¼ï¼ä»¥é«ææè·è¡¨ç¤ºï¼èæ éå å å¤å±ï¼ä»èå¨æ¯ä¸ªç½ç»é¶æ®µä¿æè½»éåé«æã</li>
<li><strong>å¤å°ºåº¦èå´-ç¹èåéª¨å¹²ç½ç»ï¼</strong> éç¨äºä¸ä¸ªå¤å°ºåº¦èåéª¨å¹²ç½ç»ï¼éè¿é«æçæ å°ï¼Pt2PxåPx2Ptï¼å¨2Dèå´å¾ååç´ ç¹å¾å3Dç¹ç¹å¾ä¹é´è¿è¡ä¿¡æ¯èåãè¿ç§èåç­ç¥å©ç¨äºå¤ä¸ªæ½è±¡çº§å«çä¿¡æ¯ï¼ä¿çäºå³é®çå ä½ç»èï¼å¢å¼ºäºæ¨¡åçå­¦ä¹ è½åååç¡®æ§ã</li>
<li><strong>è½»éçº§èåç­ç¥ï¼</strong> éç¨äºä¸ç§è½»éçº§çèåç­ç¥ï¼éè¿ç¢éåç´¢å¼åç¨çå¼ éæä½ï¼å¨3Dç¹å2Dèå´å¾åä¹é´æ å°ç¹å¾ï¼å¨ä¿çä¸°å¯çç©ºé´ä¸ä¸æçåæ¶ä¸çºç²æçã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
HARP-NeXtå¨nuScenesåSemanticKITTIåºåæµè¯ä¸åå¾äºæ¾èçæ§è½ï¼</p>
<ul>
<li><strong>åè¶çéåº¦-ç²¾åº¦æè¡¡ï¼</strong> å¨nuScenesæ°æ®éä¸ï¼HARP-NeXtå®ç°äº77.1%çmIoUï¼ä¼äºææåºäºæå½±ãç¨çå·ç§¯åèåçæ¹æ³ï¼å¹¶ä¸æåç¬¬ä¸çPTv3ï¼78.4%ï¼ç¸å½ï¼ä½éåº¦å¿«äº24åï¼å¨RTX4090ä¸ä¸º10æ¯«ç§ï¼å¨Jetson AGX Orinä¸ä¸º71æ¯«ç§ï¼ã</li>
<li><strong>èµæºåéå¹³å°ä¸çé«ææ§ï¼</strong> å¨Jetson AGX Orinç­åµå¥å¼ç³»ç»ä¸ï¼HARP-NeXtå±ç°åºåè¶çå®æ¶æ§è½ï¼å¶æ»è¿è¡æ¶ï¼é¢å¤ç+æ¨çï¼è¿ä½äºå¶ä»æ¹æ³ï¼åæ¶ä¿æäºè¾ä½çåæ°éï¼5.4Mï¼åè®¡ç®å¤æåº¦ã</li>
<li><strong>æ éTTAæéææ¨¡åï¼</strong> HARP-NeXtå¨ä¸ä¾èµæµè¯æ¶å¢å¼ºï¼TTAï¼æéææ¨¡åçæåµä¸ï¼å®ç°äºä¸æåè¿æ¹æ³ç¸å½çæ§è½ï¼è¿è¿ä¸æ­¥è¯æäºå¶åºæçé«ææ§åé²æ£æ§ã</li>
<li><strong>å®æ§ç»æï¼</strong> å®æ§åæè¡¨æï¼HARP-NeXtå¨å³é®ç±»å«ï¼å¦è¡äººãæ±½è½¦ãäººè¡éï¼ä¸çéè¯¯åç±»æ¾èå°äºå¶ä»æ¹æ³ï¼å°¤å¶æ¯å¨å¤æåºæ¯ä¸ã</li>
</ul>
<p>è¿äºç»æè¡¨æHARP-NeXtå¨LiDARè¯­ä¹åå²é¢ååå¾äºéå¤§è¿å±ï¼ä¸ºèªå¨é©¾é©¶åç§»å¨æºå¨äººç­éè¦å®æ¶ãé«ç²¾åº¦æç¥çåºç¨æä¾äºåå®å¯è¡çè§£å³æ¹æ¡ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸­æ²¡ææç¡®æåHARP-NeXtèªèº«çå±éæ§ãç¶èï¼å®é´æ¥æåºäºç°ææ¹æ³çå±éæ§ï¼è¿äºå±éæ§ä¿æäºHARP-NeXtçå¼åï¼
*   <strong>ç°ææ¹æ³çæè¡¡ï¼</strong> å¤§å¤æ°ç°ææ¹æ³å¨ç²¾åº¦åéåº¦ä¹é´å­å¨æè¡¡ï¼æ æ³åæ¶æ»¡è¶³é«ç²¾åº¦åå®æ¶å¤ççéæ±ã
*   <strong>é¢å¤çå¼éï¼</strong> ç°ææ¹æ³ä¸­çé¢å¤çé¶æ®µéå¸¸è®¡ç®éå¤§ä¸èæ¶ï¼å°¤å¶æ¯å¨åµå¥å¼å¹³å°ä¸ã
*   <strong>å ä½ä¿¡æ¯ä¸¢å¤±ï¼</strong> åºäºæå½±çæ¹æ³å¨2Dæå½±è¿ç¨ä¸­ä¼ä¸¢å¤±å³é®ç3Då ä½ä¿¡æ¯ã
*   <strong>TTAçé¢å¤å¼éï¼</strong> è®¸å¤æ¹æ³ä¾èµTTAæ¥æé«æ§è½ï¼ä½è¿ä¼è¿ä¸æ­¥åæ¢æ¨çéåº¦ã
*   <strong>ç¨çç¹äºçææï¼</strong> ç¨çæ·å¤ç¹äºç¼ºä¹è¶³å¤çå±é¨ç¸ä¼¼æ§ï¼ä½¿å¾æäºæ¹æ³ï¼å¦Superpoint Transformerï¼ææä¸ä½³ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºæä¸­æ²¡ææç¡®æåºæªæ¥çç ç©¶æ¹åï¼ä½ä»å¶è´¡ç®åç°ææ¹æ³çå±éæ§æ¥çï¼å¯ä»¥æ¨æ­åºä»¥ä¸æ½å¨æ¹åï¼
*   <strong>æ´å¹¿æ³çç¡¬ä»¶å¹³å°é¨ç½²ï¼</strong> è¿ä¸æ­¥ä¼åHARP-NeXtä»¥éåºæ´å¹¿æ³çèµæºåéç¡¬ä»¶å¹³å°ï¼æ¢ç´¢å¶å¨ä¸åè®¡ç®è½åååèéå¶ä¸çæ§è½ã
*   <strong>èªéåºæ¨¡åï¼</strong> ç ç©¶å¦ä½ä½¿HARP-NeXtè½å¤èªéåºå°è°æ´å¶æ¶ææåæ°ï¼ä»¥åºå¯¹ä¸åLiDARä¼ æå¨ç±»åãç¹äºå¯åº¦æç¯å¢æ¡ä»¶ã
*   <strong>å¤æ¨¡æèåçæ¢ç´¢ï¼</strong> è½ç¶HARP-NeXtä¸æ³¨äºLiDARæ°æ®ï¼ä½å¯ä»¥æ¢ç´¢å°å¶ä¸ç¸æºå¾åæå¶ä»ä¼ æå¨æ°æ®è¿è¡æ´æ·±å±æ¬¡çèåï¼ä»¥è¿ä¸æ­¥æé«å¨å¤æåºæ¯ä¸çé²æ£æ§ååç¡®æ§ã
*   <strong>å®æ¶å¨çº¿å­¦ä¹ ï¼</strong> ç ç©¶å¦ä½å°å¨çº¿å­¦ä¹ æå¢éå­¦ä¹ æºå¶éæå°HARP-NeXtä¸­ï¼ä½¿å¶è½å¤å¨æ°ç¯å¢ä¸­æç»­å­¦ä¹ åéåºï¼èæ ééæ°è®­ç»æ´ä¸ªæ¨¡åã
*   <strong>ä¸ç¡®å®æ§éåï¼</strong> è¿ä¸æ­¥æ¢ç´¢åæ¹è¿HARP-NeXtçä¸ç¡®å®æ§éåè½åï¼ä¸ºèªå¨é©¾é©¶ç­å®å¨å³é®åºç¨æä¾æ´å¯é çé¢æµã
*   <strong>è½»éåæ¶ææç´¢ï¼</strong> ç»åç¥ç»æ¶ææç´¢ï¼NASï¼ææ¯ï¼èªå¨æ¢ç´¢æ´è½»éãæ´é«æçHARP-NeXtåä½ï¼ä»¥è¿ä¸æ­¥ä¼åå¶å¨åµå¥å¼ç³»ç»ä¸çæ§è½ã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>Previous state-of-the-art methods often
face a trade-off between accuracy and speed.</li>
<li>Therefore, we introduce HARP-NeXt, a high-speed and accurate LiDAR
semantic segmentation network.</li>
<li>We first propose a novel pre-processing
methodology that significantly reduces computational overhead.</li>
<li>Experiments on the nuScenes and SemanticKITTI benchmarks show that
HARP-NeXt achieves a superior speed-accuracy trade-off compared to all
state-of-the-art methods, and, without relying on ensemble models or TTA, is
comparable to the top-ranked PTv3, while running 24<script type="math/tex">\times</script> faster.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.06876v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.06876v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.06827v1'></a></p>
<h2 id="stylekeeper-prevent-content-leakage-using-negative-visual-query-guidance"><a href="https://arxiv.org/abs/2510.06827v1">StyleKeeper: Prevent Content Leakage using Negative Visual Query Guidance</a></h2>
<p><strong>Authors:</strong> Jaeseok Jeong, Junho Kim, Gayoung Lee, Yunjey Choi, Youngjung Uh</p>
<p><strong>Published:</strong> 2025-10-08</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>In the domain of text-to-image generation, diffusion models have emerged as
powerful tools. Recently, studies on visual prompting, where images are used as
prompts, have enabled more precise control over style and content. However,
existing methods often suffer from content leakage, where undesired elements of
the visual style prompt are transferred along with the intended style. To
address this issue, we 1) extend classifier-free guidance (CFG) to utilize
swapping self-attention and propose 2) negative visual query guidance (NVQG) to
reduce the transfer of unwanted contents. NVQG employs negative score by
intentionally simulating content leakage scenarios that swap queries instead of
key and values of self-attention layers from visual style prompts. This simple
yet effective method significantly reduces content leakage. Furthermore, we
provide careful solutions for using a real image as visual style prompts.
Through extensive evaluation across various styles and text prompts, our method
demonstrates superiority over existing approaches, reflecting the style of the
references, and ensuring that resulting images match the text prompts. Our code
is available \href{https://github.com/naver-ai/StyleKeeper}{here}.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°å¯¹è¿ç¯è®ºææè¦è¿è¡åæã</p>
<hr />
<p><strong>è®ºææè¦åæï¼StyleKeeper: Prevent Content Leakage using Negative Visual Query Guidance</strong></p>
<p><strong>1. è®ºæä¸»è¦è´¡ç®çç®æ´æ»ç» (A concise summary of the paper's main contribution)</strong></p>
<p>è¿ç¯è®ºææåºäºä¸ç§åä¸ºâè´è§è§æ¥è¯¢å¼å¯¼ (NVQG)âçæ°æ¹æ³ï¼æ¨å¨è§£å³ææ¬å°å¾åçæä¸­è§è§é£æ ¼æç¤ºçâåå®¹æ³é²âé®é¢ãéè¿æ©å±åç±»å¨æ å³å¼å¯¼ (CFG) å¹¶å©ç¨äº¤æ¢èªæ³¨æåæºå¶ï¼NVQG ææå°åå°äºä¸å¸æçé£æ ¼æç¤ºåå®¹è½¬ç§»ï¼åæ¶ä¿æäºå¯¹ç®æ é£æ ¼çå¿ å®åæ åææ¬æç¤ºçå¹éã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®º (The key innovation or methodological approach)</strong></p>
<p>è¯¥è®ºæçå³é®åæ°å¨äºï¼
*   <strong>æ©å±åç±»å¨æ å³å¼å¯¼ (CFG) ä»¥å©ç¨äº¤æ¢èªæ³¨æå (swapping self-attention)ï¼</strong> è¿æ¯å¶æ¹æ³çåºç¡ï¼åè®¸æ´ç²¾ç»å°æ§å¶é£æ ¼ååå®¹çåç¦»ã
*   <strong>è´è§è§æ¥è¯¢å¼å¯¼ (NVQG)ï¼</strong> è¿æ¯æ ¸å¿åæ°ãNVQG éè¿âæææ¨¡æåå®¹æ³é²åºæ¯âæ¥çæè´åæ°ãå·ä½æ¥è¯´ï¼å®ä¸æ¯äº¤æ¢èªæ³¨æåå±ä¸­çé® (key) åå¼ (value)ï¼èæ¯äº¤æ¢æ¥è¯¢ (queries)ãè¿ç§æºå¶æ¨å¨æç¡®å°å­¦ä¹ åæå¶é£äºä¸é£æ ¼æç¤ºä¸­ä¸å¸æçåå®¹ç¸å³çç¹å¾ï¼ä»èå¨çæè¿ç¨ä¸­ä¸»å¨é¿åè¿äºåå®¹çè½¬ç§»ã
*   <strong>ä¸ºä½¿ç¨çå®å¾åä½ä¸ºè§è§é£æ ¼æç¤ºæä¾è§£å³æ¹æ¡ï¼</strong> è¿è¡¨æè¯¥æ¹æ³ä¸ä»éäºåææ°æ®ï¼èæ¯å·æå®éåºç¨æ½åã</p>
<p><strong>3. å¯¹è¯¥é¢åçæ½å¨å½±å (Potential impact on the field)</strong></p>
<ul>
<li><strong>æé«ææ¬å°å¾åçæçå¯æ§æ§ï¼</strong> è§£å³åå®¹æ³é²æ¯è§è§æç¤ºé¢åçä¸ä¸ªéè¦ææãStyleKeeper çæåå°æ¾èæé«ç¨æ·å¨ä½¿ç¨å¾åä½ä¸ºé£æ ¼æç¤ºæ¶å¯¹çæç»æçç²¾ç¡®æ§å¶è½åï¼åå°æå¤åä¸å¸æçåç´ ã</li>
<li><strong>ä¿è¿æ´èªç¶çè§è§æç¤ºäº¤äºï¼</strong> åè®¸ç¨æ·æ´èªä¿¡å°ä½¿ç¨çå®å¾åä½ä¸ºé£æ ¼åèï¼èæ éæå¿å¾åä¸­æ å³èæ¯æç©ä½ä¼æ±¡æçæç»æï¼ä»èä½¿è§è§æç¤ºæ´å ç´è§åå®ç¨ã</li>
<li><strong>ä¸ºæ©æ£æ¨¡åä¸­çå¼å¯¼æºå¶æä¾æ°æè·¯ï¼</strong> NVQG è¿ç§éè¿æ¨¡æâè´é¢âåºæ¯æ¥çæè´åæ°ä»¥æå¶ç¹å®ç¹å¾çæ¹æ³ï¼å¯è½ä¼å¯åå¶ä»æ©æ£æ¨¡åä¸­æ´å¤æçå¼å¯¼ç­ç¥ï¼ç¨äºè§£å³å¶ä»ç±»åççææ§å¶é®é¢ã</li>
<li><strong>æ¨å¨ä¸ªæ§ååå®å¶ååå®¹çæï¼</strong> æ´å¥½çé£æ ¼æ§å¶æå³çç¨æ·å¯ä»¥æ´ç²¾ç¡®å°å°ç¹å®èºæ¯é£æ ¼ãçº¹çæè§è§ç¹å¾åºç¨å°ä»ä»¬ççæåå®¹ä¸­ï¼è¿å¯¹äºåæäº§ä¸ãè®¾è®¡åä¸ªæ§ååªä½çæå·æå·¨å¤§ä»·å¼ã</li>
</ul>
<p><strong>4. å¯è½åçäºè¿é¡¹ç ç©¶çç¸å³é¢åæåºç¨ (Related areas or applications that might benefit from this research)</strong></p>
<ul>
<li><strong>æ°å­èºæ¯åè®¾è®¡ï¼</strong> èºæ¯å®¶åè®¾è®¡å¸å¯ä»¥ä½¿ç¨åèå¾åæ¥ç²¾ç¡®å°å°ç¹å®é£æ ¼ï¼å¦å°è±¡æ´¾ãèµåæåãæ°´å½©ç»ç­ï¼åºç¨å°ä»ä»¬çææ¬æè¿°ä¸­ï¼èæ éæå¿åèå¾åä¸­çå·ä½åå®¹è¢«å¤å¶ã</li>
<li><strong>æ¶å°åäº§åè®¾è®¡ï¼</strong> è®¾è®¡å¸å¯ä»¥å©ç¨ç¹å®æè´¨æçº¹ççå¾åä½ä¸ºé£æ ¼æç¤ºï¼çæå·æè¿äºè§è§ç¹å¾çæ°äº§åè®¾è®¡ï¼åæ¶ç¡®ä¿äº§åæ¬èº«ç¬¦åææ¬æè¿°ã</li>
<li><strong>èæç°å® (VR) åæ¸¸æå¼åï¼</strong> å¿«éçæå·æç¹å®èºæ¯é£æ ¼æç¯å¢æ°å´çèµäº§ååºæ¯ï¼åå°æå¨çº¹çåé£æ ¼åçå·¥ä½éã</li>
<li><strong>ä¸ªæ§ååå®¹åä½ï¼</strong> ç¨æ·å¯ä»¥æ ¹æ®èªå·±çåå¥½ï¼å°ç¹å®é£æ ¼åºç¨å°ç§çæè§é¢ççæä¸­ï¼ä¾å¦å°èªæç§è½¬æ¢ä¸ºæ¼«ç»é£æ ¼ï¼åæ¶ä¿æäººç©ç¹å¾ä¸åã</li>
<li><strong>å¾åç¼è¾åé£æ ¼è¿ç§»ï¼</strong> è½ç¶ä¸»è¦éå¯¹çæï¼ä½å¶æ ¸å¿ææ³å¯è½å¯åæ´é«çº§çé£æ ¼è¿ç§»ç®æ³ï¼ä½¿å¶å¨ä¿çåå®¹çåæ¶ï¼æ´ç²¾ç¡®å°åºç¨ç®æ é£æ ¼ã</li>
</ul>
<p><strong>5. ä»æè¦ä¸­å¯ä»¥æ¨æ­åºçä»»ä½å±éæ§ (Any limitations that can be inferred from the abstract)</strong></p>
<ul>
<li><strong>âç®åä½ææâçå±éæ§ï¼</strong> æè¦ä¸­æå°âThis simple yet effective methodâï¼è½ç¶å¼ºè°äºå¶æçï¼ä½ä¹å¯è½æç¤ºå¶å¨å¤çæå¶å¤ææå¾®å¦çåå®¹æ³é²åºæ¯æ¶ï¼å¯è½ä¸å¦æ´å¤æçãåºäºæ·±åº¦è¯­ä¹çè§£çæ¹æ³é£æ ·é²æ£ã</li>
<li><strong>è®¡ç®ææ¬ï¼</strong> æ©å± CFG åå¼å¥ NVQG å¯è½ä¼å¢å æ¨çæ¶çè®¡ç®ææ¬ãè½ç¶æè¦æ²¡ææç¡®è¯´æï¼ä½ä»»ä½é¢å¤çå¼å¯¼æ­¥éª¤éå¸¸é½ä¼å¸¦æ¥è®¡ç®å¼éã</li>
<li><strong>âè´è§è§æ¥è¯¢âçå®ä¹åæ³åè½åï¼</strong> NVQG ä¾èµäºâæææ¨¡æåå®¹æ³é²åºæ¯âãè¿ç§æ¨¡æçæææ§å¨å¤å¤§ç¨åº¦ä¸è½å¤è¦çæææ½å¨çåå®¹æ³é²ç±»åï¼å¦ææ³é²çæ¨¡å¼éå¸¸è§æé¾ä»¥æ¨¡æï¼è¯¥æ¹æ³çæ§è½å¯è½ä¼åå°å½±åã</li>
<li><strong>é£æ ¼ä¸åå®¹çæ¨¡ç³è¾¹çï¼</strong> å¨æäºæåµä¸ï¼é£æ ¼ååå®¹ä¹é´ççéå¯è½éå¸¸æ¨¡ç³ãä¾å¦ï¼å¦æä¸ä¸ªé£æ ¼æ¬èº«å°±åå«ç¹å®çå¾æ¡æçº¹çï¼é£ä¹åºåâä¸å¸æçåå®¹âåâææçé£æ ¼åç´ âå¯è½ä¼åå¾å°é¾ãæè¦æ²¡æè¯¦ç»è¯´æå¦ä½å¤çè¿ç§è¾¹çæ¨¡ç³çæåµã</li>
<li><strong>å¯¹ç¹å®æ©æ£æ¨¡åæ¶æçä¾èµï¼</strong> æè¦æå°äºâèªæ³¨æåå±âï¼è¿è¡¨æè¯¥æ¹æ³å¯è½ä¸åºäº Transformer çæ©æ£æ¨¡åï¼å¦ Stable Diffusionï¼ç´§å¯ç¸å³ãå®æ¯å¦è½ç´æ¥åºç¨äºææç±»åçæ©æ£æ¨¡åï¼ä¾å¦ï¼åºäº U-Net çæ¨¡åï¼å¯è½éè¦è¿ä¸æ­¥éªè¯ã</li>
</ul>
<hr />
<p>æ»çæ¥è¯´ï¼StyleKeeper æåºäºä¸ç§æ°é¢ä¸æåæ¯çæ¹æ³æ¥è§£å³ææ¬å°å¾åçæä¸­è§è§æç¤ºçå³é®ææââåå®¹æ³é²ãå¶éè¿è´è§è§æ¥è¯¢å¼å¯¼æ¥ä¸»å¨æå¶ä¸å¸æçåå®¹è½¬ç§»çæè·¯ï¼ä¸ºè¯¥é¢åå¸¦æ¥äºéè¦çè´¡ç®ï¼å¹¶æææ¾èæåæ©æ£æ¨¡åå¨é£æ ¼æ§å¶æ¹é¢çå®ç¨æ§åç¨æ·ä½éªã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Through extensive evaluation across various styles and text prompts, our method
demonstrates superiority over existing approaches, reflecting the style of the
references, and ensuring that resulting images match the text prompts.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.06827v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.06827v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.06820v1'></a></p>
<h2 id="efficient-discriminative-joint-encoders-for-large-scale-vision-language-reranking"><a href="https://arxiv.org/abs/2510.06820v1">Efficient Discriminative Joint Encoders for Large Scale Vision-Language Reranking</a></h2>
<p><strong>Authors:</strong> Mitchell Keren Taraday, Shahaf Wagner, Chaim Baskin</p>
<p><strong>Published:</strong> 2025-10-08</p>
<p><strong>Categories:</strong> cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Multimodal retrieval still leans on embedding-based models like CLIP for fast
vector search over pre-computed image embeddings. Yet, unlike text retrieval,
where joint-encoder rerankers are standard, comparable vision--language
rerankers are largely absent. We find that seminal joint encoders such as BLIP
are severely bottlenecked by an expensive visual feature-extraction stage,
preventing practical deployment at scale. Motivated by this bottleneck, we
introduce EDJE, an Efficient Discriminative Joint Encoder that precomputes
vision tokens offline and compresses them via a lightweight attention-based
adapter, so online inference runs only a compact joint encoder over a small set
of visual tokens plus the text. EDJE preserves strong retrieval performance
while drastically reducing storage and online compute, enabling high-throughput
inference. Specifically, EDJE processes 50k image--text pairs/second while
requiring 49kB of disk storage per image, matching prior art on Flickr
(zero-shot) and COCO (fine-tuned) retrieval. The implementation and checkpoints
will be made publicly available shortly.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Mitchell Keren Taraday, Shahaf Wagner, Chaim Baskinæ°åçè®ºæâEfficient Discriminative Joint Encoders for Large Scale Vision-Language Rerankingâçå¨é¢æè¦ã</p>
<hr />
<h3 id="-">è®ºææè¦ï¼é«æå¤å«å¼èåç¼ç å¨ç¨äºå¤§è§æ¨¡è§è§-è¯­è¨éæåº</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
å½åå¤æ¨¡ææ£ç´¢ä¸»è¦ä¾èµäºåºäºåµå¥çæ¨¡åï¼å¦CLIPï¼è¿è¡å¿«éåéæç´¢ï¼ä½ä¸ææ¬æ£ç´¢ä¸­èåç¼ç å¨éæåºå¨å·²æä¸ºæ åä¸åï¼è§è§-è¯­è¨éæåºå¨å¨å¤§è§æ¨¡å®éé¨ç½²ä¸­ä»æ®éç¼ºå¤±ãç°æèåç¼ç å¨ï¼å¦BLIPï¼å å¶æè´µçè§è§ç¹å¾æåé¶æ®µèåå°ä¸¥éç¶é¢ï¼å¯¼è´å¨çº¿æ¨çéåº¦æ¢ï¼æ æ³å¨å¤§è§æ¨¡åºç¨ä¸­å®ç¨ãå æ­¤ï¼è®ºææ¨å¨è§£å³çæ ¸å¿é®é¢æ¯ï¼å¦ä½å¨å®ç°å¤§è§æ¨¡æ£ç´¢æéæççåæ¶ï¼å©ç¨èåå»ºæ¨¡çä¼å¿æ¥æ¾èæåè·¨æ¨¡ææ£ç´¢æ§è½ï¼</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è®ºæå¼å¥äº<strong>EDJE (Efficient Discriminative Joint Encoder)</strong>ï¼ä¸ä¸ªé«æçå¤å«å¼èåç¼ç å¨ï¼å¶æ ¸å¿åæ°å¨äºï¼
*   <strong>è§è§ç¹å¾é¢è®¡ç®ä¸ç¦»çº¿å­å¨ï¼</strong> EDJEå°æè´µçè§è§ç¹å¾æåé¶æ®µä»å¨çº¿æ¨çè½¬ç§»å°ç¦»çº¿é¢å¤çï¼å¾åçè§è§tokenè¢«é¢åè®¡ç®å¹¶å­å¨å¨ç£çä¸ãè¿æ¾èæ¶é¤äºå¨çº¿æ¨ççç¶é¢ã
*   <strong>è½»éçº§tokenåç¼©ééå¨ï¼</strong> ä¸ºäºè§£å³å­å¨ææè§è§tokençå·¨å¤§ææ¬é®é¢ï¼EDJEæåºäºä¸ç§åºäºæ³¨æåçè½»éçº§ééå¨ãè¯¥ééå¨å°é¿åºåçè§è§tokenåç¼©æä¸ç»ç´§åä¸å¯æè¡¨è¾¾åçtokenï¼ä¾å¦ï¼ä»576ä¸ªtokenåç¼©å°64ä¸ªï¼ï¼ä»èå¤§å¹åå°äºæ¯å¾åçå­å¨éæ±åå¨çº¿æ¨çæ¶èåç¼ç å¨éè¦å¤ççtokenæ°éã
*   <strong>ç´§ååèåç¼ç å¨ï¼</strong> å¨çº¿æ¨çæ¶ï¼EDJEä»è¿è¡ä¸ä¸ªç´§åçèåç¼ç å¨ï¼å¤çå°éåç¼©çè§è§tokenåææ¬tokenï¼ä»¥çæéæåºåæ°ãè¿ä½¿å¾å¨çº¿æ¨çéåº¦æå¿«ã
*   <strong>å¤ä»»å¡è®­ç»ç­ç¥ï¼</strong> EDJEéç¨å¤é¶æ®µè®­ç»åè®®ï¼åæ¬é¢è®­ç»åå¾®è°ãé¢è®­ç»ç®æ åæ¬å¾å-ææ¬å¹éãæ©ç è¯­è¨å»ºæ¨¡åææ¬åµå¥æ¢å¤ï¼ä»¥å¢å¼ºæ¨¡åçå¤å«è½ååè·¨æ¨¡æå¯¹é½ã
*   <strong>å±é¨å°åç¼©çç¥è¯è¸é¦ï¼</strong> ä¸ºäºè¿ä¸æ­¥æååç¼©æ¨¡åçæ§è½ï¼è®ºæè¿å¼å¥äºlogitå±é¢çç¥è¯è¸é¦ï¼å©ç¨å±é¨ééå¨æ¨¡åä½ä¸ºæå¸æ¨¡åï¼å°å¤å«è½åä»å±é¨æ¨¡åè½¬ç§»å°åç¼©æ¨¡åã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æ¾èçæçæåï¼</strong> EDJEå¨ä¿æå¼ºå¤§æ£ç´¢æ§è½çåæ¶ï¼å¤§å¹éä½äºå­å¨åå¨çº¿è®¡ç®ææ¬ï¼å®ç°äºé«ååéæ¨çãå·ä½èè¨ï¼EDJEæ¯ç§å¯å¤ç5ä¸å¯¹å¾å-ææ¬ï¼æ¯å¼ å¾åä»é49KBçç£çå­å¨ã
*   <strong>ä¸ç°æææ¯åª²ç¾çæ£ç´¢æ§è½ï¼</strong> EDJEå¨Flickr30kï¼é¶æ ·æ¬ï¼åCOCOï¼å¾®è°ï¼æ£ç´¢ä»»å¡ä¸ï¼å¶æ§è½ä¸ç°ææåè¿çèåç¼ç å¨ï¼å¦BLIPï¼ç¸å½æè¶è¶ãä¾å¦ï¼å¨Flickr30ké¶æ ·æ¬æ£ç´¢ä¸­ï¼EDJEå¨Recall@1ææ ä¸æ¯åå§CLIPæ¨¡åæåé«è¾¾15%ã
*   <strong>æ¨¡åååé²æ£æ§ï¼</strong> EDJEä½ä¸ºå¯æå¥çéæåºå¨ï¼è½å¤ä¸è´å°æååç§åºäºåµå¥æ¨¡åçé¶æ ·æ¬æ£ç´¢æ§è½ï¼æ è®ºåºå±åµå¥æ¨¡åå¦ä½ãå®å¯¹éæåºæ± å¤§å°ååç¼©tokenæ°éçååè¡¨ç°åºé²æ£æ§ã
*   <strong>å­å¨ææ¬æçï¼</strong> å³ä½¿å¨æªåç¼©å½¢å¼ä¸ï¼EDJEçå­å¨ææ¬ä¹ç¸å¯¹è¾ä½ï¼ètokenåç¼©åä½åä»¥æä½çå­å¨ææ¬ï¼ä¾å¦ï¼64ä¸ªtokenä»é49KBï¼ä¿æäºå¤§é¨åæ£ç´¢ç²¾åº¦ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>æ¦å¿µéªè¯æ§è´¨ï¼</strong> è®ºæå°èªèº«å®ä½ä¸ºæ¦å¿µéªè¯ï¼æ¨å¨å¯ååç»­å·¥ä½ï¼è¿æå³çå½åçæ¬å¯è½æªæ¶µçæææ½å¨çåºç¨åºæ¯ã
*   <strong>æªæ¶µçå¤è¯­è¨-å¤æ¨¡ææ£ç´¢ï¼</strong> è®ºææªæ¶åå¤è¯­è¨-å¤æ¨¡ææ£ç´¢ï¼è¿æ¯ä¸ä¸ªæ¥çåå°å³æ³¨çé¢åã
*   <strong>æªæ¶µçå¶ä»æ¨¡æï¼</strong> è®ºæä¸»è¦å³æ³¨è§è§-è¯­è¨æ¨¡æï¼æªæ©å±å°é³é¢æè§é¢ç­å¶ä»æ¨¡æã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ©å±å°å¤è¯­è¨åå¤æ¨¡ææ£ç´¢ï¼</strong> å°EDJEçæ¡æ¶åºç¨äºå¤è¯­è¨ååå«é³é¢ãè§é¢ç­å¶ä»æ¨¡æçæ£ç´¢ä»»å¡ã
*   <strong>è¿ä¸æ­¥æ¢ç´¢èåç¼ç å¨ï¼</strong> è®ºæè®¤ä¸ºèåç¼ç å¨é¢åä»æå¤§éæªè¢«ååæ¢ç´¢çæ½åï¼æå¥æ´å¤ç²¾åæ¹è¿å®ä»¬å°æçäºé¶æ ·æ¬åç±»åè¿æ»¤å¤§åéå¯¹æ°æ®éç­å¤ç§åºç¨ã
*   <strong>ä¼åtokenåç¼©ç­ç¥ï¼</strong> è¿ä¸æ­¥ç ç©¶æ´åè¿çtokenåç¼©ææ¯ï¼ä»¥å¨æ§è½åå­å¨ä¹é´æ¾å°æ´ä¼çå¹³è¡¡ã</p>
<hr />
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæéè¿å¼å¥EDJEï¼æåå°è§£å³äºå¤§è§æ¨¡è§è§-è¯­è¨éæåºä¸­æçä¸æ§è½ççç¾ãå¶æ ¸å¿ææ³æ¯å°æè´µçè§è§ç¹å¾æåç¦»çº¿åï¼å¹¶éè¿è½»éçº§ééå¨åç¼©è§è§tokenï¼ä½¿å¾å¨çº¿æ¨çè½å¤ä»¥æé«çæçè¿è¡ï¼åæ¶ä¿æçè³è¶è¶ç°ææåè¿æ¨¡åçæ£ç´¢æ§è½ãEDJEçæ¨¡ååè®¾è®¡åæ¾èçæçæåä½¿å¶æä¸ºæªæ¥å¤§è§æ¨¡å¤æ¨¡ææ£ç´¢ç³»ç»çéè¦åºç³ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Specifically, EDJE processes 50k image--text pairs/second while
requiring 49kB of disk storage per image, matching prior art on Flickr
(zero-shot) and COCO (fine-tuned) retrieval.</li>
<li>The implementation and checkpoints
will be made publicly available shortly.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.06820v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.06820v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-10-09 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
