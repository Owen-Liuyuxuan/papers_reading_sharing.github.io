<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2026-01-06 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2026-01-05/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../../depth_completion/DNet/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2026-01-06">Arxiv Computer Vision Papers - 2026-01-06</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#exposeanyone-personalized-audio-to-expression-diffusion-models-are-robust-zero-shot-face-forgery-detectors" class="nav-link">ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors</a>
                </li>
                <li class="nav-item">
                    <a href="#vino-a-unified-visual-generator-with-interleaved-omnimodal-context" class="nav-link">VINO: A Unified Visual Generator with Interleaved OmniModal Context</a>
                </li>
                <li class="nav-item">
                    <a href="#talk2move-reinforcement-learning-for-text-instructed-object-level-geometric-transformation-in-scenes" class="nav-link">Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes</a>
                </li>
                <li class="nav-item">
                    <a href="#joint-semantic-and-rendering-enhancements-in-3d-gaussian-modeling-with-anisotropic-local-encoding" class="nav-link">Joint Semantic and Rendering Enhancements in 3D Gaussian Modeling with Anisotropic Local Encoding</a>
                </li>
                <li class="nav-item">
                    <a href="#prithvi-complimentary-adaptive-fusion-encoder-cafe-unlocking-full-potential-for-flood-inundation-mapping" class="nav-link">Prithvi-Complimentary Adaptive Fusion Encoder (CAFE): unlocking full-potential for flood inundation mapping</a>
                </li>
                <li class="nav-item">
                    <a href="#360dvo-deep-visual-odometry-for-monocular-360-degree-camera" class="nav-link">360DVO: Deep Visual Odometry for Monocular 360-Degree Camera</a>
                </li>
                <li class="nav-item">
                    <a href="#infinitevggt-visual-geometry-grounded-transformer-for-endless-streams" class="nav-link">InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams</a>
                </li>
                <li class="nav-item">
                    <a href="#diffproxy-multi-view-human-mesh-recovery-via-diffusion-generated-dense-proxies" class="nav-link">DiffProxy: Multi-View Human Mesh Recovery via Diffusion-Generated Dense Proxies</a>
                </li>
                <li class="nav-item">
                    <a href="#slgnet-synergizing-structural-priors-and-language-guided-modulation-for-multimodal-object-detection" class="nav-link">SLGNet: Synergizing Structural Priors and Language-Guided Modulation for Multimodal Object Detection</a>
                </li>
                <li class="nav-item">
                    <a href="#vibe-visual-instruction-based-editor" class="nav-link">VIBE: Visual Instruction Based Editor</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2026-01-06">Arxiv Computer Vision Papers - 2026-01-06</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，作为一名专注于计算机视觉和机器学习的研究助理，我将为您提供一份关于2026年1月5日 Arxiv 计算机视觉领域论文的简明执行摘要。</p>
<hr />
<p><strong>执行摘要：2026年1月5日 Arxiv 计算机视觉论文精选</strong></p>
<p><strong>主要主题与趋势：</strong></p>
<p>本次发布的论文展现了计算机视觉领域在<strong>多模态融合、生成模型应用、三维场景理解与重建、以及视觉导航与定位</strong>等方面的显著进展。特别值得注意的是，<strong>文本与视觉的结合</strong>在指导生成和理解任务中扮演着越来越重要的角色，同时，<strong>扩散模型</strong>在各种下游任务中展现出强大的潜力。</p>
<p><strong>亮点与创新：</strong></p>
<ul>
<li><strong>ExposeAnyone</strong> 提出了一种新颖的<strong>音频到表情扩散模型</strong>，并将其成功应用于<strong>零样本人脸伪造检测</strong>，展示了生成模型在安全领域的创新应用。</li>
<li><strong>VINO</strong> 引入了一个<strong>统一的视觉生成器</strong>，通过<strong>交织的全模态上下文</strong>，预示着更强大的跨模态生成能力。</li>
<li><strong>Talk2Move</strong> 利用<strong>强化学习</strong>实现了<strong>文本指令驱动的物体级几何变换</strong>，为机器人和虚拟环境中的交互式操作开辟了新途径。</li>
<li><strong>DiffProxy</strong> 提出了一种基于<strong>扩散模型生成密集代理</strong>的方法，用于<strong>多视角人体网格恢复</strong>，在三维人体姿态估计方面取得了重要突破。</li>
</ul>
<p><strong>新兴研究方向与技术：</strong></p>
<ul>
<li><strong>扩散模型 (Diffusion Models)</strong>：在人脸伪造检测、三维重建和人体姿态估计等多个任务中得到应用，显示出其作为强大生成和表示学习工具的通用性。</li>
<li><strong>多模态融合 (Multimodal Fusion)</strong>：文本与视觉的结合是核心趋势，体现在文本指导的生成（Talk2Move, VIBE）、多模态上下文的利用（VINO）以及语言引导的检测（SLGNet）中。</li>
<li><strong>三维场景理解与重建 (3D Scene Understanding &amp; Reconstruction)</strong>：通过改进高斯模型（Joint Semantic and Rendering Enhancements in 3D Gaussian Modeling）和利用Transformer（InfiniteVGGT）来提升三维场景的表示和处理能力。</li>
<li><strong>视觉导航与定位 (Visual Navigation &amp; Odometry)</strong>：针对特定场景（如360度相机）的视觉里程计（360DVO）研究仍在深入。</li>
</ul>
<p><strong>推荐阅读论文：</strong></p>
<p>为了快速了解当前研究热点和潜在影响，建议重点阅读以下论文：</p>
<ol>
<li><strong>ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors</strong>：鉴于其在安全领域的创新应用和对扩散模型的巧妙运用。</li>
<li><strong>VINO: A Unified Visual Generator with Interleaved OmniModal Context</strong>：代表了下一代多模态生成模型的发展方向。</li>
<li><strong>Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes</strong>：在人机交互和机器人控制领域具有重要的实际应用潜力。</li>
<li><strong>DiffProxy: Multi-View Human Mesh Recovery via Diffusion-Generated Dense Proxies</strong>：在三维人体建模这一关键领域取得了显著进展。</li>
</ol>
<hr />
<p>这份摘要旨在帮助您快速把握本次发布的 Arxiv 论文的核心内容和重要趋势。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2601.02359v1">ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors</a></li>
<li><a href="#2601.02358v1">VINO: A Unified Visual Generator with Interleaved OmniModal Context</a></li>
<li><a href="#2601.02356v1">Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes</a></li>
<li><a href="#2601.02339v1">Joint Semantic and Rendering Enhancements in 3D Gaussian Modeling with Anisotropic Local Encoding</a></li>
<li><a href="#2601.02315v1">Prithvi-Complimentary Adaptive Fusion Encoder (CAFE): unlocking full-potential for flood inundation mapping</a></li>
<li><a href="#2601.02309v1">360DVO: Deep Visual Odometry for Monocular 360-Degree Camera</a></li>
<li><a href="#2601.02281v1">InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams</a></li>
<li><a href="#2601.02267v1">DiffProxy: Multi-View Human Mesh Recovery via Diffusion-Generated Dense Proxies</a></li>
<li><a href="#2601.02249v1">SLGNet: Synergizing Structural Priors and Language-Guided Modulation for Multimodal Object Detection</a></li>
<li><a href="#2601.02242v1">VIBE: Visual Instruction Based Editor</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2601.02359v1'></a></p>
<h2 id="exposeanyone-personalized-audio-to-expression-diffusion-models-are-robust-zero-shot-face-forgery-detectors"><a href="https://arxiv.org/abs/2601.02359v1">ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors</a></h2>
<p><strong>Authors:</strong> Kaede Shiohara, Toshihiko Yamasaki, Vladislav Golyanik</p>
<p><strong>Published:</strong> 2026-01-05</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Detecting unknown deepfake manipulations remains one of the most challenging problems in face forgery detection. Current state-of-the-art approaches fail to generalize to unseen manipulations, as they primarily rely on supervised training with existing deepfakes or pseudo-fakes, which leads to overfitting to specific forgery patterns. In contrast, self-supervised methods offer greater potential for generalization, but existing work struggles to learn discriminative representations only from self-supervision. In this paper, we propose ExposeAnyone, a fully self-supervised approach based on a diffusion model that generates expression sequences from audio. The key idea is, once the model is personalized to specific subjects using reference sets, it can compute the identity distances between suspected videos and personalized subjects via diffusion reconstruction errors, enabling person-of-interest face forgery detection. Extensive experiments demonstrate that 1) our method outperforms the previous state-of-the-art method by 4.22 percentage points in the average AUC on DF-TIMIT, DFDCP, KoDF, and IDForge datasets, 2) our model is also capable of detecting Sora2-generated videos, where the previous approaches perform poorly, and 3) our method is highly robust to corruptions such as blur and compression, highlighting the applicability in real-world face forgery detection.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors”的全面中文摘要：</p>
<p><strong>论文题目：</strong> ExposeAnyone: 个性化音频到表情的扩散模型是鲁棒的零样本人脸伪造检测器</p>
<p><strong>作者：</strong> Kaede Shiohara, Toshihiko Yamasaki, Vladislav Golyanik</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
论文旨在解决当前人脸伪造检测领域面临的核心挑战：<strong>未知篡改的泛化能力不足</strong>。现有最先进的方法主要依赖于对已知深度伪造或伪造样本的监督训练，这导致模型容易过拟合到特定的伪造模式，而无法有效检测未见过的新型伪造技术。尽管自监督方法在泛化方面具有潜力，但现有研究难以仅凭自监督学习到具有判别力的表征。</p>
<p><strong>2. 关键创新/方法贡献：</strong>
ExposeAnyone 提出了一种<strong>完全自监督</strong>的人脸伪造检测框架，其核心创新在于：</p>
<ul>
<li><strong>基于音频到表情的扩散模型：</strong> 论文构建了一个名为 EXAM (ExposeAnyone Model) 的扩散模型，能够根据音频信号生成相应的面部表情序列。</li>
<li><strong>个性化（Personalization）：</strong> 模型通过引入一个<strong>主题特定的适配器（adapter token）</strong>，在少量目标人物的参考视频上进行个性化训练，从而学习到该人物特有的说话身份（talking identity）。</li>
<li><strong>内容无关的身份认证（Content-Agnostic Authentication）：</strong> 提出了一种新颖的认证机制，通过比较<strong>带有身份信息（即使用适配器）和不带身份信息（即不使用适配器）的扩散模型重建误差之间的距离</strong>来判断视频的真实性。这种方法能够有效分离身份信息和视频内容本身的影响，从而实现对伪造的检测。</li>
<li><strong>3DMM（三维可变形模型）提取策略：</strong> 引入了一种改进的 3DMM 提取策略，通过前馈初始化和迭代精炼，有效解耦了面部形状和表情，为后续的表情生成和身份识别奠定了基础。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
论文通过大量实验证明了 ExposeAnyone 的有效性：</p>
<ul>
<li><strong>卓越的泛化能力：</strong> 在 DF-TIMIT、DFDCP、KoDF 和 IDForge 等多个基准数据集上，ExposeAnyone 的平均 AUC 比现有最先进方法高出 4.22 个百分点，达到了 <strong>95.22%</strong> 的平均 AUC。</li>
<li><strong>检测新型伪造：</strong> 该模型能够有效检测由 <strong>Sora2</strong> 生成的视频，而现有方法在此类视频上表现不佳。这表明了其对新兴生成技术的适应性。</li>
<li><strong>鲁棒性强：</strong> ExposeAnyone 对模糊和压缩等常见图像扰动具有高度鲁棒性，即使在严重视频压缩的情况下，性能下降也微乎其微，这凸显了其在<strong>真实世界应用</strong>中的潜力。</li>
<li><strong>自监督的优势：</strong> 即使仅使用 <strong>VoxCeleb2</strong> 数据集进行预训练，ExposeAnyone 的性能仍然优于许多现有方法，证明了自监督学习在人脸伪造检测领域的巨大潜力。</li>
</ul>
<p><strong>4. 提及的局限性：</strong>
论文中提到了一些局限性：</p>
<ul>
<li><strong>依赖现有的模型：</strong> ExposeAnyone 的性能依赖于一些现有的“即插即用”模型，例如用于人脸表示的 FLAME、用于前馈初始化的 SPECTRE 以及用于音频编码的 Wav2Vec 2.0。</li>
<li><strong>计算开销：</strong> 模型在推理过程中存在一定的计算开销，主要源于 3DMM 提取的迭代优化以及扩散重建过程中的多时间步和多噪声序列采样。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
论文暗示了以下潜在的未来研究方向：</p>
<ul>
<li><strong>优化 3DMM 提取：</strong> 开发一个<strong>前馈式的 3DMM 提取模型</strong>，以显著减少计算开销。</li>
<li><strong>减少扩散成本：</strong> 通过使用<strong>更少数量的噪声序列</strong>来降低扩散模型的计算成本，同时保持检测精度。</li>
<li><strong>扩大数据集：</strong> 进一步<strong>扩大预训练和个性化数据集的规模和多样性</strong>，以提升模型的泛化能力和鲁棒性。</li>
<li><strong>更广泛的应用：</strong> 探索 ExposeAnyone 在<strong>其他身份识别或内容认证</strong>任务中的应用潜力。</li>
</ul>
<p><strong>总结：</strong>
ExposeAnyone 提出了一种新颖的、完全自监督的人脸伪造检测框架，通过结合音频到表情的扩散模型和个性化技术，实现了对未知伪造的强大检测能力。其核心贡献在于利用身份信息来区分真实和伪造视频，而非依赖于特定的伪造模式。该方法在多个数据集上取得了最先进的性能，并且对新型生成技术和真实世界中的图像扰动表现出高度鲁棒性，为自监督人脸伪造检测领域开辟了新的研究方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Current state-of-the-art approaches fail to generalize to unseen manipulations, as they primarily rely on supervised training with existing deepfakes or pseudo-fakes, which leads to overfitting to specific forgery patterns.</li>
<li>In this paper, we propose ExposeAnyone, a fully self-supervised approach based on a diffusion model that generates expression sequences from audio.</li>
<li>Extensive experiments demonstrate that 1) our method outperforms the previous state-of-the-art method by 4.22 percentage points in the average AUC on DF-TIMIT, DFDCP, KoDF, and IDForge datasets, 2) our model is also capable of detecting Sora2-generated videos, where the previous approaches perform poorly, and 3) our method is highly robust to corruptions such as blur and compression, highlighting the applicability in real-world face forgery detection.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.02359v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.02359v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.02358v1'></a></p>
<h2 id="vino-a-unified-visual-generator-with-interleaved-omnimodal-context"><a href="https://arxiv.org/abs/2601.02358v1">VINO: A Unified Visual Generator with Interleaved OmniModal Context</a></h2>
<p><strong>Authors:</strong> Junyi Chen, Tong He, Zhoujie Fu, Pengfei Wan, Kun Gai, Weicai Ye</p>
<p><strong>Published:</strong> 2026-01-05</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We present VINO, a unified visual generator that performs image and video generation and editing within a single framework. Instead of relying on task-specific models or independent modules for each modality, VINO uses a shared diffusion backbone that conditions on text, images and videos, enabling a broad range of visual creation and editing tasks under one model. Specifically, VINO couples a vision-language model (VLM) with a Multimodal Diffusion Transformer (MMDiT), where multimodal inputs are encoded as interleaved conditioning tokens, and then used to guide the diffusion process. This design supports multi-reference grounding, long-form instruction following, and coherent identity preservation across static and dynamic content, while avoiding modality-specific architectural components. To train such a unified system, we introduce a multi-stage training pipeline that progressively expands a video generation base model into a unified, multi-task generator capable of both image and video input and output. Across diverse generation and editing benchmarks, VINO demonstrates strong visual quality, faithful instruction following, improved reference and attribute preservation, and more controllable multi-identity edits. Our results highlight a practical path toward scalable unified visual generation, and the promise of interleaved, in-context computation as a foundation for general-purpose visual creation.</p>
<p><strong>Analysis:</strong></p>
<p>作为一名计算机视觉和机器学习领域的专家，我对这篇论文摘要进行了深入分析，并为您提供以下内容：</p>
<p><strong>1. 论文的主要贡献（2-3句话）</strong></p>
<p>VINO 提出了一种统一的视觉生成框架，能够在一个模型内完成图像和视频的生成与编辑任务。其核心在于利用共享的扩散模型骨干，并融合文本、图像和视频作为多模态上下文进行条件引导，从而实现广泛的视觉创作和编辑能力，无需依赖特定任务的模型或模块。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>VINO 的关键创新在于其<strong>统一的架构设计和多模态上下文的融合方式</strong>。具体来说：</p>
<ul>
<li><strong>共享扩散模型骨干 (Shared Diffusion Backbone):</strong> 这是 VINO 的核心，意味着一个模型能够处理多种视觉任务，避免了为不同任务构建独立模型的低效性。</li>
<li><strong>多模态扩散 Transformer (MMDiT) 与 VLM 耦合:</strong> VINO 将视觉-语言模型 (VLM) 与 MMDiT 相结合，这是实现多模态理解和生成的基础。</li>
<li><strong>交错式上下文编码 (Interleaved Conditioning Tokens):</strong> 这是 VINO 最具技术亮点的部分。它将文本、图像和视频等不同模态的输入编码成“交错式”的条件令牌，然后输入到扩散模型中。这种方式允许模型在生成过程中同时参考和整合来自不同模态的信息，从而实现更精细、更灵活的控制。</li>
<li><strong>多阶段训练管线 (Multi-stage Training Pipeline):</strong> 为了训练这样一个复杂的统一模型，论文引入了一个分阶段的训练策略，从视频生成基础模型开始，逐步扩展其能力以支持多模态输入和输出。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<p>VINO 的研究对计算机视觉领域具有重要的潜在影响：</p>
<ul>
<li><strong>推动通用视觉生成模型的发展:</strong> VINO 展示了一条通往更通用、更强大的视觉生成模型的实用路径。未来研究可以借鉴其统一架构和多模态融合思想，构建能够处理更广泛视觉任务的模型。</li>
<li><strong>提高模型效率和可扩展性:</strong> 通过单一框架处理多种任务，可以显著提高模型训练和部署的效率，降低开发成本。</li>
<li><strong>提升生成内容的质量和可控性:</strong> 摘要中提到的“强大的视觉质量”、“忠实的指令遵循”、“改进的参考和属性保留”以及“更可控的多身份编辑”都表明 VINO 在生成内容的质量和用户控制方面取得了显著进步。</li>
<li><strong>为“上下文内计算”在视觉领域的应用奠定基础:</strong> 论文强调了“交错式、上下文内计算”作为通用视觉创作基础的潜力，这可能开启新的研究方向，探索如何让模型更有效地利用上下文信息进行推理和生成。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>内容创作:</strong> 电影制作、广告、游戏开发等领域可以利用 VINO 进行更高效、更具创意的视觉内容生成和编辑。</li>
<li><strong>虚拟现实/增强现实 (VR/AR):</strong> VINO 可以用于生成逼真的虚拟场景、角色和交互元素，提升用户体验。</li>
<li><strong>个性化内容生成:</strong> 根据用户的文本描述、参考图像或视频，生成定制化的图像和视频。</li>
<li><strong>数字人/虚拟形象:</strong> 能够更精细地控制虚拟形象的表情、动作和服装，并保持身份的一致性。</li>
<li><strong>教育和培训:</strong> 生成用于教学的模拟场景、演示视频等。</li>
<li><strong>辅助设计:</strong> 帮助设计师快速生成概念图、原型等。</li>
<li><strong>图像/视频编辑工具:</strong> 提供更强大、更智能的编辑功能，如风格迁移、内容修复、对象替换等。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<p>尽管摘要描绘了 VINO 的强大能力，但仍可以推断出一些潜在的局限性：</p>
<ul>
<li><strong>计算资源需求:</strong> 训练和运行如此复杂的统一模型，尤其是基于扩散模型，通常需要巨大的计算资源（GPU/TPU）和大量的数据。</li>
<li><strong>模型复杂性与可解释性:</strong> 统一模型虽然强大，但其内部机制可能更加复杂，理解和调试其行为可能更具挑战性。</li>
<li><strong>对训练数据的依赖:</strong> 模型的性能高度依赖于训练数据的质量和多样性。如果训练数据在某些方面存在偏差，模型也可能表现出相应的局限性。</li>
<li><strong>长视频生成挑战:</strong> 尽管提到了“长形式指令遵循”和“静态和动态内容的一致性”，但生成非常长且连贯的视频仍然是一个极具挑战性的问题，摘要中并未详细说明 VINO 在此方面的具体表现。</li>
<li><strong>多身份编辑的精细度:</strong> 虽然提到了“更可控的多身份编辑”，但“可控”的程度和精细度仍需进一步验证。例如，是否能精确控制多个身份的交互和细节。</li>
<li><strong>泛化能力:</strong> 尽管是统一模型，但其在未见过或非常规的模态组合或任务上的泛化能力仍需在实际应用中进行评估。</li>
</ul>
<p>总而言之，VINO 是一项令人兴奋的研究，它通过创新的多模态上下文融合和统一架构，为通用视觉生成模型的发展开辟了新的道路。其核心技术——交错式上下文编码，是理解和生成复杂视觉内容的关键一步。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present VINO, a unified visual generator that performs image and video generation and editing within a single framework.</li>
<li>To train such a unified system, we introduce a multi-stage training pipeline that progressively expands a video generation base model into a unified, multi-task generator capable of both image and video input and output.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.02358v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.02358v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.02356v1'></a></p>
<h2 id="talk2move-reinforcement-learning-for-text-instructed-object-level-geometric-transformation-in-scenes"><a href="https://arxiv.org/abs/2601.02356v1">Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes</a></h2>
<p><strong>Authors:</strong> Jing Tan, Zhaoyang Zhang, Yantao Shen, Jiarui Cai, Shuo Yang, Jiajun Wu, Wei Xia, Zhuowen Tu, Stefano Soatto</p>
<p><strong>Published:</strong> 2026-01-05</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We introduce Talk2Move, a reinforcement learning (RL) based diffusion framework for text-instructed spatial transformation of objects within scenes. Spatially manipulating objects in a scene through natural language poses a challenge for multimodal generation systems. While existing text-based manipulation methods can adjust appearance or style, they struggle to perform object-level geometric transformations-such as translating, rotating, or resizing objects-due to scarce paired supervision and pixel-level optimization limits. Talk2Move employs Group Relative Policy Optimization (GRPO) to explore geometric actions through diverse rollouts generated from input images and lightweight textual variations, removing the need for costly paired data. A spatial reward guided model aligns geometric transformations with linguistic description, while off-policy step evaluation and active step sampling improve learning efficiency by focusing on informative transformation stages. Furthermore, we design object-centric spatial rewards that evaluate displacement, rotation, and scaling behaviors directly, enabling interpretable and coherent transformations. Experiments on curated benchmarks demonstrate that Talk2Move achieves precise, consistent, and semantically faithful object transformations, outperforming existing text-guided editing approaches in both spatial accuracy and scene coherence.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“TALK2MOVE: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes”的全面中文摘要：</p>
<p><strong>论文题目：</strong> TALK2MOVE: 强化学习驱动的文本指令对象级几何变换场景编辑</p>
<p><strong>作者：</strong> Jing Tan, Zhaoyang Zhang, Yantao Shen, Jiarui Cai, Shuo Yang, Jiajun Wu, Wei Xia, Zhuowen Tu, Stefano Soatto</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
该论文旨在解决在图像场景中，如何通过自然语言指令精确地对特定对象进行几何变换（如平移、旋转、缩放）这一核心挑战。现有文本驱动的图像编辑方法主要侧重于外观或风格的调整，而难以实现对象级别的几何变换。这主要是由于缺乏大规模的配对监督数据以及像素级优化方法的局限性。</p>
<p><strong>2. 主要创新点/方法贡献：</strong>
TALK2MOVE 提出了一种基于强化学习（RL）的扩散模型框架，专门用于处理文本指令驱动的对象级几何变换。其关键创新点包括：</p>
<ul>
<li><strong>基于强化学习的框架：</strong> 首次将文本指令驱动的几何对象变换问题形式化为强化学习问题，利用 Group Relative Policy Optimization (GRPO) 范式来探索几何变换动作。</li>
<li><strong>数据效率：</strong> 通过生成多样化的输入图像和轻量级的文本变体来探索不同的变换轨迹，从而生成大量数据，显著减少了对昂贵配对数据的依赖。</li>
<li><strong>空间感知奖励模型：</strong> 设计了专门的对象中心空间奖励，能够直接评估对象的位移、旋转和缩放行为，将几何变换与语言描述对齐，实现可解释且几何感知的优化目标。</li>
<li><strong>高效的训练策略：</strong><ul>
<li><strong>离策略步评估 (Off-policy Step Evaluation)：</strong> 通过测量每个去噪步骤的奖励方差来识别对学习信号贡献最大的步骤，从而聚焦于信息量大的变换阶段。</li>
<li><strong>主动步采样 (Active Step Sampling)：</strong> 引入一种步进式主动采样机制，利用“提前退出”策略（early-exit）来跳过冗余的去噪步骤，显著提高了训练效率（约2倍），同时保持了奖励的鲁棒性。</li>
</ul>
</li>
<li><strong>数据准备流水线：</strong> 建立了一个高效的数据收集流水线，包括参考图像生成、指令生成和目标图像合成，为 GRPO 在线训练提供了高质量的几何对象变换数据。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
实验结果表明，TALK2MOVE 在精心策划的基准测试和真实图像上，能够实现精确、一致且语义上忠实的几何对象变换。与现有的最先进（SOTA）文本引导编辑方法相比，TALK2MOVE 在空间准确性和场景连贯性方面均取得了显著的优势。</p>
<ul>
<li><strong>性能提升：</strong> 在定量评估中，TALK2MOVE 在翻译距离、旋转误差和缩放误差等指标上表现优异，并且在用户研究中获得了最高的获胜率。</li>
<li><strong>场景连贯性：</strong> 该方法能够更好地保留原始场景的细节，避免了其他方法可能出现的色调或亮度变化。</li>
<li><strong>数据效率：</strong> 即使在数据量大幅减少的情况下，TALK2MOVE 仍能达到与全数据设置相当的性能，证明了其 RL 方法在数据效率方面的优势。</li>
<li><strong>意义：</strong> 该研究为实现更自然、更直观的图像编辑交互方式开辟了道路，使得非专业用户也能通过简单的文本指令对图像中的对象进行精确的几何操作。</li>
</ul>
<p><strong>4. 提及的局限性：</strong>
*   <strong>数据集规模：</strong> 虽然论文构建了一个数据集，但作者提到“大规模数据扩展留待未来工作”，暗示当前数据集规模可能仍有提升空间。
*   <strong>计算成本：</strong> 尽管引入了效率提升策略，但 GRPO 训练本身仍然是计算密集型的，尤其是在生成大量采样轨迹时。
*   <strong>对复杂场景的泛化：</strong> 虽然在真实图像上进行了评估，但对于极其复杂或包含大量遮挡的场景，其性能仍可能受到影响。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>扩展到其他生成框架：</strong> 作者提到该 RL 配方可以扩展到其他生成框架，如 GANs 和自回归模型，用于可验证、可控的视觉生成。
*   <strong>更复杂的变换：</strong> 未来可以探索更复杂的几何变换，例如非刚性形变或对象之间的相对姿态调整。
*   <strong>交互式编辑：</strong> 进一步探索更流畅的交互式编辑体验，结合用户反馈进行迭代优化。
*   <strong>更广泛的应用：</strong> 将该技术应用于虚拟现实、增强现实、游戏开发等领域，实现更高级别的场景编辑和内容创作。</p>
<p>总而言之，TALK2MOVE 是一项重要的研究成果，它成功地将强化学习应用于文本指令驱动的对象级几何变换，解决了现有方法的关键痛点，并在性能和效率上取得了显著突破，为未来更智能、更直观的图像编辑技术奠定了基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce Talk2Move, a reinforcement learning (RL) based diffusion framework for text-instructed spatial transformation of objects within scenes.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.02356v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.02356v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.02339v1'></a></p>
<h2 id="joint-semantic-and-rendering-enhancements-in-3d-gaussian-modeling-with-anisotropic-local-encoding"><a href="https://arxiv.org/abs/2601.02339v1">Joint Semantic and Rendering Enhancements in 3D Gaussian Modeling with Anisotropic Local Encoding</a></h2>
<p><strong>Authors:</strong> Jingming He, Chongyi Li, Shiqi Wang, Sam Kwong</p>
<p><strong>Published:</strong> 2026-01-05</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent works propose extending 3DGS with semantic feature vectors for simultaneous semantic segmentation and image rendering. However, these methods often treat the semantic and rendering branches separately, relying solely on 2D supervision while ignoring the 3D Gaussian geometry. Moreover, current adaptive strategies adapt the Gaussian set depending solely on rendering gradients, which can be insufficient in subtle or textureless regions. In this work, we propose a joint enhancement framework for 3D semantic Gaussian modeling that synergizes both semantic and rendering branches. Firstly, unlike conventional point cloud shape encoding, we introduce an anisotropic 3D Gaussian Chebyshev descriptor using the Laplace-Beltrami operator to capture fine-grained 3D shape details, thereby distinguishing objects with similar appearances and reducing reliance on potentially noisy 2D guidance. In addition, without relying solely on rendering gradient, we adaptively adjust Gaussian allocation and spherical harmonics with local semantic and shape signals, enhancing rendering efficiency through selective resource allocation. Finally, we employ a cross-scene knowledge transfer module to continuously update learned shape patterns, enabling faster convergence and robust representations without relearning shape information from scratch for each new scene. Experiments on multiple datasets demonstrate improvements in segmentation accuracy and rendering quality while maintaining high rendering frame rates.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Joint Semantic and Rendering Enhancements in 3D Gaussian Modeling with Anisotropic Local Encoding”的全面中文摘要，其中包含了您要求的各个部分：</p>
<p><strong>论文题目：</strong> Joint Semantic and Rendering Enhancements in 3D Gaussian Modeling with Anisotropic Local Encoding (结合各向异性局部编码的3D高斯模型联合语义与渲染增强)</p>
<p><strong>作者：</strong> Jingming He, Chongyi Li, Shiqi Wang, Sam Kwong</p>
<hr />
<p><strong>摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>该论文旨在解决现有3D高斯模型（3DGS）在同时进行语义分割和图像渲染时存在的不足。具体而言，现有方法往往将语义和渲染分支独立处理，过度依赖2D监督信号，忽视了3D高斯几何本身的丰富信息。此外，现有的自适应策略仅依赖渲染梯度来调整高斯分布，在纹理稀疏或细节不明显的区域效果不佳。这导致了语义理解的精度和渲染质量的提升空间受限。</p>
<p><strong>2. 主要创新点/方法贡献：</strong></p>
<p>为了克服上述挑战，作者提出了一个<strong>联合增强框架</strong>，旨在协同优化3D语义高斯模型的语义和渲染两个分支。其核心创新点包括：</p>
<ul>
<li><strong>各向异性3D高斯切比雪夫描述符（Anisotropic 3D Gaussian Chebyshev descriptor）：</strong> 区别于传统的点云形状编码，该方法引入了一种基于<strong>各向异性拉普拉斯-贝尔特拉米算子（ALBO）</strong>的描述符，能够捕捉精细的3D形状细节和方向信息。这有助于区分外观相似但形状不同的物体，并减少对可能存在噪声的2D监督的依赖。</li>
<li><strong>语义感知的高斯分配与球谐函数（SH）调整：</strong> 该框架不再仅依赖渲染梯度，而是<strong>自适应地利用局部语义和形状信号</strong>来调整高斯分配（即高斯的数量和密度）以及球谐函数（SH）的阶数。这实现了<strong>选择性资源分配</strong>，提高了渲染效率和视觉保真度。</li>
<li><strong>跨场景知识迁移模块（Cross-Scene Knowledge Transfer）：</strong> 为了加速收敛并获得更鲁棒的表示，该方法引入了一个<strong>跨场景知识迁移模块</strong>。该模块通过维护一个<strong>模式基础（pattern basis）</strong>来持续更新学习到的形状模式，使得模型在处理新场景时无需从头学习形状信息。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>性能提升：</strong> 在多个数据集（如Replica和ScanNet）上的实验表明，该方法在<strong>分割精度和渲染质量</strong>上均取得了显著提升。</li>
<li><strong>效率保持：</strong> 尽管增强了语义和渲染能力，该方法仍能<strong>保持高渲染帧率</strong>，显示了其高效性。</li>
<li><strong>鲁棒性增强：</strong> 通过引入3D几何信息和跨场景知识迁移，模型对<strong>细微形状差异和复杂几何结构</strong>的识别能力得到增强，并且在处理新场景时表现出更好的鲁棒性。</li>
<li><strong>意义：</strong> 该工作展示了<strong>联合优化语义和渲染分支</strong>的潜力，并提出了一种<strong>利用3D几何信息增强语义理解和渲染</strong>的新范式，为3D场景理解和生成领域提供了新的思路。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<p>论文中虽然没有明确列出“局限性”部分，但从方法和实验结果中可以推断出一些潜在的方面：</p>
<ul>
<li><strong>对2D监督的依赖：</strong> 尽管论文强调减少对2D监督的依赖，但其方法仍需要2D语义特征（例如通过预训练模型提取）作为输入，因此完全摆脱2D监督可能仍是未来的挑战。</li>
<li><strong>计算复杂度：</strong> 引入各向异性描述符和Transformer模块可能会增加一定的计算复杂度，尽管作者通过自适应调整和高效的渲染机制来缓解。</li>
<li><strong>跨场景知识迁移的普适性：</strong> 跨场景知识迁移的效果可能依赖于场景之间的相似性，对于完全不同的场景，其迁移效果可能需要进一步研究。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<p>基于该论文的研究，可以推测以下潜在的未来研究方向：</p>
<ul>
<li><strong>更精细的3D几何特征提取：</strong> 探索更先进的3D几何描述符，以捕捉更复杂的形状和拓扑信息。</li>
<li><strong>完全摆脱2D监督的语义学习：</strong> 研究仅依赖3D几何信息或无监督/自监督的方法进行语义分割。</li>
<li><strong>更高效的跨场景知识迁移：</strong> 探索更智能的模式基础更新策略，以适应更广泛的场景变化。</li>
<li><strong>实时交互式3D语义编辑：</strong> 将该框架应用于需要实时交互的3D场景编辑和内容创作任务。</li>
<li><strong>与其他3D表示的融合：</strong> 探索将该方法与点云、体素或网格等其他3D表示形式相结合的可能性。</li>
</ul>
<p>总而言之，这篇论文通过提出一个创新的联合增强框架，有效地解决了现有3D语义高斯模型在语义理解和渲染方面的不足，为该领域的研究提供了重要的贡献。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we propose a joint enhancement framework for 3D semantic Gaussian modeling that synergizes both semantic and rendering branches.</li>
<li>Firstly, unlike conventional point cloud shape encoding, we introduce an anisotropic 3D Gaussian Chebyshev descriptor using the Laplace-Beltrami operator to capture fine-grained 3D shape details, thereby distinguishing objects with similar appearances and reducing reliance on potentially noisy 2D guidance.</li>
<li>Finally, we employ a cross-scene knowledge transfer module to continuously update learned shape patterns, enabling faster convergence and robust representations without relearning shape information from scratch for each new scene.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.02339v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.02339v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.02315v1'></a></p>
<h2 id="prithvi-complimentary-adaptive-fusion-encoder-cafe-unlocking-full-potential-for-flood-inundation-mapping"><a href="https://arxiv.org/abs/2601.02315v1">Prithvi-Complimentary Adaptive Fusion Encoder (CAFE): unlocking full-potential for flood inundation mapping</a></h2>
<p><strong>Authors:</strong> Saurabh Kaushik, Lalit Maurya, Beth Tellman</p>
<p><strong>Published:</strong> 2026-01-05</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Geo-Foundation Models (GFMs), have proven effective in diverse downstream applications, including semantic segmentation, classification, and regression tasks. However, in case of flood mapping using Sen1Flood11 dataset as a downstream task, GFMs struggles to outperform the baseline U-Net, highlighting model's limitation in capturing critical local nuances. To address this, we present the Prithvi-Complementary Adaptive Fusion Encoder (CAFE), which integrate Prithvi GFM pretrained encoder with a parallel CNN residual branch enhanced by Convolutional Attention Modules (CAM). Prithvi-CAFE enables fast and efficient fine-tuning through adapters in Prithvi and performs multi-scale, multi-level fusion with CNN features, capturing critical local details while preserving long-range dependencies. We achieve state-of-the-art results on two comprehensive flood mapping datasets: Sen1Flood11 and FloodPlanet. On Sen1Flood11 test data, Prithvi-CAFE (IoU 83.41) outperforms the original Prithvi (IoU 82.50) and other major GFMs (TerraMind 82.90, DOFA 81.54, spectralGPT: 81.02). The improvement is even more pronounced on the hold-out test site, where Prithvi-CAFE achieves an IoU of 81.37 compared to the baseline U-Net (70.57) and original Prithvi (72.42). On FloodPlanet, Prithvi-CAFE also surpasses the baseline U-Net and other GFMs, achieving an IoU of 64.70 compared to U-Net (60.14), Terramind (62.33), DOFA (59.15) and Prithvi 2.0 (61.91). Our proposed simple yet effective Prithvi-CAFE demonstrates strong potential for improving segmentation tasks where multi-channel and multi-modal data provide complementary information and local details are critical. The code is released on \href{https://github.com/Sk-2103/Prithvi-CAFE}{Prithvi-CAFE Github}</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Prithvi-Complimentary Adaptive Fusion Encoder (CAFE): unlocking full-potential for flood inundation mapping”的全面摘要，重点关注其在计算机视觉领域的贡献：</p>
<p><strong>论文摘要：Prithvi-Complimentary Adaptive Fusion Encoder (CAFE) 助力洪水淹没测绘的全部潜力</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>该论文旨在解决当前地理空间基础模型（Geo-Foundation Models, GFMs）在洪水淹没测绘任务中表现不佳的问题。尽管GFMs在语义分割、分类和回归等多种下游任务中表现出色，但在使用Sen1Flood11数据集进行洪水测绘时，它们往往难以超越经典的U-Net模型。这表明GFMs在捕捉洪水淹没区域的关键局部细节方面存在局限性。</p>
<p><strong>2. 主要创新/方法贡献：</strong></p>
<p>为了克服这一挑战，作者提出了<strong>Prithvi-Complimentary Adaptive Fusion Encoder (CAFE)</strong> 模型。其核心创新点在于：</p>
<ul>
<li><strong>混合架构设计：</strong> CAFE整合了强大的<strong>Prithvi GFM预训练编码器</strong>与一个并行的<strong>CNN残差分支</strong>，该分支增强了<strong>卷积注意力模块（CAM）</strong>。</li>
<li><strong>高效微调：</strong> 利用<strong>适配器（adapters）</strong>技术对Prithvi编码器进行参数高效的微调，显著减少了训练参数量（从6.5亿降至4550万），同时保持了预训练模型的优势。</li>
<li><strong>多尺度、多层次特征融合：</strong> CAFE通过<strong>多尺度、多层次特征注意力融合（M²FAF）模块</strong>，有效地融合了来自Transformer（Prithvi）的全局长程依赖信息和来自CNN的精细局部空间细节。</li>
<li><strong>通道扩展能力：</strong> 该模型能够处理<strong>任意数量的输入通道</strong>，克服了原始Prithvi模型仅支持六个光谱通道的限制，使其能更好地利用多光谱和多模态数据。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<p>CAFE在两个重要的洪水测绘数据集上取得了<strong>最先进（State-of-the-Art, SoTA）的成果</strong>：</p>
<ul>
<li><strong>Sen1Flood11数据集：</strong><ul>
<li>在测试数据上，CAFE达到了<strong>83.41的IoU</strong>，超越了原始Prithvi（82.50）以及TerraMind（82.90）、DOFA（81.54）等其他主流GFMs。</li>
<li>在地理上<strong>未见过的测试区域</strong>（如玻利维亚），CAFE的IoU达到了<strong>81.37</strong>，显著优于基线U-Net（70.57）和原始Prithvi（72.42），证明了其强大的空间迁移能力。</li>
</ul>
</li>
<li><strong>FloodPlanet数据集：</strong><ul>
<li>CAFE取得了<strong>64.70的IoU</strong>，同样超越了U-Net（60.14）、TerraMind（62.33）、DOFA（59.15）和Prithvi 2.0（61.91）等模型。</li>
</ul>
</li>
<li><strong>意义：</strong> 这些结果表明，CAFE能够有效地融合不同来源和尺度的信息，捕捉洪水淹没区域的关键细节，从而在洪水测绘等对局部信息要求极高的任务中展现出强大的潜力。其高效的微调方法也降低了使用大型GFMs的门槛。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>密集云层：</strong> 在处理<strong>密集云层覆盖</strong>的场景时，模型仍然会遇到挑战，导致部分区域的误分类。尽管如此，在这些困难条件下，CAFE的表现仍优于其他模型。</li>
<li><strong>局部细节捕捉：</strong> 尽管CAFE在局部细节捕捉方面有所提升，但论文也指出，大型GFMs在捕捉<strong>非常精细的局部信息</strong>方面仍有提升空间。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>SAR数据融合：</strong> 论文建议可以考虑<strong>整合SAR（合成孔径雷达）数据</strong>作为输入，因为SAR数据能提供在云层条件下有用的信息，并且CAFE模型能够处理任意数量的通道。</li>
<li><strong>进一步提升局部细节捕捉：</strong> 尽管CAFE表现出色，但未来仍可探索更先进的技术来进一步提升模型在捕捉最精细局部细节方面的能力。</li>
</ul>
<p><strong>总结：</strong></p>
<p>Prithvi-CAFE模型通过巧妙地结合强大的预训练Transformer编码器（Prithvi）和增强的CNN分支，并采用高效的适配器微调技术，成功解决了现有GFMs在洪水淹没测绘任务中捕捉局部细节不足的问题。该模型在多个基准数据集上取得了显著的性能提升，尤其是在地理上未见过的区域，证明了其强大的泛化能力和空间迁移能力。CAFE的贡献在于提供了一种简单而有效的方法，能够充分利用多通道、多模态数据，在需要精细局部信息和长程依赖的分割任务中取得优异表现，为地理空间基础模型的应用开辟了新的可能性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we present the Prithvi-Complementary Adaptive Fusion Encoder (CAFE), which integrate Prithvi GFM pretrained encoder with a parallel CNN residual branch enhanced by Convolutional Attention Modules (CAM).</li>
<li>We achieve state-of-the-art results on two comprehensive flood mapping datasets: Sen1Flood11 and FloodPlanet.</li>
<li>On Sen1Flood11 test data, Prithvi-CAFE (IoU 83.41) outperforms the original Prithvi (IoU 82.50) and other major GFMs (TerraMind 82.90, DOFA 81.54, spectralGPT: 81.02).</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.02315v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.02315v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.02309v1'></a></p>
<h2 id="360dvo-deep-visual-odometry-for-monocular-360-degree-camera"><a href="https://arxiv.org/abs/2601.02309v1">360DVO: Deep Visual Odometry for Monocular 360-Degree Camera</a></h2>
<p><strong>Authors:</strong> Xiaopeng Guo, Yinzhe Xu, Huajian Huang, Sai-Kit Yeung</p>
<p><strong>Published:</strong> 2026-01-05</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Monocular omnidirectional visual odometry (OVO) systems leverage 360-degree cameras to overcome field-of-view limitations of perspective VO systems. However, existing methods, reliant on handcrafted features or photometric objectives, often lack robustness in challenging scenarios, such as aggressive motion and varying illumination. To address this, we present 360DVO, the first deep learning-based OVO framework. Our approach introduces a distortion-aware spherical feature extractor (DAS-Feat) that adaptively learns distortion-resistant features from 360-degree images. These sparse feature patches are then used to establish constraints for effective pose estimation within a novel omnidirectional differentiable bundle adjustment (ODBA) module. To facilitate evaluation in realistic settings, we also contribute a new real-world OVO benchmark. Extensive experiments on this benchmark and public synthetic datasets (TartanAir V2 and 360VO) demonstrate that 360DVO surpasses state-of-the-art baselines (including 360VO and OpenVSLAM), improving robustness by 50% and accuracy by 37.5%. Homepage: https://chris1004336379.github.io/360DVO-homepage</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行分析。</p>
<p><strong>论文摘要分析：360DVO: Deep Visual Odometry for Monocular 360-Degree Camera</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话）：</strong></p>
<p>该论文提出了360DVO，这是首个基于深度学习的单目全景视觉里程计（OVO）框架。它引入了一种能够自适应学习抗畸变特征的球形特征提取器（DAS-Feat），并结合了一个新颖的全景可微分束调整（ODBA）模块，以实现更鲁棒和精确的位姿估计。该研究还发布了一个新的真实世界OVO基准数据集，以促进该领域的评估。</p>
<p><strong>2. 关键创新或方法论：</strong></p>
<ul>
<li><strong>失真感知球形特征提取器 (DAS-Feat):</strong> 这是论文的核心创新之一。传统的视觉里程计方法在处理全景图像时，由于其固有的几何畸变（尤其是在赤道附近），往往会遇到困难。DAS-Feat 旨在学习能够抵抗这种畸变的特征，这意味着它能够从全景图像中提取出在不同视角和不同程度畸变下都相对稳定的关键点或描述符。这对于全景图像的准确匹配至关重要。</li>
<li><strong>全景可微分束调整 (ODBA):</strong> 传统的束调整（Bundle Adjustment）算法通常是为透视相机设计的。全景图像的几何特性（如球形投影）需要专门的处理。ODBA 模块将束调整的优化过程引入了深度学习框架，并且能够直接处理全景图像的几何约束，从而实现更精确的位姿优化。可微分的特性意味着它可以与深度学习网络端到端地进行训练。</li>
<li><strong>首个深度学习驱动的全景视觉里程计框架:</strong> 尽管全景相机在克服视场限制方面有优势，但现有的OVO方法多依赖于手工特征或传统的图像处理技术，在复杂场景下鲁棒性不足。360DVO的出现标志着将深度学习的强大特征学习能力引入到全景视觉里程计领域，有望解决现有方法的瓶颈。</li>
<li><strong>新的真实世界OVO基准:</strong> 评估方法的有效性离不开高质量的数据集。发布一个新的真实世界OVO基准，能够更真实地反映全景视觉里程计在实际应用中的挑战，并为后续研究提供一个标准化的评估平台。</li>
</ul>
<p><strong>3. 对该领域的潜在影响：</strong></p>
<ul>
<li><strong>提升全景视觉里程计的性能:</strong> 360DVO通过深度学习方法，显著提高了全景视觉里程计的鲁棒性和准确性（分别提升50%和37.5%），这表明深度学习在处理全景图像的几何问题上具有巨大潜力。</li>
<li><strong>推动全景SLAM和导航的发展:</strong> 视觉里程计是SLAM（Simultaneous Localization and Mapping）和自主导航的基础。360DVO的改进将直接促进基于全景相机的SLAM系统和导航应用的性能提升，使其在更复杂的环境中更可靠。</li>
<li><strong>为全景图像处理提供新思路:</strong> DAS-Feat 和 odba 的设计思路，特别是如何处理全景图像的畸变和几何约束，可能会启发其他处理全景图像的任务，如全景图像匹配、三维重建等。</li>
<li><strong>降低对传感器和先验知识的依赖:</strong> 深度学习方法通常能从数据中学习到更通用的特征，可能减少对特定传感器模型或复杂先验知识的依赖，使系统更易于部署。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用：</strong></p>
<ul>
<li><strong>自动驾驶:</strong> 全景相机提供广阔的视野，对于自动驾驶车辆感知周围环境至关重要。360DVO可以提高自动驾驶车辆在复杂道路条件下的定位和导航精度。</li>
<li><strong>机器人导航:</strong> 在室内或室外环境中，机器人需要准确地知道自己的位置和姿态。全景相机和360DVO可以帮助机器人实现更可靠的自主导航。</li>
<li><strong>增强现实 (AR) 和虚拟现实 (VR):</strong> 精确的位姿估计是AR/VR体验的基础。360DVO可以用于构建更沉浸式和准确的AR/VR体验，尤其是在需要大范围运动追踪的场景。</li>
<li><strong>无人机 (UAV) 导航:</strong> 无人机通常配备广角或全景相机，360DVO可以帮助无人机在复杂地形或未知环境中进行自主飞行和定位。</li>
<li><strong>3D重建和场景理解:</strong> 精确的相机位姿是进行高质量3D重建和场景理解的前提。360DVO可以为基于全景图像的3D重建提供更可靠的相机轨迹。</li>
<li><strong>工业检测和监控:</strong> 在大型工业环境中，全景相机可以提供全面的监控视角。360DVO可以用于精确追踪监控设备的运动或定位。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性：</strong></p>
<ul>
<li><strong>计算成本:</strong> 深度学习模型，尤其是涉及复杂特征提取和优化的模型，通常需要较高的计算资源。虽然摘要没有直接提及，但深度学习方法的计算效率通常是一个需要考虑的因素。</li>
<li><strong>对训练数据的依赖:</strong> 深度学习方法高度依赖于训练数据的质量和数量。虽然论文提到了新的基准数据集，但其训练数据的多样性和覆盖范围可能仍然是影响模型泛化能力的一个因素。</li>
<li><strong>对特定场景的鲁棒性:</strong> 尽管论文声称提高了鲁棒性，但“挑战性场景”的定义是相对的。例如，极端的遮挡、快速的非刚性形变、极端的低光照或高动态范围（HDR）场景，可能仍然是该方法需要进一步探索的领域。</li>
<li><strong>稀疏特征的局限性:</strong> 论文提到了“稀疏特征”。稀疏特征在某些情况下可能不足以提供足够的几何约束，尤其是在纹理稀疏或重复的区域。</li>
<li><strong>单目相机的固有局限性:</strong> 即使使用了全景相机，单目系统在尺度估计上仍然存在固有挑战。摘要没有提及如何解决尺度漂移问题，这通常是单目视觉里程计的一个关键挑战。</li>
<li><strong>新基准的覆盖范围:</strong> 新发布的基准数据集虽然重要，但其规模、多样性和真实性可能需要进一步的验证和扩展，以确保其能够全面评估方法的性能。</li>
</ul>
<p>总而言之，360DVO是一项非常有前景的研究，它将深度学习的强大能力引入到全景视觉里程计领域，有望解决现有方法的瓶颈，并在多个应用领域带来显著的改进。其提出的DAS-Feat 和 odba 模块是关键的技术创新点。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we present 360DVO, the first deep learning-based OVO framework.</li>
<li>Our approach introduces a distortion-aware spherical feature extractor (DAS-Feat) that adaptively learns distortion-resistant features from 360-degree images.</li>
<li>These sparse feature patches are then used to establish constraints for effective pose estimation within a novel omnidirectional differentiable bundle adjustment (ODBA) module.</li>
<li>To facilitate evaluation in realistic settings, we also contribute a new real-world OVO benchmark.</li>
<li>Extensive experiments on this benchmark and public synthetic datasets (TartanAir V2 and 360VO) demonstrate that 360DVO surpasses state-of-the-art baselines (including 360VO and OpenVSLAM), improving robustness by 50% and accuracy by 37.5%.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.02309v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.02309v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.02281v1'></a></p>
<h2 id="infinitevggt-visual-geometry-grounded-transformer-for-endless-streams"><a href="https://arxiv.org/abs/2601.02281v1">InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams</a></h2>
<p><strong>Authors:</strong> Shuai Yuan, Yantai Yang, Xiaotian Yang, Xupeng Zhang, Zhonghao Zhao, Lingming Zhang, Zhipeng Zhang</p>
<p><strong>Published:</strong> 2026-01-05</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>The grand vision of enabling persistent, large-scale 3D visual geometry understanding is shackled by the irreconcilable demands of scalability and long-term stability. While offline models like VGGT achieve inspiring geometry capability, their batch-based nature renders them irrelevant for live systems. Streaming architectures, though the intended solution for live operation, have proven inadequate. Existing methods either fail to support truly infinite-horizon inputs or suffer from catastrophic drift over long sequences. We shatter this long-standing dilemma with InfiniteVGGT, a causal visual geometry transformer that operationalizes the concept of a rolling memory through a bounded yet adaptive and perpetually expressive KV cache. Capitalizing on this, we devise a training-free, attention-agnostic pruning strategy that intelligently discards obsolete information, effectively ``rolling'' the memory forward with each new frame. Fully compatible with FlashAttention, InfiniteVGGT finally alleviates the compromise, enabling infinite-horizon streaming while outperforming existing streaming methods in long-term stability. The ultimate test for such a system is its performance over a truly infinite horizon, a capability that has been impossible to rigorously validate due to the lack of extremely long-term, continuous benchmarks. To address this critical gap, we introduce the Long3D benchmark, which, for the first time, enables a rigorous evaluation of continuous 3D geometry estimation on sequences about 10,000 frames. This provides the definitive evaluation platform for future research in long-term 3D geometry understanding. Code is available at: https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams”的全面中文摘要，重点关注其研究问题、方法创新、主要结果、局限性和未来方向：</p>
<p><strong>论文题目：</strong> InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams</p>
<p><strong>作者：</strong> Shuai Yuan, Yantai Yang, Xiaotian Yang, Xupeng Zhang, Zhonghao Zhao, Lingming Zhang, Zhipeng Zhang</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>该论文旨在解决大规模、持久性3D视觉几何理解所面临的核心挑战：<strong>可扩展性与长期稳定性之间的矛盾</strong>。现有的离线模型（如VGGT）虽然几何能力强大，但其批处理特性使其不适用于实时系统。而现有的流式（streaming）架构虽然为实时操作而设计，却普遍存在不足：要么无法支持真正的无限视界（infinite-horizon）输入，要么在长序列处理中出现灾难性的漂移（catastrophic drift）。这使得对长期3D几何理解系统的严格评估变得困难。</p>
<p><strong>2. 关键创新/方法贡献：</strong></p>
<p>为了打破这一困境，作者提出了<strong>InfiniteVGGT</strong>，一个因果视觉几何Transformer模型，其核心创新在于引入了<strong>“滚动内存”（rolling memory）</strong>的概念，通过一个<strong>有界（bounded）、自适应且持续表达的KV缓存（KV cache）</strong>来实现。</p>
<ul>
<li><strong>训练无关的注意力无关（attention-agnostic）剪枝策略：</strong> InfiniteVGGT采用一种新颖的、无需训练的策略，能够智能地丢弃过时的信息，从而有效地将内存向前“滚动”。</li>
<li><strong>基于关键点余弦相似度的多样性度量：</strong> 作者发现，直接使用注意力分数来判断重要性会遇到计算瓶颈，因为这需要物化完整的注意力矩阵，而这与FlashAttention等优化内核的原理相悖。因此，InfiniteVGGT转而利用<strong>关键点（key）的余弦相似度</strong>作为一种高效的、与注意力无关的度量，来评估令牌（token）的重要性。通过计算关键点与平均关键点向量的负余弦相似度，可以量化其多样性，从而保留最具信息量的令牌。</li>
<li><strong>层级自适应预算分配：</strong> 为了更精细地管理内存，InfiniteVGGT引入了一个<strong>层级自适应的预算分配机制</strong>。该机制根据各层在信息多样性上的差异，为其分配非均匀的存储预算。分析表明，浅层在空间推理中放大细微的帧间差异，多样性较高；而深层则趋于整体语义理解，多样性较低。通过这种方式，模型能够更有效地利用有限的内存。</li>
<li><strong>不可变锚点令牌（Immutable Anchor Token）：</strong> 为了保证几何一致性，InfiniteVGGT将第一帧的KV缓存作为<strong>不可变的锚点集</strong>，不受剪枝策略的影响，确保了整个重建过程的全局参考系。</li>
<li><strong>与FlashAttention的兼容性：</strong> 该方法完全兼容FlashAttention，确保了在处理大量数据时仍能保持低延迟和高效的GPU内存使用。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>无限视界流式3D几何理解：</strong> InfiniteVGGT成功实现了无限视界（infinite-horizon）的流式3D几何估计，克服了现有方法的内存溢出（OOM）问题和长期漂移。</li>
<li><strong>性能超越：</strong> 在长序列场景下，InfiniteVGGT在3D重建的准确性（Acc.）、完整性（Comp.）和法线一致性（NC）等指标上，显著优于CUT3R和TTT3R等领先的流式方法。</li>
<li><strong>Long3D基准的引入：</strong> 为了解决缺乏长期、连续3D几何评估基准的难题，论文提出了<strong>Long3D基准</strong>。该基准包含约10,000帧的连续序列，为未来长期3D几何理解的研究提供了首个严格的评估平台。</li>
<li><strong>鲁棒性：</strong> InfiniteVGGT在不同数据集和序列长度下都表现出高度的鲁棒性。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li>在<strong>平均完成度（Comp.）指标</strong>上，InfiniteVGGT在某些场景下略逊于基线方法。作者将其视为未来工作的一个优化方向。</li>
<li>虽然论文强调了其在长序列上的优势，但对于<strong>短序列（50-100帧）</strong>，其性能与基线方法相比差异不大，甚至在NC指标上略有优势，这得益于其多样性驱动的内存机制。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li>进一步优化在<strong>完成度（Comp.）指标</strong>上的表现。</li>
<li>探索更广泛的应用场景，例如在机器人导航、增强现实等领域。</li>
<li>研究如何进一步提升模型在<strong>动态场景</strong>下的处理能力。</li>
<li>利用Long3D基准进行更深入的长期3D几何理解研究。</li>
</ul>
<p>总而言之，InfiniteVGGT通过创新的滚动内存机制和高效的剪枝策略，成功解决了流式3D几何理解中的关键挑战，实现了无限视界下的稳定、高精度重建，并为该领域的研究提供了重要的基准和方法论。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Capitalizing on this, we devise a training-free, attention-agnostic pruning strategy that intelligently discards obsolete information, effectively ``rolling'' the memory forward with each new frame.</li>
<li>To address this critical gap, we introduce the Long3D benchmark, which, for the first time, enables a rigorous evaluation of continuous 3D geometry estimation on sequences about 10,000 frames.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.02281v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.02281v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.02267v1'></a></p>
<h2 id="diffproxy-multi-view-human-mesh-recovery-via-diffusion-generated-dense-proxies"><a href="https://arxiv.org/abs/2601.02267v1">DiffProxy: Multi-View Human Mesh Recovery via Diffusion-Generated Dense Proxies</a></h2>
<p><strong>Authors:</strong> Renke Wang, Zhenyu Zhang, Ying Tai, Jian Yang</p>
<p><strong>Published:</strong> 2026-01-05</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Human mesh recovery from multi-view images faces a fundamental challenge: real-world datasets contain imperfect ground-truth annotations that bias the models' training, while synthetic data with precise supervision suffers from domain gap. In this paper, we propose DiffProxy, a novel framework that generates multi-view consistent human proxies for mesh recovery. Central to DiffProxy is leveraging the diffusion-based generative priors to bridge the synthetic training and real-world generalization. Its key innovations include: (1) a multi-conditional mechanism for generating multi-view consistent, pixel-aligned human proxies; (2) a hand refinement module that incorporates flexible visual prompts to enhance local details; and (3) an uncertainty-aware test-time scaling method that increases robustness to challenging cases during optimization. These designs ensure that the mesh recovery process effectively benefits from the precise synthetic ground truth and generative advantages of the diffusion-based pipeline. Trained entirely on synthetic data, DiffProxy achieves state-of-the-art performance across five real-world benchmarks, demonstrating strong zero-shot generalization particularly on challenging scenarios with occlusions and partial views. Project page: https://wrk226.github.io/DiffProxy.html</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“DiffProxy: Multi-View Human Mesh Recovery via Diffusion-Generated Dense Proxies”的全面中文摘要：</p>
<p><strong>论文题目：</strong> DiffProxy: Multi-View Human Mesh Recovery via Diffusion-Generated Dense Proxies</p>
<p><strong>作者：</strong> Renke Wang, Zhenyu Zhang, Ying Tai, Jian Yang</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题：</strong>
本文旨在解决多视角人体网格恢复（Human Mesh Recovery, HMR）中的一个核心挑战：真实世界数据集的标注不完美，容易引入模型训练偏差，而具有精确监督的合成数据又存在领域差距（domain gap）。如何有效利用合成数据的精确性，同时克服领域差距，实现对真实世界图像的鲁棒泛化，是本文研究的关键问题。</p>
<p><strong>2. 主要创新点和方法贡献：</strong>
DiffProxy 提出了一种新颖的框架，通过生成多视角一致的人体代理（proxies）来辅助网格恢复。其核心创新在于利用<strong>扩散模型（diffusion models）的生成先验</strong>来弥合合成训练与真实世界泛化之间的鸿沟。具体而言，DiffProxy 包含以下关键技术：</p>
<ul>
<li><strong>多条件机制生成多视角一致的像素对齐人体代理：</strong> 利用预训练的扩散模型，结合文本、像素对齐特征和多视角几何约束（如外极线注意力），生成精确且在不同视角下保持一致的人体代理（包括身体部位分割和 UV 坐标）。</li>
<li><strong>手部精炼模块：</strong> 针对手部细节易受低分辨率影响的问题，引入一个专门的手部精炼模块，通过将手部区域裁剪作为额外输入视图，来提升手指级别的精度。</li>
<li><strong>不确定性感知测试时域缩放（Uncertainty-Aware Test-Time Scaling）：</strong> 利用扩散模型的随机性，通过采样多个代理预测，估计像素级不确定性。这些不确定性被用来生成权重图，在网格拟合过程中对不可靠的预测区域进行降权，从而提高鲁棒性。</li>
</ul>
<p>该框架完全在合成数据上进行训练，避免了真实数据标注的偏差。</p>
<p><strong>3. 主要结果及其意义：</strong>
DiffProxy 在五个真实世界数据集上取得了<strong>最先进（state-of-the-art）的性能</strong>，尤其在具有遮挡和部分视图的挑战性场景下表现出色，展示了强大的<strong>零样本泛化（zero-shot generalization）能力</strong>。这意味着该方法能够直接应用于未曾见过的真实世界数据，而无需进行额外的微调。其意义在于：</p>
<ul>
<li><strong>克服了真实数据标注的局限性：</strong> 通过完全依赖合成数据训练，消除了真实数据中固有的标注偏差。</li>
<li><strong>实现了强大的跨领域泛化能力：</strong> 有效地弥合了合成数据与真实数据之间的领域差距。</li>
<li><strong>提升了人体网格恢复的精度和鲁棒性：</strong> 特别是在处理复杂姿势、部分可见和遮挡等具有挑战性的情况时。</li>
<li><strong>为结构化预测任务提供了新范式：</strong> 表明扩散模型在生成精确的中间表示（如密集对应）方面具有巨大潜力，可以应用于其他难以获取精确标注的计算机视觉任务。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>推理速度：</strong> 扩散生成器需要进行多次去噪步骤，并且网格拟合也需要迭代优化，导致推理时间相对较长（约 120 秒/主体）。
*   <strong>多视角依赖性：</strong> 该方法需要多个视角才能获得可靠的结果，单视角性能会受到深度模糊的影响。
*   <strong>单主体场景：</strong> 目前的研究主要集中在单主体的人体网格恢复。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>加速推理：</strong> 探索一致性模型或知识蒸馏等技术来提高推理速度。
*   <strong>单视角场景：</strong> 将框架扩展到单视角人体网格恢复。
*   <strong>多主体场景：</strong> 扩展到多主体场景，可能需要引入实例分割作为额外模态，并解决跨视角身份关联的挑战。</p>
<p>总而言之，DiffProxy 是一项重要的工作，它巧妙地利用了扩散模型的强大生成能力，通过生成高质量、多视角一致的人体代理，成功地在完全合成数据上训练了一个能够泛化到真实世界的人体网格恢复模型，并在多个基准测试中取得了领先的性能。这为解决其他需要精确监督但难以获取真实数据的计算机视觉问题提供了新的思路。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we propose DiffProxy, a novel framework that generates multi-view consistent human proxies for mesh recovery.</li>
<li>Trained entirely on synthetic data, DiffProxy achieves state-of-the-art performance across five real-world benchmarks, demonstrating strong zero-shot generalization particularly on challenging scenarios with occlusions and partial views.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.02267v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.02267v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.02249v1'></a></p>
<h2 id="slgnet-synergizing-structural-priors-and-language-guided-modulation-for-multimodal-object-detection"><a href="https://arxiv.org/abs/2601.02249v1">SLGNet: Synergizing Structural Priors and Language-Guided Modulation for Multimodal Object Detection</a></h2>
<p><strong>Authors:</strong> Xiantai Xiang, Guangyao Zhou, Zixiao Wen, Wenshuai Li, Ben Niu, Feng Wang, Lijia Huang, Qiantong Wang, Yuhan Liu, Zongxu Pan, Yuxin Hu</p>
<p><strong>Published:</strong> 2026-01-05</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Multimodal object detection leveraging RGB and Infrared (IR) images is pivotal for robust perception in all-weather scenarios. While recent adapter-based approaches efficiently transfer RGB-pretrained foundation models to this task, they often prioritize model efficiency at the expense of cross-modal structural consistency. Consequently, critical structural cues are frequently lost when significant domain gaps arise, such as in high-contrast or nighttime environments. Moreover, conventional static multimodal fusion mechanisms typically lack environmental awareness, resulting in suboptimal adaptation and constrained detection performance under complex, dynamic scene variations. To address these limitations, we propose SLGNet, a parameter-efficient framework that synergizes hierarchical structural priors and language-guided modulation within a frozen Vision Transformer (ViT)-based foundation model. Specifically, we design a Structure-Aware Adapter to extract hierarchical structural representations from both modalities and dynamically inject them into the ViT to compensate for structural degradation inherent in ViT-based backbones. Furthermore, we propose a Language-Guided Modulation module that exploits VLM-driven structured captions to dynamically recalibrate visual features, thereby endowing the model with robust environmental awareness. Extensive experiments on the LLVIP, FLIR, KAIST, and DroneVehicle datasets demonstrate that SLGNet establishes new state-of-the-art performance. Notably, on the LLVIP benchmark, our method achieves an mAP of 66.1, while reducing trainable parameters by approximately 87% compared to traditional full fine-tuning. This confirms SLGNet as a robust and efficient solution for multimodal perception.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“SLGNet: Synergizing Structural Priors and Language-Guided Modulation for Multimodal Object Detection”的全面中文摘要：</p>
<p><strong>论文摘要：SLGNet - 融合结构先验与语言引导的调制以实现多模态目标检测</strong></p>
<p><strong>1. 研究问题与动机</strong></p>
<p>在全天候场景下，利用可见光（RGB）和红外（IR）图像进行多模态目标检测至关重要。然而，现有的基于适配器（adapter）的方法虽然能高效地将RGB预训练的基础模型迁移到多模态任务，但往往牺牲了跨模态的结构一致性。这导致在存在显著领域差异（如高对比度或夜间环境）时，关键的结构信息会丢失。此外，传统的静态多模态融合机制缺乏环境感知能力，在复杂多变的场景下适应性不足，检测性能受限。</p>
<p><strong>2. 核心创新与方法贡献</strong></p>
<p>为了解决上述问题，本文提出了SLGNet，一个参数高效的框架，它在一个冻结的Vision Transformer (ViT) 基础模型中协同融合了<strong>层级结构先验</strong>和<strong>语言引导的调制</strong>。</p>
<ul>
<li><strong>结构感知适配器 (Structure-Aware Adapter)</strong>：该模块旨在从RGB和IR模态中提取层级结构表示，并将其动态注入到ViT中，以补偿ViT骨干网络固有的结构退化问题。这有助于保留几何细节，提高定位精度，尤其是在对结构敏感的航空遥感任务中。</li>
<li><strong>语言引导调制 (Language-Guided Modulation, LGM)</strong>：该模块利用<strong>视觉语言模型 (VLM)</strong> 生成的结构化文本描述（包括环境、场景、物体密度和热信号等维度），动态地重新校准视觉特征。这使得模型能够理解场景动态，并根据环境因素自适应地调整模态融合策略，从而增强模型的环境感知能力。</li>
</ul>
<p>SLGNet采用适配器微调（adapter tuning）范式，仅训练少量适配器参数，而冻结强大的预训练ViT骨干网络，从而在保持模型效率的同时，有效适应多模态任务。</p>
<p><strong>3. 主要结果与意义</strong></p>
<ul>
<li><strong>性能提升</strong>：在LLVIP、FLIR、KAIST和Drone Vehicle四个基准数据集上进行了广泛实验，SLGNet均取得了<strong>新的最先进性能 (state-of-the-art)</strong>。</li>
<li><strong>LLVIP数据集上的突出表现</strong>：在LLVIP基准上，SLGNet实现了<strong>66.1%的mAP</strong>，显著优于现有方法。</li>
<li><strong>参数效率</strong>：与传统的全模型微调（full fine-tuning）相比，SLGNet将可训练参数量<strong>减少了约87%</strong>，同时在FLIR和Drone Vehicle数据集上取得了更高的mAP和mAP50。例如，在FLIR数据集上，SLGNet的mAP比基线模型提升了2.8，而参数量减少了约95%。</li>
<li><strong>鲁棒性与泛化性</strong>：实验证明，SLGNet在低光照、复杂背景、尺度变化和模态错位等挑战性场景下表现出强大的鲁棒性和泛化能力。</li>
</ul>
<p>这些结果表明，SLGNet是一种<strong>鲁棒且高效</strong>的多模态感知解决方案，成功地在检测精度和训练效率之间取得了优异的平衡。</p>
<p><strong>4. 提及的局限性</strong></p>
<p>论文中提到，在处理某些具有高度视觉模糊性的类别（如Van）时，SLGNet的性能略有下降，这可能归因于其对显式结构线索的依赖。相比之下，一些方法可能利用更灵活但可解释性较差的特征交互来处理这些模糊类别。</p>
<p>此外，论文在实验设置中提到，VLM的上下文生成是在<strong>离线</strong>进行的，以模拟实际部署场景中VLM周期性更新调制参数而视觉检测器实时运行的情况。这暗示了实时VLM推理可能是一个需要考虑的方面。</p>
<p><strong>5. 未来研究方向</strong></p>
<p>论文的结论部分指出了未来的研究方向：</p>
<ul>
<li><strong>云边协同架构</strong>：探索如何缓解VLM推理的开销，例如通过云边协同架构，将VLM推理部署在云端，而检测器部署在边缘设备上。</li>
<li><strong>异步执行策略</strong>：实现一种异步执行策略，其中云端的VLM定期更新语义先验以指导实时边缘检测器。</li>
<li><strong>高层推理与工业级效率的平衡</strong>：进一步探索如何将大型基础模型的高层推理能力与实时工业级应用所需的效率相结合。</li>
</ul>
<p>总而言之，SLGNet通过巧妙地结合结构感知适配器和语言引导调制，有效地解决了多模态目标检测中的结构退化和环境适应性问题，并在保持参数效率的同时取得了最先进的性能。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these limitations, we propose SLGNet, a parameter-efficient framework that synergizes hierarchical structural priors and language-guided modulation within a frozen Vision Transformer (ViT)-based foundation model.</li>
<li>Furthermore, we propose a Language-Guided Modulation module that exploits VLM-driven structured captions to dynamically recalibrate visual features, thereby endowing the model with robust environmental awareness.</li>
<li>Extensive experiments on the LLVIP, FLIR, KAIST, and DroneVehicle datasets demonstrate that SLGNet establishes new state-of-the-art performance.</li>
<li>Notably, on the LLVIP benchmark, our method achieves an mAP of 66.1, while reducing trainable parameters by approximately 87% compared to traditional full fine-tuning.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.02249v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.02249v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.02242v1'></a></p>
<h2 id="vibe-visual-instruction-based-editor"><a href="https://arxiv.org/abs/2601.02242v1">VIBE: Visual Instruction Based Editor</a></h2>
<p><strong>Authors:</strong> Grigorii Alekseenko, Aleksandr Gordeev, Irina Tolstykh, Bulat Suleimanov, Vladimir Dokholyan, Georgii Fedorov, Sergey Yakubson, Aleksandra Tsybina, Mikhail Chernyshov, Maksim Kuprashevich</p>
<p><strong>Published:</strong> 2026-01-05</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Instruction-based image editing is among the fastest developing areas in generative AI. Over the past year, the field has reached a new level, with dozens of open-source models released alongside highly capable commercial systems. However, only a limited number of open-source approaches currently achieve real-world quality. In addition, diffusion backbones, the dominant choice for these pipelines, are often large and computationally expensive for many deployments and research settings, with widely used variants typically containing 6B to 20B parameters. This paper presents a compact, high-throughput instruction-based image editing pipeline that uses a modern 2B-parameter Qwen3-VL model to guide the editing process and the 1.6B-parameter diffusion model Sana1.5 for image generation. Our design decisions across architecture, data processing, training configuration, and evaluation target low-cost inference and strict source consistency while maintaining high quality across the major edit categories feasible at this scale. Evaluated on the ImgEdit and GEdit benchmarks, the proposed method matches or exceeds the performance of substantially heavier baselines, including models with several times as many parameters and higher inference cost, and is particularly strong on edits that require preserving the input image, such as an attribute adjustment, object removal, background edits, and targeted replacement. The model fits within 24 GB of GPU memory and generates edited images at up to 2K resolution in approximately 4 seconds on an NVIDIA H100 in BF16, without additional inference optimizations or distillation.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“VIBE: Visual Instruction Based Editor”的全面中文摘要：</p>
<p><strong>论文题目：</strong> VIBE: Visual Instruction Based Editor</p>
<p><strong>作者：</strong> Grigorii Alekseenko, Aleksandr Gordeev, Irina Tolstykh, Bulat Suleimanov, Vladimir Dokholyan, Georgii Fedorov, Sergey Yakubson, Aleksandra Tsybina, Mikhail Chernyshov, Maksim Kuprashevich</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/主要挑战：</strong>
指令驱动的图像编辑是生成式AI领域一个快速发展的方向。尽管涌现了大量开源模型和商业系统，但真正达到“真实世界质量”的开源方法仍然有限。此外，目前主流的扩散模型（通常包含6B至20B参数）体积庞大，计算成本高昂，限制了其在许多部署和研究场景中的应用。本文旨在解决如何构建一个<strong>紧凑、高吞吐量且低成本</strong>的指令驱动图像编辑流水线，同时保持<strong>高质量的编辑效果和严格的源图像一致性</strong>。</p>
<p><strong>2. 关键创新/方法贡献：</strong>
VIBE的核心创新在于其<strong>高效的架构设计和精心的训练策略</strong>，以实现低成本和高质量的平衡：</p>
<ul>
<li>
<p><strong>紧凑高效的架构：</strong></p>
<ul>
<li><strong>轻量级VLM指导：</strong> 使用一个2B参数的Qwen3-VL模型来理解用户指令和输入图像，生成图像感知强的条件信号。</li>
<li><strong>高效的扩散模型：</strong> 采用1.6B参数的Sana1.5扩散模型进行图像生成。</li>
<li><strong>通道级参考图像引导：</strong> 利用通道级拼接（channel-wise concatenation）将参考图像的潜在表示注入到扩散模型中，避免了增加序列长度带来的计算开销。</li>
<li><strong>元令牌（Meta Tokens）和VLM连接器：</strong> 引入可学习的元令牌，并设计了一个轻量级的Transformer连接器，将VLM的输出映射到扩散模型的条件空间，实现了VLM与扩散模型的高效接口。</li>
</ul>
</li>
<li>
<p><strong>四阶段训练流水线：</strong></p>
<ul>
<li><strong>对齐（Alignment）：</strong> 训练VLM与扩散模型的潜在空间对齐，使用文本到图像的目标函数。</li>
<li><strong>预训练（Pre-training）：</strong> 通过添加图像到图像的任务，在大量数据上学习核心编辑能力。</li>
<li><strong>监督微调（Supervised Fine-Tuning, SFT）：</strong> 在高质量、干净的三元组数据上进行精细调优。</li>
<li><strong>直接偏好优化（Direct Preference Optimization, DPO）：</strong> 利用高质量的偏好数据，进一步对齐模型以满足真实世界指令和美学要求。</li>
</ul>
</li>
<li>
<p><strong>数据处理和质量控制：</strong></p>
<ul>
<li><strong>真实世界指令：</strong> 收集和合成更符合人类表达习惯的指令，而非模板化或纯LLM生成的提示。</li>
<li><strong>混合数据策略：</strong> 将指令编辑三元组与高质量的文本到图像（T2I）数据混合训练，以防止模型在编辑任务上过拟合而损害其基础生成能力。</li>
<li><strong>严格的质量过滤：</strong> 采用多阶段过滤框架，包括学习到的三元组评分、面部嵌入约束和图像质量评分，以确保数据质量和源图像一致性。</li>
<li><strong>数据增强：</strong> 利用三元组反转（triplet inversion）和自举（bootstrapping）等技术，降低数据成本并提高鲁棒性。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>性能超越：</strong> 在ImgEdit和GEdit基准测试中，VIBE的性能<strong>匹配甚至超越了参数量和推理成本数倍于自身的大型基线模型</strong>。
*   <strong>优势领域：</strong> VIBE在需要<strong>严格保留输入图像信息</strong>的编辑任务上表现尤为出色，例如属性调整、对象移除、背景编辑和目标替换。
*   <strong>效率和可部署性：</strong> 模型<strong>仅需24GB GPU内存</strong>，在NVIDIA H100上以BF16精度<strong>约4秒即可生成2K分辨率的图像</strong>，无需额外的推理优化或蒸馏。这使其非常适合低成本推理和研究场景。
*   <strong>严格源图像一致性：</strong> VIBE在保持源图像细节方面表现出色，即使在处理全局变换（如风格迁移）等具有挑战性的编辑类别时，也能有效控制非指令要求的修改。
*   <strong>贡献总结：</strong> 论文提出了一个<strong>开源、超快、紧凑</strong>的指令驱动图像编辑系统，并提供了一个<strong>灵活的四阶段训练流水线</strong>，以及对实验设计、数据收集、增强和过滤的深入分析。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>模型复杂度限制：</strong> 尽管性能优异，但VIBE的<strong>模型复杂度相对较低</strong>，对于非常复杂的编辑操作或一些<strong>高难度的美学要求</strong>，模型可能仍会失败或表现不稳定。
*   <strong>真实世界数据的挑战：</strong> 生成模型通常在<strong>生成数据上表现更好</strong>，而真实世界照片的多样性（如不同拍摄条件、老旧手机摄像头）对模型提出了更大挑战。
*   <strong>潜在偏见：</strong> 系统依赖于预训练组件、大规模开放数据和自动生成样本，<strong>可能继承这些来源中的偏见</strong>。
*   <strong>严格源一致性难度：</strong> 对于某些编辑类型，即使是更大的闭源系统也可能难以实现严格的源图像一致性，VIBE也可能出现漂移。
*   <strong>VLM冻结的影响：</strong> 为了保持其原始知识，VLM在整个流水线中被冻结，<strong>并未研究完全端到端微调VLM对最终质量的影响</strong>。</p>
<p><strong>5. 未来研究方向：</strong>
*   <strong>降低推理成本：</strong> 通过<strong>蒸馏模型以减少扩散步数</strong>，并移除CFG（Classifier-Free Guidance）。
*   <strong>量化：</strong> 应用<strong>量化技术</strong>以提高吞吐量和内存效率，可能在低端硬件上实现更快推理。
*   <strong>真实世界数据比例：</strong> <strong>增加训练数据中真实世界信号的比例</strong>，以提高模型在真实照片上的鲁棒性。
*   <strong>VLM微调：</strong> 探索<strong>部分或完全微调VLM</strong>，以研究其对保留通用知识和提升编辑特定行为的权衡。
*   <strong>更精细的几何和视觉伪影处理：</strong> 进一步研究和解决模型在生成复杂几何变化和细微视觉伪影（如棋盘格、JPEG压缩噪声）方面的挑战。</p>
<p>总而言之，VIBE论文成功地展示了如何通过创新的架构设计（紧凑模型、高效连接）和精细的训练策略（多阶段训练、严格数据过滤、真实世界指令对齐），构建一个<strong>高效、低成本且性能强大的指令驱动图像编辑系统</strong>，为该领域的研究和实际应用提供了重要贡献。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Over the past year, the field has reached a new level, with dozens of open-source models released alongside highly capable commercial systems.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.02242v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.02242v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2026-01-06 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
