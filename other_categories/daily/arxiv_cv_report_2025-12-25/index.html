<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-12-25 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-12-24/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-12-26/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-12-25">Arxiv Computer Vision Papers - 2025-12-25</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#histream-efficient-high-resolution-video-generation-via-redundancy-eliminated-streaming" class="nav-link">HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming</a>
                </li>
                <li class="nav-item">
                    <a href="#beyond-memorization-a-multi-modal-ordinal-regression-benchmark-to-expose-popularity-bias-in-vision-language-models" class="nav-link">Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models</a>
                </li>
                <li class="nav-item">
                    <a href="#streaming-video-instruction-tuning" class="nav-link">Streaming Video Instruction Tuning</a>
                </li>
                <li class="nav-item">
                    <a href="#gridit-factorized-grid-based-diffusion-for-efficient-long-image-sequence-generation" class="nav-link">GriDiT: Factorized Grid-Based Diffusion for Efficient Long Image Sequence Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#acd-direct-conditional-control-for-video-diffusion-models-via-attention-supervision" class="nav-link">ACD: Direct Conditional Control for Video Diffusion Models via Attention Supervision</a>
                </li>
                <li class="nav-item">
                    <a href="#dreamontage-arbitrary-frame-guided-one-shot-video-generation" class="nav-link">DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#lookplangraph-embodied-instruction-following-method-with-vlm-graph-augmentation" class="nav-link">LookPlanGraph: Embodied Instruction Following Method with VLM Graph Augmentation</a>
                </li>
                <li class="nav-item">
                    <a href="#leveraging-lightweight-entity-extraction-for-scalable-event-based-image-retrieval" class="nav-link">Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval</a>
                </li>
                <li class="nav-item">
                    <a href="#latent-implicit-visual-reasoning" class="nav-link">Latent Implicit Visual Reasoning</a>
                </li>
                <li class="nav-item">
                    <a href="#human-motion-estimation-with-everyday-wearables" class="nav-link">Human Motion Estimation with Everyday Wearables</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-12-25">Arxiv Computer Vision Papers - 2025-12-25</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，作为一名专注于计算机视觉和机器学习的研究助理，我将为您提供一份简明的每日报告执行摘要，涵盖2025年12月24日发表在 Arxiv 上的10篇计算机视觉领域论文。</p>
<hr />
<p><strong>每日 Arxiv 计算机视觉论文报告 - 执行摘要 (2025-12-24)</strong></p>
<p><strong>主要主题与趋势：</strong></p>
<p>本期 Arxiv 论文集中体现了计算机视觉领域在<strong>视频生成效率、多模态理解的鲁棒性、以及具身智能的指令遵循</strong>方面的显著进展。视频生成技术正朝着更高效、更高分辨率、以及更精细的控制方向发展，同时对模型在真实世界数据中的泛化能力和偏见问题也给予了更多关注。</p>
<p><strong>亮点与创新：</strong></p>
<ul>
<li>
<p><strong>视频生成效率与控制：</strong></p>
<ul>
<li><strong>HiStream</strong> 和 <strong>GriDiT</strong> 提出了创新的流式和网格化扩散模型，显著提升了高分辨率视频和长图像序列的生成效率。</li>
<li><strong>ACD</strong> 和 <strong>DreaMontage</strong> 在视频扩散模型中引入了更直接的条件控制机制，允许用户通过注意力监督或任意帧引导来实现更灵活的视频生成。</li>
<li><strong>Streaming Video Instruction Tuning</strong> 则将指令调优的概念应用于视频生成，有望使模型更好地理解和执行视频相关的指令。</li>
</ul>
</li>
<li>
<p><strong>多模态理解与偏见：</strong></p>
<ul>
<li><strong>Beyond Memorization</strong> 提出了一个多模态序数回归基准，旨在暴露视觉语言模型在流行度偏见方面的问题，强调了模型在真实世界数据上的泛化能力和公平性。</li>
</ul>
</li>
<li>
<p><strong>具身智能与指令遵循：</strong></p>
<ul>
<li><strong>LookPlanGraph</strong> 和 <strong>Latent Implicit Visual Reasoning</strong> 在具身智能和视觉推理方面展现了新的方法，前者通过图增强提升了指令遵循能力，后者则探索了隐式视觉推理的潜力。</li>
</ul>
</li>
<li>
<p><strong>应用导向的研究：</strong></p>
<ul>
<li><strong>Human Motion Estimation with Everyday Wearables</strong> 展示了将计算机视觉技术应用于可穿戴设备进行人体运动估计的实际应用价值。</li>
<li><strong>Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval</strong> 则关注于轻量级实体提取在事件图像检索中的可扩展性，体现了对高效信息检索的需求。</li>
</ul>
</li>
</ul>
<p><strong>新兴研究方向与技术：</strong></p>
<ul>
<li><strong>流式与高效视频生成：</strong> 针对视频生成的高计算成本，流式处理和因子化扩散模型（如 GriDiT）是重要的发展方向。</li>
<li><strong>精细化视频条件控制：</strong> 通过注意力监督、任意帧引导等方式实现对视频生成过程的更精确控制。</li>
<li><strong>多模态模型的公平性与鲁棒性：</strong> 关注模型在真实世界数据中的偏见问题，并开发相应的评估基准和缓解方法。</li>
<li><strong>具身智能中的视觉推理与规划：</strong> 将视觉信息与规划、推理能力相结合，以实现更智能的代理。</li>
<li><strong>轻量级模型与高效应用：</strong> 在特定应用场景下，开发轻量级模型以实现高效的部署和检索。</li>
</ul>
<p><strong>建议阅读论文：</strong></p>
<p>考虑到这些论文的创新性和对未来研究方向的潜在影响，以下论文值得深入阅读：</p>
<ol>
<li><strong>HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming</strong> (视频生成效率的突破性进展)</li>
<li><strong>Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models</strong> (对多模态模型公平性问题的深刻洞察)</li>
<li><strong>GriDiT: Factorized Grid-Based Diffusion for Efficient Long Image Sequence Generation</strong> (长序列图像生成的效率提升)</li>
<li><strong>ACD: Direct Conditional Control for Video Diffusion Models via Attention Supervision</strong> (视频生成控制的新方法)</li>
<li><strong>LookPlanGraph: Embodied Instruction Following Method with VLM Graph Augmentation</strong> (具身智能指令遵循的创新)</li>
</ol>
<hr />
<p>这份摘要旨在为忙碌的研究人员提供一个快速了解最新研究动态的窗口。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2512.21338v1">HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming</a></li>
<li><a href="#2512.21337v1">Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models</a></li>
<li><a href="#2512.21334v1">Streaming Video Instruction Tuning</a></li>
<li><a href="#2512.21276v1">GriDiT: Factorized Grid-Based Diffusion for Efficient Long Image Sequence Generation</a></li>
<li><a href="#2512.21268v1">ACD: Direct Conditional Control for Video Diffusion Models via Attention Supervision</a></li>
<li><a href="#2512.21252v1">DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation</a></li>
<li><a href="#2512.21243v1">LookPlanGraph: Embodied Instruction Following Method with VLM Graph Augmentation</a></li>
<li><a href="#2512.21221v1">Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval</a></li>
<li><a href="#2512.21218v1">Latent Implicit Visual Reasoning</a></li>
<li><a href="#2512.21209v1">Human Motion Estimation with Everyday Wearables</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2512.21338v1'></a></p>
<h2 id="histream-efficient-high-resolution-video-generation-via-redundancy-eliminated-streaming"><a href="https://arxiv.org/abs/2512.21338v1">HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming</a></h2>
<p><strong>Authors:</strong> Haonan Qiu, Shikun Liu, Zijian Zhou, Zhaochong An, Weiming Ren, Zhiheng Liu, Jonas Schult, Sen He, Shoufa Chen, Yuren Cong, Tao Xiang, Ziwei Liu, Juan-Manuel Perez-Rua</p>
<p><strong>Published:</strong> 2025-12-24</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>High-resolution video generation, while crucial for digital media and film, is computationally bottlenecked by the quadratic complexity of diffusion models, making practical inference infeasible. To address this, we introduce HiStream, an efficient autoregressive framework that systematically reduces redundancy across three axes: i) Spatial Compression: denoising at low resolution before refining at high resolution with cached features; ii) Temporal Compression: a chunk-by-chunk strategy with a fixed-size anchor cache, ensuring stable inference speed; and iii) Timestep Compression: applying fewer denoising steps to subsequent, cache-conditioned chunks. On 1080p benchmarks, our primary HiStream model (i+ii) achieves state-of-the-art visual quality while demonstrating up to 76.2x faster denoising compared to the Wan2.1 baseline and negligible quality loss. Our faster variant, HiStream+, applies all three optimizations (i+ii+iii), achieving a 107.5x acceleration over the baseline, offering a compelling trade-off between speed and quality, thereby making high-resolution video generation both practical and scalable.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话）</strong></p>
<p>该论文提出了一种名为 HiStream 的高效自回归框架，旨在解决高分辨率视频生成中扩散模型面临的计算瓶颈。通过在空间、时间和时间步长三个维度上系统地消除冗余，HiStream 实现了显著的推理加速，同时保持了出色的视觉质量，使得高分辨率视频生成在实践中变得可行且可扩展。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>HiStream 的核心创新在于其“冗余消除流式处理”的方法论，具体体现在以下三个维度：</p>
<ul>
<li><strong>空间压缩 (Spatial Compression):</strong> 这是最核心的优化之一。它采用“先低后高”的策略，即首先在低分辨率下进行去噪，然后利用缓存的低分辨率特征来指导高分辨率的精炼过程。这种方法避免了直接在高分辨率下进行所有去噪步骤，从而大大降低了计算量。</li>
<li><strong>时间压缩 (Temporal Compression):</strong> 采用“分块处理”的策略，并引入一个固定大小的“锚点缓存 (anchor cache)”。这意味着模型不会处理整个视频序列，而是以固定大小的块为单位进行生成，并且通过锚点缓存来维持时间连贯性。这种策略确保了推理速度的稳定性，避免了随着视频长度增加而导致的计算量爆炸。</li>
<li><strong>时间步长压缩 (Timestep Compression):</strong> 针对后续的、依赖于缓存的视频块，应用更少的去噪步骤。由于前一个块（或锚点）已经提供了重要的上下文信息，后续块的去噪过程可以更加高效，不需要执行完整的去噪流程。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<p>HiStream 的提出对高分辨率视频生成领域具有重大影响：</p>
<ul>
<li><strong>实用性提升:</strong> 显著的加速比（最高可达 107.5x）使得之前因计算成本过高而难以实现的 1080p 等高分辨率视频生成任务变得实际可行。</li>
<li><strong>可扩展性增强:</strong> 这种高效的框架为生成更长、更高分辨率的视频序列打开了大门，为未来的视频内容创作和应用奠定了基础。</li>
<li><strong>推动研究方向:</strong> HiStream 的成功可能会促使更多研究关注如何通过优化模型架构和推理策略来解决扩散模型在生成任务中的效率问题，而不仅仅是提升模型本身的生成能力。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>数字媒体和电影制作:</strong> 能够以更低的成本和更快的速度生成高质量的视觉内容，极大地赋能电影特效、动画制作、虚拟现实/增强现实内容创作等。</li>
<li><strong>虚拟世界和游戏开发:</strong> 生成逼真、高分辨率的动态场景和角色动画，提升用户体验。</li>
<li><strong>科学可视化和模拟:</strong> 生成高分辨率的动态模拟结果，例如流体动力学、生物过程等。</li>
<li><strong>个性化内容生成:</strong> 为用户提供定制化的视频内容，例如个性化广告、教育视频等。</li>
<li><strong>视频编辑和增强:</strong> 更快地进行视频分辨率提升、风格迁移等操作。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<p>尽管摘要强调了 HiStream 的效率和质量，但仍可以推断出一些潜在的局限性：</p>
<ul>
<li><strong>质量与速度的权衡 (Trade-off):</strong> 摘要中提到 HiStream+ (应用所有三个优化) 提供了“速度与质量之间的权衡”。这意味着在追求极致速度时，可能仍然存在一定程度的质量损失，尽管论文声称“可忽略不计的质量损失”。具体损失的程度和可接受范围需要通过实验来验证。</li>
<li><strong>“锚点缓存”的固定大小:</strong> 固定大小的锚点缓存可能在处理非常长或复杂的时间依赖性时存在限制。如果时间依赖性超出了缓存的范围，可能会影响生成的一致性。</li>
<li><strong>自回归的固有挑战:</strong> 作为一种自回归模型，其生成过程仍然是顺序的，尽管通过分块处理有所缓解，但与并行生成模型相比，可能在某些方面仍存在效率上的差异。</li>
<li><strong>对基线模型的依赖:</strong> 摘要中将加速比与“Wan2.1 baseline”进行比较。这意味着 HiStream 的实际性能和优势在很大程度上取决于所选择的基线模型。在不同的基线模型上，其相对优势可能会有所不同。</li>
<li><strong>实现复杂性:</strong> 整合了三种不同的压缩策略，其实现和调优可能比单一优化方法更复杂。</li>
</ul>
<p>总而言之，HiStream 是一项非常有前景的研究，它通过巧妙地利用视频数据中的冗余，有效地解决了高分辨率视频生成中的效率难题，为该领域带来了实际的突破。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we introduce HiStream, an efficient autoregressive framework that systematically reduces redundancy across three axes: i) Spatial Compression: denoising at low resolution before refining at high resolution with cached features; ii) Temporal Compression: a chunk-by-chunk strategy with a fixed-size anchor cache, ensuring stable inference speed; and iii) Timestep Compression: applying fewer denoising steps to subsequent, cache-conditioned chunks.</li>
<li>On 1080p benchmarks, our primary HiStream model (i+ii) achieves state-of-the-art visual quality while demonstrating up to 76.2x faster denoising compared to the Wan2.1 baseline and negligible quality loss.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.21338v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.21338v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.21337v1'></a></p>
<h2 id="beyond-memorization-a-multi-modal-ordinal-regression-benchmark-to-expose-popularity-bias-in-vision-language-models"><a href="https://arxiv.org/abs/2512.21337v1">Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models</a></h2>
<p><strong>Authors:</strong> Li-Zhong Szu-Tu, Ting-Lin Wu, Chia-Jui Chang, He Syu, Yu-Lun Liu</p>
<p><strong>Published:</strong> 2025-12-24</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We expose a significant popularity bias in state-of-the-art vision-language models (VLMs), which achieve up to 34% higher accuracy on famous buildings compared to ordinary ones, indicating a reliance on memorization over generalizable understanding. To systematically investigate this, we introduce the largest open benchmark for this task: the YearGuessr dataset, a collection of 55,546 building images with multi-modal attributes from 157 countries, annotated with continuous ordinal labels of their construction year (1001-2024), GPS data, and page-view counts as a proxy for popularity. Using this dataset, we frame the construction year prediction task as ordinal regression and introduce popularity-aware interval accuracy metrics to quantify this bias. Our resulting benchmark of 30+ models, including our YearCLIP model, confirms that VLMs excel on popular, memorized items but struggle significantly with unrecognized subjects, exposing a critical flaw in their reasoning capabilities. Project page: https://sytwu.github.io/BeyondMemo/</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models</p>
<p><strong>作者：</strong> Li-Zhong Szu-Tu, Ting-Lin Wu, Chia-Jui Chang, He Syu, Yu-Lun Liu</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
本文旨在揭示当前最先进的视觉语言模型（VLMs）在建筑年代预测任务中存在的显著“名气偏见”（popularity bias）。研究发现，这些模型在处理著名建筑时，准确率比普通建筑高出34%，这表明它们更多地依赖于对著名地标的记忆，而非真正理解建筑的结构和历史特征。这种偏见阻碍了模型在实际应用中的泛化能力和可靠性。</p>
<p><strong>2. 主要创新与方法贡献：</strong>
*   <strong>YearGuessr 数据集：</strong> 作者构建了迄今为止最大的开放式建筑年代预测数据集，名为 YearGuessr。该数据集包含来自157个国家的55,546张建筑立面图像，并附带多模态属性，包括连续的建筑年代标签（1001-2024 CE）、GPS坐标以及维基百科页面浏览量（作为衡量建筑受欢迎程度的代理指标）。
*   <strong>序数回归框架与新颖评估指标：</strong> 研究将建筑年代预测任务重新定义为序数回归问题，并引入了“受欢迎度感知区间准确率”（popularity-aware interval accuracy）等新颖的评估指标，以量化和分析模型在不同受欢迎程度建筑上的表现差异。
*   <strong>YearCLIP 模型：</strong> 作者提出了一个名为 YearCLIP 的新模型，该模型基于 CLIP，并集成了序数回归的粗粒度到细粒度策略（受 NumCLIP 启发），同时利用 GPS 坐标作为空间先验信息。YearCLIP 还通过预定义的推理提示（如屋顶、墙体类型等）来增强其解释性，使其能够提供可验证的建筑年代预测理由。</p>
<p><strong>3. 主要研究结果与意义：</strong>
*   <strong>名气偏见的存在：</strong> 通过对30多个模型的基准测试，研究证实了 VLMs 在处理流行、易于记忆的建筑时表现出色，但在面对不熟悉的建筑时则显著挣扎，暴露了其推理能力的严重缺陷。
*   <strong>序数回归的有效性：</strong> 实验表明，将年代预测视为序数回归问题，并结合多模态信息（图像、GPS、文本描述），能够显著提升模型的性能，尤其是在减少平均绝对误差（MAE）方面。
*   <strong>YearCLIP 的优势：</strong> YearCLIP 模型在 MAE 指标上取得了优于许多现有方法的成绩，并且通过提供可解释的理由，增强了模型的透明度和可信度。
*   <strong>数据集的价值：</strong> YearGuessr 数据集为建筑年代预测这一重要但被忽视的领域提供了一个大规模、多模态、全球性的开放基准，为后续研究提供了坚实的基础。</p>
<p><strong>4. 论文提及的局限性：</strong>
*   <strong>数据偏差：</strong> 数据集在地理和时间分布上存在偏差，对现代建筑的覆盖更广，而对早期风格和代表性不足地区的覆盖相对较少。
*   <strong>年代标签的局限性：</strong> 部分年代标签基于原始建造年份，即使建筑经过大规模翻新或重建，也可能引入噪声。
*   <strong>模型泛化能力：</strong> 尽管 YearCLIP 表现出色，但模型在处理极端情况（如年代久远、翻新严重或风格模糊的建筑）时仍面临挑战。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>扩展数据覆盖：</strong> 增加对非西方地区和早期建筑风格的覆盖，并考虑整合更多历史建筑记录。
*   <strong>更精细的标签：</strong> 引入更明确的翻新/重建信息和时间分割标签。
*   <strong>数据增强与去偏：</strong> 利用扩散模型等技术进行合成数据增强，并探索更有效的去偏方法。
*   <strong>主动学习与专家验证：</strong> 结合主动学习策略，并引入专家验证来提高模型的鲁棒性和可复现性。
*   <strong>多模态融合的进一步探索：</strong> 深入研究如何更有效地融合图像、地理位置、文本描述等多种模态信息，以提升模型的泛化理解能力。</p>
<p>总而言之，这篇论文通过构建 YearGuessr 数据集和提出 YearCLIP 模型，有力地揭示了当前视觉语言模型在建筑年代预测中存在的名气偏见问题，并为解决这一问题提供了新的方法和评估框架，为该领域的研究开辟了新的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We expose a significant popularity bias in state-of-the-art vision-language models (VLMs), which achieve up to 34% higher accuracy on famous buildings compared to ordinary ones, indicating a reliance on memorization over generalizable understanding.</li>
<li>To systematically investigate this, we introduce the largest open benchmark for this task: the YearGuessr dataset, a collection of 55,546 building images with multi-modal attributes from 157 countries, annotated with continuous ordinal labels of their construction year (1001-2024), GPS data, and page-view counts as a proxy for popularity.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.21337v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.21337v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.21334v1'></a></p>
<h2 id="streaming-video-instruction-tuning"><a href="https://arxiv.org/abs/2512.21334v1">Streaming Video Instruction Tuning</a></h2>
<p><strong>Authors:</strong> Jiaer Xia, Peixian Chen, Mengdan Zhang, Xing Sun, Kaiyang Zhou</p>
<p><strong>Published:</strong> 2025-12-24</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We present Streamo, a real-time streaming video LLM that serves as a general-purpose interactive assistant. Unlike existing online video models that focus narrowly on question answering or captioning, Streamo performs a broad spectrum of streaming video tasks, including real-time narration, action understanding, event captioning, temporal event grounding, and time-sensitive question answering. To develop such versatility, we construct Streamo-Instruct-465K, a large-scale instruction-following dataset tailored for streaming video understanding. The dataset covers diverse temporal contexts and multi-task supervision, enabling unified training across heterogeneous streaming tasks. After training end-to-end on the instruction-following dataset through a streamlined pipeline, Streamo exhibits strong temporal reasoning, responsive interaction, and broad generalization across a variety of streaming benchmarks. Extensive experiments show that Streamo bridges the gap between offline video perception models and real-time multimodal assistants, making a step toward unified, intelligent video understanding in continuous video streams.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Streaming Video Instruction Tuning”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Streaming Video Instruction Tuning</p>
<p><strong>作者：</strong> Jiaer Xia, Peixian Chen, Mengdan Zhang, Xing Sun, Kaiyang Zhou</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
传统视频理解模型主要针对离线、完整的视频进行分析，这与实时交互式AI助手的需求存在根本性差异。实时助手需要处理连续、无界限的视频流，并根据事件的动态发展做出即时响应，同时受到严格的延迟限制。现有模型难以满足这一需求，主要面临两大挑战：1) 如何在不丢失上下文的情况下处理连续、无界限的数据流；2) 如何在多任务场景下管理可变的响应时机和粒度，这可能需要帧级或更长时间尺度的时序推理。</p>
<p><strong>2. 主要创新点/方法论贡献：</strong>
*   <strong>Streamo 模型：</strong> 作者提出了Streamo，一个端到端的实时流媒体视频大型语言模型（LLM）。Streamo将决策制定和响应生成统一在一个模型中，通过嵌入帧级响应状态预测（Silence, Standby, Response）来实现，从而实现一次性推理，显著提高了响应时机的准确性和生成效率。
*   <strong>Streamo-Instruct-465K 数据集：</strong> 为了解决现有数据集在时序对齐和多任务监督方面存在的不一致性问题，作者构建了一个大规模、多任务的指令遵循数据集Streamo-Instruct-465K。该数据集为流媒体视频理解和交互量身定制，标准化了响应粒度，提供了统一的时序标注，并涵盖了实时叙述、动作和事件描述、时序事件定位以及时敏问答等多种任务。
*   <strong>Streamo-Bench 基准：</strong> 作者还提出了Streamo-Bench，一个全面的流媒体视频指令遵循基准，用于评估模型在多样化交互任务中的指令理解和响应能力，超越了传统QA式评估的局限性。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>性能优越：</strong> Streamo在流媒体和离线视频基准测试中均表现出色，超越了现有在线方法，展现出强大的时序推理能力、响应式交互和广泛的泛化能力。
*   <strong>弥合差距：</strong> Streamo成功地弥合了离线视频感知模型与实时多模态助手之间的差距，朝着统一、智能的连续视频流理解迈出了重要一步。
*   <strong>数据集和基准的价值：</strong> Streamo-Instruct-465K数据集为流媒体视频理解研究提供了高质量的监督信号，而Streamo-Bench则为评估和推动该领域的研究提供了标准化的平台。
*   <strong>框架的兼容性：</strong> 作者的端到端训练框架能够有效地将多种先进的离线模型转化为流媒体助手，并且在保留离线能力的同时，提升了在线性能。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>时序上下文的挑战：</strong> 尽管Streamo在准确性方面表现良好，但其当前流水线在处理流媒体视频的无界限时序上下文时，缺乏专门的长序列优化，导致序列长度增长时内存和延迟成本显著增加。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>优化长序列处理：</strong> 作者提出可以通过集成KV-cache管理、视觉令牌剪枝、滑动窗口注意力以及自适应帧压缩等技术来降低计算开销，扩展有效上下文长度，从而实现无界限的实时数据流处理。
*   <strong>更广泛的应用：</strong> Streamo的成功为开发更通用的实时AI助手奠定了基础，未来的工作可以进一步探索其在更多交互式场景中的应用。</p>
<p><strong>总结：</strong></p>
<p>这篇论文的核心贡献在于提出了Streamo模型和Streamo-Instruct-465K数据集，成功解决了实时流媒体视频理解中的关键挑战。通过创新的端到端训练框架和精心构建的数据集，Streamo实现了强大的实时交互能力，能够处理连续视频流并执行多样化的指令任务。论文不仅在技术上取得了显著进展，还为该领域的研究提供了宝贵的资源和基准，为构建更智能、更通用的实时AI助手铺平了道路。尽管存在长序列处理的局限性，但作者也提出了切实可行的未来优化方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present Streamo, a real-time streaming video LLM that serves as a general-purpose interactive assistant.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.21334v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.21334v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.21276v1'></a></p>
<h2 id="gridit-factorized-grid-based-diffusion-for-efficient-long-image-sequence-generation"><a href="https://arxiv.org/abs/2512.21276v1">GriDiT: Factorized Grid-Based Diffusion for Efficient Long Image Sequence Generation</a></h2>
<p><strong>Authors:</strong> Snehal Singh Tomar, Alexandros Graikos, Arjun Krishna, Dimitris Samaras, Klaus Mueller</p>
<p><strong>Published:</strong> 2025-12-24</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Modern deep learning methods typically treat image sequences as large tensors of sequentially stacked frames. However, is this straightforward representation ideal given the current state-of-the-art (SoTA)? In this work, we address this question in the context of generative models and aim to devise a more effective way of modeling image sequence data. Observing the inefficiencies and bottlenecks of current SoTA image sequence generation methods, we showcase that rather than working with large tensors, we can improve the generation process by factorizing it into first generating the coarse sequence at low resolution and then refining the individual frames at high resolution. We train a generative model solely on grid images comprising subsampled frames. Yet, we learn to generate image sequences, using the strong self-attention mechanism of the Diffusion Transformer (DiT) to capture correlations between frames. In effect, our formulation extends a 2D image generator to operate as a low-resolution 3D image-sequence generator without introducing any architectural modifications. Subsequently, we super-resolve each frame individually to add the sequence-independent high-resolution details. This approach offers several advantages and can overcome key limitations of the SoTA in this domain. Compared to existing image sequence generation models, our method achieves superior synthesis quality and improved coherence across sequences. It also delivers high-fidelity generation of arbitrary-length sequences and increased efficiency in inference time and training data usage. Furthermore, our straightforward formulation enables our method to generalize effectively across diverse data domains, which typically require additional priors and supervision to model in a generative context. Our method consistently outperforms SoTA in quality and inference speed (at least twice-as-fast) across datasets.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：GriDiT: Factorized Grid-Based Diffusion for Efficient Long Image Sequence Generation</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话）：</strong></p>
<p>本研究提出了一种名为 GriDiT 的新颖方法，用于高效生成长图像序列。其核心贡献在于将图像序列生成过程分解为两个阶段：首先在低分辨率下生成粗糙的序列，然后独立地对每个帧进行高分辨率超分辨率处理。这种分解方法显著提高了生成质量、序列连贯性，并带来了更快的推理速度和更少的数据需求。</p>
<p><strong>2. 关键创新或方法论：</strong></p>
<ul>
<li><strong>分解式生成（Factorized Generation）：</strong> 这是 GriDiT 最核心的创新。论文挑战了将图像序列视为单一高维张量的传统做法，而是将其分解为低分辨率序列和高分辨率帧两个独立但相互关联的部分。</li>
<li><strong>基于网格的低分辨率序列生成：</strong> 论文训练了一个生成模型（基于 Diffusion Transformer - DiT）来处理由子采样帧组成的“网格图像”。这里的“网格”可能指的是将多个帧在空间上排列成一个网格，以便 DiT 的自注意力机制能够捕捉帧间的时序关联。</li>
<li><strong>利用 Diffusion Transformer (DiT) 的自注意力机制：</strong> 尽管模型在低分辨率下操作，但 DiT 的强大自注意力机制被用来学习和捕捉图像序列中帧与帧之间的复杂时序依赖关系。这使得模型能够“扩展”一个2D图像生成器，使其具备低分辨率3D（时序）生成能力，而无需修改 DiT 的架构。</li>
<li><strong>独立帧超分辨率：</strong> 在生成低分辨率序列后，论文采用独立的超分辨率模型来增强每个帧的高分辨率细节。这种方式避免了在高分辨率下处理整个序列的计算瓶颈。</li>
</ul>
<p><strong>3. 对该领域的潜在影响：</strong></p>
<ul>
<li><strong>提高长序列生成效率：</strong> 传统的生成模型在处理长序列时面临巨大的计算和内存挑战。GriDiT 的分解方法通过在低分辨率下处理时序信息，然后在高分辨率下并行处理帧细节，有望大幅提高生成效率，使其能够生成更长、更复杂的图像序列。</li>
<li><strong>提升生成质量和连贯性：</strong> 通过将时序建模与细节增强分离，GriDiT 可能能够更有效地捕捉序列的整体动态，同时保证每个帧的视觉质量，从而实现比现有方法更高的合成质量和更好的序列连贯性。</li>
<li><strong>降低数据和计算成本：</strong> 论文提到“训练数据使用效率的提高”，这表明 GriDiT 可能不需要像传统方法那样庞大的数据集来达到同等性能，从而降低了训练成本。</li>
<li><strong>更广泛的通用性：</strong> 论文指出其方法“能够有效地泛化到不同的数据领域”，并且“通常需要额外的先验和监督才能在生成环境中建模”。这意味着 GriDiT 可能具有更强的鲁棒性和适应性，减少了对特定领域知识的依赖。</li>
<li><strong>推动视频生成和相关应用的发展：</strong> 这种高效且高质量的图像序列生成能力将直接推动视频生成、视频预测、视频编辑等相关领域的研究和应用。</li>
</ul>
<p><strong>4. 可能受益于此研究的相关领域或应用：</strong></p>
<ul>
<li><strong>视频生成（Video Generation）：</strong> 这是最直接的应用，可以用于生成电影片段、动画、虚拟场景等。</li>
<li><strong>视频预测（Video Prediction）：</strong> 预测未来帧，用于自动驾驶、机器人导航、运动分析等。</li>
<li><strong>视频编辑和增强（Video Editing and Enhancement）：</strong> 例如，为低分辨率视频添加高分辨率细节，或者进行风格迁移。</li>
<li><strong>医学影像序列分析（Medical Imaging Sequence Analysis）：</strong> 如 MRI、CT 扫描序列的生成和预测，用于疾病诊断和治疗规划。</li>
<li><strong>科学模拟可视化（Scientific Simulation Visualization）：</strong> 生成复杂物理过程或化学反应的动态过程。</li>
<li><strong>游戏开发（Game Development）：</strong> 生成游戏中的动态场景和角色动画。</li>
<li><strong>虚拟现实/增强现实（VR/AR）：</strong> 生成逼真的动态环境和交互元素。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性：</strong></p>
<ul>
<li><strong>超分辨率的独立性：</strong> 虽然独立超分辨率提高了效率，但它也可能导致序列中不同帧之间的高分辨率细节不完全一致，或者丢失帧间微妙的、依赖于时序的高分辨率变化。论文声称“序列独立的高分辨率细节”，这可能是一个优点，但也可能是一个限制，取决于具体应用对跨帧高分辨率一致性的要求。</li>
<li><strong>低分辨率建模的潜在信息丢失：</strong> 在低分辨率下建模时序信息，虽然效率高，但可能会丢失一些在原始高分辨率图像中存在的、对时序至关重要的细微信息。</li>
<li><strong>对 DiT 架构的依赖：</strong> 论文提到“不引入任何架构修改”，这意味着其低分辨率序列生成部分仍然依赖于 DiT 的能力。如果 DiT 本身在处理某些类型的时序依赖性方面存在固有局限，那么 GriDiT 也可能继承这些局限。</li>
<li><strong>“网格图像”的具体实现细节未知：</strong> 摘要中提到“grid images comprising subsampled frames”，但具体的“网格”组织方式（例如，如何将帧排列成网格，以及这种排列是否会引入新的偏见）并未详细说明，这可能影响其泛化能力。</li>
<li><strong>“任意长度”的实现细节：</strong> 论文声称能够生成“任意长度”的序列，但实际实现中可能存在对序列长度的实际限制，或者生成长序列时计算成本会随长度线性或超线性增长。</li>
</ul>
<p>总而言之，GriDiT 提出的分解式生成方法在图像序列生成领域具有重要的理论和实践意义，它通过巧妙地将时序建模与细节增强解耦，有望在效率和质量上取得突破。其对长序列生成问题的解决思路，以及在通用性方面的优势，使其成为一个值得关注的研究方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>However, is this straightforward representation ideal given the current state-of-the-art (SoTA)?</li>
<li>Observing the inefficiencies and bottlenecks of current SoTA image sequence generation methods, we showcase that rather than working with large tensors, we can improve the generation process by factorizing it into first generating the coarse sequence at low resolution and then refining the individual frames at high resolution.</li>
<li>Compared to existing image sequence generation models, our method achieves superior synthesis quality and improved coherence across sequences.</li>
<li>Furthermore, our straightforward formulation enables our method to generalize effectively across diverse data domains, which typically require additional priors and supervision to model in a generative context.</li>
<li>Our method consistently outperforms SoTA in quality and inference speed (at least twice-as-fast) across datasets.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.21276v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.21276v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.21268v1'></a></p>
<h2 id="acd-direct-conditional-control-for-video-diffusion-models-via-attention-supervision"><a href="https://arxiv.org/abs/2512.21268v1">ACD: Direct Conditional Control for Video Diffusion Models via Attention Supervision</a></h2>
<p><strong>Authors:</strong> Weiqi Li, Zehao Zhang, Liang Lin, Guangrun Wang</p>
<p><strong>Published:</strong> 2025-12-24</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Controllability is a fundamental requirement in video synthesis, where accurate alignment with conditioning signals is essential. Existing classifier-free guidance methods typically achieve conditioning indirectly by modeling the joint distribution of data and conditions, which often results in limited controllability over the specified conditions. Classifier-based guidance enforces conditions through an external classifier, but the model may exploit this mechanism to raise the classifier score without genuinely satisfying the intended condition, resulting in adversarial artifacts and limited effective controllability. In this paper, we propose Attention-Conditional Diffusion (ACD), a novel framework for direct conditional control in video diffusion models via attention supervision. By aligning the model's attention maps with external control signals, ACD achieves better controllability. To support this, we introduce a sparse 3D-aware object layout as an efficient conditioning signal, along with a dedicated Layout ControlNet and an automated annotation pipeline for scalable layout integration. Extensive experiments on benchmark video generation datasets demonstrate that ACD delivers superior alignment with conditioning inputs while preserving temporal coherence and visual fidelity, establishing an effective paradigm for conditional video synthesis.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“ACD: Direct Conditional Control for Video Diffusion Models via Attention Supervision”的全面摘要，重点关注其研究问题、方法创新、结果、局限性和未来方向：</p>
<p><strong>论文题目：</strong> ACD: Direct Conditional Control for Video Diffusion Models via Attention Supervision</p>
<p><strong>作者：</strong> Weiqi Li, Zehao Zhang, Liang Lin, Guangrun Wang</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
该论文旨在解决当前视频生成模型在<strong>可控性（controllability）</strong>方面存在的根本性挑战。现有方法，如<strong>无分类器引导（classifier-free guidance）</strong>，通过间接建模数据和条件联合分布来引入条件，导致对指定条件的控制能力有限。而<strong>基于分类器的引导（classifier-based guidance）</strong>虽然能强制条件，但模型可能通过“欺骗”分类器来提高分数，而非真正满足条件，从而产生对抗性伪影并限制有效可控性。因此，如何实现视频生成中<strong>精确、直接且语义一致的条件控制</strong>是核心研究问题。</p>
<p><strong>2. 主要创新/方法贡献：</strong>
论文提出了<strong>注意力条件扩散（Attention-Conditional Diffusion, ACD）</strong>框架，这是其核心创新。ACD 的关键在于：</p>
<ul>
<li><strong>注意力层面的监督（Attention-Level Supervision）：</strong> ACD 不再将条件施加在输出或分数层面，而是直接将模型的<strong>内部注意力图（attention maps）</strong>与外部控制信号对齐。通过这种方式，条件信息能够更直接、更深入地影响生成过程，从而实现更强的可控性。</li>
<li><strong>稀疏 3D 感知对象布局（Sparse 3D-Aware Object Layout）：</strong> 论文引入了一种新颖的条件表示——稀疏的 3D 对象布局。这种表示比密集的图像级控制更高效，能够自然地捕捉对象的几何形状和空间关系，为场景构图和相机视角提供直观的控制。</li>
<li><strong>布局控制网络（Layout ControlNet）：</strong> 为了将稀疏 3D 对象布局信息注入到扩散模型中，论文设计了一个专门的 Layout ControlNet。</li>
<li><strong>自动化标注流程（Automated Annotation Pipeline）：</strong> 为了支持大规模的布局集成，论文开发了一个自动化的标注流程，能够从真实视频中提取结构化的 3D 布局信息。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
通过在基准视频生成数据集上的广泛实验，ACD 框架展现出以下优势：</p>
<ul>
<li><strong>卓越的可控性：</strong> ACD 在与条件输入的对齐方面表现出色，能够生成更精确地遵循指定布局和相机运动的视频。</li>
<li><strong>高质量的视频生成：</strong> 在实现高可控性的同时，ACD 还能保持视频的<strong>时间连贯性（temporal coherence）</strong>和<strong>视觉保真度（visual fidelity）</strong>。</li>
<li><strong>优于现有方法：</strong> 与包括 Stable Virtual Camera、AC3D 和 ViewCrafter 在内的多种先进方法相比，ACD 在用户研究和定量评估中均取得了更高的评分，尤其是在相机引导准确性和结构语义保持方面。</li>
<li><strong>有效范式：</strong> ACD 建立了一种有效的条件视频合成新范式，为未来研究提供了新的方向。</li>
</ul>
<p><strong>4. 局限性：</strong>
论文也指出了 ACD 的一些局限性：</p>
<ul>
<li><strong>静态场景限制：</strong> 当前的稀疏 3D 对象布局主要针对<strong>静态室内场景</strong>设计，对于动态或室外环境，需要显式地建模时间动态和物体变形。</li>
<li><strong>近似对齐：</strong> 稀疏布局提供的是<strong>近似的对象放置</strong>，而非像素级别的对齐，在复杂场景下（如长距离相机轨迹）可能导致对齐不精确。</li>
<li><strong>数据依赖性：</strong> 尽管自动化标注流程有所改进，但训练仍依赖于<strong>标注好的布局数据</strong>，大规模、多样化数据集的获取仍具挑战性。</li>
</ul>
<p><strong>5. 潜在未来研究方向：</strong>
基于上述局限性，论文暗示了未来的研究方向：</p>
<ul>
<li><strong>动态场景和室外环境：</strong> 扩展 ACD 以处理动态场景和室外环境，需要更复杂的模型来捕捉时间动态和物体运动。</li>
<li><strong>更精确的几何先验：</strong> 探索更精细的几何先验，如密集深度图或场景流，以实现更精确的像素级对齐。</li>
<li><strong>弱监督或自监督策略：</strong> 研究弱监督或自监督方法，以减少对大量标注数据的依赖，从而更容易扩展到更广泛的数据集。</li>
</ul>
<p>总而言之，ACD 论文通过将条件施加在扩散模型的注意力机制上，并结合新颖的稀疏 3D 对象布局表示，显著提升了视频生成的可控性，同时保持了高质量的视觉效果，为条件视频合成领域带来了重要的进展。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we propose Attention-Conditional Diffusion (ACD), a novel framework for direct conditional control in video diffusion models via attention supervision.</li>
<li>To support this, we introduce a sparse 3D-aware object layout as an efficient conditioning signal, along with a dedicated Layout ControlNet and an automated annotation pipeline for scalable layout integration.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.21268v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.21268v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.21252v1'></a></p>
<h2 id="dreamontage-arbitrary-frame-guided-one-shot-video-generation"><a href="https://arxiv.org/abs/2512.21252v1">DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation</a></h2>
<p><strong>Authors:</strong> Jiawei Liu, Junqiao Li, Jiangfan Deng, Gen Li, Siyu Zhou, Zetao Fang, Shanshan Lao, Zengde Deng, Jianing Zhu, Tingting Ma, Jiayi Li, Yunqiu Wang, Qian He, Xinglong Wu</p>
<p><strong>Published:</strong> 2025-12-24</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>The "one-shot" technique represents a distinct and sophisticated aesthetic in filmmaking. However, its practical realization is often hindered by prohibitive costs and complex real-world constraints. Although emerging video generation models offer a virtual alternative, existing approaches typically rely on naive clip concatenation, which frequently fails to maintain visual smoothness and temporal coherence. In this paper, we introduce DreaMontage, a comprehensive framework designed for arbitrary frame-guided generation, capable of synthesizing seamless, expressive, and long-duration one-shot videos from diverse user-provided inputs. To achieve this, we address the challenge through three primary dimensions. (i) We integrate a lightweight intermediate-conditioning mechanism into the DiT architecture. By employing an Adaptive Tuning strategy that effectively leverages base training data, we unlock robust arbitrary-frame control capabilities. (ii) To enhance visual fidelity and cinematic expressiveness, we curate a high-quality dataset and implement a Visual Expression SFT stage. In addressing critical issues such as subject motion rationality and transition smoothness, we apply a Tailored DPO scheme, which significantly improves the success rate and usability of the generated content. (iii) To facilitate the production of extended sequences, we design a Segment-wise Auto-Regressive (SAR) inference strategy that operates in a memory-efficient manner. Extensive experiments demonstrate that our approach achieves visually striking and seamlessly coherent one-shot effects while maintaining computational efficiency, empowering users to transform fragmented visual materials into vivid, cohesive one-shot cinematic experiences.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation”论文的全面中文摘要，其中包含了您要求的各个部分：</p>
<p><strong>论文题目：</strong> DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation (DreaMontage：任意帧引导的单镜头视频生成)</p>
<p><strong>作者：</strong> Jiawei Liu, Junqiao Li, Jiangfan Deng, Gen Li, Siyu Zhou, Zetao Fang, Shanshan Lao, Zengde Deng, Jianing Zhu, Tingting Ma, Jiayi Li, Yunqiu Wang, Qian He, Xinglong Wu</p>
<hr />
<p><strong>全面摘要</strong></p>
<p><strong>1. 主要问题/研究问题：</strong></p>
<p>电影制作中的“单镜头”（one-shot）或“长镜头”（long take）是一种独特的、富有沉浸感的叙事美学。然而，在现实中实现单镜头视频制作成本高昂，且受限于物理空间和专业技能。尽管现有的AI视频生成模型提供了虚拟的替代方案，但它们通常采用简单的片段拼接（clip concatenation），这难以保证视频的视觉流畅性和时间连贯性，常常导致生硬的转场和不自然的运动。因此，该研究的核心问题是如何实现一种能够<strong>灵活地、任意地由用户提供的关键帧或视频片段引导，生成流畅、连贯且富有表现力的长时长单镜头视频</strong>的生成模型。</p>
<p><strong>2. 关键创新/方法论贡献：</strong></p>
<p>DreaMontage 框架通过三个主要维度解决了上述挑战：</p>
<ul>
<li>
<p><strong>轻量级中间条件注入机制（Intermediate-Conditioning Mechanism）：</strong></p>
<ul>
<li>在DiT（Diffusion Transformer）架构中集成了一种轻量级的中间条件注入方法。</li>
<li>采用<strong>自适应调优（Adaptive Tuning）</strong>策略，有效利用基础训练数据，实现了对任意帧的鲁棒控制能力。</li>
<li>针对超分辨率模型，提出了<strong>共享旋转位置编码（Shared-RoPE）</strong>策略，以解决条件帧与生成帧之间的不匹配问题，减少闪烁和跨帧颜色偏移。</li>
</ul>
</li>
<li>
<p><strong>视觉表达增强（Visual Expression Enhancement）：</strong></p>
<ul>
<li>精心策划了一个<strong>高质量、类别均衡的数据集</strong>，并实现了<strong>视觉表达监督微调（Visual Expression SFT）</strong>阶段。</li>
<li>针对画面突变（abrupt cuts）和主体运动不合理（subject motion rationality）等关键问题，设计了<strong>定制化的对比学习数据</strong>，并应用了<strong>可微分偏好优化（Tailored DPO）</strong>技术。这显著提高了生成内容的成功率和可用性。</li>
</ul>
</li>
<li>
<p><strong>内存高效的长视频生成策略（Memory-Efficient Long Video Generation）：</strong></p>
<ul>
<li>设计了<strong>分段自回归（Segment-wise Auto-Regressive, SAR）</strong>推理策略。该策略在潜在空间（latent space）中将长视频分解为多个连续的片段，通过自回归方式逐段生成，有效避免了因视频过长而导致的计算和内存瓶颈，同时保持了单镜头内容的完整性。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<ul>
<li><strong>生成质量：</strong> DreaMontage 能够生成视觉上引人注目且无缝连贯的单镜头视频，在视觉流畅性、时间连贯性和叙事性方面表现出色。</li>
<li><strong>控制能力：</strong> 模型能够精确地响应用户提供的任意帧（图像或视频片段）作为条件，实现精细的时间控制和内容引导。</li>
<li><strong>效率：</strong> SAR策略使得生成长视频在计算上更具效率，平衡了性能和资源消耗。</li>
<li><strong>对比优势：</strong> 在与现有先进模型（如Vidu Q2, Pixverse V5, Kling 2.5）的多关键帧和首尾帧条件生成任务的比较中，DreaMontage 在整体用户偏好、提示跟随（Prompt Following）和运动效果（Motion Effects）方面均取得了显著优势。</li>
<li><strong>意义：</strong> 该研究为电影制作、内容创作等领域提供了一个强大的新工具，使用户能够将零散的视觉素材转化为生动、连贯的单镜头电影体验，极大地降低了创作门槛，并拓展了创意表达的可能性。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong></p>
<ul>
<li>论文中并未明确列出具体的局限性，但从其研究内容和方法来看，可以推测：<ul>
<li>虽然模型在运动效果和提示跟随上表现优异，但在<strong>视觉质量（Visual Quality）</strong>方面，与某些顶级模型相比，可能存在微小的权衡（如与Vidu Q2相比有-2.63%的GSB得分）。</li>
<li><strong>数据收集和标注</strong>（如用于DPO的对比对生成）可能仍然是耗时且需要专业知识的过程。</li>
<li><strong>计算资源</strong>虽然通过SAR策略得到优化，但生成高质量、长时长的视频仍需要一定的计算能力。</li>
</ul>
</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>更精细的语义控制：</strong> 进一步探索如何更细致地控制视频的语义内容，例如特定物体的行为、场景的演变等。</li>
<li><strong>交互式编辑和迭代：</strong> 开发更强大的交互式工具，允许用户在生成过程中进行更灵活的编辑和迭代调整。</li>
<li><strong>跨模态融合的深化：</strong> 探索将更多模态的信息（如音频、更复杂的文本描述）更深入地融合到视频生成过程中。</li>
<li><strong>实时生成和低延迟应用：</strong> 针对需要实时反馈的应用场景（如游戏、虚拟现实），进一步优化模型的推理速度。</li>
<li><strong>更广泛的艺术风格迁移：</strong> 探索将模型应用于更广泛的艺术风格迁移和视频风格转换任务。</li>
<li><strong>伦理和社会影响：</strong> 随着AI生成内容能力的增强，研究和讨论相关的伦理问题和潜在的社会影响将变得越来越重要。</li>
</ul>
<hr />
<p>这份摘要力求在保持技术准确性的同时，清晰地传达DreaMontage论文的核心贡献和意义。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we introduce DreaMontage, a comprehensive framework designed for arbitrary frame-guided generation, capable of synthesizing seamless, expressive, and long-duration one-shot videos from diverse user-provided inputs.</li>
<li>Extensive experiments demonstrate that our approach achieves visually striking and seamlessly coherent one-shot effects while maintaining computational efficiency, empowering users to transform fragmented visual materials into vivid, cohesive one-shot cinematic experiences.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.21252v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.21252v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.21243v1'></a></p>
<h2 id="lookplangraph-embodied-instruction-following-method-with-vlm-graph-augmentation"><a href="https://arxiv.org/abs/2512.21243v1">LookPlanGraph: Embodied Instruction Following Method with VLM Graph Augmentation</a></h2>
<p><strong>Authors:</strong> Anatoly O. Onishchenko, Alexey K. Kovalev, Aleksandr I. Panov</p>
<p><strong>Published:</strong> 2025-12-24</p>
<p><strong>Categories:</strong> cs.RO, cs.AI, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Methods that use Large Language Models (LLM) as planners for embodied instruction following tasks have become widespread. To successfully complete tasks, the LLM must be grounded in the environment in which the robot operates. One solution is to use a scene graph that contains all the necessary information. Modern methods rely on prebuilt scene graphs and assume that all task-relevant information is available at the start of planning. However, these approaches do not account for changes in the environment that may occur between the graph construction and the task execution. We propose LookPlanGraph - a method that leverages a scene graph composed of static assets and object priors. During plan execution, LookPlanGraph continuously updates the graph with relevant objects, either by verifying existing priors or discovering new entities. This is achieved by processing the agents egocentric camera view using a Vision Language Model. We conducted experiments with changed object positions VirtualHome and OmniGibson simulated environments, demonstrating that LookPlanGraph outperforms methods based on predefined static scene graphs. To demonstrate the practical applicability of our approach, we also conducted experiments in a real-world setting. Additionally, we introduce the GraSIF (Graph Scenes for Instruction Following) dataset with automated validation framework, comprising 514 tasks drawn from SayPlan Office, BEHAVIOR-1K, and VirtualHome RobotHow. Project page available at https://lookplangraph.github.io .</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“LookPlanGraph: Embodied Instruction Following Method with VLM Graph Augmentation”的全面中文摘要：</p>
<p><strong>论文题目：</strong> LookPlanGraph: Embodied Instruction Following Method with VLM Graph Augmentation
<strong>作者：</strong> Anatoly O. Onishchenko, Alexey K. Kovalev, Aleksandr I. Panov</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题/研究问题：</strong>
该论文旨在解决当前基于大型语言模型（LLM）的具身指令跟随方法在动态环境中执行任务时遇到的挑战。现有方法通常依赖于预先构建的静态场景图，并假设环境信息在规划开始时是完全已知的。然而，这种方法无法应对在图构建和任务执行之间可能发生的<strong>环境变化</strong>（如物体位置改变）。这限制了LLM在真实、动态的机器人操作环境中的有效性，因为它们需要与环境进行<strong>有效交互和感知</strong>才能成功执行任务。</p>
<p><strong>2. 关键创新/方法贡献：</strong>
作者提出了<strong>LookPlanGraph</strong>方法，其核心创新在于：</p>
<ul>
<li><strong>动态场景图更新：</strong> LookPlanGraph能够<strong>实时更新场景图</strong>。它结合了静态资产和物体先验信息，并在任务执行过程中，利用<strong>视觉语言模型（VLM）</strong>处理代理的<strong>自主视角图像</strong>，动态地验证现有先验信息或发现新实体，从而不断更新场景图。</li>
<li><strong>记忆图（Memory Graph）：</strong> 引入了一个记忆图，包含通用场景结构、已交互的不可移动资产以及可移动物体的可能位置（先验）。</li>
<li><strong>场景图模拟器（SGS）：</strong> SGS用于验证和精炼LLM生成的动作，并根据环境变化更新记忆图，使LLM能够根据当前环境状态动态选择动作。</li>
<li><strong>图增强模块（Graph Augmentation Module）：</strong> 该模块利用VLM处理代理的视角图像，识别新物体或更新物体位置，并将这些信息整合到场景表示中。</li>
<li><strong>GraSIF数据集：</strong> 为了解决缺乏图基准指令跟随数据集的问题，作者构建了一个包含514个任务的GraSIF数据集，涵盖了家庭操作场景，并提供了自动验证框架。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>性能提升：</strong> 在虚拟家庭（VirtualHome）和OmniGibson模拟器中进行的实验表明，LookPlanGraph在<strong>动态环境</strong>中显著优于基于预定义静态场景图的方法，在<strong>成功率（SR）和平均规划精度（APP）</strong>方面均表现出色。
*   <strong>鲁棒性：</strong> 在真实世界办公室环境中的实验也证明了该方法的<strong>实际应用潜力</strong>，成功率达到了80%。
*   <strong>效率与准确性：</strong> 该方法在动态环境中实现了<strong>高成功率和高规划精度</strong>，同时通过记忆图和动态更新机制，在<strong>计算效率</strong>（TPA）方面也取得了良好的平衡。
*   <strong>数据集贡献：</strong> GraSIF数据集的发布为未来研究提供了重要的基准，促进了对具身指令跟随方法在图表示和动态环境下的评估。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>低级动作执行依赖：</strong> 该方法在很大程度上依赖于<strong>低级动作的完美执行</strong>。在真实世界中，传感器噪声、抓取失败和导航错误等问题可能会影响整体性能。
*   <strong>动作幻觉和无效发现：</strong> LLM有时会生成<strong>语义上无效的动作</strong>，例如尝试“发现”已知物体，或将发现操作应用于不适用的节点类型。
*   <strong>领域知识限制：</strong> LLM在<strong>领域特定推理</strong>方面存在局限性，例如在区分可消耗物品和可重复使用物品时可能出错。
*   <strong>模型能力影响：</strong> 方法的性能<strong>随模型能力而变化</strong>，虽然GPT-4o表现最佳，但较小的模型（如Gemma3-12b）性能会显著下降。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>集成鲁棒的错误恢复机制：</strong> 开发能够处理低级控制器执行失败的机制，以增强系统的<strong>韧性</strong>。
*   <strong>增强领域特定推理能力：</strong> 进一步提升LLM在复杂领域推理方面的能力，以解决语义知识限制的问题。
*   <strong>多模态融合与交互：</strong> 探索更深层次的多模态融合，以及更精细的低级控制与高级规划之间的交互。
*   <strong>实时适应性：</strong> 进一步研究如何在更广泛的动态环境中实现更快速、更自适应的规划和执行。</p>
<p><strong>对计算机视觉领域的贡献：</strong>
LookPlanGraph在计算机视觉领域的重要贡献在于其<strong>创新的视觉语言模型（VLM）应用</strong>，用于<strong>动态地增强和更新场景图表示</strong>。它展示了如何利用VLM处理代理的自主视角图像，从而实现对动态环境中物体位置和状态的实时感知和理解。这不仅提升了具身智能体在复杂、变化环境中的任务执行能力，也为<strong>视觉场景理解、物体识别与关系推理</strong>在机器人导航和操作中的实际应用提供了新的思路和方法。GraSIF数据集的构建也为评估和推动该领域的研究提供了宝贵的资源。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose LookPlanGraph - a method that leverages a scene graph composed of static assets and object priors.</li>
<li>During plan execution, LookPlanGraph continuously updates the graph with relevant objects, either by verifying existing priors or discovering new entities.</li>
<li>We conducted experiments with changed object positions VirtualHome and OmniGibson simulated environments, demonstrating that LookPlanGraph outperforms methods based on predefined static scene graphs.</li>
<li>To demonstrate the practical applicability of our approach, we also conducted experiments in a real-world setting.</li>
<li>Additionally, we introduce the GraSIF (Graph Scenes for Instruction Following) dataset with automated validation framework, comprising 514 tasks drawn from SayPlan Office, BEHAVIOR-1K, and VirtualHome RobotHow.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.21243v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.21243v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.21221v1'></a></p>
<h2 id="leveraging-lightweight-entity-extraction-for-scalable-event-based-image-retrieval"><a href="https://arxiv.org/abs/2512.21221v1">Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval</a></h2>
<p><strong>Authors:</strong> Dao Sy Duy Minh, Huynh Trung Kiet, Nguyen Lam Phu Quy, Phu-Hoa Pham, Tran Chi Nguyen</p>
<p><strong>Published:</strong> 2025-12-24</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Retrieving images from natural language descriptions is a core task at the intersection of computer vision and natural language processing, with wide-ranging applications in search engines, media archiving, and digital content management. However, real-world image-text retrieval remains challenging due to vague or context-dependent queries, linguistic variability, and the need for scalable solutions. In this work, we propose a lightweight two-stage retrieval pipeline that leverages event-centric entity extraction to incorporate temporal and contextual signals from real-world captions. The first stage performs efficient candidate filtering using BM25 based on salient entities, while the second stage applies BEiT-3 models to capture deep multimodal semantics and rerank the results. Evaluated on the OpenEvents v1 benchmark, our method achieves a mean average precision of 0.559, substantially outperforming prior baselines. These results highlight the effectiveness of combining event-guided filtering with long-text vision-language modeling for accurate and efficient retrieval in complex, real-world scenarios. Our code is available at https://github.com/PhamPhuHoa-23/Event-Based-Image-Retrieval</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval”的全面中文摘要，重点关注其在计算机视觉和机器学习领域的贡献：</p>
<p><strong>论文题目：</strong> Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval (利用轻量级实体提取实现可扩展的基于事件的图像检索)</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
该论文旨在解决现实世界中图像-文本检索的挑战，特别是针对自然语言描述的图像检索任务。现有方法在处理模糊、依赖上下文的查询、语言多样性以及对可扩展解决方案的需求方面存在困难。尤其是在新闻和事件检索等领域，查询往往比典型的图像描述更复杂，包含多个命名实体、时间参考，并且需要事件中心的理解。</p>
<p><strong>2. 主要创新/方法贡献：</strong>
作者提出了一种<strong>轻量级的两阶段检索流水线</strong>，其核心创新在于<strong>利用事件中心的实体提取</strong>来整合真实世界标题中的时间信息和上下文信号。</p>
<ul>
<li>
<p><strong>第一阶段：事件引导的候选过滤 (Event-guided Filtering)</strong></p>
<ul>
<li>利用<strong>spaCy</strong>等工具进行<strong>命名实体识别 (NER)</strong>，提取文本中的关键实体（如人名、地点、日期、组织等）。</li>
<li>基于提取出的<strong>显著实体</strong>，使用<strong>BM25</strong>算法进行高效的候选文章过滤。这大大减少了计算开销，同时保留了语义相关性。</li>
<li>通过<strong>实体加权</strong>和<strong>同义词扩展</strong>（使用NLTK的WordNet）来增强实体匹配的鲁棒性。</li>
<li>采用<strong>倒数排名融合 (Reciprocal Rank Fusion, RRF)</strong> 来融合来自实体匹配和BM25文本检索的结果，以提高候选文章的多样性和相关性。</li>
</ul>
</li>
<li>
<p><strong>第二阶段：多模态图像重排序 (Multimodal Image Reranking)</strong></p>
<ul>
<li>使用<strong>BEiT-3</strong>模型（一种强大的视觉-语言Transformer模型）来捕捉深层的多模态语义。</li>
<li>采用<strong>双模型配置</strong>：<ul>
<li><strong>事件对齐BEiT-3 (Event-Aligned BEiT-3)</strong>：针对事件查询和图像文本之间的字面相似性进行微调，特别擅长匹配命名实体、时间参考和事实描述。</li>
<li><strong>BEiT-3 ITC (Image-Text Contrastive)</strong>：利用对比学习目标进行预训练，能够编码超越文本重叠的高层视觉语义，用于检索具有潜在视觉线索（如情感、象征意义）的图像。</li>
</ul>
</li>
<li><strong>冻结视觉编码器</strong>，选择性更新语言模型组件，以提高计算效率并保留预训练的视觉表示能力。</li>
<li>结合<strong>sigmoid增强</strong>机制，考虑了<strong>原始相似度分数</strong>和<strong>文章排名位置</strong>，以平衡模型相似度和文章的整体排名。</li>
<li>最终通过<strong>倒数排名融合 (RRF)</strong> 整合两个BEiT-3模型的重排序分数，确保高排名图像获得最高分数。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
该方法在<strong>OpenEvents v1</strong>基准测试上取得了显著的性能提升。在公共测试集上，实现了<strong>0.559的平均精度 (mAP)</strong>，显著优于现有基线方法（最佳基线mAP为0.323），相对提升了73%。在私有测试集上也表现出良好的泛化能力。</p>
<p>这些结果表明：
*   <strong>事件引导的过滤</strong>能够有效地缩小搜索空间，同时保持语义相关性，显著提高了检索效率。
*   <strong>长文本视觉-语言建模</strong>（如BEiT-3支持512个token的输入）对于理解复杂、实体密集的事件查询至关重要。
*   <strong>双模型重排序</strong>策略能够捕捉到字面和抽象的语义线索，从而实现更准确的图像检索。
*   该方法在<strong>复杂、真实世界的场景</strong>中，能够实现<strong>准确且高效</strong>的检索。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>实体提取的挑战：</strong> 尽管spaCy表现良好，但对于描述性强的事件（如动词表达或隐含在子句中的事件），实体提取器可能难以捕捉到关键的时间和上下文线索。
*   <strong>抽象/象征性描述的理解：</strong> 对于不直接对应具体对象的抽象或象征性视觉描述，当前的视觉-语言模型（包括BEiT-3）在准确表示和对齐方面仍有困难，这反映了将文本与人类对语言的细致理解对齐的挑战。</p>
<p><strong>5. 未来研究方向：</strong>
*   <strong>更先进的事件提取方法：</strong> 探索利用大型语言模型（LLMs）来改进对隐式或非字面事件表达的捕捉能力。
*   <strong>专门识别抽象/细微事件线索的模型：</strong> 开发能够识别和整合抽象或细微事件线索的模型，以提高检索的精确度和泛化能力。
*   <strong>检索增强生成 (RAG) 和其他重排序技术：</strong> 探索将RAG或其他文本重排序技术整合到流水线中，以进一步缩小文章池并减少图像匹配阶段的噪声。
*   <strong>知识增强推理：</strong> 结合符号过滤、上下文抽象和知识增强推理，以推动多模态事件检索的边界。</p>
<p>总而言之，这篇论文提出了一种创新的、高效的、可扩展的事件图像检索框架，通过结合轻量级的实体提取和强大的多模态模型，有效解决了现实世界中复杂事件查询的检索难题，并在实验中取得了显著的成果。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we propose a lightweight two-stage retrieval pipeline that leverages event-centric entity extraction to incorporate temporal and contextual signals from real-world captions.</li>
<li>Evaluated on the OpenEvents v1 benchmark, our method achieves a mean average precision of 0.559, substantially outperforming prior baselines.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.21221v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.21221v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.21218v1'></a></p>
<h2 id="latent-implicit-visual-reasoning"><a href="https://arxiv.org/abs/2512.21218v1">Latent Implicit Visual Reasoning</a></h2>
<p><strong>Authors:</strong> Kelvin Li, Chuyi Shang, Leonid Karlinsky, Rogerio Feris, Trevor Darrell, Roei Herzig</p>
<p><strong>Published:</strong> 2025-12-24</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what "useful" visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Latent Implicit Visual Reasoning”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Latent Implicit Visual Reasoning (潜在隐式视觉推理)</p>
<p><strong>作者：</strong> Kelvin Li, Chuyi Shang, Leonid Karlinsky, Rogerio Feris, Trevor Darrell, Roei Herzig</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>大型多模态模型（LMMs）在视觉理解方面取得了显著进展，但它们在处理以视觉为主的推理任务时仍然存在局限性。这主要是因为它们高度依赖文本作为核心推理模态，导致在需要复杂视觉抽象的任务中表现不佳。现有方法试图通过监督中间视觉步骤来解决这个问题，例如使用辅助图像、深度图或图像裁剪，但这带来了高昂的标注成本、限制了“有用”视觉抽象的定义，并且难以泛化到不同任务。论文旨在解决LMMs在视觉推理任务中，尤其是在缺乏明确中间步骤指导的情况下，如何自主发现和利用视觉信息的问题。</p>
<p><strong>2. 关键创新/方法贡献：</strong></p>
<p>该论文提出了一种名为 <strong>Latent Implicit Visual Reasoning (LIVR)</strong> 的新方法，其核心创新在于：</p>
<ul>
<li><strong>任务无关的隐式视觉推理机制：</strong> LIVR引入了一种任务无关的机制，使LMM能够<strong>自主发现和使用视觉推理的潜在（latent）令牌（tokens）</strong>，而无需显式的监督。</li>
<li><strong>视觉瓶颈（Visual Bottlenecking）：</strong> 通过一种新颖的视觉瓶颈方法，强制视觉信息必须<strong>通过这些潜在令牌</strong>才能传递给模型的回答部分。这迫使潜在令牌承担起提取关键视觉信息的功能，并充当视觉信息传递的瓶颈。</li>
<li><strong>潜在令牌（Latent Tokens）：</strong> 在LMM的词汇表中添加K个新的特殊令牌，这些令牌在训练过程中被<strong>隐式学习</strong>，用于表示重要的视觉信息。模型不需要学习如何生成这些令牌，而是学习如何利用它们。</li>
<li><strong>两阶段训练（Multi-Stage Training）：</strong><ul>
<li><strong>第一阶段（视觉瓶颈）：</strong> 使用修改后的注意力掩码，强制回答令牌只能关注提示令牌和潜在令牌，而不能直接关注原始图像令牌。这使得潜在令牌成为视觉信息的唯一通道。</li>
<li><strong>第二阶段（标准掩码）：</strong> 恢复标准的注意力掩码，允许回答令牌同时关注原始图像令牌和经过丰富后的潜在令牌，以实现联合推理。</li>
</ul>
</li>
</ul>
<p>这种方法使得模型能够以一种<strong>自适应的方式</strong>重新编码图像，从而提取相关的视觉信息，而无需人工设计的监督。</p>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>性能提升：</strong> LIVR在九项感知密集型任务上均取得了显著的性能提升，<strong>优于直接监督微调（Direct SFT）</strong>。</li>
<li><strong>状态最优（State-of-the-art）：</strong> 在多种单任务微调设置下，LIVR取得了<strong>当前最优（state-of-the-art）的结果</strong>，尤其是在那些中间抽象难以明确指定的视觉推理任务上。</li>
<li><strong>泛化能力：</strong> LIVR在<strong>多任务指令微调</strong>设置下也展现出强大的泛化能力，优于监督微调。</li>
<li><strong>任务无关性：</strong> LIVR的方法具有<strong>任务无关性</strong>，可以轻松应用于不同的视觉推理任务，而无需为每个任务设计特定的辅助图像或中间目标。</li>
<li><strong>与现有方法的对比：</strong> 与依赖显式监督（如Mirage方法）的方法相比，LIVR在Jigsaw和Visual Spatial Planning任务上取得了显著的性能优势，证明了其隐式学习的有效性。</li>
</ul>
<p><strong>4. 论文提及的局限性：</strong></p>
<ul>
<li><strong>可解释性：</strong> 论文提到，潜在令牌的可解释性<strong>可能不如文本解释</strong>。虽然通过注意力可视化可以部分理解其关注区域，但其内部表示的含义仍需进一步探索。</li>
<li><strong>模型规模和数据：</strong> 未来工作可以考虑<strong>扩展到更大的模型</strong>，并<strong>在更大的数据集上进行训练</strong>，以进一步提升性能。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>提高可解释性：</strong> 探索更有效的方法来理解和解释潜在令牌所学习到的视觉表示。</li>
<li><strong>扩展模型和数据：</strong> 将LIVR应用于更大规模的模型和更广泛、更复杂的视觉推理任务。</li>
<li><strong>增加潜在令牌容量：</strong> 研究如何调整潜在令牌的数量（K）以及其结构，以适应更复杂的推理需求。</li>
<li><strong>架构和训练策略的进一步优化：</strong> 探索更精细的训练策略和注意力掩码设计，以进一步提升LIVR的性能。</li>
<li><strong>将LIVR应用于其他模态：</strong> 探索将这种隐式推理机制扩展到其他模态（如音频、视频）的可能性。</li>
</ul>
<p><strong>总结：</strong></p>
<p>“Latent Implicit Visual Reasoning”论文提出了一种创新的方法，通过引入<strong>任务无关的潜在视觉推理令牌</strong>和<strong>视觉瓶颈机制</strong>，使大型多模态模型能够<strong>隐式地学习和利用视觉信息</strong>，从而显著提升其在复杂视觉推理任务上的性能。该方法无需额外的标注数据或人工设计的中间步骤，展现了强大的泛化能力和在多种视觉任务上的优越性，为提升LMMs的视觉推理能力开辟了新的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision.</li>
<li>Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.21218v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.21218v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.21209v1'></a></p>
<h2 id="human-motion-estimation-with-everyday-wearables"><a href="https://arxiv.org/abs/2512.21209v1">Human Motion Estimation with Everyday Wearables</a></h2>
<p><strong>Authors:</strong> Siqi Zhu, Yixuan Li, Junfu Li, Qi Wu, Zan Wang, Haozhe Ma, Wei Liang</p>
<p><strong>Published:</strong> 2025-12-24</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>While on-body device-based human motion estimation is crucial for applications such as XR interaction, existing methods often suffer from poor wearability, expensive hardware, and cumbersome calibration, which hinder their adoption in daily life. To address these challenges, we present EveryWear, a lightweight and practical human motion capture approach based entirely on everyday wearables: a smartphone, smartwatch, earbuds, and smart glasses equipped with one forward-facing and two downward-facing cameras, requiring no explicit calibration before use. We introduce Ego-Elec, a 9-hour real-world dataset covering 56 daily activities across 17 diverse indoor and outdoor environments, with ground-truth 3D annotations provided by the motion capture (MoCap), to facilitate robust research and benchmarking in this direction. Our approach employs a multimodal teacher-student framework that integrates visual cues from egocentric cameras with inertial signals from consumer devices. By training directly on real-world data rather than synthetic data, our model effectively eliminates the sim-to-real gap that constrains prior work. Experiments demonstrate that our method outperforms baseline models, validating its effectiveness for practical full-body motion estimation.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Human Motion Estimation with Everyday Wearables”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Human Motion Estimation with Everyday Wearables</p>
<p><strong>作者：</strong> Siqi Zhu, Yixuan Li, Junfu Li, Qi Wu, Zan Wang, Haozhe Ma, Wei Liang</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
本文旨在解决当前基于可穿戴设备的人体运动估计方法在实际应用中面临的挑战，包括：
*   <strong>可穿戴性差：</strong> 现有方法常依赖笨重的头戴式设备或密集的IMU（惯性测量单元）配置。
*   <strong>硬件成本高：</strong> 专业级IMU设备价格昂贵。
*   <strong>校准复杂：</strong> 需要跨多个异构传感器进行仔细校准。
这些因素严重阻碍了人体运动估计技术在日常生活中的普及。</p>
<p><strong>2. 主要创新与方法贡献：</strong>
为了克服上述挑战，作者提出了 <strong>EveryWear</strong>，一个轻量级且实用的全身人体运动捕捉方法，其核心创新包括：</p>
<ul>
<li><strong>轻量级日常可穿戴设备：</strong> EveryWear完全基于日常消费级可穿戴设备，包括智能手机、智能手表、无线耳机以及配备一个前视和两个下视摄像头的智能眼镜。</li>
<li><strong>无需显式校准：</strong> 该系统无需用户进行繁琐的校准过程即可使用。</li>
<li><strong>多模态教师-学生框架：</strong><ul>
<li><strong>教师模型：</strong> 利用来自智能眼镜的RGB图像（前视和两个下视摄像头）以及来自智能手机、智能手表和耳机的IMU信号，结合运动捕捉（MoCap）系统提供的密集IMU数据，学习精确的运动估计。</li>
<li><strong>学生模型：</strong> 仅使用日常可穿戴设备提供的稀疏、有噪声的IMU测量和相机输入，通过从教师模型“蒸馏”知识来学习运动估计。</li>
</ul>
</li>
<li><strong>Ego-Elec 数据集：</strong> 作者构建了一个大规模、真实世界的<strong>Ego-Elec</strong>数据集，包含9小时的日常活动数据，涵盖56种不同的日常活动，分布在17个室内外环境中。该数据集提供了由MoCap系统标注的精确3D人体姿态和全局位姿真值，为该领域的研究和基准测试提供了重要资源。</li>
<li><strong>克服“仿真到真实”差距：</strong> 通过直接在真实世界数据上进行训练，该方法有效消除了传统基于合成数据的方法所面临的“仿真到真实”（sim-to-real）差距。</li>
<li><strong>多模态融合与跨模态补偿：</strong> 系统通过融合来自不同传感器的信息，实现了强大的跨模态补偿能力，即使在某些传感器不可靠（如相机被遮挡或IMU漂移）的情况下，也能维持鲁棒的运动估计。</li>
<li><strong>SLAM模块集成：</strong> 集成了一个现成的SLAM（同步定位与地图构建）模块，利用前视摄像头提供全局定位，并补偿头部姿态估计的漂移。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>性能优越：</strong> 实验结果表明，EveryWear在各种指标上（如MPJPE、PA-MPJPE、MPJVE等）均显著优于现有的IMU-only和Camera-only基线方法。
*   <strong>鲁棒性强：</strong> 该方法在具有挑战性的场景（如遮挡、身体部分离开视野）下表现出强大的鲁棒性，这得益于多模态融合和教师-学生蒸馏。
*   <strong>实用性高：</strong> 使用日常消费级设备、无需校准的特性使其非常适合实际应用。
*   <strong>数据集贡献：</strong> Ego-Elec数据集的发布为该领域的研究提供了宝贵的资源，促进了对真实世界人体运动估计的深入研究。</p>
<p><strong>4. 局限性：</strong>
*   <strong>离线处理：</strong> 目前的方法是离线运行的，虽然可以适配实时应用，但需要进一步优化。
*   <strong>固定传感器位置：</strong> 当前的IMU配置仅支持固定的传感器位置，并且需要预先指定，未来可以改进为支持自适应的传感器配置处理。</p>
<p><strong>5. 未来研究方向：</strong>
*   <strong>实时应用：</strong> 将现有离线方法适配到实时应用，以支持更广泛的现实世界场景。
*   <strong>自适应传感器配置：</strong> 开发能够处理可变传感器配置的自适应方法。
*   <strong>更丰富的交互模态：</strong> 进一步整合更丰富的交互模态，以增强在复杂真实世界环境中的鲁棒性。</p>
<p>总而言之，这篇论文提出了一种创新的、基于日常可穿戴设备的轻量级人体运动估计方法EveryWear，并通过构建大规模真实世界数据集Ego-Elec，在性能、鲁棒性和实用性方面取得了显著进展，为未来人体运动估计的研究和应用奠定了坚实的基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these challenges, we present EveryWear, a lightweight and practical human motion capture approach based entirely on everyday wearables: a smartphone, smartwatch, earbuds, and smart glasses equipped with one forward-facing and two downward-facing cameras, requiring no explicit calibration before use.</li>
<li>We introduce Ego-Elec, a 9-hour real-world dataset covering 56 daily activities across 17 diverse indoor and outdoor environments, with ground-truth 3D annotations provided by the motion capture (MoCap), to facilitate robust research and benchmarking in this direction.</li>
<li>Our approach employs a multimodal teacher-student framework that integrates visual cues from egocentric cameras with inertial signals from consumer devices.</li>
<li>Experiments demonstrate that our method outperforms baseline models, validating its effectiveness for practical full-body motion estimation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.21209v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.21209v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-12-25 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
