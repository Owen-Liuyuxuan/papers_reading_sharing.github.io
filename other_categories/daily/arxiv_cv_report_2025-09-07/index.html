<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-09-07 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-09-05/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-09-08/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-09-07">Arxiv Computer Vision Papers - 2025-09-07</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#one-flight-over-the-gap-a-survey-from-perspective-to-panoramic-vision" class="nav-link">One Flight Over the Gap: A Survey from Perspective to Panoramic Vision</a>
                </li>
                <li class="nav-item">
                    <a href="#human-motion-video-generation-a-survey" class="nav-link">Human Motion Video Generation: A Survey</a>
                </li>
                <li class="nav-item">
                    <a href="#the-telephone-game-evaluating-semantic-drift-in-unified-models" class="nav-link">The Telephone Game: Evaluating Semantic Drift in Unified Models</a>
                </li>
                <li class="nav-item">
                    <a href="#transition-models-rethinking-the-generative-learning-objective" class="nav-link">Transition Models: Rethinking the Generative Learning Objective</a>
                </li>
                <li class="nav-item">
                    <a href="#ssgaussian-semantic-aware-and-structure-preserving-3d-style-transfer" class="nav-link">SSGaussian: Semantic-Aware and Structure-Preserving 3D Style Transfer</a>
                </li>
                <li class="nav-item">
                    <a href="#from-editor-to-dense-geometry-estimator" class="nav-link">From Editor to Dense Geometry Estimator</a>
                </li>
                <li class="nav-item">
                    <a href="#geoarena-an-open-platform-for-benchmarking-large-vision-language-models-on-worldwide-image-geolocalization" class="nav-link">GeoArena: An Open Platform for Benchmarking Large Vision-language Models on WorldWide Image Geolocalization</a>
                </li>
                <li class="nav-item">
                    <a href="#efficient-odd-one-out-anomaly-detection" class="nav-link">Efficient Odd-One-Out Anomaly Detection</a>
                </li>
                <li class="nav-item">
                    <a href="#visiofirm-cross-platform-ai-assisted-annotation-tool-for-computer-vision" class="nav-link">VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer Vision</a>
                </li>
                <li class="nav-item">
                    <a href="#revisiting-simple-baselines-for-in-the-wild-deepfake-detection" class="nav-link">Revisiting Simple Baselines for In-The-Wild Deepfake Detection</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-09-07">Arxiv Computer Vision Papers - 2025-09-07</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，这是一份针对2025年9月4日Arxiv计算机视觉论文的每日报告执行摘要，旨在帮助忙碌的研究人员快速掌握关键信息。</p>
<hr />
<p><strong>每日Arxiv计算机视觉论文报告执行摘要 (2025-09-04)</strong></p>
<p><strong>1. 主要主题与趋势概述：</strong></p>
<p>今天的论文集呈现出计算机视觉领域几个活跃且交叉的趋势。<strong>生成式AI</strong>依然是核心，尤其体现在视频生成、3D内容创建以及对生成模型底层机制的重新思考上。<strong>多模态学习</strong>，特别是视觉-语言模型的地理定位能力，显示出其在理解复杂世界信息方面的潜力。此外，对<strong>基础任务的重新审视</strong>，如异常检测和深度伪造检测，表明研究人员在寻求更高效、更鲁棒的解决方案。<strong>工具和平台开发</strong>也占据一席之地，旨在提升研究和应用效率。</p>
<p><strong>2. 显著或创新论文亮点：</strong></p>
<ul>
<li><strong>"Transition Models: Rethinking the Generative Learning Objective" (Zidong Wang et al.)</strong>：这篇论文可能对生成模型的基础理论产生重要影响。通过重新思考生成学习目标，它有望为更稳定、更高效的生成模型训练提供新的范式，值得深入关注。</li>
<li><strong>"SSGaussian: Semantic-Aware and Structure-Preserving 3D Style Transfer" (Jimin Xu et al.)</strong>：在3D内容生成日益重要的背景下，这篇论文提出的语义感知和结构保持的3D风格迁移方法，解决了3D内容创作中的一个关键挑战，具有很高的实用价值和创新性。</li>
<li><strong>"GeoArena: An Open Platform for Benchmarking Large Vision-language Models on WorldWide Image Geolocalization" (Pengyue Jia et al.)</strong>：这是一个重要的贡献，不仅提出了一个具体的应用（地理定位），更提供了一个开放平台来基准测试大型视觉-语言模型。这对于推动该领域的研究和公平比较不同模型的性能至关重要。</li>
</ul>
<p><strong>3. 新兴研究方向或技术：</strong></p>
<ul>
<li><strong>生成模型理论的深层探索：</strong> "Transition Models"表明研究者不再仅仅关注生成效果，而是开始深入探讨生成学习的底层机制和目标函数，这可能带来新的理论突破。</li>
<li><strong>3D内容生成与编辑的精细化控制：</strong> "SSGaussian"强调了在3D风格迁移中结合语义和结构的重要性，预示着未来3D内容创作将更加注重精细化、智能化的控制。</li>
<li><strong>视觉-语言模型在特定复杂任务中的应用与评估：</strong> "GeoArena"展示了视觉-语言模型在地理定位这类需要复杂世界知识的任务中的潜力，并强调了建立标准化评估平台的重要性。</li>
<li><strong>高效与鲁棒的基础视觉任务：</strong> "Efficient Odd-One-Out Anomaly Detection" 和 "Revisiting Simple Baselines for In-The-Wild Deepfake Detection" 提示研究者仍在寻求在资源受限或真实世界复杂场景下，提升传统任务的效率和鲁棒性。</li>
</ul>
<p><strong>4. 建议完整阅读的论文：</strong></p>
<p>基于其潜在影响、创新性和实用性，建议优先完整阅读以下论文：</p>
<ol>
<li><strong>"Transition Models: Rethinking the Generative Learning Objective" (Zidong Wang et al.)</strong> - 理论突破潜力。</li>
<li><strong>"SSGaussian: Semantic-Aware and Structure-Preserving 3D Style Transfer" (Jimin Xu et al.)</strong> - 3D内容生成与编辑的实用创新。</li>
<li><strong>"GeoArena: An Open Platform for Benchmarking Large Vision-language Models on WorldWide Image Geolocalization" (Pengyue Jia et al.)</strong> - 平台与基准测试，对领域发展有重要推动作用。</li>
<li><strong>"Human Motion Video Generation: A Survey" (Haiwei Xue et al.)</strong> - 如果您对视频生成，特别是人体运动方面感兴趣，这篇综述将提供全面的背景和方向。</li>
</ol>
<hr />
<p>这份摘要旨在提供一个高层次的概览，帮助您快速识别与您研究方向最相关的论文。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2509.04444v1">One Flight Over the Gap: A Survey from Perspective to Panoramic Vision</a></li>
<li><a href="#2509.03883v1">Human Motion Video Generation: A Survey</a></li>
<li><a href="#2509.04438v1">The Telephone Game: Evaluating Semantic Drift in Unified Models</a></li>
<li><a href="#2509.04394v1">Transition Models: Rethinking the Generative Learning Objective</a></li>
<li><a href="#2509.04379v1">SSGaussian: Semantic-Aware and Structure-Preserving 3D Style Transfer</a></li>
<li><a href="#2509.04338v1">From Editor to Dense Geometry Estimator</a></li>
<li><a href="#2509.04334v1">GeoArena: An Open Platform for Benchmarking Large Vision-language Models on WorldWide Image Geolocalization</a></li>
<li><a href="#2509.04326v1">Efficient Odd-One-Out Anomaly Detection</a></li>
<li><a href="#2509.04180v1">VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer Vision</a></li>
<li><a href="#2509.04150v1">Revisiting Simple Baselines for In-The-Wild Deepfake Detection</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2509.04444v1'></a></p>
<h2 id="one-flight-over-the-gap-a-survey-from-perspective-to-panoramic-vision"><a href="https://arxiv.org/abs/2509.04444v1">One Flight Over the Gap: A Survey from Perspective to Panoramic Vision</a></h2>
<p><strong>Authors:</strong> Xin Lin, Xian Ge, Dizhe Zhang, Zhaoliang Wan, Xianshun Wang, Xiangtai Li, Wenjie Jiang, Bo Du, Dacheng Tao, Ming-Hsuan Yang, Lu Qi</p>
<p><strong>Published:</strong> 2025-09-04</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Driven by the demand for spatial intelligence and holistic scene perception,
omnidirectional images (ODIs), which provide a complete 360\textdegree{} field
of view, are receiving growing attention across diverse applications such as
virtual reality, autonomous driving, and embodied robotics. Despite their
unique characteristics, ODIs exhibit remarkable differences from perspective
images in geometric projection, spatial distribution, and boundary continuity,
making it challenging for direct domain adaption from perspective methods. This
survey reviews recent panoramic vision techniques with a particular emphasis on
the perspective-to-panorama adaptation. We first revisit the panoramic imaging
pipeline and projection methods to build the prior knowledge required for
analyzing the structural disparities. Then, we summarize three challenges of
domain adaptation: severe geometric distortions near the poles, non-uniform
sampling in Equirectangular Projection (ERP), and periodic boundary continuity.
Building on this, we cover 20+ representative tasks drawn from more than 300
research papers in two dimensions. On one hand, we present a cross-method
analysis of representative strategies for addressing panoramic specific
challenges across different tasks. On the other hand, we conduct a cross-task
comparison and classify panoramic vision into four major categories: visual
quality enhancement and assessment, visual understanding, multimodal
understanding, and visual generation. In addition, we discuss open challenges
and future directions in data, models, and applications that will drive the
advancement of panoramic vision research. We hope that our work can provide new
insight and forward looking perspectives to advance the development of
panoramic vision technologies. Our project page is
https://insta360-research-team.github.io/Survey-of-Panorama</p>
<p><strong>Analysis:</strong></p>
<p>这篇由Xin Lin等人撰写的论文“One Flight Over the Gap: A Survey from Perspective to Panoramic Vision”全面回顾了全景视觉领域，特别关注了如何弥合传统透视图像与全景图像（ODIs）之间的领域鸿沟。</p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决全景图像（ODIs）在几何投影、空间分布和边界连续性方面与传统透视图像存在的显著差异，这些差异导致基于透视图像的方法难以直接应用于全景场景。核心研究问题是如何克服这些结构性挑战，实现从透视到全景的有效领域适应，从而推动全景视觉技术在虚拟现实、自动驾驶和具身机器人等应用中的发展。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
论文首先回顾了全景成像管线和投影方法，为理解ODIs与透视图像之间的结构差异奠定了基础。随后，它总结了领域适应的三个主要挑战：
*   <strong>极点附近的严重几何畸变：</strong> 在等距柱状投影（ERP）中，极点附近的物体会严重拉伸和变形。
*   <strong>等距柱状投影（ERP）中的非均匀空间采样：</strong> 导致像素密度在不同纬度上变化，赤道区域采样密集，而极点区域采样稀疏。
*   <strong>全景边界的周期性连续性：</strong> 传统卷积神经网络（CNNs）通常将ERP图像视为平面，未能有效处理水平边界的无缝连续性。</p>
<p>为了应对这些挑战，论文提出了两种核心策略：
*   <strong>畸变感知方法（Distortion-Aware Methods）：</strong> 保持ERP格式，但将畸变信息嵌入网络设计中，例如通过自适应卷积核、注意力机制或畸变图来指导特征学习。
*   <strong>投影驱动方法（Projection-Driven Methods）：</strong> 将全景图像重新投影到其他畸变较小的视图（如立方体投影、切线投影）中，然后融合多投影特征。</p>
<p>论文还对20多个代表性任务进行了跨方法和跨任务分析，将全景视觉任务分为四大类：视觉质量增强与评估、视觉理解、多模态理解和视觉生成。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>系统性分类和分析：</strong> 论文首次从“透视-全景鸿沟”这一根本视角出发，对全景视觉领域的300多篇研究论文进行了全面的回顾和分类，揭示了不同任务中解决全景特有挑战的策略。
*   <strong>方法论洞察：</strong> 畸变感知方法在需要全局语义一致性和感知质量的任务（如超分辨率、图像修复、分割、检测）中表现出色；投影驱动方法在几何敏感任务（如深度估计、光流、新视角合成）和多模态融合中具有优势。
*   <strong>新兴技术整合：</strong> 论文讨论了扩散模型、3D高斯泼溅和多模态融合等新兴技术在全景视觉中的应用潜力，这些技术有望在生成式任务和场景建模中发挥关键作用。
*   <strong>统一的视角：</strong> 论文提供了一个统一且不断演进的全景视觉学习图景，有助于研究人员更好地理解和选择适合特定任务的方法。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>数据稀缺性：</strong> 相比透视视觉，全景数据集在规模、多样性、质量和模态方面仍然有限，这严重制约了模型的泛化能力和公平的基准测试。
*   <strong>计算成本和推理速度：</strong> 某些生成模型驱动的方法（如扩散模型）仍面临高计算成本和慢推理速度的挑战。
*   <strong>几何精度：</strong> 畸变感知方法在高度变形区域的精度仍有待提高，尤其是在几何敏感任务中。
*   <strong>信息碎片化：</strong> 投影驱动方法可能导致跨投影的信息碎片化，需要额外的融合机制。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>数据方面：</strong> 构建大规模、标准化、多样化、高质量且富含标注的多模态全景数据集，以增强模型的泛化能力和可比性。
*   <strong>模型方面：</strong>
    *   <strong>基础模型：</strong> 发展具有强大泛化能力和零样本迁移能力的全景基础模型，实现统一的多任务架构。
    *   <strong>生成模型：</strong> 发展能够进行开放世界理解和场景生成的模型，特别是结合全景先验的扩散模型和3D高斯泼溅，以生成更真实、可控且具有时空一致性的全景内容。
    *   <strong>多模态融合：</strong> 进一步整合音频、LiDAR和文本等多种模态，实现更丰富的全景场景理解和人机交互。
*   <strong>应用方面：</strong> 将全景视觉技术扩展到更广泛的下游应用，如具身智能、自动驾驶、沉浸式媒体、3D重建和数字孪生，以及安全、教育和医疗等社会应用。</p>
<p>总而言之，这篇论文为全景视觉领域提供了一个全面的路线图，不仅总结了现有技术，还指出了未来的发展方向，强调了弥合领域鸿沟、利用新兴技术和构建更完善数据集的重要性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>On one hand, we present a cross-method
analysis of representative strategies for addressing panoramic specific
challenges across different tasks.</li>
<li>We hope that our work can provide new
insight and forward looking perspectives to advance the development of
panoramic vision technologies.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.04444v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.04444v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.03883v1'></a></p>
<h2 id="human-motion-video-generation-a-survey"><a href="https://arxiv.org/abs/2509.03883v1">Human Motion Video Generation: A Survey</a></h2>
<p><strong>Authors:</strong> Haiwei Xue, Xiangyang Luo, Zhanghao Hu, Xin Zhang, Xunzhi Xiang, Yuqin Dai, Jianzhuang Liu, Zhensong Zhang, Minglei Li, Jian Yang, Fei Ma, Zhiyong Wu, Changpeng Yang, Zonghong Dai, Fei Richard Yu</p>
<p><strong>Published:</strong> 2025-09-04</p>
<p><strong>Categories:</strong> cs.CV, cs.MM</p>
<p><strong>Abstract:</strong></p>
<p>Human motion video generation has garnered significant research interest due
to its broad applications, enabling innovations such as photorealistic singing
heads or dynamic avatars that seamlessly dance to music. However, existing
surveys in this field focus on individual methods, lacking a comprehensive
overview of the entire generative process. This paper addresses this gap by
providing an in-depth survey of human motion video generation, encompassing
over ten sub-tasks, and detailing the five key phases of the generation
process: input, motion planning, motion video generation, refinement, and
output. Notably, this is the first survey that discusses the potential of large
language models in enhancing human motion video generation. Our survey reviews
the latest developments and technological trends in human motion video
generation across three primary modalities: vision, text, and audio. By
covering over two hundred papers, we offer a thorough overview of the field and
highlight milestone works that have driven significant technological
breakthroughs. Our goal for this survey is to unveil the prospects of human
motion video generation and serve as a valuable resource for advancing the
comprehensive applications of digital humans. A complete list of the models
examined in this survey is available in Our Repository
https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“Human Motion Video Generation: A Survey”的全面摘要，涵盖了您要求的所有要点：</p>
<p><strong>论文摘要：Human Motion Video Generation: A Survey</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决现有关于人体运动视频生成领域综述的不足。当前综述通常侧重于单一方法，缺乏对整个生成过程的全面概述。因此，本文的核心问题是提供一个系统、深入的综述，涵盖人体运动视频生成的完整生成流程、多模态驱动方式、最新技术进展、挑战以及未来方向，特别是大型语言模型（LLMs）在该领域的应用潜力。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>五阶段生成流程框架：</strong> 论文首次将人体运动视频生成任务分解为五个关键阶段：输入（Input）、运动规划（Motion Planning）、运动视频生成（Motion Video Generation）、精修（Refinement）和输出（Output）。这一框架为理解和分析该领域的复杂性提供了系统化的视角。
*   <strong>LLMs在运动规划中的应用：</strong> 首次探讨了大型语言模型（LLMs）在运动规划阶段的潜力，包括利用LLMs生成精细的运动描述以进行检索，或将运动条件投影到潜在空间以指导生成模型。这代表了该领域的一个重要创新方向。
*   <strong>多模态驱动分类：</strong> 将人体运动视频生成方法分为视觉驱动、文本驱动和音频驱动三大主要模态，并详细分析了每种模态下的子任务和技术进展。
*   <strong>生成模型和注意力机制的深入分析：</strong> 详细分析了基于扩散模型（DMs）、生成对抗网络（GANs）和变分自编码器（VAEs）的生成框架，并对扩散模型中的空间注意力（SA）、交叉注意力（CA）、时间注意力（TA）和跨帧自注意力（CFSA）等注意力融合方法进行了细致分类。
*   <strong>全面的数据集和评估指标：</strong> 收集并概述了64个人体相关视频数据集，并总结了常用的单帧图像质量、视频质量评估、视频特性评估以及LLM规划器评估指标。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>领域快速发展：</strong> 综述揭示了人体运动视频生成领域，特别是“说话人头部”（talking head）和“舞蹈视频”（dance video）生成方面的快速增长和技术突破。
*   <strong>LLMs的潜力：</strong> 强调了LLMs在理解语义细微差别和推理情感方面的优势，能够生成更精细、更符合上下文的运动规划，从而提升数字人的真实感和交互性。
*   <strong>技术趋势：</strong> 扩散模型在生成高质量视频方面表现出色，但计算成本较高；GANs在生成质量上有所提升，但多样性有限；VAEs在数据表示方面具有优势，但容易出现模式崩溃。多模态融合是主流趋势。
*   <strong>挑战与未来方向：</strong> 明确指出了当前面临的挑战，并为未来的研究提供了启发性方向，有助于推动数字人综合应用的发展。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>LLMs在运动规划中的应用仍处于早期阶段：</strong> 尽管LLMs显示出潜力，但目前主要通过文本作为中介与生成模型集成，缺乏更有效和新颖的中间表示探索。
*   <strong>现有方法在复杂运动和精细控制上的不足：</strong> 文本输入在复杂运动中效果有限，难以捕捉精细的运动模式。多模态方法在精细控制特定身体部位（如手部和面部细节）方面仍面临挑战。
*   <strong>数据稀缺和质量问题：</strong> 人体运动视频生成领域受到数据可用性、隐私问题、数据质量和高收集成本的限制，影响了模型的鲁棒性和真实世界可靠性。
*   <strong>光照真实感不足：</strong> 生成的人体形态，特别是面部和手部的光照真实感，仍需改进。
*   <strong>视频持续时间和控制范围有限：</strong> 大多数方法只能生成短视频片段，扩展到更长时间的视频仍是重大挑战。
*   <strong>实时性能和成本：</strong> 扩散模型计算成本高昂，实时流媒体应用面临低延迟和高带宽的挑战。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>更有效的LLM中间表示：</strong> 探索超越文本描述的、更有效和新颖的中间表示，以增强LLMs在运动规划中的理解和行为规划能力。
*   <strong>多人物驱动和交互：</strong> 探索多人物面部驱动和多人物交互任务的方法，以实现更复杂的数字人场景。
*   <strong>高效的端到端训练范式：</strong> 开发更高效的训练范式，以降低计算开销，同时保持视频质量和一致性。
*   <strong>少样本学习：</strong> 探索少样本学习方法，以解决训练数据量大的问题，尤其是在视频驱动的舞蹈视频生成中。
*   <strong>替代视频生成架构：</strong> 探索DiT或VAR等替代架构，以实现更稳定和连贯的视频生成，解决基于Unet的扩散模型中帧间不一致和闪烁伪影的问题。
*   <strong>精细化控制和长视频生成：</strong> 提升对特定身体部位（如手部、面部细节）的精细控制，并开发能够生成长时间、高质量视频的技术。
*   <strong>实时部署和成本优化：</strong> 研发更高效的模型和流媒体技术，以降低计算成本，实现数字人在实时平台上的部署。
*   <strong>伦理框架和隐私保护：</strong> 建立健全的伦理框架，确保生物识别数据使用的知情同意，并解决数字人生成中的隐私和负面影响问题。</p>
<p>这篇综述为人体运动视频生成领域提供了一个全面的路线图，不仅总结了现有技术，还指明了未来的研究方向，对于推动数字人技术的发展具有重要价值。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Our goal for this survey is to unveil the prospects of human
motion video generation and serve as a valuable resource for advancing the
comprehensive applications of digital humans.</li>
<li>A complete list of the models
examined in this survey is available in Our Repository
https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.03883v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.03883v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.04438v1'></a></p>
<h2 id="the-telephone-game-evaluating-semantic-drift-in-unified-models"><a href="https://arxiv.org/abs/2509.04438v1">The Telephone Game: Evaluating Semantic Drift in Unified Models</a></h2>
<p><strong>Authors:</strong> Sabbir Mollah, Rohit Gupta, Sirnam Swetha, Qingyang Liu, Ahnaf Munir, Mubarak Shah</p>
<p><strong>Published:</strong> 2025-09-04</p>
<p><strong>Categories:</strong> cs.CV, cs.CL</p>
<p><strong>Abstract:</strong></p>
<p>Employing a single, unified model (UM) for both visual understanding
(image-to-text: I2T) and and visual generation (text-to-image: T2I) has opened
a new direction in Visual Language Model (VLM) research. While UMs can also
support broader unimodal tasks (e.g., text-to-text, image-to-image), we focus
on the core cross-modal pair T2I and I2T, as consistency between understanding
and generation is critical for downstream use. Existing evaluations consider
these capabilities in isolation: FID and GenEval for T2I, and benchmarks such
as MME, MMBench for I2T. These single-pass metrics do not reveal whether a
model that understands a concept can also render it, nor whether meaning is
preserved when cycling between image and text modalities. To address this, we
introduce the Unified Consistency Framework for Unified Models (UCF-UM), a
cyclic evaluation protocol that alternates I2T and T2I over multiple
generations to quantify semantic drift. UCF formulates 3 metrics: (i) Mean
Cumulative Drift (MCD), an embedding-based measure of overall semantic loss;
(ii) Semantic Drift Rate (SDR), that summarizes semantic decay rate; and (iii)
Multi-Generation GenEval (MGG), an object-level compliance score extending
GenEval. To assess generalization beyond COCO, which is widely used in
training; we create a new benchmark ND400, sampled from NoCaps and DOCCI and
evaluate on seven recent models. UCF-UM reveals substantial variation in
cross-modal stability: some models like BAGEL maintain semantics over many
alternations, whereas others like Vila-u drift quickly despite strong
single-pass scores. Our results highlight cyclic consistency as a necessary
complement to standard I2T and T2I evaluations, and provide practical metrics
to consistently assess unified model's cross-modal stability and strength of
their shared representations. Code:
https://github.com/mollahsabbir/Semantic-Drift-in-Unified-Models</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Sabbir Mollah等人撰写的论文“The Telephone Game: Evaluating Semantic Drift in Unified Models”的全面摘要。</p>
<hr />
<h3 id="the-telephone-game-evaluating-semantic-drift-in-unified-models_1">论文摘要：The Telephone Game: Evaluating Semantic Drift in Unified Models</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决统一模型（Unified Models, UMs）在视觉理解（I2T，图像到文本）和视觉生成（T2I，文本到图像）任务之间进行交替转换时，语义信息是否能被有效保留的问题。现有的评估方法通常孤立地衡量I2T和T2I能力（例如，T2I使用FID和GenEval，I2T使用MME和MMBench），这些单次通过的指标无法揭示模型在图像和文本模态之间循环时，是否能持续理解和生成同一概念，以及语义是否会发生漂移。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
为了解决上述问题，作者提出了<strong>统一模型一致性框架（Unified Consistency Framework for Unified Models, UCF-UM）</strong>。这是一个循环评估协议，通过在多代I2T和T2I交替转换中量化语义漂移。UCF-UM引入了三项核心指标：
*   <strong>(i) 平均累积漂移（Mean Cumulative Drift, MCD）</strong>：一种基于嵌入的度量，用于量化整体语义损失。
*   <strong>(ii) 语义漂移率（Semantic Drift Rate, SDR）</strong>：总结语义衰减速率。
*   <strong>(iii) 多代GenEval（Multi-Generation GenEval, MGG）</strong>：扩展了GenEval的对象级合规性得分，以评估多代生成中的对象级保真度。</p>
<p>此外，为了评估模型在COCO之外的泛化能力，作者创建了一个新的基准数据集<strong>ND400</strong>，该数据集从NoCaps和DOCCI中采样，包含新颖对象和细粒度视觉细节，以更好地探测模型的泛化性。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   UCF-UM评估揭示了统一模型在跨模态稳定性方面存在显著差异。
*   一些模型，如<strong>BAGEL</strong>，在多次交替转换中能很好地保持语义。
*   而另一些模型，如<strong>Vila-u</strong>和<strong>Janus</strong>系列模型，尽管在单次通过评估中表现强劲，但语义漂移迅速。这表明单次通过指标可能高估了模型的鲁棒性。
*   研究结果强调，循环一致性是标准I2T和T2I评估的必要补充，并提供了评估统一模型跨模态稳定性和共享表示强度的实用指标。
*   MGG结果显示，在复杂任务（如定位和属性绑定）上，模型的性能下降最为显著，这可能是语义漂移的原因。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   现有评估方法未能捕捉到模型在多轮转换中实体、属性、关系和计数信息的保留情况。
*   论文主要关注I2T和T2I任务，而对更广泛的单模态任务（如文本到文本、图像到图像）的语义漂移分析较少。
*   CLIPScore等指标虽然使用嵌入来衡量语义对齐，但可能不总是与人类感知一致。
*   GenEval虽然检查对象和关系级合规性，但未评估整体视觉质量或真实感。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   进一步探索不同架构设计（共享权重、部分共享、解耦）对语义稳定性的影响。
*   研究如何通过改进模型架构、训练方法和数据集来增强统一模型的跨模态一致性和语义保真度。
*   将UCF-UM框架扩展到更广泛的单模态任务，以全面评估统一模型的语义漂移。
*   开发新的指标，能够更好地反映人类对语义漂移的感知。
*   深入分析模型在特定复杂任务（如组合属性绑定）上的脆弱性，并寻找解决方案。</p>
<hr />
<p>这篇论文通过引入一个新颖的循环评估框架，为统一模型的评估提供了一个全新的视角，强调了语义一致性在实际应用中的重要性，并揭示了现有单次通过指标无法捕捉到的模型行为。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Employing a single, unified model (UM) for both visual understanding
(image-to-text: I2T) and and visual generation (text-to-image: T2I) has opened
a new direction in Visual Language Model (VLM) research.</li>
<li>To assess generalization beyond COCO, which is widely used in
training; we create a new benchmark ND400, sampled from NoCaps and DOCCI and
evaluate on seven recent models.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.04438v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.04438v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.04394v1'></a></p>
<h2 id="transition-models-rethinking-the-generative-learning-objective"><a href="https://arxiv.org/abs/2509.04394v1">Transition Models: Rethinking the Generative Learning Objective</a></h2>
<p><strong>Authors:</strong> Zidong Wang, Yiyuan Zhang, Xiaoyu Yue, Xiangyu Yue, Yangguang Li, Wanli Ouyang, Lei Bai</p>
<p><strong>Published:</strong> 2025-09-04</p>
<p><strong>Categories:</strong> cs.LG, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>A fundamental dilemma in generative modeling persists: iterative diffusion
models achieve outstanding fidelity, but at a significant computational cost,
while efficient few-step alternatives are constrained by a hard quality
ceiling. This conflict between generation steps and output quality arises from
restrictive training objectives that focus exclusively on either infinitesimal
dynamics (PF-ODEs) or direct endpoint prediction. We address this challenge by
introducing an exact, continuous-time dynamics equation that analytically
defines state transitions across any finite time interval. This leads to a
novel generative paradigm, Transition Models (TiM), which adapt to
arbitrary-step transitions, seamlessly traversing the generative trajectory
from single leaps to fine-grained refinement with more steps. Despite having
only 865M parameters, TiM achieves state-of-the-art performance, surpassing
leading models such as SD3.5 (8B parameters) and FLUX.1 (12B parameters) across
all evaluated step counts. Importantly, unlike previous few-step generators,
TiM demonstrates monotonic quality improvement as the sampling budget
increases. Additionally, when employing our native-resolution strategy, TiM
delivers exceptional fidelity at resolutions up to 4096x4096.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Zidong Wang等人撰写的论文“Transition Models: Rethinking the Generative Learning Objective”的全面摘要。</p>
<hr />
<h3 id="transition-models-rethinking-the-generative-learning-objective_1">论文摘要：Transition Models: Rethinking the Generative Learning Objective</h3>
<p><strong>1. 主要问题或研究问题：</strong>
生成模型领域存在一个核心困境：迭代扩散模型虽然能实现卓越的图像生成质量，但计算成本高昂；而高效的少步生成方法则受限于固定的质量上限。这种生成步数与输出质量之间的冲突源于现有训练目标过于局限，它们要么只关注无穷小动态（PF-ODEs），要么只关注直接端点预测。论文旨在解决这一问题，即如何开发一种生成模型，既能实现少步高效生成，又能通过增加采样步数实现质量的单调提升，并支持任意分辨率的生成。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
该论文引入了一种名为<strong>Transition Models (TiM)</strong> 的新型生成范式，其核心创新在于：
*   <strong>精确的连续时间动态方程：</strong> TiM 提出一个精确的、连续时间动态方程，能够分析性地定义任意有限时间间隔 <script type="math/tex">\Delta t</script> 内的状态转换。这使得模型不再仅仅是近似微分方程或统计映射，而是学习生成过程本身的“解流形”。
*   <strong>任意步长转换能力：</strong> TiM 能够适应任意步长的转换，无缝地在单次大跳跃和多步精细化之间遍历生成轨迹。这统一了少步和多步生成范式。
*   <strong>隐式轨迹一致性与时间斜率匹配：</strong> 论文推导出了“状态转换恒等式”（State Transition Identity），它强制模型在任意起始时间 <script type="math/tex">t</script> 到相同目标 <script type="math/tex">x_r</script> 的路径上保持一致性，并要求模型不仅最小化残差值，还要最小化残差的时间导数，从而学习更平滑的解流形，确保大步采样时的连贯性和小步精细化时的稳定性。
*   <strong>可扩展且稳定的训练：</strong> 针对计算网络时间导数带来的可扩展性挑战，TiM 提出了“微分推导方程”（Differential Derivation Equation, DDE）作为一种高效的有限差分近似方法，其前向传播结构与FSDP等分布式训练优化兼容，使得训练数十亿参数的模型成为可能。此外，通过引入损失加权方案，优先处理短间隔转换，解决了梯度方差问题，提高了训练稳定性。
*   <strong>改进的架构：</strong> 引入了解耦时间嵌入（Decoupled Time Embedding）和间隔感知注意力（Interval-Aware Attention），使模型能够明确地同时考虑绝对时间 <script type="math/tex">t</script> 和转换间隔 <script type="math/tex">\Delta t</script> 的影响，从而在所有采样步长上获得显著性能提升。</p>
<p><strong>3. 主要结果及其重要性：</strong>
*   <strong>卓越的性能：</strong> 尽管TiM模型仅有8.65亿参数，但在GenEval基准测试中，其性能超越了参数量更大的领先模型，如SD3.5（80亿参数）和FLUX.1（120亿参数），并在所有评估的步数下都达到了最先进水平。
*   <strong>单调质量提升：</strong> 与以往的少步生成器不同，TiM 随着采样预算的增加，质量表现出单调提升，解决了少步模型质量饱和的问题。
*   <strong>高分辨率和多样化长宽比支持：</strong> 采用原生分辨率训练策略，TiM 在高达 4096x4096 的分辨率下以及各种长宽比下均能提供卓越的图像生成质量。
*   <strong>效率和可扩展性：</strong> DDE方法比传统的JVP方法快约2倍，且与FSDP兼容，使得从头开始训练数十亿参数的TiM模型成为可能。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>内容安全和可控性：</strong> 尽管TiM在基础生成模型方面做出了重大贡献，但确保内容安全和可控性仍然是一个开放的挑战。
*   <strong>精细细节退化：</strong> 在需要精细细节（如渲染文本和手部）的场景中，模型保真度可能会下降。
*   <strong>高分辨率下的伪影：</strong> 在高分辨率（例如3072x4096）下，偶尔会观察到伪影，这可能归因于底层自编码器中的偏差。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
论文的成功为新一代基础模型铺平了道路，这些模型既高效、可扩展，又在创意潜力方面充满前景。未来的研究可以集中于：
*   进一步提升模型在精细细节生成方面的能力，以解决文本和手部渲染等场景中的保真度问题。
*   探索如何减少高分辨率生成中的伪影，可能通过改进底层自编码器或调整训练策略。
*   研究如何将内容安全和可控性机制整合到TiM框架中，以应对生成模型面临的伦理和社会挑战。</p>
<hr />
<p>总而言之，这篇论文通过引入Transition Models (TiM) 及其创新的状态转换恒等式和高效的DDE计算方法，成功地重新思考了生成学习目标。TiM不仅在性能上超越了现有模型，还在效率、可扩展性和多功能性方面取得了显著进展，为生成式AI的未来发展奠定了坚实基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This leads to a
novel generative paradigm, Transition Models (TiM), which adapt to
arbitrary-step transitions, seamlessly traversing the generative trajectory
from single leaps to fine-grained refinement with more steps.</li>
<li>Despite having
only 865M parameters, TiM achieves state-of-the-art performance, surpassing
leading models such as SD3.5 (8B parameters) and FLUX.1 (12B parameters) across
all evaluated step counts.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.04394v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.04394v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.04379v1'></a></p>
<h2 id="ssgaussian-semantic-aware-and-structure-preserving-3d-style-transfer"><a href="https://arxiv.org/abs/2509.04379v1">SSGaussian: Semantic-Aware and Structure-Preserving 3D Style Transfer</a></h2>
<p><strong>Authors:</strong> Jimin Xu, Bosheng Qin, Tao Jin, Zhou Zhao, Zhenhui Ye, Jun Yu, Fei Wu</p>
<p><strong>Published:</strong> 2025-09-04</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Recent advancements in neural representations, such as Neural Radiance Fields
and 3D Gaussian Splatting, have increased interest in applying style transfer
to 3D scenes. While existing methods can transfer style patterns onto
3D-consistent neural representations, they struggle to effectively extract and
transfer high-level style semantics from the reference style image.
Additionally, the stylized results often lack structural clarity and
separation, making it difficult to distinguish between different instances or
objects within the 3D scene. To address these limitations, we propose a novel
3D style transfer pipeline that effectively integrates prior knowledge from
pretrained 2D diffusion models. Our pipeline consists of two key stages: First,
we leverage diffusion priors to generate stylized renderings of key viewpoints.
Then, we transfer the stylized key views onto the 3D representation. This
process incorporates two innovative designs. The first is cross-view style
alignment, which inserts cross-view attention into the last upsampling block of
the UNet, allowing feature interactions across multiple key views. This ensures
that the diffusion model generates stylized key views that maintain both style
fidelity and instance-level consistency. The second is instance-level style
transfer, which effectively leverages instance-level consistency across
stylized key views and transfers it onto the 3D representation. This results in
a more structured, visually coherent, and artistically enriched stylization.
Extensive qualitative and quantitative experiments demonstrate that our 3D
style transfer pipeline significantly outperforms state-of-the-art methods
across a wide range of scenes, from forward-facing to challenging 360-degree
environments. Visit our project page https://jm-xu.github.io/SSGaussian for
immersive visualization.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Jimin Xu等人撰写的论文“SSGaussian: Semantic-Aware and Structure-Preserving 3D Style Transfer”的全面摘要。</p>
<hr />
<h3 id="ssgaussian-semantic-aware-and-structure-preserving-3d-style-transfer_1">SSGaussian: Semantic-Aware and Structure-Preserving 3D Style Transfer 论文摘要</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决3D场景风格迁移中的核心挑战。现有方法虽然能将风格模式迁移到3D一致的神经表示上，但在提取和迁移参考风格图像中的高层风格语义方面表现不足。此外，风格化结果往往缺乏结构清晰度和对象分离性，导致难以区分3D场景中的不同实例或对象。</p>
<p><strong>2. 关键创新或方法贡献：</strong>
SSGaussian提出了一种新颖的3D风格迁移流程，有效整合了预训练2D扩散模型的先验知识，并包含两个关键创新：</p>
<ul>
<li><strong>跨视图风格对齐（Cross-View Style Alignment, CVSA）模块：</strong> 为了解决2D扩散模型在多视图一致性方面的挑战，CVSA模块将跨视图注意力机制引入到U-Net的最后一个上采样块中。这使得特征能够在多个关键视图之间进行交互，确保扩散模型生成的风格化关键视图在保持风格保真度的同时，也具备实例级别的（而非像素级别的）一致性。</li>
<li><strong>实例级别风格迁移（Instance-level Style Transfer, IST）方法：</strong> 该方法利用高斯分组（Gaussian Grouping）提供的身份编码参数，建立训练视图与风格化关键视图之间局部区域（即实例）的对应关系。通过在匹配的局部组内执行最近邻特征匹配（NNFM），IST方法能将风格化关键视图的实例级别一致性有效迁移到3D表示上，从而实现更具结构化、视觉连贯且艺术性丰富的风格化。</li>
</ul>
<p>整个流程分为两个阶段：首先，利用扩散先验生成关键视角的风格化渲染；然后，将这些风格化的关键视图迁移到3D表示上。</p>
<p><strong>3. 主要结果及其意义：</strong>
论文通过广泛的定性和定量实验证明，SSGaussian在多种场景（从前向场景到具有挑战性的360度环境）下的3D风格迁移性能显著优于现有最先进的方法。</p>
<ul>
<li><strong>定性结果：</strong> 风格化结果在保持原始内容结构的同时，能更好地提取和迁移高层风格语义及精细笔触细节。特别是，在高度细节化的场景中，SSGaussian能更有效地保持精细细节，并实现局部风格化，使得3D场景中的不同区域（实例）区分更清晰，视觉连贯性更强。</li>
<li><strong>定量结果：</strong> 在多视图一致性（短程和长程一致性）方面，SSGaussian在LPIPS和RMSE指标上均优于基线方法。在渲染质量评估（内容损失和风格损失）方面，SSGaussian也表现出卓越性能，表明其在保持内容结构的同时，能更好地捕捉风格特征。</li>
<li><strong>用户研究：</strong> 用户研究结果进一步证实，SSGaussian在“结构完整性”、“风格相似性”和“视觉质量”三个维度上均优于所有对比方法，证明了其在语义感知和结构保持3D风格迁移方面的卓越性能。</li>
<li><strong>速度：</strong> SSGaussian实现了高效的风格化和实时渲染性能，与最快的替代方法相当。</li>
</ul>
<p><strong>4. 论文中提及的局限性：</strong>
论文中没有明确提及当前方法的具体局限性。然而，从方法描述和实验设置中可以推断出一些潜在的方面：
*   <strong>计算资源：</strong> 虽然论文提到速度与最快方法相当，但3D风格迁移，尤其是涉及扩散模型和3D高斯表示的迭代优化，通常仍需要较高的计算资源。
*   <strong>风格泛化性：</strong> 尽管论文使用了多样化的风格参考图像，但对于某些极端或高度抽象的风格，其效果可能仍有提升空间。
*   <strong>对高斯分组的依赖：</strong> 该方法依赖于高斯分组来获取实例分割信息。如果高斯分组在某些复杂场景中表现不佳，可能会影响风格迁移的准确性。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
论文中没有明确提出未来的研究方向。但基于其贡献和潜在局限性，可以推断出以下方向：
*   <strong>实时性能优化：</strong> 进一步探索更高效的算法或模型架构，以实现更快的训练和渲染速度，使其更适用于交互式应用。
*   <strong>更复杂的风格语义理解：</strong> 探索如何从参考图像中提取更抽象、更复杂的风格语义，并将其更鲁斯地迁移到3D场景中。
*   <strong>用户可控性：</strong> 增加用户对风格迁移过程的精细控制能力，例如允许用户指定特定区域的风格或调整风格强度。
*   <strong>结合其他3D表示：</strong> 探索将该方法扩展到其他3D表示，如神经辐射场（NeRF）或其他隐式表示，以评估其通用性。
*   <strong>动态场景风格迁移：</strong> 将方法扩展到动态3D场景，处理时间一致性问题。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these limitations, we propose a novel
3D style transfer pipeline that effectively integrates prior knowledge from
pretrained 2D diffusion models.</li>
<li>Extensive qualitative and quantitative experiments demonstrate that our 3D
style transfer pipeline significantly outperforms state-of-the-art methods
across a wide range of scenes, from forward-facing to challenging 360-degree
environments.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.04379v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.04379v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.04338v1'></a></p>
<h2 id="from-editor-to-dense-geometry-estimator"><a href="https://arxiv.org/abs/2509.04338v1">From Editor to Dense Geometry Estimator</a></h2>
<p><strong>Authors:</strong> JiYuan Wang, Chunyu Lin, Lei Sun, Rongying Liu, Lang Nie, Mingxing Li, Kang Liao, Xiangxiang Chu, Yao Zhao</p>
<p><strong>Published:</strong> 2025-09-04</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Leveraging visual priors from pre-trained text-to-image (T2I) generative
models has shown success in dense prediction. However, dense prediction is
inherently an image-to-image task, suggesting that image editing models, rather
than T2I generative models, may be a more suitable foundation for fine-tuning.
  Motivated by this, we conduct a systematic analysis of the fine-tuning
behaviors of both editors and generators for dense geometry estimation. Our
findings show that editing models possess inherent structural priors, which
enable them to converge more stably by <code>refining" their innate features, and
ultimately achieve higher performance than their generative counterparts.
  Based on these findings, we introduce \textbf{FE2E}, a framework that
pioneeringly adapts an advanced editing model based on Diffusion Transformer
(DiT) architecture for dense geometry prediction. Specifically, to tailor the
editor for this deterministic task, we reformulate the editor's original flow
matching loss into the</code>consistent velocity" training objective. And we use
logarithmic quantization to resolve the precision conflict between the editor's
native BFloat16 format and the high precision demand of our tasks.
Additionally, we leverage the DiT's global attention for a cost-free joint
estimation of depth and normals in a single forward pass, enabling their
supervisory signals to mutually enhance each other.
  Without scaling up the training data, FE2E achieves impressive performance
improvements in zero-shot monocular depth and normal estimation across multiple
datasets. Notably, it achieves over 35\% performance gains on the ETH3D dataset
and outperforms the DepthAnything series, which is trained on 100<script type="math/tex">\times</script> data.
The project page can be accessed \href{https://amap-ml.github.io/FE2E/}{here}.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供JiYuan Wang等人撰写的论文“From Editor to Dense Geometry Estimator”的全面摘要。</p>
<hr />
<h3 id="from-editor-to-dense-geometry-estimator_1">《From Editor to Dense Geometry Estimator》论文摘要</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决单目稠密几何估计（如深度和法线估计）中的一个核心问题：如何更有效地利用预训练的视觉模型来提升稠密预测任务的性能，尤其是在数据有限的情况下。现有研究多利用文本到图像（T2I）生成模型作为基础，但作者提出，稠密预测本质上是图像到图像（I2I）任务，因此图像编辑模型可能比T2I生成模型更适合作为微调的基础。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
该论文的核心创新在于提出了<strong>FE2E</strong>（From Editor to Estimator）框架，该框架首次将基于Diffusion Transformer (DiT) 架构的先进图像编辑模型应用于稠密几何预测任务。具体贡献包括：</p>
<ul>
<li><strong>系统性分析与洞察：</strong> 作者对图像编辑模型和生成模型在稠密几何估计任务上的微调行为进行了系统性分析。研究发现，编辑模型具有固有的结构先验，能够更稳定地收敛，并通过“提炼”其内在特征，最终实现比生成模型更高的性能。</li>
<li><strong>“一致速度”训练目标：</strong> 针对稠密预测的确定性性质，FE2E将编辑模型原始的流匹配损失重新表述为“一致速度”（consistent velocity）训练目标。这通过固定起始点并确保速度方向和大小的一致性，消除了离散曲线轨迹和随机起始点引入的误差，显著提高了推理效率和性能。</li>
<li><strong>对数式量化：</strong> 为了解决编辑模型原生BFloat16格式与稠密几何任务高精度需求之间的冲突，论文引入了对数式量化方法。这种方法在近距离和远距离范围内都能保持合理且几乎恒定的相对误差，有效解决了传统均匀量化和逆量化方案的精度问题。</li>
<li><strong>无成本深度与法线联合估计：</strong> FE2E利用DiT的全局注意力机制，在单次前向传播中实现了深度和法线的联合估计，无需额外计算成本。这使得两种监督信号能够相互增强，提升整体性能。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
FE2E在多个数据集上的零样本单目深度和法线估计任务中取得了显著的性能提升：</p>
<ul>
<li><strong>深度估计：</strong> 在ETH3D数据集上，FE2E的性能提升超过35%。尽管训练数据量远小于DepthAnything系列（FE2E使用71K数据，而DepthAnything使用62.6M数据，相差100倍），FE2E的平均排名仍超越了DepthAnything系列。在KITTI数据集上，AbsRel误差降低了10%。</li>
<li><strong>法线估计：</strong> FE2E在零样本法线估计任务中也取得了最先进的性能，尤其擅长重建复杂的几何细节，如表面褶皱和小型物体。</li>
<li><strong>定性结果：</strong> 定性比较显示，FE2E在挑战性光照条件（极亮、低光）下表现优异，并能更好地保留远距离细节。</li>
</ul>
<p>这些结果的意义在于，FE2E验证了“从编辑模型到估计器”的范式，证明了利用编辑模型固有的能力是一种有效且数据高效的稠密预测方法，为未来稠密几何估计任务提供了新的基础。</p>
<p><strong>4. 论文中提及的局限性：</strong>
论文中提到了FE2E的一个主要局限性是<strong>计算负载较大</strong>。尽管FE2E在性能和计算效率之间取得了平衡，但相对于其他自监督方法，DiT架构的引入确实导致了计算复杂度的显著增加。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
论文指出了以下未来研究方向：</p>
<ul>
<li><strong>多样化基础模型：</strong> 图像编辑领域发展迅速，FE2E的方法设计是模型无关的。未来工作计划整合更广泛的编辑模型，以进一步证实本文提出的动机和结论。</li>
<li><strong>扩大训练数据规模：</strong> 尽管FE2E在有限数据量下展示了强大的泛化性能，但作者仍预期扩大训练数据集可以进一步提升模型的性能。对于那些对计算复杂度不敏感但要求极高预测精度的领域，这是一个有意义的方向。</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>Based on these findings, we introduce \textbf{FE2E}, a framework that
pioneeringly adapts an advanced editing model based on Diffusion Transformer
(DiT) architecture for dense geometry prediction.</li>
<li>Notably, it achieves over 35\% performance gains on the ETH3D dataset
and outperforms the DepthAnything series, which is trained on 100<script type="math/tex">\times</script> data.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.04338v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.04338v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.04334v1'></a></p>
<h2 id="geoarena-an-open-platform-for-benchmarking-large-vision-language-models-on-worldwide-image-geolocalization"><a href="https://arxiv.org/abs/2509.04334v1">GeoArena: An Open Platform for Benchmarking Large Vision-language Models on WorldWide Image Geolocalization</a></h2>
<p><strong>Authors:</strong> Pengyue Jia, Yingyi Zhang, Xiangyu Zhao, Yixuan Li</p>
<p><strong>Published:</strong> 2025-09-04</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Image geolocalization aims to predict the geographic location of images
captured anywhere on Earth, but its global nature presents significant
challenges. Current evaluation methodologies suffer from two major limitations.
First, data leakage: advanced approaches often rely on large vision-language
models (LVLMs) to predict image locations, yet these models are frequently
pretrained on the test datasets, compromising the accuracy of evaluating a
model's actual geolocalization capability. Second, existing metrics primarily
rely on exact geographic coordinates to assess predictions, which not only
neglects the reasoning process but also raises privacy concerns when user-level
location data is required. To address these issues, we propose GeoArena, a
first open platform for evaluating LVLMs on worldwide image geolocalization
tasks, offering true in-the-wild and human-centered benchmarking. GeoArena
enables users to upload in-the-wild images for a more diverse evaluation
corpus, and it leverages pairwise human judgments to determine which model
output better aligns with human expectations. Our platform has been deployed
online for two months, during which we collected over thousands voting records.
Based on this data, we conduct a detailed analysis and establish a leaderboard
of different LVLMs on the image geolocalization task.</p>
<p><strong>Analysis:</strong></p>
<p>好的，以下是Pengyue Jia, Yingyi Zhang, Xiangyu Zhao, Yixuan Li撰写的论文“GeoArena: An Open Platform for Benchmarking Large Vision-language Models on Worldwide Image Geolocalization”的全面摘要：</p>
<p><strong>论文题目：</strong> GeoArena: 一个用于基准测试全球图像地理定位大型视觉-语言模型的开放平台</p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决当前图像全球地理定位任务评估方法存在的两个主要限制：
1. <strong>数据泄露（Data Leakage）：</strong> 现有的评估基准通常使用静态数据集，而大型视觉-语言模型（LVLMs）在预训练阶段可能已经接触过这些测试数据，导致评估结果无法真实反映模型的地理定位能力。
2. <strong>基于GPS的评估局限性（GPS-based Evaluation Limitations）：</strong> 现有的评估指标主要依赖精确的地理坐标来衡量预测准确性，这不仅忽略了模型的推理过程，而且在需要用户级位置数据时引发隐私问题。</p>
<p>为了解决这些问题，论文提出了一个更动态、以用户为中心且注重隐私的评估框架，以更好地基准测试LVLMs在全球图像地理定位任务上的性能。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
GeoArena平台引入了以下关键创新和方法论贡献：
*   <strong>开放平台与真实世界数据收集：</strong> GeoArena是一个开放的在线平台，允许用户上传真实世界的图像进行地理定位，从而构建一个多样化且动态的评估语料库，有效缓解了静态数据集的数据泄露问题。
*   <strong>以人为中心的基准测试：</strong> 平台采用成对的人类判断来评估模型输出的质量，用户对两个匿名模型生成的响应进行投票，选择更符合人类期望的答案。这种方法超越了单纯依赖GPS准确性的评估，并减轻了对精确用户位置数据的隐私担忧。
*   <strong>Elo排名系统与Bradley-Terry模型：</strong> GeoArena利用Elo排名系统和Bradley-Terry模型来计算模型的相对强度和最终排名。Elo系统根据成对比较结果迭代更新模型分数，而Bradley-Terry模型则提供了一种更稳定、与顺序无关的排名估计方法，确保了排名的可靠性。
*   <strong>GeoArena-1K数据集发布：</strong> 基于平台收集的数据，论文发布了GeoArena-1K数据集，这是首个用于图像地理定位领域LVLMs的人类偏好数据集，包含用户上传的图像、文本指令、模型响应和人类投票结果，为奖励建模和地理基础模型等相关研究提供了宝贵资源。
*   <strong>风格特征分析：</strong> 论文通过将风格相关特征（如响应长度、列表数量、标题数量、强调数量和GPS输出比例）纳入Bradley-Terry回归框架，分析了模型响应的哪些特征会影响用户偏好，揭示了用户更倾向于更长、更结构化、包含GPS信息的响应。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>GeoArena排行榜：</strong> 部署两个月以来，GeoArena收集了数千条投票记录，并基于此建立了LVLMs在图像地理定位任务上的排行榜。结果显示，Gemini系列模型（如Gemini-2.5-pro和Gemini-2.5-flash）表现最强，显著优于其他系统，凸显了大规模多模态预训练的优势。
*   <strong>开源模型的竞争力：</strong> Qwen2.5和Gemma-3等开源模型也取得了有竞争力的排名，例如Qwen2.5-VL-72B-Instruct与GPT-4.1系列表现相当，表明开源社区正在迅速缩小与专有前沿系统的差距。
*   <strong>模型容量与性能：</strong> 较小容量的模型（如gemma-3-4b-it、qwen2.5-vl-7b-instruct、gpt-4.1-nano和gpt-40-mini）表现普遍不佳，凸显了图像地理定位任务的固有难度，以及模型容量和训练数据对泛化能力的重要性。
*   <strong>人类偏好分析：</strong> 响应长度、列表数量和GPS输出比例与人类偏好呈正相关，表明用户更喜欢更长、更结构化、包含GPS预测的响应，这强调了推理质量的重要性。
*   <strong>LVLM与人类判断的一致性：</strong> 对比研究显示，Gemini 2.5 Pro与人类评估的一致性（65.79%）显著高于Qwen-VL-72B（46.67%），表明顶级专有模型在评估地理定位任务响应时与人类判断更一致，但仍存在显著差距，激励未来研究设计更忠实、鲁棒的LLM评估器。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>Elo排名对近期匹配的敏感性：</strong> 传统的Elo排名系统对近期匹配结果高度敏感，可能导致排名受匹配顺序影响，这在评估模型能力时是不理想的。论文通过采用Bradley-Terry模型来缓解这一问题。
*   <strong>稀疏采样单元的可靠性：</strong> 在成对比较中，如果某些模型对之间的战斗次数较少（稀疏采样），其胜率对比的可靠性可能较低，需要谨慎解释。
*   <strong>LVLM作为评估器的局限性：</strong> 尽管顶级LVLM在评估地理定位响应时与人类判断有一定一致性，但仍存在显著差距，表明它们在捕捉人类看重的细微标准（准确性、证据和清晰度）方面仍有挑战。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>奖励建模与地理基础模型：</strong> GeoArena-1K数据集的发布将支持奖励建模和地理基础模型等相关领域的研究进展。
*   <strong>更忠实、鲁棒的LLM评估器：</strong> 鉴于LVLM作为评估器与人类判断之间仍存在差距，未来的工作可以专注于设计更忠实、鲁棒的LLM评估器。
*   <strong>模型开发：</strong> 针对“困难案例”进行训练和评估，以提高模型在视觉线索不明显或不直观的场景下的鲁棒性和一致性性能。
*   <strong>多模态推理：</strong> 进一步探索LVLMs在多模态推理中的能力，特别是文本识别如何增强位置预测准确性。</p>
<p>总而言之，GeoArena为全球图像地理定位任务提供了一个开创性的、以用户为中心、注重隐私的动态基准测试平台，通过结合真实世界数据和人类偏好评估，克服了现有静态基准的局限性，并为LVLMs在地理空间推理领域的研究和发展奠定了坚实基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these issues, we propose GeoArena, a
first open platform for evaluating LVLMs on worldwide image geolocalization
tasks, offering true in-the-wild and human-centered benchmarking.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.04334v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.04334v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.04326v1'></a></p>
<h2 id="efficient-odd-one-out-anomaly-detection"><a href="https://arxiv.org/abs/2509.04326v1">Efficient Odd-One-Out Anomaly Detection</a></h2>
<p><strong>Authors:</strong> Silvio Chito, Paolo Rabino, Tatiana Tommasi</p>
<p><strong>Published:</strong> 2025-09-04</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>The recently introduced odd-one-out anomaly detection task involves
identifying the odd-looking instances within a multi-object scene. This problem
presents several challenges for modern deep learning models, demanding spatial
reasoning across multiple views and relational reasoning to understand context
and generalize across varying object categories and layouts. We argue that
these challenges must be addressed with efficiency in mind. To this end, we
propose a DINO-based model that reduces the number of parameters by one third
and shortens training time by a factor of three compared to the current
state-of-the-art, while maintaining competitive performance. Our experimental
evaluation also introduces a Multimodal Large Language Model baseline,
providing insights into its current limitations in structured visual reasoning
tasks. The project page can be found at
https://silviochito.github.io/EfficientOddOneOut/</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Silvio Chito, Paolo Rabino和Tatiana Tommasi撰写的论文“Efficient Odd-One-Out Anomaly Detection”的全面摘要。</p>
<hr />
<h3 id="_1">《高效的异类异常检测》论文摘要</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决“异类异常检测”（odd-one-out anomaly detection）任务，即在包含多个物体的场景中识别出与众不同的异常实例。这项任务对现代深度学习模型提出了多重挑战，包括需要跨多个视角进行空间推理、理解上下文的关联推理，以及在不同物体类别和布局之间进行泛化。作者强调，解决这些挑战时必须兼顾效率。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>DINOv2-based高效模型：</strong> 论文提出了一种基于DINOv2的模型，该模型在保持竞争性性能的同时，将参数数量减少了三分之一，并将训练时间缩短了三倍，显著优于当前最先进的方法（OOO）。这通过直接从多视图输入图像中提取DINOv2特征，并将其投影到3D体素空间，避免了OOO模型中不必要的两步映射复杂性。
*   <strong>上下文匹配头（Context Match Head）与残差异常头（Residual Anomaly Head）：</strong> 模型通过ROI池化将3D体素网格中的物体裁剪归一化，然后使用Transformer编码器进行上下文匹配，以推理场景中物体间的相对外观。此外，引入了残差异常头，通过一个可学习的token作为场景特定正常原型，引导模型关注物体与平均正常状态的偏差，进一步提升了场景特定异常的识别能力。
*   <strong>多模态大语言模型（MLLM）基线：</strong> 论文首次引入了使用多模态大语言模型（如Gemini-Flash 2.0）作为异类异常检测任务的基线，并采用Set-of-Mark (SoM) 提示策略来支持视觉定位。这为评估MLLM在结构化视觉推理任务中的当前局限性提供了重要见解。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>性能优势：</strong> 提出的模型在Toys Seen数据集上与OOO模型表现相当，在Toys Unseen数据集上略逊一筹，但在更具挑战性的Parts Unseen数据集上显著优于OOO模型。这表明该模型在处理几何形状差异大、语义多样性低的机械部件场景时具有更强的泛化能力和效率。
*   <strong>效率提升：</strong> 与OOO模型相比，新模型在参数数量和训练时间上实现了显著优化（参数减少三分之一，训练时间缩短三倍），同时保持了竞争性的性能，这对于工业应用中对推理时间敏感的场景至关重要。
*   <strong>MLLM局限性：</strong> MLLM基线在Toys和Parts数据集上的表现不佳，其准确率接近于传统的检测方法（ImVoxelNet和DETR3D）。MLLM表现出对正常数据的偏见，擅长识别大型裂缝和断裂，但在需要跨多个视图进行组内比较时表现挣扎。这揭示了当前MLLM在精细结构化视觉推理和多视图一致性方面的局限性。
*   <strong>鲁棒性分析：</strong> 模型在不同视图数量和物体数量变化下表现出相对鲁棒性，尤其是在视图数量增加时性能有所提升。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>MLLM的视觉定位和精细推理能力：</strong> MLLM在异类异常检测任务中的表现揭示了其在视觉定位和处理精细异常细节方面的局限性，尤其是在需要跨多个视图进行比较时。它们倾向于识别全局外观异常，而非细粒度的结构化差异。
*   <strong>3D特定异常检测：</strong> 提出的模型在检测3D特定异常（如缺失部件、平移和变形）方面仍有不足，这些异常需要更全面的3D推理。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>结合预训练基础模型与效率：</strong> 未来的解决方案应继续利用预训练基础模型的能力，并以效率为关键目标，将异常检测扩展到完整的3D异常定位。
*   <strong>自然语言解释：</strong> 模型应能够通过自然语言阐明其预测背后的推理过程。
*   <strong>逻辑和功能方面扩展：</strong> 任务可以扩展到包含逻辑和功能方面，从而连接感知与机器人技术，并演变为对自主代理的真正智能测试。
*   <strong>引导通用模型进行多视图一致性推理：</strong> 需要新的定制模块来引导通用模型处理需要多视图一致性的任务。</p>
<hr />
<p>总而言之，这篇论文在异类异常检测任务中取得了重要进展，通过提出一个高效的DINOv2-based模型，显著提升了效率并保持了竞争性性能。同时，通过引入MLLM基线，为理解多模态大语言模型在复杂视觉推理任务中的当前能力和局限性提供了宝贵见解，为未来的研究指明了方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To this end, we
propose a DINO-based model that reduces the number of parameters by one third
and shortens training time by a factor of three compared to the current
state-of-the-art, while maintaining competitive performance.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.04326v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.04326v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.04180v1'></a></p>
<h2 id="visiofirm-cross-platform-ai-assisted-annotation-tool-for-computer-vision"><a href="https://arxiv.org/abs/2509.04180v1">VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer Vision</a></h2>
<p><strong>Authors:</strong> Safouane El Ghazouali, Umberto Michelucci</p>
<p><strong>Published:</strong> 2025-09-04</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>AI models rely on annotated data to learn pattern and perform prediction.
Annotation is usually a labor-intensive step that require associating labels
ranging from a simple classification label to more complex tasks such as object
detection, oriented bounding box estimation, and instance segmentation.
Traditional tools often require extensive manual input, limiting scalability
for large datasets. To address this, we introduce VisioFirm, an open-source web
application designed to streamline image labeling through AI-assisted
automation. VisioFirm integrates state-of-the-art foundation models into an
interface with a filtering pipeline to reduce human-in-the-loop efforts. This
hybrid approach employs CLIP combined with pre-trained detectors like
Ultralytics models for common classes and zero-shot models such as Grounding
DINO for custom labels, generating initial annotations with low-confidence
thresholding to maximize recall. Through this framework, when tested on
COCO-type of classes, initial prediction have been proven to be mostly correct
though the users can refine these via interactive tools supporting bounding
boxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has
on-the-fly segmentation powered by Segment Anything accelerated through WebGPU
for browser-side efficiency. The tool supports multiple export formats (YOLO,
COCO, Pascal VOC, CSV) and operates offline after model caching, enhancing
accessibility. VisioFirm demonstrates up to 90\% reduction in manual effort
through benchmarks on diverse datasets, while maintaining high annotation
accuracy via clustering of connected CLIP-based disambiguate components and
IoU-graph for redundant detection suppression. VisioFirm can be accessed from
\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于Safouane El Ghazouali和Umberto Michelucci撰写的论文“VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer Vision”的全面摘要。</p>
<p><strong>论文摘要：VisioFirm：跨平台AI辅助计算机视觉标注工具</strong></p>
<p><strong>1. 主要问题或研究问题</strong>
计算机视觉（CV）领域中，高质量的标注数据对于训练鲁棒的机器学习模型至关重要，尤其是在目标检测、定向边界框估计和实例分割等复杂任务中。然而，传统的数据标注过程通常是劳动密集型、耗时且难以扩展的，需要大量人工输入，并且容易出现主观性错误和数据偏差。现有工具在自动化和处理复杂任务方面存在局限性，难以满足大规模数据集的需求。</p>
<p><strong>2. 关键创新或方法论贡献</strong>
VisioFirm引入了一个开源、跨平台的Web应用程序，旨在通过AI辅助自动化来简化图像标注过程。其核心创新和方法论贡献包括：</p>
<ul>
<li><strong>混合AI辅助标注管道：</strong> VisioFirm结合了预训练检测模型（如Ultralytics YOLO模型）用于常见类别，以及零样本模型（如Grounding DINO）用于自定义标签。这种方法以低置信度阈值生成初始高召回率的预标注，以最大化潜在对象的捕获。</li>
<li><strong>CLIP语义验证与过滤：</strong> 初始检测结果通过基于CLIP的语义验证和IoU图连接组件聚类进行过滤，以消除冗余检测并确保标签准确性。CLIP用于验证预测标签与裁剪图像内容的语义一致性。</li>
<li><strong>高效的交互式精炼工具：</strong> 用户可以通过交互式工具（支持边界框、定向边界框和多边形）精炼初始预测。</li>
<li><strong>WebGPU加速的实时分割：</strong> VisioFirm集成了WebGPU加速的Segment Anything Model (SAM2)，实现了浏览器端的实时分割，提高了效率。</li>
<li><strong>多格式导出与离线操作：</strong> 该工具支持多种标准导出格式（YOLO、COCO、Pascal VOC、CSV），并能在模型缓存后离线操作，增强了可访问性。</li>
<li><strong>用户友好的Web界面：</strong> 提供直观的界面，支持项目管理、键盘快捷键、缩放/平移控制以及多种标注模式（矩形、多边形、魔术棒）。</li>
</ul>
<p><strong>3. 主要结果及其意义</strong>
VisioFirm在多个数据集上的基准测试表明，它能够将手动标注工作量减少高达90%，同时保持高标注准确性。具体结果包括：</p>
<ul>
<li><strong>效率提升：</strong> 通过GPU加速，YOLOv10和Grounding DINO模型的推理延迟显著降低。例如，YOLOv10在0%阈值下，GPU模式比CPU模式快2.9倍；在50%阈值下，GPU模式快17倍。Grounding DINO在0%阈值下，GPU模式快5.7倍；在50%阈值下，GPU模式快4.1倍。这表明AI辅助自动化显著提高了标注效率。</li>
<li><strong>准确性保持：</strong> 尽管减少了人工干预，但通过CLIP验证和IoU图聚类等后处理步骤，VisioFirm能够保持高标注准确性。</li>
<li><strong>灵活性：</strong> 该工具能够处理COCO等常见类别以及通过零样本模型处理的自定义或领域特定类别，展现了其在多样化标注任务中的灵活性。</li>
<li><strong>可访问性：</strong> 作为开源项目，VisioFirm易于安装和部署，支持多种硬件配置（CPU或GPU），并提供离线操作能力。</li>
</ul>
<p><strong>4. 论文中提及的局限性</strong>
论文中提及了一些局限性，主要集中在AI模型的泛化能力和性能方面：</p>
<ul>
<li><strong>模型在领域特定或异常对象上的表现：</strong> 预训练和零样本模型在遇到训练数据集中未充分表示的复杂形状、纹理或遮挡的领域特定或异常对象时，可能会表现不佳，产生低置信度或错误检测。VisioFirm通过动态评估检测质量来应对此问题，并在必要时切换到AI辅助或纯手动模式。</li>
<li><strong>零样本模型的计算成本：</strong> 尽管Grounding DINO在生成不常见类别标签提案方面非常有效，但它在完整数据集上的计算时间相对较长，尤其是在低阈值下。</li>
</ul>
<p><strong>5. 潜在的未来研究方向</strong>
VisioFirm的未来发展方向旨在扩展其功能范围和兼容性：</p>
<ul>
<li><strong>集成更多高级模型：</strong> 计划集成Detectron2等框架，以支持更高级的实例分割工作流。</li>
<li><strong>支持多模态任务：</strong> 将纳入图像分类和图像字幕等CV相关标注，以支持多模态任务。</li>
<li><strong>视频标注支持：</strong> 扩展工具以支持视频数据，包括帧提取和基于跟踪的标注，从而处理时间序列数据。</li>
<li><strong>社区贡献：</strong> 鼓励社区通过拉取请求贡献，以扩展模型兼容性（例如，集成新兴检测器）和增强跨平台鲁棒性。</li>
</ul>
<p>总而言之，VisioFirm通过结合先进的AI模型、智能过滤管道和用户友好的交互式界面，为计算机视觉数据标注提供了一个高效、准确且可扩展的解决方案，显著减少了手动工作量，并为未来的研究和应用奠定了基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we introduce VisioFirm, an open-source web
application designed to streamline image labeling through AI-assisted
automation.</li>
<li>VisioFirm integrates state-of-the-art foundation models into an
interface with a filtering pipeline to reduce human-in-the-loop efforts.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.04180v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.04180v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.04150v1'></a></p>
<h2 id="revisiting-simple-baselines-for-in-the-wild-deepfake-detection"><a href="https://arxiv.org/abs/2509.04150v1">Revisiting Simple Baselines for In-The-Wild Deepfake Detection</a></h2>
<p><strong>Authors:</strong> Orlando Castaneda, Kevin So-Tang, Kshitij Gurung</p>
<p><strong>Published:</strong> 2025-09-04</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>The widespread adoption of synthetic media demands accessible deepfake
detectors and realistic benchmarks. While most existing research evaluates
deepfake detectors on highly controlled datasets, we focus on the recently
released "in-the-wild" benchmark, Deepfake-Eval-2024. Initial reporting on
Deepfake-Eval-2024 showed that three finetuned open-source models achieve
accuracies between 61% and 69%, significantly lagging behind the leading
commercial deepfake detector with 82% accuracy. Our work revisits one of these
baseline approaches, originally introduced by Ojha et al., which adapts
standard pretrained vision backbones to produce generalizable deepfake
detectors. We demonstrate that with better-tuned hyperparameters, this simple
approach actually yields much higher performance -- 81% accuracy on
Deepfake-Eval-2024 -- surpassing the previously reported accuracy of this
baseline approach by 18% and competing with commercial deepfake detectors. We
discuss tradeoffs in accuracy, computational costs, and interpretability,
focusing on how practical these deepfake detectors might be when deployed in
real-world settings. Our code can be found at
https://github.com/Deepfake-Detection-KKO/deepfake-detection.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“Revisiting Simple Baselines for In-The-Wild Deepfake Detection”的摘要，重点关注其核心内容和对计算机视觉领域的影响：</p>
<p><strong>论文摘要：重新审视野外深度伪造检测的简单基线</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决在“野外”（in-the-wild）真实世界场景中，深度伪造检测器性能不足的问题。现有的研究大多在高度受控的数据集上评估深度伪造检测器，导致其在真实世界中表现不佳。特别是，针对最近发布的“Deepfake-Eval-2024”基准测试，开源模型的准确率显著落后于领先的商业检测器（61%-69% vs 82%），这表明开源解决方案在实际应用中存在明显差距。论文的核心问题是：通过优化现有简单基线方法，能否显著提升其在野外深度伪造检测上的性能，使其与商业检测器相媲美？</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
该论文的核心贡献在于重新审视并优化了Ojha等人[17]提出的简单基线方法，即通过在标准预训练视觉骨干网络（如ResNet-50、ViT-b32和ConvNeXt-base）之上添加一个dropout层和一个线性分类器来构建深度伪造检测器。关键的创新和方法论改进包括：
*   <strong>超参数优化：</strong> 论文通过对学习率调度（余弦退火优于步长衰减）、预训练任务（CLIP预训练优于ImageNet或无预训练）以及适度的L2正则化和dropout率进行系统性优化，显著提升了模型性能。
*   <strong>模型架构选择：</strong> 评估了ResNet-50、ViT-b32和ConvNeXt-base三种不同的骨干网络，并发现CLIP预训练的ConvNeXt-base和ViT-b32表现最佳，这表明这些架构的表示能力和从CLIP学习到的特征对于深度伪造检测非常有效。
*   <strong>数据处理：</strong> 采用了一系列数据处理程序，包括将图像缩放到384像素，以及在训练时进行随机裁剪和缩放以增强数据。
*   <strong>性能权衡分析：</strong> 论文不仅关注准确率，还深入讨论了模型的计算成本（GFLOPs）、推理速度和可解释性（通过GradCAM可视化），为实际部署提供了重要见解。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>显著的性能提升：</strong> 经过优化的简单基线方法在Deepfake-Eval-2024数据集上实现了81%的准确率，比之前报告的该基线方法（63%）提高了18%，并与领先的商业深度伪造检测器（82%）的性能非常接近。
*   <strong>开源模型的竞争力：</strong> 这一结果表明，通过适当的超参数调优和预训练策略，开源模型也能在野外深度伪造检测任务中达到与商业解决方案相当的水平，从而降低了对专有技术的依赖。
*   <strong>预训练的重要性：</strong> CLIP预训练被证明对性能提升至关重要，尤其是在处理社交媒体来源的“野外”图像时，其效果优于ImageNet预训练。
*   <strong>模型权衡：</strong> ConvNeXt-base和ViT-b32在准确率上表现最佳，但ViT-b32在推理速度上更快，参数量更少，使其在时间敏感或高吞吐量场景中更具优势。ResNet-50虽然准确率略低，但模型尺寸更小，适用于资源受限的设备。
*   <strong>可解释性：</strong> GradCAM可视化揭示了不同模型关注图像的不同区域，ViT-b32倾向于关注更大、更分散的区域，而ConvNeXt-base和ResNet-50则更集中于较小的区域，这有助于理解模型决策机制。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>数据集规模和分布：</strong> Deepfake-Eval-2024数据集的规模相对较小，且其分布可能无法完全代表所有真实世界场景中的深度伪造。
*   <strong>泛化能力：</strong> 尽管该基准测试的结果值得关注，但其收集渠道和标注方法可能意味着这些检测器在所有真实世界设置中的表现不一定能完全代表。
*   <strong>技术快速发展：</strong> 深度伪造生成和检测技术发展迅速，需要持续投入资源开发新的、更真实的基准和模型。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>更大规模、更多样化的野外数据集：</strong> 开发更大、更具代表性的数据集，以更好地反映真实世界的深度伪造挑战。
*   <strong>多模态深度伪造检测：</strong> 探索结合视觉、音频和文本信息的多模态检测方法，以应对更复杂的深度伪造形式。
*   <strong>模型鲁棒性：</strong> 进一步研究如何提高深度伪造检测器对新型生成技术和对抗性攻击的鲁棒性。
*   <strong>轻量级和高效模型：</strong> 针对资源受限设备和实时应用，继续开发更轻量级、更高效的深度伪造检测模型。
*   <strong>可解释性与信任：</strong> 深入研究模型的可解释性，以增强用户对深度伪造检测器的信任，并帮助内容审核员和分析师更好地理解模型决策。</p>
<p>总而言之，这篇论文通过对简单基线方法的深入优化，为野外深度伪造检测领域设定了新的标准，证明了开源解决方案在真实世界应用中的巨大潜力，并为未来的研究提供了宝贵的见解。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We demonstrate that with better-tuned hyperparameters, this simple
approach actually yields much higher performance -- 81% accuracy on
Deepfake-Eval-2024 -- surpassing the previously reported accuracy of this
baseline approach by 18% and competing with commercial deepfake detectors.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.04150v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.04150v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-09-07 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
