<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-09-07 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DEâ«¶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Birdâs-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">è®ºæéè¯»</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-Î±
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-09-05/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-09-08/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-09-07">Arxiv Computer Vision Papers - 2025-09-07</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#one-flight-over-the-gap-a-survey-from-perspective-to-panoramic-vision" class="nav-link">One Flight Over the Gap: A Survey from Perspective to Panoramic Vision</a>
                </li>
                <li class="nav-item">
                    <a href="#human-motion-video-generation-a-survey" class="nav-link">Human Motion Video Generation: A Survey</a>
                </li>
                <li class="nav-item">
                    <a href="#the-telephone-game-evaluating-semantic-drift-in-unified-models" class="nav-link">The Telephone Game: Evaluating Semantic Drift in Unified Models</a>
                </li>
                <li class="nav-item">
                    <a href="#transition-models-rethinking-the-generative-learning-objective" class="nav-link">Transition Models: Rethinking the Generative Learning Objective</a>
                </li>
                <li class="nav-item">
                    <a href="#ssgaussian-semantic-aware-and-structure-preserving-3d-style-transfer" class="nav-link">SSGaussian: Semantic-Aware and Structure-Preserving 3D Style Transfer</a>
                </li>
                <li class="nav-item">
                    <a href="#from-editor-to-dense-geometry-estimator" class="nav-link">From Editor to Dense Geometry Estimator</a>
                </li>
                <li class="nav-item">
                    <a href="#geoarena-an-open-platform-for-benchmarking-large-vision-language-models-on-worldwide-image-geolocalization" class="nav-link">GeoArena: An Open Platform for Benchmarking Large Vision-language Models on WorldWide Image Geolocalization</a>
                </li>
                <li class="nav-item">
                    <a href="#efficient-odd-one-out-anomaly-detection" class="nav-link">Efficient Odd-One-Out Anomaly Detection</a>
                </li>
                <li class="nav-item">
                    <a href="#visiofirm-cross-platform-ai-assisted-annotation-tool-for-computer-vision" class="nav-link">VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer Vision</a>
                </li>
                <li class="nav-item">
                    <a href="#revisiting-simple-baselines-for-in-the-wild-deepfake-detection" class="nav-link">Revisiting Simple Baselines for In-The-Wild Deepfake Detection</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-09-07">Arxiv Computer Vision Papers - 2025-09-07</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>å¥½çï¼è¿æ¯ä¸ä»½éå¯¹2025å¹´9æ4æ¥Arxivè®¡ç®æºè§è§è®ºæçæ¯æ¥æ¥åæ§è¡æè¦ï¼æ¨å¨å¸®å©å¿ç¢çç ç©¶äººåå¿«éææ¡å³é®ä¿¡æ¯ã</p>
<hr />
<p><strong>æ¯æ¥Arxivè®¡ç®æºè§è§è®ºææ¥åæ§è¡æè¦ (2025-09-04)</strong></p>
<p><strong>1. ä¸»è¦ä¸»é¢ä¸è¶å¿æ¦è¿°ï¼</strong></p>
<p>ä»å¤©çè®ºæéåç°åºè®¡ç®æºè§è§é¢åå ä¸ªæ´»è·ä¸äº¤åçè¶å¿ã<strong>çæå¼AI</strong>ä¾ç¶æ¯æ ¸å¿ï¼å°¤å¶ä½ç°å¨è§é¢çæã3Dåå®¹åå»ºä»¥åå¯¹çææ¨¡ååºå±æºå¶çéæ°æèä¸ã<strong>å¤æ¨¡æå­¦ä¹ </strong>ï¼ç¹å«æ¯è§è§-è¯­è¨æ¨¡åçå°çå®ä½è½åï¼æ¾ç¤ºåºå¶å¨çè§£å¤æä¸çä¿¡æ¯æ¹é¢çæ½åãæ­¤å¤ï¼å¯¹<strong>åºç¡ä»»å¡çéæ°å®¡è§</strong>ï¼å¦å¼å¸¸æ£æµåæ·±åº¦ä¼ªé æ£æµï¼è¡¨æç ç©¶äººåå¨å¯»æ±æ´é«æãæ´é²æ£çè§£å³æ¹æ¡ã<strong>å·¥å·åå¹³å°å¼å</strong>ä¹å æ®ä¸å¸­ä¹å°ï¼æ¨å¨æåç ç©¶ååºç¨æçã</p>
<p><strong>2. æ¾èæåæ°è®ºæäº®ç¹ï¼</strong></p>
<ul>
<li><strong>"Transition Models: Rethinking the Generative Learning Objective" (Zidong Wang et al.)</strong>ï¼è¿ç¯è®ºæå¯è½å¯¹çææ¨¡åçåºç¡çè®ºäº§çéè¦å½±åãéè¿éæ°æèçæå­¦ä¹ ç®æ ï¼å®ææä¸ºæ´ç¨³å®ãæ´é«æççææ¨¡åè®­ç»æä¾æ°çèå¼ï¼å¼å¾æ·±å¥å³æ³¨ã</li>
<li><strong>"SSGaussian: Semantic-Aware and Structure-Preserving 3D Style Transfer" (Jimin Xu et al.)</strong>ï¼å¨3Dåå®¹çææ¥çéè¦çèæ¯ä¸ï¼è¿ç¯è®ºææåºçè¯­ä¹æç¥åç»æä¿æç3Dé£æ ¼è¿ç§»æ¹æ³ï¼è§£å³äº3Dåå®¹åä½ä¸­çä¸ä¸ªå³é®ææï¼å·æå¾é«çå®ç¨ä»·å¼ååæ°æ§ã</li>
<li><strong>"GeoArena: An Open Platform for Benchmarking Large Vision-language Models on WorldWide Image Geolocalization" (Pengyue Jia et al.)</strong>ï¼è¿æ¯ä¸ä¸ªéè¦çè´¡ç®ï¼ä¸ä»æåºäºä¸ä¸ªå·ä½çåºç¨ï¼å°çå®ä½ï¼ï¼æ´æä¾äºä¸ä¸ªå¼æ¾å¹³å°æ¥åºåæµè¯å¤§åè§è§-è¯­è¨æ¨¡åãè¿å¯¹äºæ¨å¨è¯¥é¢åçç ç©¶åå¬å¹³æ¯è¾ä¸åæ¨¡åçæ§è½è³å³éè¦ã</li>
</ul>
<p><strong>3. æ°å´ç ç©¶æ¹åæææ¯ï¼</strong></p>
<ul>
<li><strong>çææ¨¡åçè®ºçæ·±å±æ¢ç´¢ï¼</strong> "Transition Models"è¡¨æç ç©¶èä¸åä»ä»å³æ³¨çæææï¼èæ¯å¼å§æ·±å¥æ¢è®¨çæå­¦ä¹ çåºå±æºå¶åç®æ å½æ°ï¼è¿å¯è½å¸¦æ¥æ°ççè®ºçªç ´ã</li>
<li><strong>3Dåå®¹çæä¸ç¼è¾çç²¾ç»åæ§å¶ï¼</strong> "SSGaussian"å¼ºè°äºå¨3Dé£æ ¼è¿ç§»ä¸­ç»åè¯­ä¹åç»æçéè¦æ§ï¼é¢ç¤ºçæªæ¥3Dåå®¹åä½å°æ´å æ³¨éç²¾ç»åãæºè½åçæ§å¶ã</li>
<li><strong>è§è§-è¯­è¨æ¨¡åå¨ç¹å®å¤æä»»å¡ä¸­çåºç¨ä¸è¯ä¼°ï¼</strong> "GeoArena"å±ç¤ºäºè§è§-è¯­è¨æ¨¡åå¨å°çå®ä½è¿ç±»éè¦å¤æä¸çç¥è¯çä»»å¡ä¸­çæ½åï¼å¹¶å¼ºè°äºå»ºç«æ ååè¯ä¼°å¹³å°çéè¦æ§ã</li>
<li><strong>é«æä¸é²æ£çåºç¡è§è§ä»»å¡ï¼</strong> "Efficient Odd-One-Out Anomaly Detection" å "Revisiting Simple Baselines for In-The-Wild Deepfake Detection" æç¤ºç ç©¶èä»å¨å¯»æ±å¨èµæºåéæçå®ä¸çå¤æåºæ¯ä¸ï¼æåä¼ ç»ä»»å¡çæçåé²æ£æ§ã</li>
</ul>
<p><strong>4. å»ºè®®å®æ´éè¯»çè®ºæï¼</strong></p>
<p>åºäºå¶æ½å¨å½±åãåæ°æ§åå®ç¨æ§ï¼å»ºè®®ä¼åå®æ´éè¯»ä»¥ä¸è®ºæï¼</p>
<ol>
<li><strong>"Transition Models: Rethinking the Generative Learning Objective" (Zidong Wang et al.)</strong> - çè®ºçªç ´æ½åã</li>
<li><strong>"SSGaussian: Semantic-Aware and Structure-Preserving 3D Style Transfer" (Jimin Xu et al.)</strong> - 3Dåå®¹çæä¸ç¼è¾çå®ç¨åæ°ã</li>
<li><strong>"GeoArena: An Open Platform for Benchmarking Large Vision-language Models on WorldWide Image Geolocalization" (Pengyue Jia et al.)</strong> - å¹³å°ä¸åºåæµè¯ï¼å¯¹é¢ååå±æéè¦æ¨å¨ä½ç¨ã</li>
<li><strong>"Human Motion Video Generation: A Survey" (Haiwei Xue et al.)</strong> - å¦ææ¨å¯¹è§é¢çæï¼ç¹å«æ¯äººä½è¿å¨æ¹é¢æå´è¶£ï¼è¿ç¯ç»¼è¿°å°æä¾å¨é¢çèæ¯åæ¹åã</li>
</ol>
<hr />
<p>è¿ä»½æè¦æ¨å¨æä¾ä¸ä¸ªé«å±æ¬¡çæ¦è§ï¼å¸®å©æ¨å¿«éè¯å«ä¸æ¨ç ç©¶æ¹åæç¸å³çè®ºæã</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2509.04444v1">One Flight Over the Gap: A Survey from Perspective to Panoramic Vision</a></li>
<li><a href="#2509.03883v1">Human Motion Video Generation: A Survey</a></li>
<li><a href="#2509.04438v1">The Telephone Game: Evaluating Semantic Drift in Unified Models</a></li>
<li><a href="#2509.04394v1">Transition Models: Rethinking the Generative Learning Objective</a></li>
<li><a href="#2509.04379v1">SSGaussian: Semantic-Aware and Structure-Preserving 3D Style Transfer</a></li>
<li><a href="#2509.04338v1">From Editor to Dense Geometry Estimator</a></li>
<li><a href="#2509.04334v1">GeoArena: An Open Platform for Benchmarking Large Vision-language Models on WorldWide Image Geolocalization</a></li>
<li><a href="#2509.04326v1">Efficient Odd-One-Out Anomaly Detection</a></li>
<li><a href="#2509.04180v1">VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer Vision</a></li>
<li><a href="#2509.04150v1">Revisiting Simple Baselines for In-The-Wild Deepfake Detection</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2509.04444v1'></a></p>
<h2 id="one-flight-over-the-gap-a-survey-from-perspective-to-panoramic-vision"><a href="https://arxiv.org/abs/2509.04444v1">One Flight Over the Gap: A Survey from Perspective to Panoramic Vision</a></h2>
<p><strong>Authors:</strong> Xin Lin, Xian Ge, Dizhe Zhang, Zhaoliang Wan, Xianshun Wang, Xiangtai Li, Wenjie Jiang, Bo Du, Dacheng Tao, Ming-Hsuan Yang, Lu Qi</p>
<p><strong>Published:</strong> 2025-09-04</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Driven by the demand for spatial intelligence and holistic scene perception,
omnidirectional images (ODIs), which provide a complete 360\textdegree{} field
of view, are receiving growing attention across diverse applications such as
virtual reality, autonomous driving, and embodied robotics. Despite their
unique characteristics, ODIs exhibit remarkable differences from perspective
images in geometric projection, spatial distribution, and boundary continuity,
making it challenging for direct domain adaption from perspective methods. This
survey reviews recent panoramic vision techniques with a particular emphasis on
the perspective-to-panorama adaptation. We first revisit the panoramic imaging
pipeline and projection methods to build the prior knowledge required for
analyzing the structural disparities. Then, we summarize three challenges of
domain adaptation: severe geometric distortions near the poles, non-uniform
sampling in Equirectangular Projection (ERP), and periodic boundary continuity.
Building on this, we cover 20+ representative tasks drawn from more than 300
research papers in two dimensions. On one hand, we present a cross-method
analysis of representative strategies for addressing panoramic specific
challenges across different tasks. On the other hand, we conduct a cross-task
comparison and classify panoramic vision into four major categories: visual
quality enhancement and assessment, visual understanding, multimodal
understanding, and visual generation. In addition, we discuss open challenges
and future directions in data, models, and applications that will drive the
advancement of panoramic vision research. We hope that our work can provide new
insight and forward looking perspectives to advance the development of
panoramic vision technologies. Our project page is
https://insta360-research-team.github.io/Survey-of-Panorama</p>
<p><strong>Analysis:</strong></p>
<p>è¿ç¯ç±Xin Linç­äººæ°åçè®ºæâOne Flight Over the Gap: A Survey from Perspective to Panoramic Visionâå¨é¢åé¡¾äºå¨æ¯è§è§é¢åï¼ç¹å«å³æ³¨äºå¦ä½å¼¥åä¼ ç»éè§å¾åä¸å¨æ¯å¾åï¼ODIsï¼ä¹é´çé¢åé¸¿æ²ã</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³å¨æ¯å¾åï¼ODIsï¼å¨å ä½æå½±ãç©ºé´åå¸åè¾¹çè¿ç»­æ§æ¹é¢ä¸ä¼ ç»éè§å¾åå­å¨çæ¾èå·®å¼ï¼è¿äºå·®å¼å¯¼è´åºäºéè§å¾åçæ¹æ³é¾ä»¥ç´æ¥åºç¨äºå¨æ¯åºæ¯ãæ ¸å¿ç ç©¶é®é¢æ¯å¦ä½åæè¿äºç»ææ§ææï¼å®ç°ä»éè§å°å¨æ¯çææé¢åéåºï¼ä»èæ¨å¨å¨æ¯è§è§ææ¯å¨èæç°å®ãèªå¨é©¾é©¶åå·èº«æºå¨äººç­åºç¨ä¸­çåå±ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è®ºæé¦ååé¡¾äºå¨æ¯æåç®¡çº¿åæå½±æ¹æ³ï¼ä¸ºçè§£ODIsä¸éè§å¾åä¹é´çç»æå·®å¼å¥ å®äºåºç¡ãéåï¼å®æ»ç»äºé¢åéåºçä¸ä¸ªä¸»è¦ææï¼
*   <strong>æç¹éè¿çä¸¥éå ä½ç¸åï¼</strong> å¨ç­è·æ±ç¶æå½±ï¼ERPï¼ä¸­ï¼æç¹éè¿çç©ä½ä¼ä¸¥éæä¼¸ååå½¢ã
*   <strong>ç­è·æ±ç¶æå½±ï¼ERPï¼ä¸­çéååç©ºé´éæ ·ï¼</strong> å¯¼è´åç´ å¯åº¦å¨ä¸åçº¬åº¦ä¸ååï¼èµ¤éåºåéæ ·å¯éï¼èæç¹åºåéæ ·ç¨çã
*   <strong>å¨æ¯è¾¹ççå¨ææ§è¿ç»­æ§ï¼</strong> ä¼ ç»å·ç§¯ç¥ç»ç½ç»ï¼CNNsï¼éå¸¸å°ERPå¾åè§ä¸ºå¹³é¢ï¼æªè½ææå¤çæ°´å¹³è¾¹ççæ ç¼è¿ç»­æ§ã</p>
<p>ä¸ºäºåºå¯¹è¿äºææï¼è®ºææåºäºä¸¤ç§æ ¸å¿ç­ç¥ï¼
*   <strong>ç¸åæç¥æ¹æ³ï¼Distortion-Aware Methodsï¼ï¼</strong> ä¿æERPæ ¼å¼ï¼ä½å°ç¸åä¿¡æ¯åµå¥ç½ç»è®¾è®¡ä¸­ï¼ä¾å¦éè¿èªéåºå·ç§¯æ ¸ãæ³¨æåæºå¶æç¸åå¾æ¥æå¯¼ç¹å¾å­¦ä¹ ã
*   <strong>æå½±é©±å¨æ¹æ³ï¼Projection-Driven Methodsï¼ï¼</strong> å°å¨æ¯å¾åéæ°æå½±å°å¶ä»ç¸åè¾å°çè§å¾ï¼å¦ç«æ¹ä½æå½±ãåçº¿æå½±ï¼ä¸­ï¼ç¶åèåå¤æå½±ç¹å¾ã</p>
<p>è®ºæè¿å¯¹20å¤ä¸ªä»£è¡¨æ§ä»»å¡è¿è¡äºè·¨æ¹æ³åè·¨ä»»å¡åæï¼å°å¨æ¯è§è§ä»»å¡åä¸ºåå¤§ç±»ï¼è§è§è´¨éå¢å¼ºä¸è¯ä¼°ãè§è§çè§£ãå¤æ¨¡æçè§£åè§è§çæã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>ç³»ç»æ§åç±»ååæï¼</strong> è®ºæé¦æ¬¡ä»âéè§-å¨æ¯é¸¿æ²âè¿ä¸æ ¹æ¬è§è§åºåï¼å¯¹å¨æ¯è§è§é¢åç300å¤ç¯ç ç©¶è®ºæè¿è¡äºå¨é¢çåé¡¾ååç±»ï¼æ­ç¤ºäºä¸åä»»å¡ä¸­è§£å³å¨æ¯ç¹æææçç­ç¥ã
*   <strong>æ¹æ³è®ºæ´å¯ï¼</strong> ç¸åæç¥æ¹æ³å¨éè¦å¨å±è¯­ä¹ä¸è´æ§åæç¥è´¨éçä»»å¡ï¼å¦è¶åè¾¨çãå¾åä¿®å¤ãåå²ãæ£æµï¼ä¸­è¡¨ç°åºè²ï¼æå½±é©±å¨æ¹æ³å¨å ä½ææä»»å¡ï¼å¦æ·±åº¦ä¼°è®¡ãåæµãæ°è§è§åæï¼åå¤æ¨¡æèåä¸­å·æä¼å¿ã
*   <strong>æ°å´ææ¯æ´åï¼</strong> è®ºæè®¨è®ºäºæ©æ£æ¨¡åã3Dé«æ¯æ³¼æºåå¤æ¨¡æèåç­æ°å´ææ¯å¨å¨æ¯è§è§ä¸­çåºç¨æ½åï¼è¿äºææ¯ææå¨çæå¼ä»»å¡ååºæ¯å»ºæ¨¡ä¸­åæ¥å³é®ä½ç¨ã
*   <strong>ç»ä¸çè§è§ï¼</strong> è®ºææä¾äºä¸ä¸ªç»ä¸ä¸ä¸æ­æ¼è¿çå¨æ¯è§è§å­¦ä¹ å¾æ¯ï¼æå©äºç ç©¶äººåæ´å¥½å°çè§£åéæ©éåç¹å®ä»»å¡çæ¹æ³ã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
*   <strong>æ°æ®ç¨ç¼ºæ§ï¼</strong> ç¸æ¯éè§è§è§ï¼å¨æ¯æ°æ®éå¨è§æ¨¡ãå¤æ ·æ§ãè´¨éåæ¨¡ææ¹é¢ä»ç¶æéï¼è¿ä¸¥éå¶çº¦äºæ¨¡åçæ³åè½ååå¬å¹³çåºåæµè¯ã
*   <strong>è®¡ç®ææ¬åæ¨çéåº¦ï¼</strong> æäºçææ¨¡åé©±å¨çæ¹æ³ï¼å¦æ©æ£æ¨¡åï¼ä»é¢ä¸´é«è®¡ç®ææ¬åæ¢æ¨çéåº¦çææã
*   <strong>å ä½ç²¾åº¦ï¼</strong> ç¸åæç¥æ¹æ³å¨é«åº¦åå½¢åºåçç²¾åº¦ä»æå¾æé«ï¼å°¤å¶æ¯å¨å ä½ææä»»å¡ä¸­ã
*   <strong>ä¿¡æ¯ç¢çåï¼</strong> æå½±é©±å¨æ¹æ³å¯è½å¯¼è´è·¨æå½±çä¿¡æ¯ç¢çåï¼éè¦é¢å¤çèåæºå¶ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ°æ®æ¹é¢ï¼</strong> æå»ºå¤§è§æ¨¡ãæ ååãå¤æ ·åãé«è´¨éä¸å¯å«æ æ³¨çå¤æ¨¡æå¨æ¯æ°æ®éï¼ä»¥å¢å¼ºæ¨¡åçæ³åè½ååå¯æ¯æ§ã
*   <strong>æ¨¡åæ¹é¢ï¼</strong>
    *   <strong>åºç¡æ¨¡åï¼</strong> åå±å·æå¼ºå¤§æ³åè½ååé¶æ ·æ¬è¿ç§»è½åçå¨æ¯åºç¡æ¨¡åï¼å®ç°ç»ä¸çå¤ä»»å¡æ¶æã
    *   <strong>çææ¨¡åï¼</strong> åå±è½å¤è¿è¡å¼æ¾ä¸ççè§£ååºæ¯çæçæ¨¡åï¼ç¹å«æ¯ç»åå¨æ¯åéªçæ©æ£æ¨¡åå3Dé«æ¯æ³¼æºï¼ä»¥çææ´çå®ãå¯æ§ä¸å·ææ¶ç©ºä¸è´æ§çå¨æ¯åå®¹ã
    *   <strong>å¤æ¨¡æèåï¼</strong> è¿ä¸æ­¥æ´åé³é¢ãLiDARåææ¬ç­å¤ç§æ¨¡æï¼å®ç°æ´ä¸°å¯çå¨æ¯åºæ¯çè§£åäººæºäº¤äºã
*   <strong>åºç¨æ¹é¢ï¼</strong> å°å¨æ¯è§è§ææ¯æ©å±å°æ´å¹¿æ³çä¸æ¸¸åºç¨ï¼å¦å·èº«æºè½ãèªå¨é©¾é©¶ãæ²æµ¸å¼åªä½ã3Déå»ºåæ°å­å­ªçï¼ä»¥åå®å¨ãæè²åå»çç­ç¤¾ä¼åºç¨ã</p>
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæä¸ºå¨æ¯è§è§é¢åæä¾äºä¸ä¸ªå¨é¢çè·¯çº¿å¾ï¼ä¸ä»æ»ç»äºç°æææ¯ï¼è¿æåºäºæªæ¥çåå±æ¹åï¼å¼ºè°äºå¼¥åé¢åé¸¿æ²ãå©ç¨æ°å´ææ¯åæå»ºæ´å®åæ°æ®éçéè¦æ§ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>On one hand, we present a cross-method
analysis of representative strategies for addressing panoramic specific
challenges across different tasks.</li>
<li>We hope that our work can provide new
insight and forward looking perspectives to advance the development of
panoramic vision technologies.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.04444v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.04444v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.03883v1'></a></p>
<h2 id="human-motion-video-generation-a-survey"><a href="https://arxiv.org/abs/2509.03883v1">Human Motion Video Generation: A Survey</a></h2>
<p><strong>Authors:</strong> Haiwei Xue, Xiangyang Luo, Zhanghao Hu, Xin Zhang, Xunzhi Xiang, Yuqin Dai, Jianzhuang Liu, Zhensong Zhang, Minglei Li, Jian Yang, Fei Ma, Zhiyong Wu, Changpeng Yang, Zonghong Dai, Fei Richard Yu</p>
<p><strong>Published:</strong> 2025-09-04</p>
<p><strong>Categories:</strong> cs.CV, cs.MM</p>
<p><strong>Abstract:</strong></p>
<p>Human motion video generation has garnered significant research interest due
to its broad applications, enabling innovations such as photorealistic singing
heads or dynamic avatars that seamlessly dance to music. However, existing
surveys in this field focus on individual methods, lacking a comprehensive
overview of the entire generative process. This paper addresses this gap by
providing an in-depth survey of human motion video generation, encompassing
over ten sub-tasks, and detailing the five key phases of the generation
process: input, motion planning, motion video generation, refinement, and
output. Notably, this is the first survey that discusses the potential of large
language models in enhancing human motion video generation. Our survey reviews
the latest developments and technological trends in human motion video
generation across three primary modalities: vision, text, and audio. By
covering over two hundred papers, we offer a thorough overview of the field and
highlight milestone works that have driven significant technological
breakthroughs. Our goal for this survey is to unveil the prospects of human
motion video generation and serve as a valuable resource for advancing the
comprehensive applications of digital humans. A complete list of the models
examined in this survey is available in Our Repository
https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºâHuman Motion Video Generation: A Surveyâçå¨é¢æè¦ï¼æ¶µçäºæ¨è¦æ±çææè¦ç¹ï¼</p>
<p><strong>è®ºææè¦ï¼Human Motion Video Generation: A Survey</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³ç°æå³äºäººä½è¿å¨è§é¢çæé¢åç»¼è¿°çä¸è¶³ãå½åç»¼è¿°éå¸¸ä¾§éäºåä¸æ¹æ³ï¼ç¼ºä¹å¯¹æ´ä¸ªçæè¿ç¨çå¨é¢æ¦è¿°ãå æ­¤ï¼æ¬æçæ ¸å¿é®é¢æ¯æä¾ä¸ä¸ªç³»ç»ãæ·±å¥çç»¼è¿°ï¼æ¶µçäººä½è¿å¨è§é¢çæçå®æ´çææµç¨ãå¤æ¨¡æé©±å¨æ¹å¼ãææ°ææ¯è¿å±ãææä»¥åæªæ¥æ¹åï¼ç¹å«æ¯å¤§åè¯­è¨æ¨¡åï¼LLMsï¼å¨è¯¥é¢åçåºç¨æ½åã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>äºé¶æ®µçææµç¨æ¡æ¶ï¼</strong> è®ºæé¦æ¬¡å°äººä½è¿å¨è§é¢çæä»»å¡åè§£ä¸ºäºä¸ªå³é®é¶æ®µï¼è¾å¥ï¼Inputï¼ãè¿å¨è§åï¼Motion Planningï¼ãè¿å¨è§é¢çæï¼Motion Video Generationï¼ãç²¾ä¿®ï¼Refinementï¼åè¾åºï¼Outputï¼ãè¿ä¸æ¡æ¶ä¸ºçè§£ååæè¯¥é¢åçå¤ææ§æä¾äºç³»ç»åçè§è§ã
*   <strong>LLMså¨è¿å¨è§åä¸­çåºç¨ï¼</strong> é¦æ¬¡æ¢è®¨äºå¤§åè¯­è¨æ¨¡åï¼LLMsï¼å¨è¿å¨è§åé¶æ®µçæ½åï¼åæ¬å©ç¨LLMsçæç²¾ç»çè¿å¨æè¿°ä»¥è¿è¡æ£ç´¢ï¼æå°è¿å¨æ¡ä»¶æå½±å°æ½å¨ç©ºé´ä»¥æå¯¼çææ¨¡åãè¿ä»£è¡¨äºè¯¥é¢åçä¸ä¸ªéè¦åæ°æ¹åã
*   <strong>å¤æ¨¡æé©±å¨åç±»ï¼</strong> å°äººä½è¿å¨è§é¢çææ¹æ³åä¸ºè§è§é©±å¨ãææ¬é©±å¨åé³é¢é©±å¨ä¸å¤§ä¸»è¦æ¨¡æï¼å¹¶è¯¦ç»åæäºæ¯ç§æ¨¡æä¸çå­ä»»å¡åææ¯è¿å±ã
*   <strong>çææ¨¡ååæ³¨æåæºå¶çæ·±å¥åæï¼</strong> è¯¦ç»åæäºåºäºæ©æ£æ¨¡åï¼DMsï¼ãçæå¯¹æç½ç»ï¼GANsï¼åååèªç¼ç å¨ï¼VAEsï¼ççææ¡æ¶ï¼å¹¶å¯¹æ©æ£æ¨¡åä¸­çç©ºé´æ³¨æåï¼SAï¼ãäº¤åæ³¨æåï¼CAï¼ãæ¶é´æ³¨æåï¼TAï¼åè·¨å¸§èªæ³¨æåï¼CFSAï¼ç­æ³¨æåèåæ¹æ³è¿è¡äºç»è´åç±»ã
*   <strong>å¨é¢çæ°æ®éåè¯ä¼°ææ ï¼</strong> æ¶éå¹¶æ¦è¿°äº64ä¸ªäººä½ç¸å³è§é¢æ°æ®éï¼å¹¶æ»ç»äºå¸¸ç¨çåå¸§å¾åè´¨éãè§é¢è´¨éè¯ä¼°ãè§é¢ç¹æ§è¯ä¼°ä»¥åLLMè§åå¨è¯ä¼°ææ ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>é¢åå¿«éåå±ï¼</strong> ç»¼è¿°æ­ç¤ºäºäººä½è¿å¨è§é¢çæé¢åï¼ç¹å«æ¯âè¯´è¯äººå¤´é¨âï¼talking headï¼åâèè¹è§é¢âï¼dance videoï¼çææ¹é¢çå¿«éå¢é¿åææ¯çªç ´ã
*   <strong>LLMsçæ½åï¼</strong> å¼ºè°äºLLMså¨çè§£è¯­ä¹ç»å¾®å·®å«åæ¨çæææ¹é¢çä¼å¿ï¼è½å¤çææ´ç²¾ç»ãæ´ç¬¦åä¸ä¸æçè¿å¨è§åï¼ä»èæåæ°å­äººççå®æåäº¤äºæ§ã
*   <strong>ææ¯è¶å¿ï¼</strong> æ©æ£æ¨¡åå¨çæé«è´¨éè§é¢æ¹é¢è¡¨ç°åºè²ï¼ä½è®¡ç®ææ¬è¾é«ï¼GANså¨çæè´¨éä¸æææåï¼ä½å¤æ ·æ§æéï¼VAEså¨æ°æ®è¡¨ç¤ºæ¹é¢å·æä¼å¿ï¼ä½å®¹æåºç°æ¨¡å¼å´©æºãå¤æ¨¡æèåæ¯ä¸»æµè¶å¿ã
*   <strong>ææä¸æªæ¥æ¹åï¼</strong> æç¡®æåºäºå½åé¢ä¸´çææï¼å¹¶ä¸ºæªæ¥çç ç©¶æä¾äºå¯åæ§æ¹åï¼æå©äºæ¨å¨æ°å­äººç»¼ååºç¨çåå±ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>LLMså¨è¿å¨è§åä¸­çåºç¨ä»å¤äºæ©æé¶æ®µï¼</strong> å°½ç®¡LLMsæ¾ç¤ºåºæ½åï¼ä½ç®åä¸»è¦éè¿ææ¬ä½ä¸ºä¸­ä»ä¸çææ¨¡åéæï¼ç¼ºä¹æ´ææåæ°é¢çä¸­é´è¡¨ç¤ºæ¢ç´¢ã
*   <strong>ç°ææ¹æ³å¨å¤æè¿å¨åç²¾ç»æ§å¶ä¸çä¸è¶³ï¼</strong> ææ¬è¾å¥å¨å¤æè¿å¨ä¸­æææéï¼é¾ä»¥ææç²¾ç»çè¿å¨æ¨¡å¼ãå¤æ¨¡ææ¹æ³å¨ç²¾ç»æ§å¶ç¹å®èº«ä½é¨ä½ï¼å¦æé¨åé¢é¨ç»èï¼æ¹é¢ä»é¢ä¸´ææã
*   <strong>æ°æ®ç¨ç¼ºåè´¨éé®é¢ï¼</strong> äººä½è¿å¨è§é¢çæé¢ååå°æ°æ®å¯ç¨æ§ãéç§é®é¢ãæ°æ®è´¨éåé«æ¶éææ¬çéå¶ï¼å½±åäºæ¨¡åçé²æ£æ§åçå®ä¸çå¯é æ§ã
*   <strong>åç§çå®æä¸è¶³ï¼</strong> çæçäººä½å½¢æï¼ç¹å«æ¯é¢é¨åæé¨çåç§çå®æï¼ä»éæ¹è¿ã
*   <strong>è§é¢æç»­æ¶é´åæ§å¶èå´æéï¼</strong> å¤§å¤æ°æ¹æ³åªè½çæç­è§é¢çæ®µï¼æ©å±å°æ´é¿æ¶é´çè§é¢ä»æ¯éå¤§ææã
*   <strong>å®æ¶æ§è½åææ¬ï¼</strong> æ©æ£æ¨¡åè®¡ç®ææ¬é«æï¼å®æ¶æµåªä½åºç¨é¢ä¸´ä½å»¶è¿åé«å¸¦å®½çææã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ´ææçLLMä¸­é´è¡¨ç¤ºï¼</strong> æ¢ç´¢è¶è¶ææ¬æè¿°çãæ´ææåæ°é¢çä¸­é´è¡¨ç¤ºï¼ä»¥å¢å¼ºLLMså¨è¿å¨è§åä¸­ççè§£åè¡ä¸ºè§åè½åã
*   <strong>å¤äººç©é©±å¨åäº¤äºï¼</strong> æ¢ç´¢å¤äººç©é¢é¨é©±å¨åå¤äººç©äº¤äºä»»å¡çæ¹æ³ï¼ä»¥å®ç°æ´å¤æçæ°å­äººåºæ¯ã
*   <strong>é«æçç«¯å°ç«¯è®­ç»èå¼ï¼</strong> å¼åæ´é«æçè®­ç»èå¼ï¼ä»¥éä½è®¡ç®å¼éï¼åæ¶ä¿æè§é¢è´¨éåä¸è´æ§ã
*   <strong>å°æ ·æ¬å­¦ä¹ ï¼</strong> æ¢ç´¢å°æ ·æ¬å­¦ä¹ æ¹æ³ï¼ä»¥è§£å³è®­ç»æ°æ®éå¤§çé®é¢ï¼å°¤å¶æ¯å¨è§é¢é©±å¨çèè¹è§é¢çæä¸­ã
*   <strong>æ¿ä»£è§é¢çææ¶æï¼</strong> æ¢ç´¢DiTæVARç­æ¿ä»£æ¶æï¼ä»¥å®ç°æ´ç¨³å®åè¿è´¯çè§é¢çæï¼è§£å³åºäºUnetçæ©æ£æ¨¡åä¸­å¸§é´ä¸ä¸è´åéªçä¼ªå½±çé®é¢ã
*   <strong>ç²¾ç»åæ§å¶åé¿è§é¢çæï¼</strong> æåå¯¹ç¹å®èº«ä½é¨ä½ï¼å¦æé¨ãé¢é¨ç»èï¼çç²¾ç»æ§å¶ï¼å¹¶å¼åè½å¤çæé¿æ¶é´ãé«è´¨éè§é¢çææ¯ã
*   <strong>å®æ¶é¨ç½²åææ¬ä¼åï¼</strong> ç åæ´é«æçæ¨¡ååæµåªä½ææ¯ï¼ä»¥éä½è®¡ç®ææ¬ï¼å®ç°æ°å­äººå¨å®æ¶å¹³å°ä¸çé¨ç½²ã
*   <strong>ä¼¦çæ¡æ¶åéç§ä¿æ¤ï¼</strong> å»ºç«å¥å¨çä¼¦çæ¡æ¶ï¼ç¡®ä¿çç©è¯å«æ°æ®ä½¿ç¨çç¥æåæï¼å¹¶è§£å³æ°å­äººçæä¸­çéç§åè´é¢å½±åé®é¢ã</p>
<p>è¿ç¯ç»¼è¿°ä¸ºäººä½è¿å¨è§é¢çæé¢åæä¾äºä¸ä¸ªå¨é¢çè·¯çº¿å¾ï¼ä¸ä»æ»ç»äºç°æææ¯ï¼è¿ææäºæªæ¥çç ç©¶æ¹åï¼å¯¹äºæ¨å¨æ°å­äººææ¯çåå±å·æéè¦ä»·å¼ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Our goal for this survey is to unveil the prospects of human
motion video generation and serve as a valuable resource for advancing the
comprehensive applications of digital humans.</li>
<li>A complete list of the models
examined in this survey is available in Our Repository
https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.03883v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.03883v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.04438v1'></a></p>
<h2 id="the-telephone-game-evaluating-semantic-drift-in-unified-models"><a href="https://arxiv.org/abs/2509.04438v1">The Telephone Game: Evaluating Semantic Drift in Unified Models</a></h2>
<p><strong>Authors:</strong> Sabbir Mollah, Rohit Gupta, Sirnam Swetha, Qingyang Liu, Ahnaf Munir, Mubarak Shah</p>
<p><strong>Published:</strong> 2025-09-04</p>
<p><strong>Categories:</strong> cs.CV, cs.CL</p>
<p><strong>Abstract:</strong></p>
<p>Employing a single, unified model (UM) for both visual understanding
(image-to-text: I2T) and and visual generation (text-to-image: T2I) has opened
a new direction in Visual Language Model (VLM) research. While UMs can also
support broader unimodal tasks (e.g., text-to-text, image-to-image), we focus
on the core cross-modal pair T2I and I2T, as consistency between understanding
and generation is critical for downstream use. Existing evaluations consider
these capabilities in isolation: FID and GenEval for T2I, and benchmarks such
as MME, MMBench for I2T. These single-pass metrics do not reveal whether a
model that understands a concept can also render it, nor whether meaning is
preserved when cycling between image and text modalities. To address this, we
introduce the Unified Consistency Framework for Unified Models (UCF-UM), a
cyclic evaluation protocol that alternates I2T and T2I over multiple
generations to quantify semantic drift. UCF formulates 3 metrics: (i) Mean
Cumulative Drift (MCD), an embedding-based measure of overall semantic loss;
(ii) Semantic Drift Rate (SDR), that summarizes semantic decay rate; and (iii)
Multi-Generation GenEval (MGG), an object-level compliance score extending
GenEval. To assess generalization beyond COCO, which is widely used in
training; we create a new benchmark ND400, sampled from NoCaps and DOCCI and
evaluate on seven recent models. UCF-UM reveals substantial variation in
cross-modal stability: some models like BAGEL maintain semantics over many
alternations, whereas others like Vila-u drift quickly despite strong
single-pass scores. Our results highlight cyclic consistency as a necessary
complement to standard I2T and T2I evaluations, and provide practical metrics
to consistently assess unified model's cross-modal stability and strength of
their shared representations. Code:
https://github.com/mollahsabbir/Semantic-Drift-in-Unified-Models</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Sabbir Mollahç­äººæ°åçè®ºæâThe Telephone Game: Evaluating Semantic Drift in Unified Modelsâçå¨é¢æè¦ã</p>
<hr />
<h3 id="the-telephone-game-evaluating-semantic-drift-in-unified-models_1">è®ºææè¦ï¼The Telephone Game: Evaluating Semantic Drift in Unified Models</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³ç»ä¸æ¨¡åï¼Unified Models, UMsï¼å¨è§è§çè§£ï¼I2Tï¼å¾åå°ææ¬ï¼åè§è§çæï¼T2Iï¼ææ¬å°å¾åï¼ä»»å¡ä¹é´è¿è¡äº¤æ¿è½¬æ¢æ¶ï¼è¯­ä¹ä¿¡æ¯æ¯å¦è½è¢«ææä¿ççé®é¢ãç°æçè¯ä¼°æ¹æ³éå¸¸å­¤ç«å°è¡¡éI2TåT2Iè½åï¼ä¾å¦ï¼T2Iä½¿ç¨FIDåGenEvalï¼I2Tä½¿ç¨MMEåMMBenchï¼ï¼è¿äºåæ¬¡éè¿çææ æ æ³æ­ç¤ºæ¨¡åå¨å¾ååææ¬æ¨¡æä¹é´å¾ªç¯æ¶ï¼æ¯å¦è½æç»­çè§£åçæåä¸æ¦å¿µï¼ä»¥åè¯­ä¹æ¯å¦ä¼åçæ¼ç§»ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ä¸ºäºè§£å³ä¸è¿°é®é¢ï¼ä½èæåºäº<strong>ç»ä¸æ¨¡åä¸è´æ§æ¡æ¶ï¼Unified Consistency Framework for Unified Models, UCF-UMï¼</strong>ãè¿æ¯ä¸ä¸ªå¾ªç¯è¯ä¼°åè®®ï¼éè¿å¨å¤ä»£I2TåT2Iäº¤æ¿è½¬æ¢ä¸­éåè¯­ä¹æ¼ç§»ãUCF-UMå¼å¥äºä¸é¡¹æ ¸å¿ææ ï¼
*   <strong>(i) å¹³åç´¯ç§¯æ¼ç§»ï¼Mean Cumulative Drift, MCDï¼</strong>ï¼ä¸ç§åºäºåµå¥çåº¦éï¼ç¨äºéåæ´ä½è¯­ä¹æå¤±ã
*   <strong>(ii) è¯­ä¹æ¼ç§»çï¼Semantic Drift Rate, SDRï¼</strong>ï¼æ»ç»è¯­ä¹è¡°åéçã
*   <strong>(iii) å¤ä»£GenEvalï¼Multi-Generation GenEval, MGGï¼</strong>ï¼æ©å±äºGenEvalçå¯¹è±¡çº§åè§æ§å¾åï¼ä»¥è¯ä¼°å¤ä»£çæä¸­çå¯¹è±¡çº§ä¿çåº¦ã</p>
<p>æ­¤å¤ï¼ä¸ºäºè¯ä¼°æ¨¡åå¨COCOä¹å¤çæ³åè½åï¼ä½èåå»ºäºä¸ä¸ªæ°çåºåæ°æ®é<strong>ND400</strong>ï¼è¯¥æ°æ®éä»NoCapsåDOCCIä¸­éæ ·ï¼åå«æ°é¢å¯¹è±¡åç»ç²åº¦è§è§ç»èï¼ä»¥æ´å¥½å°æ¢æµæ¨¡åçæ³åæ§ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   UCF-UMè¯ä¼°æ­ç¤ºäºç»ä¸æ¨¡åå¨è·¨æ¨¡æç¨³å®æ§æ¹é¢å­å¨æ¾èå·®å¼ã
*   ä¸äºæ¨¡åï¼å¦<strong>BAGEL</strong>ï¼å¨å¤æ¬¡äº¤æ¿è½¬æ¢ä¸­è½å¾å¥½å°ä¿æè¯­ä¹ã
*   èå¦ä¸äºæ¨¡åï¼å¦<strong>Vila-u</strong>å<strong>Janus</strong>ç³»åæ¨¡åï¼å°½ç®¡å¨åæ¬¡éè¿è¯ä¼°ä¸­è¡¨ç°å¼ºå²ï¼ä½è¯­ä¹æ¼ç§»è¿éãè¿è¡¨æåæ¬¡éè¿ææ å¯è½é«ä¼°äºæ¨¡åçé²æ£æ§ã
*   ç ç©¶ç»æå¼ºè°ï¼å¾ªç¯ä¸è´æ§æ¯æ åI2TåT2Iè¯ä¼°çå¿è¦è¡¥åï¼å¹¶æä¾äºè¯ä¼°ç»ä¸æ¨¡åè·¨æ¨¡æç¨³å®æ§åå±äº«è¡¨ç¤ºå¼ºåº¦çå®ç¨ææ ã
*   MGGç»ææ¾ç¤ºï¼å¨å¤æä»»å¡ï¼å¦å®ä½åå±æ§ç»å®ï¼ä¸ï¼æ¨¡åçæ§è½ä¸éæä¸ºæ¾èï¼è¿å¯è½æ¯è¯­ä¹æ¼ç§»çåå ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   ç°æè¯ä¼°æ¹æ³æªè½ææå°æ¨¡åå¨å¤è½®è½¬æ¢ä¸­å®ä½ãå±æ§ãå³ç³»åè®¡æ°ä¿¡æ¯çä¿çæåµã
*   è®ºæä¸»è¦å³æ³¨I2TåT2Iä»»å¡ï¼èå¯¹æ´å¹¿æ³çåæ¨¡æä»»å¡ï¼å¦ææ¬å°ææ¬ãå¾åå°å¾åï¼çè¯­ä¹æ¼ç§»åæè¾å°ã
*   CLIPScoreç­ææ è½ç¶ä½¿ç¨åµå¥æ¥è¡¡éè¯­ä¹å¯¹é½ï¼ä½å¯è½ä¸æ»æ¯ä¸äººç±»æç¥ä¸è´ã
*   GenEvalè½ç¶æ£æ¥å¯¹è±¡åå³ç³»çº§åè§æ§ï¼ä½æªè¯ä¼°æ´ä½è§è§è´¨éæçå®æã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   è¿ä¸æ­¥æ¢ç´¢ä¸åæ¶æè®¾è®¡ï¼å±äº«æéãé¨åå±äº«ãè§£è¦ï¼å¯¹è¯­ä¹ç¨³å®æ§çå½±åã
*   ç ç©¶å¦ä½éè¿æ¹è¿æ¨¡åæ¶æãè®­ç»æ¹æ³åæ°æ®éæ¥å¢å¼ºç»ä¸æ¨¡åçè·¨æ¨¡æä¸è´æ§åè¯­ä¹ä¿çåº¦ã
*   å°UCF-UMæ¡æ¶æ©å±å°æ´å¹¿æ³çåæ¨¡æä»»å¡ï¼ä»¥å¨é¢è¯ä¼°ç»ä¸æ¨¡åçè¯­ä¹æ¼ç§»ã
*   å¼åæ°çææ ï¼è½å¤æ´å¥½å°åæ äººç±»å¯¹è¯­ä¹æ¼ç§»çæç¥ã
*   æ·±å¥åææ¨¡åå¨ç¹å®å¤æä»»å¡ï¼å¦ç»åå±æ§ç»å®ï¼ä¸çèå¼±æ§ï¼å¹¶å¯»æ¾è§£å³æ¹æ¡ã</p>
<hr />
<p>è¿ç¯è®ºæéè¿å¼å¥ä¸ä¸ªæ°é¢çå¾ªç¯è¯ä¼°æ¡æ¶ï¼ä¸ºç»ä¸æ¨¡åçè¯ä¼°æä¾äºä¸ä¸ªå¨æ°çè§è§ï¼å¼ºè°äºè¯­ä¹ä¸è´æ§å¨å®éåºç¨ä¸­çéè¦æ§ï¼å¹¶æ­ç¤ºäºç°æåæ¬¡éè¿ææ æ æ³ææå°çæ¨¡åè¡ä¸ºã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Employing a single, unified model (UM) for both visual understanding
(image-to-text: I2T) and and visual generation (text-to-image: T2I) has opened
a new direction in Visual Language Model (VLM) research.</li>
<li>To assess generalization beyond COCO, which is widely used in
training; we create a new benchmark ND400, sampled from NoCaps and DOCCI and
evaluate on seven recent models.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.04438v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.04438v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.04394v1'></a></p>
<h2 id="transition-models-rethinking-the-generative-learning-objective"><a href="https://arxiv.org/abs/2509.04394v1">Transition Models: Rethinking the Generative Learning Objective</a></h2>
<p><strong>Authors:</strong> Zidong Wang, Yiyuan Zhang, Xiaoyu Yue, Xiangyu Yue, Yangguang Li, Wanli Ouyang, Lei Bai</p>
<p><strong>Published:</strong> 2025-09-04</p>
<p><strong>Categories:</strong> cs.LG, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>A fundamental dilemma in generative modeling persists: iterative diffusion
models achieve outstanding fidelity, but at a significant computational cost,
while efficient few-step alternatives are constrained by a hard quality
ceiling. This conflict between generation steps and output quality arises from
restrictive training objectives that focus exclusively on either infinitesimal
dynamics (PF-ODEs) or direct endpoint prediction. We address this challenge by
introducing an exact, continuous-time dynamics equation that analytically
defines state transitions across any finite time interval. This leads to a
novel generative paradigm, Transition Models (TiM), which adapt to
arbitrary-step transitions, seamlessly traversing the generative trajectory
from single leaps to fine-grained refinement with more steps. Despite having
only 865M parameters, TiM achieves state-of-the-art performance, surpassing
leading models such as SD3.5 (8B parameters) and FLUX.1 (12B parameters) across
all evaluated step counts. Importantly, unlike previous few-step generators,
TiM demonstrates monotonic quality improvement as the sampling budget
increases. Additionally, when employing our native-resolution strategy, TiM
delivers exceptional fidelity at resolutions up to 4096x4096.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Zidong Wangç­äººæ°åçè®ºæâTransition Models: Rethinking the Generative Learning Objectiveâçå¨é¢æè¦ã</p>
<hr />
<h3 id="transition-models-rethinking-the-generative-learning-objective_1">è®ºææè¦ï¼Transition Models: Rethinking the Generative Learning Objective</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
çææ¨¡åé¢åå­å¨ä¸ä¸ªæ ¸å¿å°å¢ï¼è¿­ä»£æ©æ£æ¨¡åè½ç¶è½å®ç°åè¶çå¾åçæè´¨éï¼ä½è®¡ç®ææ¬é«æï¼èé«æçå°æ­¥çææ¹æ³ååéäºåºå®çè´¨éä¸éãè¿ç§çææ­¥æ°ä¸è¾åºè´¨éä¹é´çå²çªæºäºç°æè®­ç»ç®æ è¿äºå±éï¼å®ä»¬è¦ä¹åªå³æ³¨æ ç©·å°å¨æï¼PF-ODEsï¼ï¼è¦ä¹åªå³æ³¨ç´æ¥ç«¯ç¹é¢æµãè®ºææ¨å¨è§£å³è¿ä¸é®é¢ï¼å³å¦ä½å¼åä¸ç§çææ¨¡åï¼æ¢è½å®ç°å°æ­¥é«æçæï¼åè½éè¿å¢å éæ ·æ­¥æ°å®ç°è´¨éçåè°æåï¼å¹¶æ¯æä»»æåè¾¨çççæã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥è®ºæå¼å¥äºä¸ç§åä¸º<strong>Transition Models (TiM)</strong> çæ°åçæèå¼ï¼å¶æ ¸å¿åæ°å¨äºï¼
*   <strong>ç²¾ç¡®çè¿ç»­æ¶é´å¨ææ¹ç¨ï¼</strong> TiM æåºä¸ä¸ªç²¾ç¡®çãè¿ç»­æ¶é´å¨ææ¹ç¨ï¼è½å¤åææ§å°å®ä¹ä»»ææéæ¶é´é´é <script type="math/tex">\Delta t</script> åçç¶æè½¬æ¢ãè¿ä½¿å¾æ¨¡åä¸åä»ä»æ¯è¿ä¼¼å¾®åæ¹ç¨æç»è®¡æ å°ï¼èæ¯å­¦ä¹ çæè¿ç¨æ¬èº«çâè§£æµå½¢âã
*   <strong>ä»»ææ­¥é¿è½¬æ¢è½åï¼</strong> TiM è½å¤éåºä»»ææ­¥é¿çè½¬æ¢ï¼æ ç¼å°å¨åæ¬¡å¤§è·³è·åå¤æ­¥ç²¾ç»åä¹é´éåçæè½¨è¿¹ãè¿ç»ä¸äºå°æ­¥åå¤æ­¥çæèå¼ã
*   <strong>éå¼è½¨è¿¹ä¸è´æ§ä¸æ¶é´æçå¹éï¼</strong> è®ºææ¨å¯¼åºäºâç¶æè½¬æ¢æç­å¼âï¼State Transition Identityï¼ï¼å®å¼ºå¶æ¨¡åå¨ä»»æèµ·å§æ¶é´ <script type="math/tex">t</script> å°ç¸åç®æ  <script type="math/tex">x_r</script> çè·¯å¾ä¸ä¿æä¸è´æ§ï¼å¹¶è¦æ±æ¨¡åä¸ä»æå°åæ®å·®å¼ï¼è¿è¦æå°åæ®å·®çæ¶é´å¯¼æ°ï¼ä»èå­¦ä¹ æ´å¹³æ»çè§£æµå½¢ï¼ç¡®ä¿å¤§æ­¥éæ ·æ¶çè¿è´¯æ§åå°æ­¥ç²¾ç»åæ¶çç¨³å®æ§ã
*   <strong>å¯æ©å±ä¸ç¨³å®çè®­ç»ï¼</strong> éå¯¹è®¡ç®ç½ç»æ¶é´å¯¼æ°å¸¦æ¥çå¯æ©å±æ§ææï¼TiM æåºäºâå¾®åæ¨å¯¼æ¹ç¨âï¼Differential Derivation Equation, DDEï¼ä½ä¸ºä¸ç§é«æçæéå·®åè¿ä¼¼æ¹æ³ï¼å¶ååä¼ æ­ç»æä¸FSDPç­åå¸å¼è®­ç»ä¼åå¼å®¹ï¼ä½¿å¾è®­ç»æ°åäº¿åæ°çæ¨¡åæä¸ºå¯è½ãæ­¤å¤ï¼éè¿å¼å¥æå¤±å ææ¹æ¡ï¼ä¼åå¤çç­é´éè½¬æ¢ï¼è§£å³äºæ¢¯åº¦æ¹å·®é®é¢ï¼æé«äºè®­ç»ç¨³å®æ§ã
*   <strong>æ¹è¿çæ¶æï¼</strong> å¼å¥äºè§£è¦æ¶é´åµå¥ï¼Decoupled Time Embeddingï¼åé´éæç¥æ³¨æåï¼Interval-Aware Attentionï¼ï¼ä½¿æ¨¡åè½å¤æç¡®å°åæ¶èèç»å¯¹æ¶é´ <script type="math/tex">t</script> åè½¬æ¢é´é <script type="math/tex">\Delta t</script> çå½±åï¼ä»èå¨ææéæ ·æ­¥é¿ä¸è·å¾æ¾èæ§è½æåã</p>
<p><strong>3. ä¸»è¦ç»æåå¶éè¦æ§ï¼</strong>
*   <strong>åè¶çæ§è½ï¼</strong> å°½ç®¡TiMæ¨¡åä»æ8.65äº¿åæ°ï¼ä½å¨GenEvalåºåæµè¯ä¸­ï¼å¶æ§è½è¶è¶äºåæ°éæ´å¤§çé¢åæ¨¡åï¼å¦SD3.5ï¼80äº¿åæ°ï¼åFLUX.1ï¼120äº¿åæ°ï¼ï¼å¹¶å¨ææè¯ä¼°çæ­¥æ°ä¸é½è¾¾å°äºæåè¿æ°´å¹³ã
*   <strong>åè°è´¨éæåï¼</strong> ä¸ä»¥å¾çå°æ­¥çæå¨ä¸åï¼TiM éçéæ ·é¢ç®çå¢å ï¼è´¨éè¡¨ç°åºåè°æåï¼è§£å³äºå°æ­¥æ¨¡åè´¨éé¥±åçé®é¢ã
*   <strong>é«åè¾¨çåå¤æ ·åé¿å®½æ¯æ¯æï¼</strong> éç¨åçåè¾¨çè®­ç»ç­ç¥ï¼TiM å¨é«è¾¾ 4096x4096 çåè¾¨çä¸ä»¥ååç§é¿å®½æ¯ä¸åè½æä¾åè¶çå¾åçæè´¨éã
*   <strong>æçåå¯æ©å±æ§ï¼</strong> DDEæ¹æ³æ¯ä¼ ç»çJVPæ¹æ³å¿«çº¦2åï¼ä¸ä¸FSDPå¼å®¹ï¼ä½¿å¾ä»å¤´å¼å§è®­ç»æ°åäº¿åæ°çTiMæ¨¡åæä¸ºå¯è½ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>åå®¹å®å¨åå¯æ§æ§ï¼</strong> å°½ç®¡TiMå¨åºç¡çææ¨¡åæ¹é¢ååºäºéå¤§è´¡ç®ï¼ä½ç¡®ä¿åå®¹å®å¨åå¯æ§æ§ä»ç¶æ¯ä¸ä¸ªå¼æ¾çææã
*   <strong>ç²¾ç»ç»èéåï¼</strong> å¨éè¦ç²¾ç»ç»èï¼å¦æ¸²æææ¬åæé¨ï¼çåºæ¯ä¸­ï¼æ¨¡åä¿çåº¦å¯è½ä¼ä¸éã
*   <strong>é«åè¾¨çä¸çä¼ªå½±ï¼</strong> å¨é«åè¾¨çï¼ä¾å¦3072x4096ï¼ä¸ï¼å¶å°ä¼è§å¯å°ä¼ªå½±ï¼è¿å¯è½å½å äºåºå±èªç¼ç å¨ä¸­çåå·®ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºæçæåä¸ºæ°ä¸ä»£åºç¡æ¨¡åéºå¹³äºéè·¯ï¼è¿äºæ¨¡åæ¢é«æãå¯æ©å±ï¼åå¨åææ½åæ¹é¢åæ»¡åæ¯ãæªæ¥çç ç©¶å¯ä»¥éä¸­äºï¼
*   è¿ä¸æ­¥æåæ¨¡åå¨ç²¾ç»ç»èçææ¹é¢çè½åï¼ä»¥è§£å³ææ¬åæé¨æ¸²æç­åºæ¯ä¸­çä¿çåº¦é®é¢ã
*   æ¢ç´¢å¦ä½åå°é«åè¾¨ççæä¸­çä¼ªå½±ï¼å¯è½éè¿æ¹è¿åºå±èªç¼ç å¨æè°æ´è®­ç»ç­ç¥ã
*   ç ç©¶å¦ä½å°åå®¹å®å¨åå¯æ§æ§æºå¶æ´åå°TiMæ¡æ¶ä¸­ï¼ä»¥åºå¯¹çææ¨¡åé¢ä¸´çä¼¦çåç¤¾ä¼ææã</p>
<hr />
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæéè¿å¼å¥Transition Models (TiM) åå¶åæ°çç¶æè½¬æ¢æç­å¼åé«æçDDEè®¡ç®æ¹æ³ï¼æåå°éæ°æèäºçæå­¦ä¹ ç®æ ãTiMä¸ä»å¨æ§è½ä¸è¶è¶äºç°ææ¨¡åï¼è¿å¨æçãå¯æ©å±æ§åå¤åè½æ§æ¹é¢åå¾äºæ¾èè¿å±ï¼ä¸ºçæå¼AIçæªæ¥åå±å¥ å®äºåå®åºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This leads to a
novel generative paradigm, Transition Models (TiM), which adapt to
arbitrary-step transitions, seamlessly traversing the generative trajectory
from single leaps to fine-grained refinement with more steps.</li>
<li>Despite having
only 865M parameters, TiM achieves state-of-the-art performance, surpassing
leading models such as SD3.5 (8B parameters) and FLUX.1 (12B parameters) across
all evaluated step counts.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.04394v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.04394v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.04379v1'></a></p>
<h2 id="ssgaussian-semantic-aware-and-structure-preserving-3d-style-transfer"><a href="https://arxiv.org/abs/2509.04379v1">SSGaussian: Semantic-Aware and Structure-Preserving 3D Style Transfer</a></h2>
<p><strong>Authors:</strong> Jimin Xu, Bosheng Qin, Tao Jin, Zhou Zhao, Zhenhui Ye, Jun Yu, Fei Wu</p>
<p><strong>Published:</strong> 2025-09-04</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Recent advancements in neural representations, such as Neural Radiance Fields
and 3D Gaussian Splatting, have increased interest in applying style transfer
to 3D scenes. While existing methods can transfer style patterns onto
3D-consistent neural representations, they struggle to effectively extract and
transfer high-level style semantics from the reference style image.
Additionally, the stylized results often lack structural clarity and
separation, making it difficult to distinguish between different instances or
objects within the 3D scene. To address these limitations, we propose a novel
3D style transfer pipeline that effectively integrates prior knowledge from
pretrained 2D diffusion models. Our pipeline consists of two key stages: First,
we leverage diffusion priors to generate stylized renderings of key viewpoints.
Then, we transfer the stylized key views onto the 3D representation. This
process incorporates two innovative designs. The first is cross-view style
alignment, which inserts cross-view attention into the last upsampling block of
the UNet, allowing feature interactions across multiple key views. This ensures
that the diffusion model generates stylized key views that maintain both style
fidelity and instance-level consistency. The second is instance-level style
transfer, which effectively leverages instance-level consistency across
stylized key views and transfers it onto the 3D representation. This results in
a more structured, visually coherent, and artistically enriched stylization.
Extensive qualitative and quantitative experiments demonstrate that our 3D
style transfer pipeline significantly outperforms state-of-the-art methods
across a wide range of scenes, from forward-facing to challenging 360-degree
environments. Visit our project page https://jm-xu.github.io/SSGaussian for
immersive visualization.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Jimin Xuç­äººæ°åçè®ºæâSSGaussian: Semantic-Aware and Structure-Preserving 3D Style Transferâçå¨é¢æè¦ã</p>
<hr />
<h3 id="ssgaussian-semantic-aware-and-structure-preserving-3d-style-transfer_1">SSGaussian: Semantic-Aware and Structure-Preserving 3D Style Transfer è®ºææè¦</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³3Dåºæ¯é£æ ¼è¿ç§»ä¸­çæ ¸å¿ææãç°ææ¹æ³è½ç¶è½å°é£æ ¼æ¨¡å¼è¿ç§»å°3Dä¸è´çç¥ç»è¡¨ç¤ºä¸ï¼ä½å¨æååè¿ç§»åèé£æ ¼å¾åä¸­çé«å±é£æ ¼è¯­ä¹æ¹é¢è¡¨ç°ä¸è¶³ãæ­¤å¤ï¼é£æ ¼åç»æå¾å¾ç¼ºä¹ç»ææ¸æ°åº¦åå¯¹è±¡åç¦»æ§ï¼å¯¼è´é¾ä»¥åºå3Dåºæ¯ä¸­çä¸åå®ä¾æå¯¹è±¡ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è´¡ç®ï¼</strong>
SSGaussianæåºäºä¸ç§æ°é¢ç3Dé£æ ¼è¿ç§»æµç¨ï¼æææ´åäºé¢è®­ç»2Dæ©æ£æ¨¡åçåéªç¥è¯ï¼å¹¶åå«ä¸¤ä¸ªå³é®åæ°ï¼</p>
<ul>
<li><strong>è·¨è§å¾é£æ ¼å¯¹é½ï¼Cross-View Style Alignment, CVSAï¼æ¨¡åï¼</strong> ä¸ºäºè§£å³2Dæ©æ£æ¨¡åå¨å¤è§å¾ä¸è´æ§æ¹é¢çææï¼CVSAæ¨¡åå°è·¨è§å¾æ³¨æåæºå¶å¼å¥å°U-Netçæåä¸ä¸ªä¸éæ ·åä¸­ãè¿ä½¿å¾ç¹å¾è½å¤å¨å¤ä¸ªå³é®è§å¾ä¹é´è¿è¡äº¤äºï¼ç¡®ä¿æ©æ£æ¨¡åçæçé£æ ¼åå³é®è§å¾å¨ä¿æé£æ ¼ä¿çåº¦çåæ¶ï¼ä¹å·å¤å®ä¾çº§å«çï¼èéåç´ çº§å«çï¼ä¸è´æ§ã</li>
<li><strong>å®ä¾çº§å«é£æ ¼è¿ç§»ï¼Instance-level Style Transfer, ISTï¼æ¹æ³ï¼</strong> è¯¥æ¹æ³å©ç¨é«æ¯åç»ï¼Gaussian Groupingï¼æä¾çèº«ä»½ç¼ç åæ°ï¼å»ºç«è®­ç»è§å¾ä¸é£æ ¼åå³é®è§å¾ä¹é´å±é¨åºåï¼å³å®ä¾ï¼çå¯¹åºå³ç³»ãéè¿å¨å¹éçå±é¨ç»åæ§è¡æè¿é»ç¹å¾å¹éï¼NNFMï¼ï¼ISTæ¹æ³è½å°é£æ ¼åå³é®è§å¾çå®ä¾çº§å«ä¸è´æ§ææè¿ç§»å°3Dè¡¨ç¤ºä¸ï¼ä»èå®ç°æ´å·ç»æåãè§è§è¿è´¯ä¸èºæ¯æ§ä¸°å¯çé£æ ¼åã</li>
</ul>
<p>æ´ä¸ªæµç¨åä¸ºä¸¤ä¸ªé¶æ®µï¼é¦åï¼å©ç¨æ©æ£åéªçæå³é®è§è§çé£æ ¼åæ¸²æï¼ç¶åï¼å°è¿äºé£æ ¼åçå³é®è§å¾è¿ç§»å°3Dè¡¨ç¤ºä¸ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
è®ºæéè¿å¹¿æ³çå®æ§åå®éå®éªè¯æï¼SSGaussianå¨å¤ç§åºæ¯ï¼ä»åååºæ¯å°å·ææææ§ç360åº¦ç¯å¢ï¼ä¸ç3Dé£æ ¼è¿ç§»æ§è½æ¾èä¼äºç°ææåè¿çæ¹æ³ã</p>
<ul>
<li><strong>å®æ§ç»æï¼</strong> é£æ ¼åç»æå¨ä¿æåå§åå®¹ç»æçåæ¶ï¼è½æ´å¥½å°æååè¿ç§»é«å±é£æ ¼è¯­ä¹åç²¾ç»ç¬è§¦ç»èãç¹å«æ¯ï¼å¨é«åº¦ç»èåçåºæ¯ä¸­ï¼SSGaussianè½æ´ææå°ä¿æç²¾ç»ç»èï¼å¹¶å®ç°å±é¨é£æ ¼åï¼ä½¿å¾3Dåºæ¯ä¸­çä¸ååºåï¼å®ä¾ï¼åºåæ´æ¸æ°ï¼è§è§è¿è´¯æ§æ´å¼ºã</li>
<li><strong>å®éç»æï¼</strong> å¨å¤è§å¾ä¸è´æ§ï¼ç­ç¨åé¿ç¨ä¸è´æ§ï¼æ¹é¢ï¼SSGaussianå¨LPIPSåRMSEææ ä¸åä¼äºåºçº¿æ¹æ³ãå¨æ¸²æè´¨éè¯ä¼°ï¼åå®¹æå¤±åé£æ ¼æå¤±ï¼æ¹é¢ï¼SSGaussianä¹è¡¨ç°åºåè¶æ§è½ï¼è¡¨æå¶å¨ä¿æåå®¹ç»æçåæ¶ï¼è½æ´å¥½å°ææé£æ ¼ç¹å¾ã</li>
<li><strong>ç¨æ·ç ç©¶ï¼</strong> ç¨æ·ç ç©¶ç»æè¿ä¸æ­¥è¯å®ï¼SSGaussianå¨âç»æå®æ´æ§âãâé£æ ¼ç¸ä¼¼æ§âåâè§è§è´¨éâä¸ä¸ªç»´åº¦ä¸åä¼äºææå¯¹æ¯æ¹æ³ï¼è¯æäºå¶å¨è¯­ä¹æç¥åç»æä¿æ3Dé£æ ¼è¿ç§»æ¹é¢çåè¶æ§è½ã</li>
<li><strong>éåº¦ï¼</strong> SSGaussianå®ç°äºé«æçé£æ ¼ååå®æ¶æ¸²ææ§è½ï¼ä¸æå¿«çæ¿ä»£æ¹æ³ç¸å½ã</li>
</ul>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸­æ²¡ææç¡®æåå½åæ¹æ³çå·ä½å±éæ§ãç¶èï¼ä»æ¹æ³æè¿°åå®éªè®¾ç½®ä¸­å¯ä»¥æ¨æ­åºä¸äºæ½å¨çæ¹é¢ï¼
*   <strong>è®¡ç®èµæºï¼</strong> è½ç¶è®ºææå°éåº¦ä¸æå¿«æ¹æ³ç¸å½ï¼ä½3Dé£æ ¼è¿ç§»ï¼å°¤å¶æ¯æ¶åæ©æ£æ¨¡åå3Dé«æ¯è¡¨ç¤ºçè¿­ä»£ä¼åï¼éå¸¸ä»éè¦è¾é«çè®¡ç®èµæºã
*   <strong>é£æ ¼æ³åæ§ï¼</strong> å°½ç®¡è®ºæä½¿ç¨äºå¤æ ·åçé£æ ¼åèå¾åï¼ä½å¯¹äºæäºæç«¯æé«åº¦æ½è±¡çé£æ ¼ï¼å¶ææå¯è½ä»ææåç©ºé´ã
*   <strong>å¯¹é«æ¯åç»çä¾èµï¼</strong> è¯¥æ¹æ³ä¾èµäºé«æ¯åç»æ¥è·åå®ä¾åå²ä¿¡æ¯ãå¦æé«æ¯åç»å¨æäºå¤æåºæ¯ä¸­è¡¨ç°ä¸ä½³ï¼å¯è½ä¼å½±åé£æ ¼è¿ç§»çåç¡®æ§ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºæä¸­æ²¡ææç¡®æåºæªæ¥çç ç©¶æ¹åãä½åºäºå¶è´¡ç®åæ½å¨å±éæ§ï¼å¯ä»¥æ¨æ­åºä»¥ä¸æ¹åï¼
*   <strong>å®æ¶æ§è½ä¼åï¼</strong> è¿ä¸æ­¥æ¢ç´¢æ´é«æçç®æ³ææ¨¡åæ¶æï¼ä»¥å®ç°æ´å¿«çè®­ç»åæ¸²æéåº¦ï¼ä½¿å¶æ´éç¨äºäº¤äºå¼åºç¨ã
*   <strong>æ´å¤æçé£æ ¼è¯­ä¹çè§£ï¼</strong> æ¢ç´¢å¦ä½ä»åèå¾åä¸­æåæ´æ½è±¡ãæ´å¤æçé£æ ¼è¯­ä¹ï¼å¹¶å°å¶æ´é²æ¯å°è¿ç§»å°3Dåºæ¯ä¸­ã
*   <strong>ç¨æ·å¯æ§æ§ï¼</strong> å¢å ç¨æ·å¯¹é£æ ¼è¿ç§»è¿ç¨çç²¾ç»æ§å¶è½åï¼ä¾å¦åè®¸ç¨æ·æå®ç¹å®åºåçé£æ ¼æè°æ´é£æ ¼å¼ºåº¦ã
*   <strong>ç»åå¶ä»3Dè¡¨ç¤ºï¼</strong> æ¢ç´¢å°è¯¥æ¹æ³æ©å±å°å¶ä»3Dè¡¨ç¤ºï¼å¦ç¥ç»è¾å°åºï¼NeRFï¼æå¶ä»éå¼è¡¨ç¤ºï¼ä»¥è¯ä¼°å¶éç¨æ§ã
*   <strong>å¨æåºæ¯é£æ ¼è¿ç§»ï¼</strong> å°æ¹æ³æ©å±å°å¨æ3Dåºæ¯ï¼å¤çæ¶é´ä¸è´æ§é®é¢ã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these limitations, we propose a novel
3D style transfer pipeline that effectively integrates prior knowledge from
pretrained 2D diffusion models.</li>
<li>Extensive qualitative and quantitative experiments demonstrate that our 3D
style transfer pipeline significantly outperforms state-of-the-art methods
across a wide range of scenes, from forward-facing to challenging 360-degree
environments.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.04379v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.04379v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.04338v1'></a></p>
<h2 id="from-editor-to-dense-geometry-estimator"><a href="https://arxiv.org/abs/2509.04338v1">From Editor to Dense Geometry Estimator</a></h2>
<p><strong>Authors:</strong> JiYuan Wang, Chunyu Lin, Lei Sun, Rongying Liu, Lang Nie, Mingxing Li, Kang Liao, Xiangxiang Chu, Yao Zhao</p>
<p><strong>Published:</strong> 2025-09-04</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Leveraging visual priors from pre-trained text-to-image (T2I) generative
models has shown success in dense prediction. However, dense prediction is
inherently an image-to-image task, suggesting that image editing models, rather
than T2I generative models, may be a more suitable foundation for fine-tuning.
  Motivated by this, we conduct a systematic analysis of the fine-tuning
behaviors of both editors and generators for dense geometry estimation. Our
findings show that editing models possess inherent structural priors, which
enable them to converge more stably by <code>refining" their innate features, and
ultimately achieve higher performance than their generative counterparts.
  Based on these findings, we introduce \textbf{FE2E}, a framework that
pioneeringly adapts an advanced editing model based on Diffusion Transformer
(DiT) architecture for dense geometry prediction. Specifically, to tailor the
editor for this deterministic task, we reformulate the editor's original flow
matching loss into the</code>consistent velocity" training objective. And we use
logarithmic quantization to resolve the precision conflict between the editor's
native BFloat16 format and the high precision demand of our tasks.
Additionally, we leverage the DiT's global attention for a cost-free joint
estimation of depth and normals in a single forward pass, enabling their
supervisory signals to mutually enhance each other.
  Without scaling up the training data, FE2E achieves impressive performance
improvements in zero-shot monocular depth and normal estimation across multiple
datasets. Notably, it achieves over 35\% performance gains on the ETH3D dataset
and outperforms the DepthAnything series, which is trained on 100<script type="math/tex">\times</script> data.
The project page can be accessed \href{https://amap-ml.github.io/FE2E/}{here}.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾JiYuan Wangç­äººæ°åçè®ºæâFrom Editor to Dense Geometry Estimatorâçå¨é¢æè¦ã</p>
<hr />
<h3 id="from-editor-to-dense-geometry-estimator_1">ãFrom Editor to Dense Geometry Estimatorãè®ºææè¦</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³åç®ç¨ å¯å ä½ä¼°è®¡ï¼å¦æ·±åº¦åæ³çº¿ä¼°è®¡ï¼ä¸­çä¸ä¸ªæ ¸å¿é®é¢ï¼å¦ä½æ´ææå°å©ç¨é¢è®­ç»çè§è§æ¨¡åæ¥æåç¨ å¯é¢æµä»»å¡çæ§è½ï¼å°¤å¶æ¯å¨æ°æ®æéçæåµä¸ãç°æç ç©¶å¤å©ç¨ææ¬å°å¾åï¼T2Iï¼çææ¨¡åä½ä¸ºåºç¡ï¼ä½ä½èæåºï¼ç¨ å¯é¢æµæ¬è´¨ä¸æ¯å¾åå°å¾åï¼I2Iï¼ä»»å¡ï¼å æ­¤å¾åç¼è¾æ¨¡åå¯è½æ¯T2Içææ¨¡åæ´éåä½ä¸ºå¾®è°çåºç¡ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥è®ºæçæ ¸å¿åæ°å¨äºæåºäº<strong>FE2E</strong>ï¼From Editor to Estimatorï¼æ¡æ¶ï¼è¯¥æ¡æ¶é¦æ¬¡å°åºäºDiffusion Transformer (DiT) æ¶æçåè¿å¾åç¼è¾æ¨¡ååºç¨äºç¨ å¯å ä½é¢æµä»»å¡ãå·ä½è´¡ç®åæ¬ï¼</p>
<ul>
<li><strong>ç³»ç»æ§åæä¸æ´å¯ï¼</strong> ä½èå¯¹å¾åç¼è¾æ¨¡ååçææ¨¡åå¨ç¨ å¯å ä½ä¼°è®¡ä»»å¡ä¸çå¾®è°è¡ä¸ºè¿è¡äºç³»ç»æ§åæãç ç©¶åç°ï¼ç¼è¾æ¨¡åå·æåºæçç»æåéªï¼è½å¤æ´ç¨³å®å°æ¶æï¼å¹¶éè¿âæç¼âå¶åå¨ç¹å¾ï¼æç»å®ç°æ¯çææ¨¡åæ´é«çæ§è½ã</li>
<li><strong>âä¸è´éåº¦âè®­ç»ç®æ ï¼</strong> éå¯¹ç¨ å¯é¢æµçç¡®å®æ§æ§è´¨ï¼FE2Eå°ç¼è¾æ¨¡ååå§çæµå¹éæå¤±éæ°è¡¨è¿°ä¸ºâä¸è´éåº¦âï¼consistent velocityï¼è®­ç»ç®æ ãè¿éè¿åºå®èµ·å§ç¹å¹¶ç¡®ä¿éåº¦æ¹ååå¤§å°çä¸è´æ§ï¼æ¶é¤äºç¦»æ£æ²çº¿è½¨è¿¹åéæºèµ·å§ç¹å¼å¥çè¯¯å·®ï¼æ¾èæé«äºæ¨çæçåæ§è½ã</li>
<li><strong>å¯¹æ°å¼éåï¼</strong> ä¸ºäºè§£å³ç¼è¾æ¨¡ååçBFloat16æ ¼å¼ä¸ç¨ å¯å ä½ä»»å¡é«ç²¾åº¦éæ±ä¹é´çå²çªï¼è®ºæå¼å¥äºå¯¹æ°å¼éåæ¹æ³ãè¿ç§æ¹æ³å¨è¿è·ç¦»åè¿è·ç¦»èå´åé½è½ä¿æåçä¸å ä¹æå®çç¸å¯¹è¯¯å·®ï¼ææè§£å³äºä¼ ç»ååéååééåæ¹æ¡çç²¾åº¦é®é¢ã</li>
<li><strong>æ ææ¬æ·±åº¦ä¸æ³çº¿èåä¼°è®¡ï¼</strong> FE2Eå©ç¨DiTçå¨å±æ³¨æåæºå¶ï¼å¨åæ¬¡ååä¼ æ­ä¸­å®ç°äºæ·±åº¦åæ³çº¿çèåä¼°è®¡ï¼æ éé¢å¤è®¡ç®ææ¬ãè¿ä½¿å¾ä¸¤ç§çç£ä¿¡å·è½å¤ç¸äºå¢å¼ºï¼æåæ´ä½æ§è½ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
FE2Eå¨å¤ä¸ªæ°æ®éä¸çé¶æ ·æ¬åç®æ·±åº¦åæ³çº¿ä¼°è®¡ä»»å¡ä¸­åå¾äºæ¾èçæ§è½æåï¼</p>
<ul>
<li><strong>æ·±åº¦ä¼°è®¡ï¼</strong> å¨ETH3Dæ°æ®éä¸ï¼FE2Eçæ§è½æåè¶è¿35%ãå°½ç®¡è®­ç»æ°æ®éè¿å°äºDepthAnythingç³»åï¼FE2Eä½¿ç¨71Kæ°æ®ï¼èDepthAnythingä½¿ç¨62.6Mæ°æ®ï¼ç¸å·®100åï¼ï¼FE2Eçå¹³åæåä»è¶è¶äºDepthAnythingç³»åãå¨KITTIæ°æ®éä¸ï¼AbsRelè¯¯å·®éä½äº10%ã</li>
<li><strong>æ³çº¿ä¼°è®¡ï¼</strong> FE2Eå¨é¶æ ·æ¬æ³çº¿ä¼°è®¡ä»»å¡ä¸­ä¹åå¾äºæåè¿çæ§è½ï¼å°¤å¶æé¿éå»ºå¤æçå ä½ç»èï¼å¦è¡¨é¢è¤¶ç±åå°åç©ä½ã</li>
<li><strong>å®æ§ç»æï¼</strong> å®æ§æ¯è¾æ¾ç¤ºï¼FE2Eå¨æææ§åç§æ¡ä»¶ï¼æäº®ãä½åï¼ä¸è¡¨ç°ä¼å¼ï¼å¹¶è½æ´å¥½å°ä¿çè¿è·ç¦»ç»èã</li>
</ul>
<p>è¿äºç»æçæä¹å¨äºï¼FE2Eéªè¯äºâä»ç¼è¾æ¨¡åå°ä¼°è®¡å¨âçèå¼ï¼è¯æäºå©ç¨ç¼è¾æ¨¡ååºæçè½åæ¯ä¸ç§ææä¸æ°æ®é«æçç¨ å¯é¢æµæ¹æ³ï¼ä¸ºæªæ¥ç¨ å¯å ä½ä¼°è®¡ä»»å¡æä¾äºæ°çåºç¡ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸­æå°äºFE2Eçä¸ä¸ªä¸»è¦å±éæ§æ¯<strong>è®¡ç®è´è½½è¾å¤§</strong>ãå°½ç®¡FE2Eå¨æ§è½åè®¡ç®æçä¹é´åå¾äºå¹³è¡¡ï¼ä½ç¸å¯¹äºå¶ä»èªçç£æ¹æ³ï¼DiTæ¶æçå¼å¥ç¡®å®å¯¼è´äºè®¡ç®å¤æåº¦çæ¾èå¢å ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºææåºäºä»¥ä¸æªæ¥ç ç©¶æ¹åï¼</p>
<ul>
<li><strong>å¤æ ·ååºç¡æ¨¡åï¼</strong> å¾åç¼è¾é¢ååå±è¿éï¼FE2Eçæ¹æ³è®¾è®¡æ¯æ¨¡åæ å³çãæªæ¥å·¥ä½è®¡åæ´åæ´å¹¿æ³çç¼è¾æ¨¡åï¼ä»¥è¿ä¸æ­¥è¯å®æ¬ææåºçå¨æºåç»è®ºã</li>
<li><strong>æ©å¤§è®­ç»æ°æ®è§æ¨¡ï¼</strong> å°½ç®¡FE2Eå¨æéæ°æ®éä¸å±ç¤ºäºå¼ºå¤§çæ³åæ§è½ï¼ä½ä½èä»é¢ææ©å¤§è®­ç»æ°æ®éå¯ä»¥è¿ä¸æ­¥æåæ¨¡åçæ§è½ãå¯¹äºé£äºå¯¹è®¡ç®å¤æåº¦ä¸ææä½è¦æ±æé«é¢æµç²¾åº¦çé¢åï¼è¿æ¯ä¸ä¸ªææä¹çæ¹åã</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>Based on these findings, we introduce \textbf{FE2E}, a framework that
pioneeringly adapts an advanced editing model based on Diffusion Transformer
(DiT) architecture for dense geometry prediction.</li>
<li>Notably, it achieves over 35\% performance gains on the ETH3D dataset
and outperforms the DepthAnything series, which is trained on 100<script type="math/tex">\times</script> data.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.04338v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.04338v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.04334v1'></a></p>
<h2 id="geoarena-an-open-platform-for-benchmarking-large-vision-language-models-on-worldwide-image-geolocalization"><a href="https://arxiv.org/abs/2509.04334v1">GeoArena: An Open Platform for Benchmarking Large Vision-language Models on WorldWide Image Geolocalization</a></h2>
<p><strong>Authors:</strong> Pengyue Jia, Yingyi Zhang, Xiangyu Zhao, Yixuan Li</p>
<p><strong>Published:</strong> 2025-09-04</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Image geolocalization aims to predict the geographic location of images
captured anywhere on Earth, but its global nature presents significant
challenges. Current evaluation methodologies suffer from two major limitations.
First, data leakage: advanced approaches often rely on large vision-language
models (LVLMs) to predict image locations, yet these models are frequently
pretrained on the test datasets, compromising the accuracy of evaluating a
model's actual geolocalization capability. Second, existing metrics primarily
rely on exact geographic coordinates to assess predictions, which not only
neglects the reasoning process but also raises privacy concerns when user-level
location data is required. To address these issues, we propose GeoArena, a
first open platform for evaluating LVLMs on worldwide image geolocalization
tasks, offering true in-the-wild and human-centered benchmarking. GeoArena
enables users to upload in-the-wild images for a more diverse evaluation
corpus, and it leverages pairwise human judgments to determine which model
output better aligns with human expectations. Our platform has been deployed
online for two months, during which we collected over thousands voting records.
Based on this data, we conduct a detailed analysis and establish a leaderboard
of different LVLMs on the image geolocalization task.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä»¥ä¸æ¯Pengyue Jia, Yingyi Zhang, Xiangyu Zhao, Yixuan Liæ°åçè®ºæâGeoArena: An Open Platform for Benchmarking Large Vision-language Models on Worldwide Image Geolocalizationâçå¨é¢æè¦ï¼</p>
<p><strong>è®ºæé¢ç®ï¼</strong> GeoArena: ä¸ä¸ªç¨äºåºåæµè¯å¨çå¾åå°çå®ä½å¤§åè§è§-è¯­è¨æ¨¡åçå¼æ¾å¹³å°</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³å½åå¾åå¨çå°çå®ä½ä»»å¡è¯ä¼°æ¹æ³å­å¨çä¸¤ä¸ªä¸»è¦éå¶ï¼
1. <strong>æ°æ®æ³é²ï¼Data Leakageï¼ï¼</strong> ç°æçè¯ä¼°åºåéå¸¸ä½¿ç¨éææ°æ®éï¼èå¤§åè§è§-è¯­è¨æ¨¡åï¼LVLMsï¼å¨é¢è®­ç»é¶æ®µå¯è½å·²ç»æ¥è§¦è¿è¿äºæµè¯æ°æ®ï¼å¯¼è´è¯ä¼°ç»ææ æ³çå®åæ æ¨¡åçå°çå®ä½è½åã
2. <strong>åºäºGPSçè¯ä¼°å±éæ§ï¼GPS-based Evaluation Limitationsï¼ï¼</strong> ç°æçè¯ä¼°ææ ä¸»è¦ä¾èµç²¾ç¡®çå°çåæ æ¥è¡¡éé¢æµåç¡®æ§ï¼è¿ä¸ä»å¿½ç¥äºæ¨¡åçæ¨çè¿ç¨ï¼èä¸å¨éè¦ç¨æ·çº§ä½ç½®æ°æ®æ¶å¼åéç§é®é¢ã</p>
<p>ä¸ºäºè§£å³è¿äºé®é¢ï¼è®ºææåºäºä¸ä¸ªæ´å¨æãä»¥ç¨æ·ä¸ºä¸­å¿ä¸æ³¨ééç§çè¯ä¼°æ¡æ¶ï¼ä»¥æ´å¥½å°åºåæµè¯LVLMså¨å¨çå¾åå°çå®ä½ä»»å¡ä¸çæ§è½ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
GeoArenaå¹³å°å¼å¥äºä»¥ä¸å³é®åæ°åæ¹æ³è®ºè´¡ç®ï¼
*   <strong>å¼æ¾å¹³å°ä¸çå®ä¸çæ°æ®æ¶éï¼</strong> GeoArenaæ¯ä¸ä¸ªå¼æ¾çå¨çº¿å¹³å°ï¼åè®¸ç¨æ·ä¸ä¼ çå®ä¸ççå¾åè¿è¡å°çå®ä½ï¼ä»èæå»ºä¸ä¸ªå¤æ ·åä¸å¨æçè¯ä¼°è¯­æåºï¼ææç¼è§£äºéææ°æ®éçæ°æ®æ³é²é®é¢ã
*   <strong>ä»¥äººä¸ºä¸­å¿çåºåæµè¯ï¼</strong> å¹³å°éç¨æå¯¹çäººç±»å¤æ­æ¥è¯ä¼°æ¨¡åè¾åºçè´¨éï¼ç¨æ·å¯¹ä¸¤ä¸ªå¿åæ¨¡åçæçååºè¿è¡æç¥¨ï¼éæ©æ´ç¬¦åäººç±»ææçç­æ¡ãè¿ç§æ¹æ³è¶è¶äºåçº¯ä¾èµGPSåç¡®æ§çè¯ä¼°ï¼å¹¶åè½»äºå¯¹ç²¾ç¡®ç¨æ·ä½ç½®æ°æ®çéç§æå¿§ã
*   <strong>Eloæåç³»ç»ä¸Bradley-Terryæ¨¡åï¼</strong> GeoArenaå©ç¨Eloæåç³»ç»åBradley-Terryæ¨¡åæ¥è®¡ç®æ¨¡åçç¸å¯¹å¼ºåº¦åæç»æåãEloç³»ç»æ ¹æ®æå¯¹æ¯è¾ç»æè¿­ä»£æ´æ°æ¨¡ååæ°ï¼èBradley-Terryæ¨¡ååæä¾äºä¸ç§æ´ç¨³å®ãä¸é¡ºåºæ å³çæåä¼°è®¡æ¹æ³ï¼ç¡®ä¿äºæåçå¯é æ§ã
*   <strong>GeoArena-1Kæ°æ®éåå¸ï¼</strong> åºäºå¹³å°æ¶éçæ°æ®ï¼è®ºæåå¸äºGeoArena-1Kæ°æ®éï¼è¿æ¯é¦ä¸ªç¨äºå¾åå°çå®ä½é¢åLVLMsçäººç±»åå¥½æ°æ®éï¼åå«ç¨æ·ä¸ä¼ çå¾åãææ¬æä»¤ãæ¨¡åååºåäººç±»æç¥¨ç»æï¼ä¸ºå¥å±å»ºæ¨¡åå°çåºç¡æ¨¡åç­ç¸å³ç ç©¶æä¾äºå®è´µèµæºã
*   <strong>é£æ ¼ç¹å¾åæï¼</strong> è®ºæéè¿å°é£æ ¼ç¸å³ç¹å¾ï¼å¦ååºé¿åº¦ãåè¡¨æ°éãæ é¢æ°éãå¼ºè°æ°éåGPSè¾åºæ¯ä¾ï¼çº³å¥Bradley-Terryåå½æ¡æ¶ï¼åæäºæ¨¡åååºçåªäºç¹å¾ä¼å½±åç¨æ·åå¥½ï¼æ­ç¤ºäºç¨æ·æ´å¾åäºæ´é¿ãæ´ç»æåãåå«GPSä¿¡æ¯çååºã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>GeoArenaæè¡æ¦ï¼</strong> é¨ç½²ä¸¤ä¸ªæä»¥æ¥ï¼GeoArenaæ¶éäºæ°åæ¡æç¥¨è®°å½ï¼å¹¶åºäºæ­¤å»ºç«äºLVLMså¨å¾åå°çå®ä½ä»»å¡ä¸çæè¡æ¦ãç»ææ¾ç¤ºï¼Geminiç³»åæ¨¡åï¼å¦Gemini-2.5-proåGemini-2.5-flashï¼è¡¨ç°æå¼ºï¼æ¾èä¼äºå¶ä»ç³»ç»ï¼å¸æ¾äºå¤§è§æ¨¡å¤æ¨¡æé¢è®­ç»çä¼å¿ã
*   <strong>å¼æºæ¨¡åçç«äºåï¼</strong> Qwen2.5åGemma-3ç­å¼æºæ¨¡åä¹åå¾äºæç«äºåçæåï¼ä¾å¦Qwen2.5-VL-72B-Instructä¸GPT-4.1ç³»åè¡¨ç°ç¸å½ï¼è¡¨æå¼æºç¤¾åºæ­£å¨è¿éç¼©å°ä¸ä¸æåæ²¿ç³»ç»çå·®è·ã
*   <strong>æ¨¡åå®¹éä¸æ§è½ï¼</strong> è¾å°å®¹éçæ¨¡åï¼å¦gemma-3-4b-itãqwen2.5-vl-7b-instructãgpt-4.1-nanoågpt-40-miniï¼è¡¨ç°æ®éä¸ä½³ï¼å¸æ¾äºå¾åå°çå®ä½ä»»å¡çåºæé¾åº¦ï¼ä»¥åæ¨¡åå®¹éåè®­ç»æ°æ®å¯¹æ³åè½åçéè¦æ§ã
*   <strong>äººç±»åå¥½åæï¼</strong> ååºé¿åº¦ãåè¡¨æ°éåGPSè¾åºæ¯ä¾ä¸äººç±»åå¥½åæ­£ç¸å³ï¼è¡¨æç¨æ·æ´åæ¬¢æ´é¿ãæ´ç»æåãåå«GPSé¢æµçååºï¼è¿å¼ºè°äºæ¨çè´¨éçéè¦æ§ã
*   <strong>LVLMä¸äººç±»å¤æ­çä¸è´æ§ï¼</strong> å¯¹æ¯ç ç©¶æ¾ç¤ºï¼Gemini 2.5 Proä¸äººç±»è¯ä¼°çä¸è´æ§ï¼65.79%ï¼æ¾èé«äºQwen-VL-72Bï¼46.67%ï¼ï¼è¡¨æé¡¶çº§ä¸ææ¨¡åå¨è¯ä¼°å°çå®ä½ä»»å¡ååºæ¶ä¸äººç±»å¤æ­æ´ä¸è´ï¼ä½ä»å­å¨æ¾èå·®è·ï¼æ¿å±æªæ¥ç ç©¶è®¾è®¡æ´å¿ å®ãé²æ£çLLMè¯ä¼°å¨ã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
*   <strong>Eloæåå¯¹è¿æå¹éçæææ§ï¼</strong> ä¼ ç»çEloæåç³»ç»å¯¹è¿æå¹éç»æé«åº¦ææï¼å¯è½å¯¼è´æååå¹éé¡ºåºå½±åï¼è¿å¨è¯ä¼°æ¨¡åè½åæ¶æ¯ä¸çæ³çãè®ºæéè¿éç¨Bradley-Terryæ¨¡åæ¥ç¼è§£è¿ä¸é®é¢ã
*   <strong>ç¨çéæ ·ååçå¯é æ§ï¼</strong> å¨æå¯¹æ¯è¾ä¸­ï¼å¦ææäºæ¨¡åå¯¹ä¹é´çæææ¬¡æ°è¾å°ï¼ç¨çéæ ·ï¼ï¼å¶èçå¯¹æ¯çå¯é æ§å¯è½è¾ä½ï¼éè¦è°¨æè§£éã
*   <strong>LVLMä½ä¸ºè¯ä¼°å¨çå±éæ§ï¼</strong> å°½ç®¡é¡¶çº§LVLMå¨è¯ä¼°å°çå®ä½ååºæ¶ä¸äººç±»å¤æ­æä¸å®ä¸è´æ§ï¼ä½ä»å­å¨æ¾èå·®è·ï¼è¡¨æå®ä»¬å¨ææäººç±»çéçç»å¾®æ åï¼åç¡®æ§ãè¯æ®åæ¸æ°åº¦ï¼æ¹é¢ä»æææã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>å¥å±å»ºæ¨¡ä¸å°çåºç¡æ¨¡åï¼</strong> GeoArena-1Kæ°æ®éçåå¸å°æ¯æå¥å±å»ºæ¨¡åå°çåºç¡æ¨¡åç­ç¸å³é¢åçç ç©¶è¿å±ã
*   <strong>æ´å¿ å®ãé²æ£çLLMè¯ä¼°å¨ï¼</strong> é´äºLVLMä½ä¸ºè¯ä¼°å¨ä¸äººç±»å¤æ­ä¹é´ä»å­å¨å·®è·ï¼æªæ¥çå·¥ä½å¯ä»¥ä¸æ³¨äºè®¾è®¡æ´å¿ å®ãé²æ£çLLMè¯ä¼°å¨ã
*   <strong>æ¨¡åå¼åï¼</strong> éå¯¹âå°é¾æ¡ä¾âè¿è¡è®­ç»åè¯ä¼°ï¼ä»¥æé«æ¨¡åå¨è§è§çº¿ç´¢ä¸ææ¾æä¸ç´è§çåºæ¯ä¸çé²æ£æ§åä¸è´æ§æ§è½ã
*   <strong>å¤æ¨¡ææ¨çï¼</strong> è¿ä¸æ­¥æ¢ç´¢LVLMså¨å¤æ¨¡ææ¨çä¸­çè½åï¼ç¹å«æ¯ææ¬è¯å«å¦ä½å¢å¼ºä½ç½®é¢æµåç¡®æ§ã</p>
<p>æ»èè¨ä¹ï¼GeoArenaä¸ºå¨çå¾åå°çå®ä½ä»»å¡æä¾äºä¸ä¸ªå¼åæ§çãä»¥ç¨æ·ä¸ºä¸­å¿ãæ³¨ééç§çå¨æåºåæµè¯å¹³å°ï¼éè¿ç»åçå®ä¸çæ°æ®åäººç±»åå¥½è¯ä¼°ï¼åæäºç°æéæåºåçå±éæ§ï¼å¹¶ä¸ºLVLMså¨å°çç©ºé´æ¨çé¢åçç ç©¶ååå±å¥ å®äºåå®åºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these issues, we propose GeoArena, a
first open platform for evaluating LVLMs on worldwide image geolocalization
tasks, offering true in-the-wild and human-centered benchmarking.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.04334v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.04334v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.04326v1'></a></p>
<h2 id="efficient-odd-one-out-anomaly-detection"><a href="https://arxiv.org/abs/2509.04326v1">Efficient Odd-One-Out Anomaly Detection</a></h2>
<p><strong>Authors:</strong> Silvio Chito, Paolo Rabino, Tatiana Tommasi</p>
<p><strong>Published:</strong> 2025-09-04</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>The recently introduced odd-one-out anomaly detection task involves
identifying the odd-looking instances within a multi-object scene. This problem
presents several challenges for modern deep learning models, demanding spatial
reasoning across multiple views and relational reasoning to understand context
and generalize across varying object categories and layouts. We argue that
these challenges must be addressed with efficiency in mind. To this end, we
propose a DINO-based model that reduces the number of parameters by one third
and shortens training time by a factor of three compared to the current
state-of-the-art, while maintaining competitive performance. Our experimental
evaluation also introduces a Multimodal Large Language Model baseline,
providing insights into its current limitations in structured visual reasoning
tasks. The project page can be found at
https://silviochito.github.io/EfficientOddOneOut/</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Silvio Chito, Paolo RabinoåTatiana Tommasiæ°åçè®ºæâEfficient Odd-One-Out Anomaly Detectionâçå¨é¢æè¦ã</p>
<hr />
<h3 id="_1">ãé«æçå¼ç±»å¼å¸¸æ£æµãè®ºææè¦</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³âå¼ç±»å¼å¸¸æ£æµâï¼odd-one-out anomaly detectionï¼ä»»å¡ï¼å³å¨åå«å¤ä¸ªç©ä½çåºæ¯ä¸­è¯å«åºä¸ä¼ä¸åçå¼å¸¸å®ä¾ãè¿é¡¹ä»»å¡å¯¹ç°ä»£æ·±åº¦å­¦ä¹ æ¨¡åæåºäºå¤éææï¼åæ¬éè¦è·¨å¤ä¸ªè§è§è¿è¡ç©ºé´æ¨çãçè§£ä¸ä¸æçå³èæ¨çï¼ä»¥åå¨ä¸åç©ä½ç±»å«åå¸å±ä¹é´è¿è¡æ³åãä½èå¼ºè°ï¼è§£å³è¿äºæææ¶å¿é¡»å¼é¡¾æçã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>DINOv2-basedé«ææ¨¡åï¼</strong> è®ºææåºäºä¸ç§åºäºDINOv2çæ¨¡åï¼è¯¥æ¨¡åå¨ä¿æç«äºæ§æ§è½çåæ¶ï¼å°åæ°æ°éåå°äºä¸åä¹ä¸ï¼å¹¶å°è®­ç»æ¶é´ç¼©ç­äºä¸åï¼æ¾èä¼äºå½åæåè¿çæ¹æ³ï¼OOOï¼ãè¿éè¿ç´æ¥ä»å¤è§å¾è¾å¥å¾åä¸­æåDINOv2ç¹å¾ï¼å¹¶å°å¶æå½±å°3Dä½ç´ ç©ºé´ï¼é¿åäºOOOæ¨¡åä¸­ä¸å¿è¦çä¸¤æ­¥æ å°å¤ææ§ã
*   <strong>ä¸ä¸æå¹éå¤´ï¼Context Match Headï¼ä¸æ®å·®å¼å¸¸å¤´ï¼Residual Anomaly Headï¼ï¼</strong> æ¨¡åéè¿ROIæ± åå°3Dä½ç´ ç½æ ¼ä¸­çç©ä½è£åªå½ä¸åï¼ç¶åä½¿ç¨Transformerç¼ç å¨è¿è¡ä¸ä¸æå¹éï¼ä»¥æ¨çåºæ¯ä¸­ç©ä½é´çç¸å¯¹å¤è§ãæ­¤å¤ï¼å¼å¥äºæ®å·®å¼å¸¸å¤´ï¼éè¿ä¸ä¸ªå¯å­¦ä¹ çtokenä½ä¸ºåºæ¯ç¹å®æ­£å¸¸ååï¼å¼å¯¼æ¨¡åå³æ³¨ç©ä½ä¸å¹³åæ­£å¸¸ç¶æçåå·®ï¼è¿ä¸æ­¥æåäºåºæ¯ç¹å®å¼å¸¸çè¯å«è½åã
*   <strong>å¤æ¨¡æå¤§è¯­è¨æ¨¡åï¼MLLMï¼åºçº¿ï¼</strong> è®ºæé¦æ¬¡å¼å¥äºä½¿ç¨å¤æ¨¡æå¤§è¯­è¨æ¨¡åï¼å¦Gemini-Flash 2.0ï¼ä½ä¸ºå¼ç±»å¼å¸¸æ£æµä»»å¡çåºçº¿ï¼å¹¶éç¨Set-of-Mark (SoM) æç¤ºç­ç¥æ¥æ¯æè§è§å®ä½ãè¿ä¸ºè¯ä¼°MLLMå¨ç»æåè§è§æ¨çä»»å¡ä¸­çå½åå±éæ§æä¾äºéè¦è§è§£ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æ§è½ä¼å¿ï¼</strong> æåºçæ¨¡åå¨Toys Seenæ°æ®éä¸ä¸OOOæ¨¡åè¡¨ç°ç¸å½ï¼å¨Toys Unseenæ°æ®éä¸ç¥éä¸ç­¹ï¼ä½å¨æ´å·æææ§çParts Unseenæ°æ®éä¸æ¾èä¼äºOOOæ¨¡åãè¿è¡¨æè¯¥æ¨¡åå¨å¤çå ä½å½¢ç¶å·®å¼å¤§ãè¯­ä¹å¤æ ·æ§ä½çæºæ¢°é¨ä»¶åºæ¯æ¶å·ææ´å¼ºçæ³åè½ååæçã
*   <strong>æçæåï¼</strong> ä¸OOOæ¨¡åç¸æ¯ï¼æ°æ¨¡åå¨åæ°æ°éåè®­ç»æ¶é´ä¸å®ç°äºæ¾èä¼åï¼åæ°åå°ä¸åä¹ä¸ï¼è®­ç»æ¶é´ç¼©ç­ä¸åï¼ï¼åæ¶ä¿æäºç«äºæ§çæ§è½ï¼è¿å¯¹äºå·¥ä¸åºç¨ä¸­å¯¹æ¨çæ¶é´ææçåºæ¯è³å³éè¦ã
*   <strong>MLLMå±éæ§ï¼</strong> MLLMåºçº¿å¨ToysåPartsæ°æ®éä¸çè¡¨ç°ä¸ä½³ï¼å¶åç¡®çæ¥è¿äºä¼ ç»çæ£æµæ¹æ³ï¼ImVoxelNetåDETR3Dï¼ãMLLMè¡¨ç°åºå¯¹æ­£å¸¸æ°æ®çåè§ï¼æé¿è¯å«å¤§åè£ç¼åæ­è£ï¼ä½å¨éè¦è·¨å¤ä¸ªè§å¾è¿è¡ç»åæ¯è¾æ¶è¡¨ç°æ£æãè¿æ­ç¤ºäºå½åMLLMå¨ç²¾ç»ç»æåè§è§æ¨çåå¤è§å¾ä¸è´æ§æ¹é¢çå±éæ§ã
*   <strong>é²æ£æ§åæï¼</strong> æ¨¡åå¨ä¸åè§å¾æ°éåç©ä½æ°éååä¸è¡¨ç°åºç¸å¯¹é²æ£æ§ï¼å°¤å¶æ¯å¨è§å¾æ°éå¢å æ¶æ§è½æææåã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>MLLMçè§è§å®ä½åç²¾ç»æ¨çè½åï¼</strong> MLLMå¨å¼ç±»å¼å¸¸æ£æµä»»å¡ä¸­çè¡¨ç°æ­ç¤ºäºå¶å¨è§è§å®ä½åå¤çç²¾ç»å¼å¸¸ç»èæ¹é¢çå±éæ§ï¼å°¤å¶æ¯å¨éè¦è·¨å¤ä¸ªè§å¾è¿è¡æ¯è¾æ¶ãå®ä»¬å¾åäºè¯å«å¨å±å¤è§å¼å¸¸ï¼èéç»ç²åº¦çç»æåå·®å¼ã
*   <strong>3Dç¹å®å¼å¸¸æ£æµï¼</strong> æåºçæ¨¡åå¨æ£æµ3Dç¹å®å¼å¸¸ï¼å¦ç¼ºå¤±é¨ä»¶ãå¹³ç§»ååå½¢ï¼æ¹é¢ä»æä¸è¶³ï¼è¿äºå¼å¸¸éè¦æ´å¨é¢ç3Dæ¨çã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>ç»åé¢è®­ç»åºç¡æ¨¡åä¸æçï¼</strong> æªæ¥çè§£å³æ¹æ¡åºç»§ç»­å©ç¨é¢è®­ç»åºç¡æ¨¡åçè½åï¼å¹¶ä»¥æçä¸ºå³é®ç®æ ï¼å°å¼å¸¸æ£æµæ©å±å°å®æ´ç3Då¼å¸¸å®ä½ã
*   <strong>èªç¶è¯­è¨è§£éï¼</strong> æ¨¡ååºè½å¤éè¿èªç¶è¯­è¨éæå¶é¢æµèåçæ¨çè¿ç¨ã
*   <strong>é»è¾ååè½æ¹é¢æ©å±ï¼</strong> ä»»å¡å¯ä»¥æ©å±å°åå«é»è¾ååè½æ¹é¢ï¼ä»èè¿æ¥æç¥ä¸æºå¨äººææ¯ï¼å¹¶æ¼åä¸ºå¯¹èªä¸»ä»£çççæ­£æºè½æµè¯ã
*   <strong>å¼å¯¼éç¨æ¨¡åè¿è¡å¤è§å¾ä¸è´æ§æ¨çï¼</strong> éè¦æ°çå®å¶æ¨¡åæ¥å¼å¯¼éç¨æ¨¡åå¤çéè¦å¤è§å¾ä¸è´æ§çä»»å¡ã</p>
<hr />
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæå¨å¼ç±»å¼å¸¸æ£æµä»»å¡ä¸­åå¾äºéè¦è¿å±ï¼éè¿æåºä¸ä¸ªé«æçDINOv2-basedæ¨¡åï¼æ¾èæåäºæçå¹¶ä¿æäºç«äºæ§æ§è½ãåæ¶ï¼éè¿å¼å¥MLLMåºçº¿ï¼ä¸ºçè§£å¤æ¨¡æå¤§è¯­è¨æ¨¡åå¨å¤æè§è§æ¨çä»»å¡ä¸­çå½åè½ååå±éæ§æä¾äºå®è´µè§è§£ï¼ä¸ºæªæ¥çç ç©¶ææäºæ¹åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To this end, we
propose a DINO-based model that reduces the number of parameters by one third
and shortens training time by a factor of three compared to the current
state-of-the-art, while maintaining competitive performance.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.04326v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.04326v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.04180v1'></a></p>
<h2 id="visiofirm-cross-platform-ai-assisted-annotation-tool-for-computer-vision"><a href="https://arxiv.org/abs/2509.04180v1">VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer Vision</a></h2>
<p><strong>Authors:</strong> Safouane El Ghazouali, Umberto Michelucci</p>
<p><strong>Published:</strong> 2025-09-04</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>AI models rely on annotated data to learn pattern and perform prediction.
Annotation is usually a labor-intensive step that require associating labels
ranging from a simple classification label to more complex tasks such as object
detection, oriented bounding box estimation, and instance segmentation.
Traditional tools often require extensive manual input, limiting scalability
for large datasets. To address this, we introduce VisioFirm, an open-source web
application designed to streamline image labeling through AI-assisted
automation. VisioFirm integrates state-of-the-art foundation models into an
interface with a filtering pipeline to reduce human-in-the-loop efforts. This
hybrid approach employs CLIP combined with pre-trained detectors like
Ultralytics models for common classes and zero-shot models such as Grounding
DINO for custom labels, generating initial annotations with low-confidence
thresholding to maximize recall. Through this framework, when tested on
COCO-type of classes, initial prediction have been proven to be mostly correct
though the users can refine these via interactive tools supporting bounding
boxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has
on-the-fly segmentation powered by Segment Anything accelerated through WebGPU
for browser-side efficiency. The tool supports multiple export formats (YOLO,
COCO, Pascal VOC, CSV) and operates offline after model caching, enhancing
accessibility. VisioFirm demonstrates up to 90\% reduction in manual effort
through benchmarks on diverse datasets, while maintaining high annotation
accuracy via clustering of connected CLIP-based disambiguate components and
IoU-graph for redundant detection suppression. VisioFirm can be accessed from
\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºSafouane El GhazoualiåUmberto Michelucciæ°åçè®ºæâVisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer Visionâçå¨é¢æè¦ã</p>
<p><strong>è®ºææè¦ï¼VisioFirmï¼è·¨å¹³å°AIè¾å©è®¡ç®æºè§è§æ æ³¨å·¥å·</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢</strong>
è®¡ç®æºè§è§ï¼CVï¼é¢åä¸­ï¼é«è´¨éçæ æ³¨æ°æ®å¯¹äºè®­ç»é²æ£çæºå¨å­¦ä¹ æ¨¡åè³å³éè¦ï¼å°¤å¶æ¯å¨ç®æ æ£æµãå®åè¾¹çæ¡ä¼°è®¡åå®ä¾åå²ç­å¤æä»»å¡ä¸­ãç¶èï¼ä¼ ç»çæ°æ®æ æ³¨è¿ç¨éå¸¸æ¯å³å¨å¯éåãèæ¶ä¸é¾ä»¥æ©å±çï¼éè¦å¤§éäººå·¥è¾å¥ï¼å¹¶ä¸å®¹æåºç°ä¸»è§æ§éè¯¯åæ°æ®åå·®ãç°æå·¥å·å¨èªå¨ååå¤çå¤æä»»å¡æ¹é¢å­å¨å±éæ§ï¼é¾ä»¥æ»¡è¶³å¤§è§æ¨¡æ°æ®éçéæ±ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®</strong>
VisioFirmå¼å¥äºä¸ä¸ªå¼æºãè·¨å¹³å°çWebåºç¨ç¨åºï¼æ¨å¨éè¿AIè¾å©èªå¨åæ¥ç®åå¾åæ æ³¨è¿ç¨ãå¶æ ¸å¿åæ°åæ¹æ³è®ºè´¡ç®åæ¬ï¼</p>
<ul>
<li><strong>æ··åAIè¾å©æ æ³¨ç®¡éï¼</strong> VisioFirmç»åäºé¢è®­ç»æ£æµæ¨¡åï¼å¦Ultralytics YOLOæ¨¡åï¼ç¨äºå¸¸è§ç±»å«ï¼ä»¥åé¶æ ·æ¬æ¨¡åï¼å¦Grounding DINOï¼ç¨äºèªå®ä¹æ ç­¾ãè¿ç§æ¹æ³ä»¥ä½ç½®ä¿¡åº¦éå¼çæåå§é«å¬åççé¢æ æ³¨ï¼ä»¥æå¤§åæ½å¨å¯¹è±¡çæè·ã</li>
<li><strong>CLIPè¯­ä¹éªè¯ä¸è¿æ»¤ï¼</strong> åå§æ£æµç»æéè¿åºäºCLIPçè¯­ä¹éªè¯åIoUå¾è¿æ¥ç»ä»¶èç±»è¿è¡è¿æ»¤ï¼ä»¥æ¶é¤åä½æ£æµå¹¶ç¡®ä¿æ ç­¾åç¡®æ§ãCLIPç¨äºéªè¯é¢æµæ ç­¾ä¸è£åªå¾ååå®¹çè¯­ä¹ä¸è´æ§ã</li>
<li><strong>é«æçäº¤äºå¼ç²¾ç¼å·¥å·ï¼</strong> ç¨æ·å¯ä»¥éè¿äº¤äºå¼å·¥å·ï¼æ¯æè¾¹çæ¡ãå®åè¾¹çæ¡åå¤è¾¹å½¢ï¼ç²¾ç¼åå§é¢æµã</li>
<li><strong>WebGPUå éçå®æ¶åå²ï¼</strong> VisioFirméæäºWebGPUå éçSegment Anything Model (SAM2)ï¼å®ç°äºæµè§å¨ç«¯çå®æ¶åå²ï¼æé«äºæçã</li>
<li><strong>å¤æ ¼å¼å¯¼åºä¸ç¦»çº¿æä½ï¼</strong> è¯¥å·¥å·æ¯æå¤ç§æ åå¯¼åºæ ¼å¼ï¼YOLOãCOCOãPascal VOCãCSVï¼ï¼å¹¶è½å¨æ¨¡åç¼å­åç¦»çº¿æä½ï¼å¢å¼ºäºå¯è®¿é®æ§ã</li>
<li><strong>ç¨æ·åå¥½çWebçé¢ï¼</strong> æä¾ç´è§ççé¢ï¼æ¯æé¡¹ç®ç®¡çãé®çå¿«æ·é®ãç¼©æ¾/å¹³ç§»æ§å¶ä»¥åå¤ç§æ æ³¨æ¨¡å¼ï¼ç©å½¢ãå¤è¾¹å½¢ãé­æ¯æ£ï¼ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹</strong>
VisioFirmå¨å¤ä¸ªæ°æ®éä¸çåºåæµè¯è¡¨æï¼å®è½å¤å°æå¨æ æ³¨å·¥ä½éåå°é«è¾¾90%ï¼åæ¶ä¿æé«æ æ³¨åç¡®æ§ãå·ä½ç»æåæ¬ï¼</p>
<ul>
<li><strong>æçæåï¼</strong> éè¿GPUå éï¼YOLOv10åGrounding DINOæ¨¡åçæ¨çå»¶è¿æ¾èéä½ãä¾å¦ï¼YOLOv10å¨0%éå¼ä¸ï¼GPUæ¨¡å¼æ¯CPUæ¨¡å¼å¿«2.9åï¼å¨50%éå¼ä¸ï¼GPUæ¨¡å¼å¿«17åãGrounding DINOå¨0%éå¼ä¸ï¼GPUæ¨¡å¼å¿«5.7åï¼å¨50%éå¼ä¸ï¼GPUæ¨¡å¼å¿«4.1åãè¿è¡¨æAIè¾å©èªå¨åæ¾èæé«äºæ æ³¨æçã</li>
<li><strong>åç¡®æ§ä¿æï¼</strong> å°½ç®¡åå°äºäººå·¥å¹²é¢ï¼ä½éè¿CLIPéªè¯åIoUå¾èç±»ç­åå¤çæ­¥éª¤ï¼VisioFirmè½å¤ä¿æé«æ æ³¨åç¡®æ§ã</li>
<li><strong>çµæ´»æ§ï¼</strong> è¯¥å·¥å·è½å¤å¤çCOCOç­å¸¸è§ç±»å«ä»¥åéè¿é¶æ ·æ¬æ¨¡åå¤ççèªå®ä¹æé¢åç¹å®ç±»å«ï¼å±ç°äºå¶å¨å¤æ ·åæ æ³¨ä»»å¡ä¸­ççµæ´»æ§ã</li>
<li><strong>å¯è®¿é®æ§ï¼</strong> ä½ä¸ºå¼æºé¡¹ç®ï¼VisioFirmæäºå®è£åé¨ç½²ï¼æ¯æå¤ç§ç¡¬ä»¶éç½®ï¼CPUæGPUï¼ï¼å¹¶æä¾ç¦»çº¿æä½è½åã</li>
</ul>
<p><strong>4. è®ºæä¸­æåçå±éæ§</strong>
è®ºæä¸­æåäºä¸äºå±éæ§ï¼ä¸»è¦éä¸­å¨AIæ¨¡åçæ³åè½ååæ§è½æ¹é¢ï¼</p>
<ul>
<li><strong>æ¨¡åå¨é¢åç¹å®æå¼å¸¸å¯¹è±¡ä¸çè¡¨ç°ï¼</strong> é¢è®­ç»åé¶æ ·æ¬æ¨¡åå¨éå°è®­ç»æ°æ®éä¸­æªååè¡¨ç¤ºçå¤æå½¢ç¶ãçº¹çæé®æ¡çé¢åç¹å®æå¼å¸¸å¯¹è±¡æ¶ï¼å¯è½ä¼è¡¨ç°ä¸ä½³ï¼äº§çä½ç½®ä¿¡åº¦æéè¯¯æ£æµãVisioFirméè¿å¨æè¯ä¼°æ£æµè´¨éæ¥åºå¯¹æ­¤é®é¢ï¼å¹¶å¨å¿è¦æ¶åæ¢å°AIè¾å©æçº¯æå¨æ¨¡å¼ã</li>
<li><strong>é¶æ ·æ¬æ¨¡åçè®¡ç®ææ¬ï¼</strong> å°½ç®¡Grounding DINOå¨çæä¸å¸¸è§ç±»å«æ ç­¾ææ¡æ¹é¢éå¸¸ææï¼ä½å®å¨å®æ´æ°æ®éä¸çè®¡ç®æ¶é´ç¸å¯¹è¾é¿ï¼å°¤å¶æ¯å¨ä½éå¼ä¸ã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å</strong>
VisioFirmçæªæ¥åå±æ¹åæ¨å¨æ©å±å¶åè½èå´åå¼å®¹æ§ï¼</p>
<ul>
<li><strong>éææ´å¤é«çº§æ¨¡åï¼</strong> è®¡åéæDetectron2ç­æ¡æ¶ï¼ä»¥æ¯ææ´é«çº§çå®ä¾åå²å·¥ä½æµã</li>
<li><strong>æ¯æå¤æ¨¡æä»»å¡ï¼</strong> å°çº³å¥å¾ååç±»åå¾åå­å¹ç­CVç¸å³æ æ³¨ï¼ä»¥æ¯æå¤æ¨¡æä»»å¡ã</li>
<li><strong>è§é¢æ æ³¨æ¯æï¼</strong> æ©å±å·¥å·ä»¥æ¯æè§é¢æ°æ®ï¼åæ¬å¸§æåååºäºè·è¸ªçæ æ³¨ï¼ä»èå¤çæ¶é´åºåæ°æ®ã</li>
<li><strong>ç¤¾åºè´¡ç®ï¼</strong> é¼å±ç¤¾åºéè¿æåè¯·æ±è´¡ç®ï¼ä»¥æ©å±æ¨¡åå¼å®¹æ§ï¼ä¾å¦ï¼éææ°å´æ£æµå¨ï¼åå¢å¼ºè·¨å¹³å°é²æ£æ§ã</li>
</ul>
<p>æ»èè¨ä¹ï¼VisioFirméè¿ç»ååè¿çAIæ¨¡åãæºè½è¿æ»¤ç®¡éåç¨æ·åå¥½çäº¤äºå¼çé¢ï¼ä¸ºè®¡ç®æºè§è§æ°æ®æ æ³¨æä¾äºä¸ä¸ªé«æãåç¡®ä¸å¯æ©å±çè§£å³æ¹æ¡ï¼æ¾èåå°äºæå¨å·¥ä½éï¼å¹¶ä¸ºæªæ¥çç ç©¶ååºç¨å¥ å®äºåºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we introduce VisioFirm, an open-source web
application designed to streamline image labeling through AI-assisted
automation.</li>
<li>VisioFirm integrates state-of-the-art foundation models into an
interface with a filtering pipeline to reduce human-in-the-loop efforts.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.04180v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.04180v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.04150v1'></a></p>
<h2 id="revisiting-simple-baselines-for-in-the-wild-deepfake-detection"><a href="https://arxiv.org/abs/2509.04150v1">Revisiting Simple Baselines for In-The-Wild Deepfake Detection</a></h2>
<p><strong>Authors:</strong> Orlando Castaneda, Kevin So-Tang, Kshitij Gurung</p>
<p><strong>Published:</strong> 2025-09-04</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>The widespread adoption of synthetic media demands accessible deepfake
detectors and realistic benchmarks. While most existing research evaluates
deepfake detectors on highly controlled datasets, we focus on the recently
released "in-the-wild" benchmark, Deepfake-Eval-2024. Initial reporting on
Deepfake-Eval-2024 showed that three finetuned open-source models achieve
accuracies between 61% and 69%, significantly lagging behind the leading
commercial deepfake detector with 82% accuracy. Our work revisits one of these
baseline approaches, originally introduced by Ojha et al., which adapts
standard pretrained vision backbones to produce generalizable deepfake
detectors. We demonstrate that with better-tuned hyperparameters, this simple
approach actually yields much higher performance -- 81% accuracy on
Deepfake-Eval-2024 -- surpassing the previously reported accuracy of this
baseline approach by 18% and competing with commercial deepfake detectors. We
discuss tradeoffs in accuracy, computational costs, and interpretability,
focusing on how practical these deepfake detectors might be when deployed in
real-world settings. Our code can be found at
https://github.com/Deepfake-Detection-KKO/deepfake-detection.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºâRevisiting Simple Baselines for In-The-Wild Deepfake Detectionâçæè¦ï¼éç¹å³æ³¨å¶æ ¸å¿åå®¹åå¯¹è®¡ç®æºè§è§é¢åçå½±åï¼</p>
<p><strong>è®ºææè¦ï¼éæ°å®¡è§éå¤æ·±åº¦ä¼ªé æ£æµçç®ååºçº¿</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³å¨âéå¤âï¼in-the-wildï¼çå®ä¸çåºæ¯ä¸­ï¼æ·±åº¦ä¼ªé æ£æµå¨æ§è½ä¸è¶³çé®é¢ãç°æçç ç©¶å¤§å¤å¨é«åº¦åæ§çæ°æ®éä¸è¯ä¼°æ·±åº¦ä¼ªé æ£æµå¨ï¼å¯¼è´å¶å¨çå®ä¸çä¸­è¡¨ç°ä¸ä½³ãç¹å«æ¯ï¼éå¯¹æè¿åå¸çâDeepfake-Eval-2024âåºåæµè¯ï¼å¼æºæ¨¡åçåç¡®çæ¾èè½åäºé¢åçåä¸æ£æµå¨ï¼61%-69% vs 82%ï¼ï¼è¿è¡¨æå¼æºè§£å³æ¹æ¡å¨å®éåºç¨ä¸­å­å¨ææ¾å·®è·ãè®ºæçæ ¸å¿é®é¢æ¯ï¼éè¿ä¼åç°æç®ååºçº¿æ¹æ³ï¼è½å¦æ¾èæåå¶å¨éå¤æ·±åº¦ä¼ªé æ£æµä¸çæ§è½ï¼ä½¿å¶ä¸åä¸æ£æµå¨ç¸åª²ç¾ï¼</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥è®ºæçæ ¸å¿è´¡ç®å¨äºéæ°å®¡è§å¹¶ä¼åäºOjhaç­äºº[17]æåºçç®ååºçº¿æ¹æ³ï¼å³éè¿å¨æ åé¢è®­ç»è§è§éª¨å¹²ç½ç»ï¼å¦ResNet-50ãViT-b32åConvNeXt-baseï¼ä¹ä¸æ·»å ä¸ä¸ªdropoutå±åä¸ä¸ªçº¿æ§åç±»å¨æ¥æå»ºæ·±åº¦ä¼ªé æ£æµå¨ãå³é®çåæ°åæ¹æ³è®ºæ¹è¿åæ¬ï¼
*   <strong>è¶åæ°ä¼åï¼</strong> è®ºæéè¿å¯¹å­¦ä¹ çè°åº¦ï¼ä½å¼¦éç«ä¼äºæ­¥é¿è¡°åï¼ãé¢è®­ç»ä»»å¡ï¼CLIPé¢è®­ç»ä¼äºImageNetææ é¢è®­ç»ï¼ä»¥åéåº¦çL2æ­£ååådropoutçè¿è¡ç³»ç»æ§ä¼åï¼æ¾èæåäºæ¨¡åæ§è½ã
*   <strong>æ¨¡åæ¶æéæ©ï¼</strong> è¯ä¼°äºResNet-50ãViT-b32åConvNeXt-baseä¸ç§ä¸åçéª¨å¹²ç½ç»ï¼å¹¶åç°CLIPé¢è®­ç»çConvNeXt-baseåViT-b32è¡¨ç°æä½³ï¼è¿è¡¨æè¿äºæ¶æçè¡¨ç¤ºè½ååä»CLIPå­¦ä¹ å°çç¹å¾å¯¹äºæ·±åº¦ä¼ªé æ£æµéå¸¸ææã
*   <strong>æ°æ®å¤çï¼</strong> éç¨äºä¸ç³»åæ°æ®å¤çç¨åºï¼åæ¬å°å¾åç¼©æ¾å°384åç´ ï¼ä»¥åå¨è®­ç»æ¶è¿è¡éæºè£åªåç¼©æ¾ä»¥å¢å¼ºæ°æ®ã
*   <strong>æ§è½æè¡¡åæï¼</strong> è®ºæä¸ä»å³æ³¨åç¡®çï¼è¿æ·±å¥è®¨è®ºäºæ¨¡åçè®¡ç®ææ¬ï¼GFLOPsï¼ãæ¨çéåº¦åå¯è§£éæ§ï¼éè¿GradCAMå¯è§åï¼ï¼ä¸ºå®éé¨ç½²æä¾äºéè¦è§è§£ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æ¾èçæ§è½æåï¼</strong> ç»è¿ä¼åçç®ååºçº¿æ¹æ³å¨Deepfake-Eval-2024æ°æ®éä¸å®ç°äº81%çåç¡®çï¼æ¯ä¹åæ¥åçè¯¥åºçº¿æ¹æ³ï¼63%ï¼æé«äº18%ï¼å¹¶ä¸é¢åçåä¸æ·±åº¦ä¼ªé æ£æµå¨ï¼82%ï¼çæ§è½éå¸¸æ¥è¿ã
*   <strong>å¼æºæ¨¡åçç«äºåï¼</strong> è¿ä¸ç»æè¡¨æï¼éè¿éå½çè¶åæ°è°ä¼åé¢è®­ç»ç­ç¥ï¼å¼æºæ¨¡åä¹è½å¨éå¤æ·±åº¦ä¼ªé æ£æµä»»å¡ä¸­è¾¾å°ä¸åä¸è§£å³æ¹æ¡ç¸å½çæ°´å¹³ï¼ä»èéä½äºå¯¹ä¸æææ¯çä¾èµã
*   <strong>é¢è®­ç»çéè¦æ§ï¼</strong> CLIPé¢è®­ç»è¢«è¯æå¯¹æ§è½æåè³å³éè¦ï¼å°¤å¶æ¯å¨å¤çç¤¾äº¤åªä½æ¥æºçâéå¤âå¾åæ¶ï¼å¶ææä¼äºImageNeté¢è®­ç»ã
*   <strong>æ¨¡åæè¡¡ï¼</strong> ConvNeXt-baseåViT-b32å¨åç¡®çä¸è¡¨ç°æä½³ï¼ä½ViT-b32å¨æ¨çéåº¦ä¸æ´å¿«ï¼åæ°éæ´å°ï¼ä½¿å¶å¨æ¶é´æææé«ååéåºæ¯ä¸­æ´å·ä¼å¿ãResNet-50è½ç¶åç¡®çç¥ä½ï¼ä½æ¨¡åå°ºå¯¸æ´å°ï¼éç¨äºèµæºåéçè®¾å¤ã
*   <strong>å¯è§£éæ§ï¼</strong> GradCAMå¯è§åæ­ç¤ºäºä¸åæ¨¡åå³æ³¨å¾åçä¸ååºåï¼ViT-b32å¾åäºå³æ³¨æ´å¤§ãæ´åæ£çåºåï¼èConvNeXt-baseåResNet-50åæ´éä¸­äºè¾å°çåºåï¼è¿æå©äºçè§£æ¨¡åå³ç­æºå¶ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>æ°æ®éè§æ¨¡ååå¸ï¼</strong> Deepfake-Eval-2024æ°æ®éçè§æ¨¡ç¸å¯¹è¾å°ï¼ä¸å¶åå¸å¯è½æ æ³å®å¨ä»£è¡¨ææçå®ä¸çåºæ¯ä¸­çæ·±åº¦ä¼ªé ã
*   <strong>æ³åè½åï¼</strong> å°½ç®¡è¯¥åºåæµè¯çç»æå¼å¾å³æ³¨ï¼ä½å¶æ¶éæ¸ éåæ æ³¨æ¹æ³å¯è½æå³çè¿äºæ£æµå¨å¨ææçå®ä¸çè®¾ç½®ä¸­çè¡¨ç°ä¸ä¸å®è½å®å¨ä»£è¡¨ã
*   <strong>ææ¯å¿«éåå±ï¼</strong> æ·±åº¦ä¼ªé çæåæ£æµææ¯åå±è¿éï¼éè¦æç»­æå¥èµæºå¼åæ°çãæ´çå®çåºååæ¨¡åã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ´å¤§è§æ¨¡ãæ´å¤æ ·åçéå¤æ°æ®éï¼</strong> å¼åæ´å¤§ãæ´å·ä»£è¡¨æ§çæ°æ®éï¼ä»¥æ´å¥½å°åæ çå®ä¸ççæ·±åº¦ä¼ªé ææã
*   <strong>å¤æ¨¡ææ·±åº¦ä¼ªé æ£æµï¼</strong> æ¢ç´¢ç»åè§è§ãé³é¢åææ¬ä¿¡æ¯çå¤æ¨¡ææ£æµæ¹æ³ï¼ä»¥åºå¯¹æ´å¤æçæ·±åº¦ä¼ªé å½¢å¼ã
*   <strong>æ¨¡åé²æ£æ§ï¼</strong> è¿ä¸æ­¥ç ç©¶å¦ä½æé«æ·±åº¦ä¼ªé æ£æµå¨å¯¹æ°åçæææ¯åå¯¹ææ§æ»å»çé²æ£æ§ã
*   <strong>è½»éçº§åé«ææ¨¡åï¼</strong> éå¯¹èµæºåéè®¾å¤åå®æ¶åºç¨ï¼ç»§ç»­å¼åæ´è½»éçº§ãæ´é«æçæ·±åº¦ä¼ªé æ£æµæ¨¡åã
*   <strong>å¯è§£éæ§ä¸ä¿¡ä»»ï¼</strong> æ·±å¥ç ç©¶æ¨¡åçå¯è§£éæ§ï¼ä»¥å¢å¼ºç¨æ·å¯¹æ·±åº¦ä¼ªé æ£æµå¨çä¿¡ä»»ï¼å¹¶å¸®å©åå®¹å®¡æ ¸åååæå¸æ´å¥½å°çè§£æ¨¡åå³ç­ã</p>
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæéè¿å¯¹ç®ååºçº¿æ¹æ³çæ·±å¥ä¼åï¼ä¸ºéå¤æ·±åº¦ä¼ªé æ£æµé¢åè®¾å®äºæ°çæ åï¼è¯æäºå¼æºè§£å³æ¹æ¡å¨çå®ä¸çåºç¨ä¸­çå·¨å¤§æ½åï¼å¹¶ä¸ºæªæ¥çç ç©¶æä¾äºå®è´µçè§è§£ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We demonstrate that with better-tuned hyperparameters, this simple
approach actually yields much higher performance -- 81% accuracy on
Deepfake-Eval-2024 -- surpassing the previously reported accuracy of this
baseline approach by 18% and competing with commercial deepfake detectors.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.04150v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.04150v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-09-07 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
