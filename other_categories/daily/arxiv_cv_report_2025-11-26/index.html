<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-11-26 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-22
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-11-25/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-11-27/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-11-26">Arxiv Computer Vision Papers - 2025-11-26</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#rubricrl-simple-generalizable-rewards-for-text-to-image-generation" class="nav-link">RubricRL: Simple Generalizable Rewards for Text-to-Image Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#infinity-rope-action-controllable-infinite-video-generation-emerges-from-autoregressive-self-rollout" class="nav-link">Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout</a>
                </li>
                <li class="nav-item">
                    <a href="#diverse-video-generation-with-determinantal-point-process-guided-policy-optimization" class="nav-link">Diverse Video Generation with Determinantal Point Process-Guided Policy Optimization</a>
                </li>
                <li class="nav-item">
                    <a href="#locateanything3d-vision-language-3d-detection-with-chain-of-sight" class="nav-link">LocateAnything3D: Vision-Language 3D Detection with Chain-of-Sight</a>
                </li>
                <li class="nav-item">
                    <a href="#pixeldit-pixel-diffusion-transformers-for-image-generation" class="nav-link">PixelDiT: Pixel Diffusion Transformers for Image Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#vision-language-memory-for-spatial-reasoning" class="nav-link">Vision-Language Memory for Spatial Reasoning</a>
                </li>
                <li class="nav-item">
                    <a href="#concept-aware-batch-sampling-improves-language-image-pretraining" class="nav-link">Concept-Aware Batch Sampling Improves Language-Image Pretraining</a>
                </li>
                <li class="nav-item">
                    <a href="#unleashing-the-power-of-vision-language-models-for-long-tailed-multi-label-visual-recognition" class="nav-link">Unleashing the Power of Vision-Language Models for Long-Tailed Multi-Label Visual Recognition</a>
                </li>
                <li class="nav-item">
                    <a href="#motionv2v-editing-motion-in-a-video" class="nav-link">MotionV2V: Editing Motion in a Video</a>
                </li>
                <li class="nav-item">
                    <a href="#imontage-unified-versatile-highly-dynamic-many-to-many-image-generation" class="nav-link">iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-11-26">Arxiv Computer Vision Papers - 2025-11-26</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，作为一名专注于计算机视觉和机器学习的研究助理，我将为您提供一份关于2025年11月25日 Arxiv 计算机视觉领域论文的简明执行摘要。</p>
<hr />
<p><strong>执行摘要：2025年11月25日 Arxiv 计算机视觉论文速览</strong></p>
<p><strong>日期：</strong> 2025年11月25日</p>
<p><strong>主要主题与趋势：</strong></p>
<p>本期 Arxiv 论文集中体现了计算机视觉领域在<strong>生成模型（尤其是文本到图像和视频生成）</strong>、<strong>多模态理解（视觉-语言模型）</strong>以及<strong>三维视觉</strong>方面的持续进步。生成模型正朝着更精细的控制、更高的多样性和更强的泛化能力发展。多模态模型在空间推理和长尾识别等复杂任务上展现出更强的潜力。三维视觉领域则在利用语言指令进行三维检测方面取得了突破。</p>
<p><strong>亮点与创新：</strong></p>
<ul>
<li>
<p><strong>生成模型控制与泛化：</strong></p>
<ul>
<li><strong>RubricRL</strong> 提出了一种简单且可泛化的奖励机制，用于文本到图像生成，有望提升生成质量和可控性。</li>
<li><strong>Infinity-RoPE</strong> 在视频生成领域引入了“自回归自回滚”机制，实现了动作可控的无限视频生成，这是视频生成领域的一大进步。</li>
<li><strong>PixelDiT</strong> 提出了基于像素的扩散 Transformer 模型，为图像生成提供了新的架构思路。</li>
<li><strong>iMontage</strong> 则致力于统一、通用且高度动态的“多对多”图像生成，预示着更灵活的图像合成能力。</li>
</ul>
</li>
<li>
<p><strong>多模态理解与应用：</strong></p>
<ul>
<li><strong>LocateAnything3D</strong> 实现了基于视觉-语言的 3D 检测，并引入了“视线链”（Chain-of-Sight）的概念，为理解和操作三维场景提供了新的方法。</li>
<li><strong>Vision-Language Memory for Spatial Reasoning</strong> 探索了利用视觉-语言模型进行空间推理，这对于构建更具理解力的 AI 系统至关重要。</li>
<li><strong>Unleashing the Power of Vision-Language Models for Long-Tailed Multi-Label Visual Recognition</strong> 展示了视觉-语言模型在处理长尾多标签识别等具有挑战性的任务上的强大能力。</li>
<li><strong>Concept-Aware Batch Sampling</strong> 提出了一种概念感知批次采样方法，以改进语言-图像预训练的效果。</li>
</ul>
</li>
<li>
<p><strong>视频编辑与生成：</strong></p>
<ul>
<li><strong>MotionV2V</strong> 专注于视频中的运动编辑，为视频内容创作和修改提供了新的工具。</li>
<li><strong>Diverse Video Generation with Determinantal Point Process-Guided Policy Optimization</strong> 则通过引导策略优化来提升视频生成的多样性。</li>
</ul>
</li>
</ul>
<p><strong>新兴研究方向与技术：</strong></p>
<ul>
<li><strong>更精细的生成控制：</strong> 从文本到图像到视频，对生成内容（如动作、风格、细节）的精细控制是核心趋势。</li>
<li><strong>三维视觉与语言的融合：</strong> 将语言理解能力应用于三维场景的检测、定位和推理，是未来三维视觉的重要发展方向。</li>
<li><strong>长尾数据处理：</strong> 利用强大的预训练模型（如视觉-语言模型）来解决现实世界中普遍存在的长尾数据问题。</li>
<li><strong>视频内容生成与编辑的自动化：</strong> 进一步提升视频生成的多样性和可控性，并开发更智能的视频编辑工具。</li>
<li><strong>Transformer 架构在生成模型中的应用：</strong> PixelDiT 的出现表明 Transformer 架构在像素级图像生成方面仍有巨大潜力。</li>
</ul>
<p><strong>建议阅读全文的论文：</strong></p>
<p>考虑到其潜在的广泛影响和技术创新性，以下论文值得深入阅读：</p>
<ol>
<li><strong>RubricRL: Simple Generalizable Rewards for Text-to-Image Generation</strong> (对文本到图像生成的可控性和泛化能力有重要贡献)</li>
<li><strong>Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout</strong> (在视频生成领域具有开创性，解决了动作控制和无限生成的问题)</li>
<li><strong>LocateAnything3D: Vision-Language 3D Detection with Chain-of-Sight</strong> (将语言理解与三维检测结合，是三维视觉领域的重要进展)</li>
<li><strong>PixelDiT: Pixel Diffusion Transformers for Image Generation</strong> (为图像生成提供了新的 Transformer 架构思路，可能带来性能提升)</li>
</ol>
<hr />
<p>希望这份执行摘要能帮助您快速了解近期 Arxiv 计算机视觉领域的最新动态。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2511.20651v1">RubricRL: Simple Generalizable Rewards for Text-to-Image Generation</a></li>
<li><a href="#2511.20649v1">Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout</a></li>
<li><a href="#2511.20647v1">Diverse Video Generation with Determinantal Point Process-Guided Policy Optimization</a></li>
<li><a href="#2511.20648v1">LocateAnything3D: Vision-Language 3D Detection with Chain-of-Sight</a></li>
<li><a href="#2511.20645v1">PixelDiT: Pixel Diffusion Transformers for Image Generation</a></li>
<li><a href="#2511.20644v1">Vision-Language Memory for Spatial Reasoning</a></li>
<li><a href="#2511.20643v1">Concept-Aware Batch Sampling Improves Language-Image Pretraining</a></li>
<li><a href="#2511.20641v1">Unleashing the Power of Vision-Language Models for Long-Tailed Multi-Label Visual Recognition</a></li>
<li><a href="#2511.20640v1">MotionV2V: Editing Motion in a Video</a></li>
<li><a href="#2511.20635v1">iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2511.20651v1'></a></p>
<h2 id="rubricrl-simple-generalizable-rewards-for-text-to-image-generation"><a href="https://arxiv.org/abs/2511.20651v1">RubricRL: Simple Generalizable Rewards for Text-to-Image Generation</a></h2>
<p><strong>Authors:</strong> Xuelu Feng, Yunsheng Li, Ziyu Wan, Zixuan Gao, Junsong Yuan, Dongdong Chen, Chunming Qiao</p>
<p><strong>Published:</strong> 2025-11-25</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Reinforcement learning (RL) has recently emerged as a promising approach for aligning text-to-image generative models with human preferences. A key challenge, however, lies in designing effective and interpretable rewards. Existing methods often rely on either composite metrics (e.g., CLIP, OCR, and realism scores) with fixed weights or a single scalar reward distilled from human preference models, which can limit interpretability and flexibility. We propose RubricRL, a simple and general framework for rubric-based reward design that offers greater interpretability, composability, and user control. Instead of using a black-box scalar signal, RubricRL dynamically constructs a structured rubric for each prompt--a decomposable checklist of fine-grained visual criteria such as object correctness, attribute accuracy, OCR fidelity, and realism--tailored to the input text. Each criterion is independently evaluated by a multimodal judge (e.g., o4-mini), and a prompt-adaptive weighting mechanism emphasizes the most relevant dimensions. This design not only produces interpretable and modular supervision signals for policy optimization (e.g., GRPO or PPO), but also enables users to directly adjust which aspects to reward or penalize. Experiments with an autoregressive text-to-image model demonstrate that RubricRL improves prompt faithfulness, visual detail, and generalizability, while offering a flexible and extensible foundation for interpretable RL alignment across text-to-image architectures.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“RubricRL: Simple Generalizable Rewards for Text-to-Image Generation”的全面中文摘要：</p>
<p><strong>论文题目：</strong> RubricRL: Simple Generalizable Rewards for Text-to-Image Generation (RubricRL：用于文本到图像生成的简单可泛化奖励)</p>
<p><strong>作者：</strong> Xuelu Feng, Yunsheng Li, Ziyu Wan, Zixuan Gao, Junsong Yuan, Dongdong Chen, Chunming Qiao</p>
<hr />
<p><strong>摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>文本到图像（Text-to-Image, T2I）生成模型在生成逼真且与文本描述高度一致的图像方面取得了显著进展。然而，如何使这些模型生成符合人类偏好的图像是一个关键挑战。现有的方法主要有两种：一种是<strong>复合奖励系统</strong>，将CLIP分数、OCR准确率、真实感等多个指标进行加权组合，但这种方法需要精细的权重调整，且可能难以解释；另一种是<strong>统一标量奖励模型</strong>，通过学习人类偏好来预测一个单一的奖励分数，这种方法简化了优化过程，但牺牲了可解释性和灵活性，用户难以控制模型关注的特定方面。因此，设计一种<strong>有效、可解释且灵活的奖励机制</strong>是当前研究的重点。</p>
<p><strong>2. 关键创新/方法贡献：</strong></p>
<p>本文提出了<strong>RubricRL</strong>，一个简单且通用的<strong>基于规则（rubric）的奖励设计框架</strong>，旨在解决上述挑战。其核心创新点在于：</p>
<ul>
<li><strong>动态结构化规则（Rubric）生成：</strong> RubricRL不依赖于固定的奖励函数，而是为每个文本提示（prompt）动态地构建一个<strong>结构化的、可分解的规则清单</strong>。这些规则是细粒度的视觉标准，例如<strong>对象正确性、属性准确性、OCR保真度、空间关系、美学质量和一致性</strong>等，并且这些规则是<strong>提示自适应（prompt-adaptive）</strong>的，能够根据输入文本的语义和粒度进行调整。</li>
<li><strong>多模态评判（Multimodal Judge）：</strong> 每个规则标准由一个强大的<strong>多模态语言模型（VLM）</strong>（例如GPT-4-mini）独立评估。这使得评估过程更加灵活和准确。</li>
<li><strong>可解释、可组合和用户可控：</strong> RubricRL将奖励设计从一个“黑箱”过程转变为一个<strong>可审计的过程</strong>。用户可以直观地理解每个奖励项的含义，并<strong>直接调整</strong>哪些方面应该被奖励或惩罚，从而实现<strong>可控的奖励塑造（reward shaping）</strong>。这种模块化的设计也使得奖励信号更容易集成到现有的策略优化框架（如GRPO或PPO）中。</li>
<li><strong>提示自适应加权：</strong> 模型会根据提示的重要性动态调整不同规则的权重，从而<strong>强调最相关的维度</strong>。</li>
</ul>
<p><strong>3. 主要结果及意义：</strong></p>
<p>通过在自回归（AR）文本到图像模型上的实验，RubricRL取得了显著的成果：</p>
<ul>
<li><strong>提升图像质量和提示保真度：</strong> RubricRL显著提高了生成图像的<strong>提示保真度（prompt faithfulness）</strong>、<strong>视觉细节（visual detail）</strong>和<strong>整体质量</strong>。</li>
<li><strong>增强泛化能力：</strong> RubricRL在不同数据集和模型架构上都表现出良好的<strong>泛化能力</strong>。</li>
<li><strong>可解释性和可控性：</strong> 与现有方法相比，RubricRL提供了<strong>更高的可解释性</strong>，用户可以清晰地了解模型为何会生成特定图像，并能<strong>灵活地控制</strong>生成过程。</li>
<li><strong>简化奖励设计：</strong> 避免了手动调整复杂权重或依赖昂贵的人类偏好数据，大大<strong>简化了奖励设计过程</strong>。</li>
<li><strong>为可解释的RL对齐奠定基础：</strong> RubricRL为文本到图像生成模型的可解释的强化学习对齐提供了一个<strong>灵活且可扩展的框架</strong>。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong></p>
<ul>
<li><strong>对评判模型的依赖：</strong> RubricRL的性能在很大程度上<strong>依赖于多模态评判模型（VLM）的质量</strong>。如果评判模型存在偏差或错误（例如，在计数任务中可能出现误判），则可能导致奖励信号不准确，进而影响模型训练的稳定性和最终性能。论文中提到，在某些计数场景下，GPT-4-mini的评分可能受到基础模型生成图像质量的影响。</li>
<li><strong>计算成本：</strong> 虽然RubricRL简化了奖励设计，但动态生成规则和使用大型VLM进行评估可能带来一定的<strong>计算开销</strong>。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>更鲁棒的评判模型：</strong> 研究如何开发更鲁棒、更准确、对生成图像质量不敏感的评判模型，以进一步提高RubricRL的稳定性。</li>
<li><strong>更精细的规则粒度控制：</strong> 探索更精细的规则粒度控制机制，允许用户定义更具体、更个性化的评估标准。</li>
<li><strong>跨领域和跨模态的泛化：</strong> 将RubricRL框架扩展到其他视觉生成任务（如图像编辑、视频生成）或多模态生成任务中。</li>
<li><strong>自动化规则优化：</strong> 研究如何自动化规则的生成和优化过程，使其更加高效和智能。</li>
<li><strong>与用户交互的深度融合：</strong> 进一步探索RubricRL在人机协作生成场景中的应用，例如允许用户在生成过程中实时调整规则以引导模型。</li>
</ul>
<hr />
<p>总而言之，RubricRL通过引入动态、可解释的基于规则的奖励机制，有效地解决了文本到图像生成模型在对齐人类偏好方面的挑战，为构建更可控、更透明的生成模型提供了新的思路和方法。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose RubricRL, a simple and general framework for rubric-based reward design that offers greater interpretability, composability, and user control.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.20651v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.20651v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.20649v1'></a></p>
<h2 id="infinity-rope-action-controllable-infinite-video-generation-emerges-from-autoregressive-self-rollout"><a href="https://arxiv.org/abs/2511.20649v1">Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout</a></h2>
<p><strong>Authors:</strong> Hidir Yesiltepe, Tuna Han Salih Meral, Adil Kaan Akan, Kaan Oktay, Pinar Yanardag</p>
<p><strong>Published:</strong> 2025-11-25</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Current autoregressive video diffusion models are constrained by three core bottlenecks: (i) the finite temporal horizon imposed by the base model's 3D Rotary Positional Embedding (3D-RoPE), (ii) slow prompt responsiveness in maintaining fine-grained action control during long-form rollouts, and (iii) the inability to realize discontinuous cinematic transitions within a single generation stream. We introduce <script type="math/tex">\infty</script>-RoPE, a unified inference-time framework that addresses all three limitations through three interconnected components: Block-Relativistic RoPE, KV Flush, and RoPE Cut. Block-Relativistic RoPE reformulates temporal encoding as a moving local reference frame, where each newly generated latent block is rotated relative to the base model's maximum frame horizon while earlier blocks are rotated backward to preserve relative temporal geometry. This relativistic formulation eliminates fixed temporal positions, enabling continuous video generation far beyond the base positional limits. To obtain fine-grained action control without re-encoding, KV Flush renews the KV cache by retaining only two latent frames, the global sink and the last generated latent frame, thereby ensuring immediate prompt responsiveness. Finally, RoPE Cut introduces controlled discontinuities in temporal RoPE coordinates, enabling multi-cut scene transitions within a single continuous rollout. Together, these components establish <script type="math/tex">\infty</script>-RoPE as a training-free foundation for infinite-horizon, controllable, and cinematic video diffusion. Comprehensive experiments show that <script type="math/tex">\infty</script>-RoPE consistently surpasses previous autoregressive models in overall VBench scores.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout</p>
<p><strong>作者：</strong> Hidir Yesiltepe, Tuna Han Salih Meral, Adil Kaan Akan, Kaan Oktay, Pinar Yanardag</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
当前自回归视频扩散模型在生成长视频时面临三大瓶颈：(i) 受限于基础模型的3D旋转位置嵌入（3D-RoPE）的有限时间跨度；(ii) 在长视频生成过程中，维持精细动作控制的提示响应速度慢；(iii) 无法在单一生成流中实现不连续的电影化场景切换。</p>
<p><strong>2. 关键创新/方法贡献：</strong>
为了解决上述问题，论文提出了一种名为 <strong><script type="math/tex">\infty</script>-RoPE</strong> 的统一推理时框架，它包含三个相互关联的组件：</p>
<ul>
<li><strong>块相对位置编码（Block-Relativistic RoPE）：</strong> 论文重新定义了时间编码，将其视为一个移动的局部参考系。新生成的潜在块会相对于基础模型的最大帧跨度进行旋转，而较早的块则向后旋转以保持相对时间几何。这种相对主义的表述消除了固定的时间位置，使得视频生成能够远远超出基础模型的固定位置限制，实现无限时间跨度的生成。</li>
<li><strong>KV Flush：</strong> 为了在不重新编码的情况下实现精细的动作控制，KV Flush通过仅保留两个潜在帧（全局汇聚点和最后一个生成的潜在帧）来更新KV缓存。这确保了对新提示的即时响应，从而实现快速的动作控制。</li>
<li><strong>RoPE Cut：</strong> RoPE Cut在时间RoPE坐标中引入了受控的不连续性，从而在单一连续的生成过程中实现多镜头场景切换。这使得模型能够生成具有电影化剪辑效果的视频。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
<script type="math/tex">\infty</script>-RoPE 是一个无需训练的框架，能够将现有的短时间跨度自回归自回归视频扩散模型转化为无限时间跨度、可控且电影化的视频生成器。
论文通过广泛的实验证明，<script type="math/tex">\infty</script>-RoPE 在 VBench 分数上始终优于之前最先进的自回归模型。具体而言：
*   在长视频生成方面，<script type="math/tex">\infty</script>-RoPE 在主体一致性、背景一致性和动态度方面表现出色，能够生成长达240秒的稳定、高质量视频。
*   在精细动作控制方面，<script type="math/tex">\infty</script>-RoPE 能够实现即时响应，并保持主体身份和背景的稳定性，优于其他方法。
*   在电影化场景切换方面，RoPE Cut 能够实现平滑的场景过渡，同时保持主体身份和风格的一致性。</p>
<p><strong>4. 提及的局限性：</strong>
论文指出，作为一个无需训练的框架，<script type="math/tex">\infty</script>-RoPE 直接继承了其底层基础模型的局限性，例如可能存在的物理不准确性或偶尔的纹理闪烁。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
论文认为，这项工作为用户可控的长视频生成提供了一个实用的步骤，并为未来研究可扩展、时间鲁棒且面向电影制作的生成式视频模型奠定了基础。未来的研究可以进一步探索如何克服基础模型的固有局限性，以及如何更精细地控制视频的艺术风格和叙事结构。</p>
<p><strong>总结：</strong>
Infinity-RoPE 论文提出了一种创新的、无需训练的框架，有效解决了现有自回归视频扩散模型在长视频生成中的核心痛点。通过引入块相对位置编码、KV Flush 和 RoPE Cut 三个关键组件，该方法实现了无限时间跨度、精细动作控制和电影化场景切换的能力。实验结果表明，<script type="math/tex">\infty</script>-RoPE 在多项指标上均取得了显著的领先优势，为生成高质量、长时序、可控的视频内容开辟了新的可能性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce <script type="math/tex">\infty</script>-RoPE, a unified inference-time framework that addresses all three limitations through three interconnected components: Block-Relativistic RoPE, KV Flush, and RoPE Cut.</li>
<li>Block-Relativistic RoPE reformulates temporal encoding as a moving local reference frame, where each newly generated latent block is rotated relative to the base model's maximum frame horizon while earlier blocks are rotated backward to preserve relative temporal geometry.</li>
<li>To obtain fine-grained action control without re-encoding, KV Flush renews the KV cache by retaining only two latent frames, the global sink and the last generated latent frame, thereby ensuring immediate prompt responsiveness.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.20649v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.20649v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.20647v1'></a></p>
<h2 id="diverse-video-generation-with-determinantal-point-process-guided-policy-optimization"><a href="https://arxiv.org/abs/2511.20647v1">Diverse Video Generation with Determinantal Point Process-Guided Policy Optimization</a></h2>
<p><strong>Authors:</strong> Tahira Kazimi, Connor Dunlop, Pinar Yanardag</p>
<p><strong>Published:</strong> 2025-11-25</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>While recent text-to-video (T2V) diffusion models have achieved impressive quality and prompt alignment, they often produce low-diversity outputs when sampling multiple videos from a single text prompt. We tackle this challenge by formulating it as a set-level policy optimization problem, with the goal of training a policy that can cover the diverse range of plausible outcomes for a given prompt. To address this, we introduce DPP-GRPO, a novel framework for diverse video generation that combines Determinantal Point Processes (DPPs) and Group Relative Policy Optimization (GRPO) theories to enforce explicit reward on diverse generations. Our objective turns diversity into an explicit signal by imposing diminishing returns on redundant samples (via DPP) while supplies groupwise feedback over candidate sets (via GRPO). Our framework is plug-and-play and model-agnostic, and encourages diverse generations across visual appearance, camera motions, and scene structure without sacrificing prompt fidelity or perceptual quality. We implement our method on WAN and CogVideoX, and show that our method consistently improves video diversity on state-of-the-art benchmarks such as VBench, VideoScore, and human preference studies. Moreover, we release our code and a new benchmark dataset of 30,000 diverse prompts to support future research.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Diverse Video Generation with Determinantal Point Process-Guided Policy Optimization”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Diverse Video Generation with Determinantal Point Process-Guided Policy Optimization (基于行列式点过程引导的策略优化的多样化视频生成)</p>
<p><strong>作者：</strong> Tahira Kazimi, Connor Dunlop, Pinar Yanardag</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
尽管近期的文本到视频（T2V）扩散模型在生成视频的质量和与文本提示的对齐方面取得了显著进展，但它们在从单个文本提示生成多个视频时，往往输出结果的多样性较低。这导致用户需要花费大量时间和精力进行提示工程或多次尝试才能获得足够多样化的结果。本文旨在解决这一核心问题，即如何生成一组既忠实于文本提示，又在视觉外观、摄像机运动和场景结构等方面具有高度多样性的视频。</p>
<p><strong>2. 主要创新点/方法贡献：</strong>
作者提出了一个名为 <strong>DPP-GRPO</strong> 的新颖框架，用于解决多样化视频生成的问题。其核心创新在于：</p>
<ul>
<li><strong>将多样化视频生成建模为集合级策略优化问题：</strong> 目标是训练一个策略，使其能够覆盖给定提示下所有可能的多样化结果。</li>
<li><strong>结合行列式点过程（DPP）和组相对策略优化（GRPO）理论：</strong><ul>
<li><strong>DPP（Determinantal Point Processes）：</strong> 用于引入“递减回报”机制，对冗余的样本进行惩罚，从而鼓励生成具有内在多样性的视频集合。DPP通过计算集合的行列式来衡量其多样性，行列式越大，集合越多样。</li>
<li><strong>GRPO（Group Relative Policy Optimization）：</strong> 提供基于组的反馈，鼓励策略生成能够跨越用户输入中语义多样化维度的候选集。GRPO通过对一组候选视频的奖励进行归一化来计算优势，从而指导策略优化。</li>
</ul>
</li>
<li><strong>模型无关且即插即用：</strong> DPP-GRPO框架不依赖于特定的T2V模型架构，可以应用于现有的开源模型（如WAN、CogVideoX）或黑盒API（如Veo），无需修改模型本身。</li>
<li><strong>双重奖励机制：</strong> 结合了多样性奖励（Δdiv）和相关性奖励（Rrel）。多样性奖励通过DPP衡量新样本对现有集合的贡献，而相关性奖励则确保生成的视频与原始提示以及参考集中的样本保持语义一致性。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>显著提升视频多样性：</strong> 在WAN和CogVideoX等模型上，DPP-GRPO在VBench、VideoScore等基准测试以及人类偏好研究中，一致地提高了生成视频集合的多样性，涵盖了视觉外观、摄像机运动和场景结构等多个维度。
*   <strong>保持提示保真度和感知质量：</strong> 实验表明，DPP-GRPO在提升多样性的同时，并未牺牲视频的提示保真度或感知质量，甚至在某些指标上有所提升。
*   <strong>模型无关性验证：</strong> 在不同T2V模型上的实验证明了该框架的通用性和适应性。
*   <strong>发布新数据集和代码：</strong> 作者发布了一个包含30,000个多样化提示-变体对的新基准数据集，以及代码，以支持该领域未来的研究。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>继承基础模型限制：</strong> 论文指出，DPP-GRPO的性能在一定程度上受限于基础T2V模型的时序动态能力。如果基础模型在处理复杂动作（如“剥苹果”）的精细运动方面存在困难，DPP-GRPO也可能难以克服这些限制。
*   <strong>计算效率：</strong> 虽然作者声称该方法实现了最小的计算开销，但与基础模型相比，仍有一定程度的额外计算成本。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>进一步提升时序动态的生成质量：</strong> 探索如何更好地控制和生成复杂、精细的时序动态，以克服基础模型的局限性。
*   <strong>更精细化的多样性控制：</strong> 研究如何更细粒度地控制生成视频在不同维度（如风格、情感、叙事等）上的多样性。
*   <strong>更广泛的模型集成：</strong> 探索将DPP-GRPO应用于更多类型的生成模型，如图像生成、3D内容生成等。
*   <strong>用户反馈的整合：</strong> 进一步研究如何更有效地整合用户反馈来指导多样化生成过程。</p>
<p><strong>总结：</strong>
DPP-GRPO是文本到视频生成领域的一项重要贡献，它通过创新的方法将多样性作为核心优化目标，有效解决了现有T2V模型输出多样性不足的问题。该框架的通用性、即插即用性和在多个维度上提升多样性的能力，使其成为未来研究和应用的重要基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we introduce DPP-GRPO, a novel framework for diverse video generation that combines Determinantal Point Processes (DPPs) and Group Relative Policy Optimization (GRPO) theories to enforce explicit reward on diverse generations.</li>
<li>We implement our method on WAN and CogVideoX, and show that our method consistently improves video diversity on state-of-the-art benchmarks such as VBench, VideoScore, and human preference studies.</li>
<li>Moreover, we release our code and a new benchmark dataset of 30,000 diverse prompts to support future research.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.20647v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.20647v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.20648v1'></a></p>
<h2 id="locateanything3d-vision-language-3d-detection-with-chain-of-sight"><a href="https://arxiv.org/abs/2511.20648v1">LocateAnything3D: Vision-Language 3D Detection with Chain-of-Sight</a></h2>
<p><strong>Authors:</strong> Yunze Man, Shihao Wang, Guowen Zhang, Johan Bjorck, Zhiqi Li, Liang-Yan Gui, Jim Fan, Jan Kautz, Yu-Xiong Wang, Zhiding Yu</p>
<p><strong>Published:</strong> 2025-11-25</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>To act in the world, a model must name what it sees and know where it is in 3D. Today's vision-language models (VLMs) excel at open-ended 2D description and grounding, yet multi-object 3D detection remains largely missing from the VLM toolbox. We present LocateAnything3D, a VLM-native recipe that casts 3D detection as a next-token prediction problem. The key is a short, explicit Chain-of-Sight (CoS) sequence that mirrors how human reason from images: find an object in 2D, then infer its distance, size, and pose. The decoder first emits 2D detections as a visual chain-of-thought, then predicts 3D boxes under an easy-to-hard curriculum: across objects, a near-to-far order reduces early ambiguity and matches ego-centric utility; within each object, a center-from-camera, dimensions, and rotation factorization ranks information by stability and learnability. This VLM-native interface preserves open-vocabulary and visual-prompting capability without specialized heads. On the challenging Omni3D benchmark, our model achieves state-of-the-art results, with 49.89 AP_3D, surpassing the previous best by +15.51 absolute improvement even when the baseline is given ground-truth 2D boxes. It also generalizes zero-shot to held-out categories with strong robustness. By turning 3D detection into a disciplined next-token problem, LocateAnything3D offers a practical foundation for models to perceive in 3D.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“LocateAnything3D: Vision-Language 3D Detection with Chain-of-Sight”的全面中文摘要：</p>
<p><strong>论文题目：</strong> LocateAnything3D: Vision-Language 3D Detection with Chain-of-Sight</p>
<p><strong>作者：</strong> Yunze Man, Shihao Wang, Guowen Zhang, Johan Bjorck, Zhiqi Li, Liang-Yan Gui, Jim Fan, Jan Kautz, Yu-Xiong Wang, Zhiding Yu</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题/研究问题：</strong>
该论文旨在解决当前视觉语言模型（VLMs）在开放式2D描述和定位方面表现出色，但在多目标3D检测方面仍存在显著不足的问题。现有的单目3D检测器通常依赖于特定任务的头部、封闭的标签空间和精细校准的相机，缺乏VLMs的通用性、组合性和指令遵循能力。因此，研究的核心问题是：<strong>如何设计一种最符合VLM范式的、能够直接从单目图像生成可靠的多目标3D边界框的方法？</strong></p>
<p><strong>2. 关键创新/方法贡献：</strong>
LocateAnything3D的核心创新在于将3D检测转化为一个<strong>VLM原生的“下一个词元预测”问题</strong>，并引入了<strong>“视线链”（Chain-of-Sight, CoS）</strong>的解码和监督方案。其主要贡献包括：</p>
<ul>
<li><strong>视线链（CoS）公式化：</strong> 将3D检测分解为一个短的、显式的词元序列，模仿人类从图像推理的方式：首先在2D中定位对象，然后推断其距离、尺寸和姿态。解码器首先输出2D检测框作为“视觉思维链”，然后预测对应的3D边界框。这种2D到3D的顺序预测，利用了2D定位的高置信度来约束3D推理，减少了幻觉，并与自回归解码器自然契合。</li>
<li><strong>分层课程学习（Curriculum Learning）：</strong><ul>
<li><strong>对象间课程（Inter-Object Curriculum）：</strong> 按照<strong>“近到远”</strong>的顺序序列化3D检测框。这符合以自我为中心的效用（近处物体更重要）、提供了高证据的早期词元，并利用近处物体的几何信息来约束远处物体的尺度和距离。</li>
<li><strong>对象内因子分解（Intra-Object Factorization）：</strong> 将每个3D边界框分解为<strong>中心（center）、尺寸（dimensions）和旋转（rotation）</strong>，并按此顺序进行预测。这种顺序基于线索的可观测性和学习稳定性，即先确定“在哪里”，再确定“多大”，最后确定“如何朝向”。</li>
</ul>
</li>
<li><strong>VLM原生接口：</strong> 整个过程在单一的VLM解码器中完成，无需专门的3D头部，保留了VLMs的开放词汇和视觉提示能力。用户可以通过文本指令或视觉提示（如框选、点击）来驱动3D检测。</li>
<li><strong>大规模、相机中心数据集：</strong> 论文构建了一个包含约1.74M训练样本的、相机中心的语料库，其数据格式与CoS解码顺序完全一致，为模型训练提供了高质量的监督信号。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>性能提升：</strong> 在具有挑战性的Omni3D基准测试上，LocateAnything3D取得了<strong>49.89 AP3D</strong>的<strong>state-of-the-art</strong>结果，比之前的最佳方法（即使是使用了地面真实2D框作为输入的基线）<strong>绝对提升了15.51个百分点</strong>。
*   <strong>零样本泛化能力：</strong> 模型在从未见过的类别上表现出<strong>强大的零样本泛化能力</strong>，证明了其开放词汇的优势。
*   <strong>设计验证：</strong> 消融实验表明，CoS的各个组成部分（如2D作为中间步骤、近到远排序、中心-尺寸-旋转顺序）都对性能有显著贡献。移除2D步骤或改变排序策略都会导致性能大幅下降。
*   <strong>数据效率：</strong> CoS模型在数据效率方面表现出色，即使只使用10%-40%的数据，也能达到甚至超过纯3D解码器在全部数据上的性能。
*   <strong>意义：</strong> 该工作将多目标3D检测这一关键的3D感知任务，成功地整合到了VLMs的统一框架中，<strong>弥合了开放词汇识别与度量3D理解之间的长期差距</strong>。它为通用VLMs实现3D感知提供了<strong>实用且高效的基础</strong>。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>对显式深度先验的依赖不足：</strong> 目前模型主要依赖单目RGB线索和语义上下文来推断深度，尚未直接利用显式的深度图。
*   <strong>相机内参的显式条件化不足：</strong> 模型目前隐式地依赖视觉编码器处理相机内参的变化，但未将其显式地作为提示输入。
*   <strong>单帧设置：</strong> 当前框架主要针对单张图像进行3D检测，尚未扩展到多帧或视频序列。</p>
<p><strong>5. 未来研究方向：</strong>
基于上述局限性，论文提出了以下未来研究方向：
*   <strong>集成显式深度先验：</strong> 引入深度编码器或使用深度图作为额外条件，以提高在纹理稀疏或模糊场景下的度量精度。
*   <strong>显式相机内参条件化：</strong> 将相机内参矩阵（如焦距、主点）作为位置提示进行编码，使模型能更精确地调整尺寸和深度预测。
*   <strong>扩展到多帧和视频设置：</strong> 利用自回归解码器的时序特性，扩展上下文窗口以包含前一帧的视觉信息，从而实现物体跟踪、速度估计和解决单帧中的深度歧义。
*   <strong>更广泛的应用：</strong> 将LocateAnything3D作为基础，进一步探索在视频理解、多视角推理和具身智能等领域的应用。</p>
<p>总而言之，LocateAnything3D通过将3D检测巧妙地转化为VLM的下一个词元预测任务，并引入创新的“视线链”和分层课程学习策略，显著提升了单目3D检测的性能和泛化能力，为VLMs在3D感知领域的发展奠定了坚实基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present LocateAnything3D, a VLM-native recipe that casts 3D detection as a next-token prediction problem.</li>
<li>On the challenging Omni3D benchmark, our model achieves state-of-the-art results, with 49.89 AP_3D, surpassing the previous best by +15.51 absolute improvement even when the baseline is given ground-truth 2D boxes.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.20648v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.20648v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.20645v1'></a></p>
<h2 id="pixeldit-pixel-diffusion-transformers-for-image-generation"><a href="https://arxiv.org/abs/2511.20645v1">PixelDiT: Pixel Diffusion Transformers for Image Generation</a></h2>
<p><strong>Authors:</strong> Yongsheng Yu, Wei Xiong, Weili Nie, Yichen Sheng, Shiqiu Liu, Jiebo Luo</p>
<p><strong>Published:</strong> 2025-11-25</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Latent-space modeling has been the standard for Diffusion Transformers (DiTs). However, it relies on a two-stage pipeline where the pretrained autoencoder introduces lossy reconstruction, leading to error accumulation while hindering joint optimization. To address these issues, we propose PixelDiT, a single-stage, end-to-end model that eliminates the need for the autoencoder and learns the diffusion process directly in the pixel space. PixelDiT adopts a fully transformer-based architecture shaped by a dual-level design: a patch-level DiT that captures global semantics and a pixel-level DiT that refines texture details, enabling efficient training of a pixel-space diffusion model while preserving fine details. Our analysis reveals that effective pixel-level token modeling is essential to the success of pixel diffusion. PixelDiT achieves 1.61 FID on ImageNet 256x256, surpassing existing pixel generative models by a large margin. We further extend PixelDiT to text-to-image generation and pretrain it at the 1024x1024 resolution in pixel space. It achieves 0.74 on GenEval and 83.5 on DPG-bench, approaching the best latent diffusion models.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“PixelDiT: Pixel Diffusion Transformers for Image Generation”的全面中文摘要，其中包含了您要求的各个部分：</p>
<p><strong>论文题目：</strong> PixelDiT: Pixel Diffusion Transformers for Image Generation</p>
<p><strong>作者：</strong> Yongsheng Yu, Wei Xiong, Weili Nie, Yichen Sheng, Shiqiu Liu, Jiebo Luo</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>该论文旨在解决当前主流的潜在空间扩散模型（Latent Diffusion Models, LDMs）在图像生成中存在的两个主要问题：
*   <strong>两阶段流水线依赖预训练自编码器（Autoencoder）：</strong> LDMs通常依赖一个预先训练好的自编码器将图像压缩到潜在空间进行扩散处理。这个自编码器引入了有损重建，可能导致误差累积，并且阻碍了自编码器和扩散模型之间的联合优化。
*   <strong>高频细节丢失与重建误差：</strong> 自编码器的有损重建会丢失图像的高频细节，限制了生成图像的精细度和编辑任务的保真度，即使扩散模型本身很强大。</p>
<p>因此，研究的核心问题是如何在不依赖预训练自编码器的情况下，直接在像素空间进行高效且高质量的图像生成，并解决像素建模的计算瓶颈。</p>
<p><strong>2. 主要创新点/方法贡献：</strong></p>
<p>PixelDiT 提出了一种<strong>单阶段、端到端、纯 Transformer 的像素空间扩散模型</strong>，其核心创新点在于：</p>
<ul>
<li><strong>消除自编码器依赖：</strong> 模型直接在原始像素空间进行扩散过程的学习和采样，避免了自编码器带来的重建误差和优化限制。</li>
<li><strong>双层级 Transformer 架构：</strong><ul>
<li><strong>块（Patch）级 DiT：</strong> 负责捕获全局语义信息，通过较大的块大小和长距离注意力来处理较短的 token 序列，从而高效地学习全局布局。</li>
<li><strong>像素（Pixel）级 DiT：</strong> 负责精炼纹理细节，通过密集的像素级 token 建模来处理局部细节。</li>
</ul>
</li>
<li><strong>像素级 AdaLN 调制：</strong> 引入一种像素级的自适应层归一化（AdaLN）机制，将语义 token 的全局上下文信息精确地注入到每个像素的更新中，实现更精细的纹理控制。</li>
<li><strong>像素 Token 压缩机制（Pixel Token Compaction）：</strong> 为了解决像素级注意力计算成本过高的问题，该机制将每个块内的 p² 个像素 token 压缩成一个 token，显著缩短了全局注意力的序列长度，从而在保持效率的同时实现密集的像素级建模。</li>
<li><strong>多模态 DiT 块（MM-DiT）：</strong> 在文本到图像生成任务中，通过在块级路径中引入 MM-DiT 块来融合文本信息，实现了高效的文本条件生成。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<ul>
<li><strong>图像生成质量：</strong><ul>
<li>在 ImageNet 256x256 的类条件生成任务上，PixelDiT 取得了 <strong>1.61 的 gFID</strong>，显著优于现有的像素生成模型，并且接近甚至超越了许多先进的潜在扩散模型。</li>
<li>在文本到图像生成任务上，PixelDiT-T2I 在 1024x1024 分辨率下取得了 <strong>0.74 的 GenEval 和 83.5 的 DPG-bench 分数</strong>，接近最佳的潜在扩散模型。</li>
</ul>
</li>
<li><strong>效率与可扩展性：</strong><ul>
<li>PixelDiT 在像素空间实现了高效的训练和采样，即使在 1024x1024 的高分辨率下也能进行端到端训练，这对于之前的像素模型来说是一个重大突破。</li>
<li>通过模型规模（B, L, XL）的消融研究，证明了 PixelDiT 的可扩展性，更大的模型带来了更好的图像质量。</li>
</ul>
</li>
<li><strong>细节保留与图像编辑：</strong><ul>
<li>由于消除了 VAE 的使用，PixelDiT 在图像编辑任务中能够更好地保留图像的精细细节，例如场景中的文字，避免了 VAE 重建引入的失真。</li>
</ul>
</li>
<li><strong>意义：</strong> PixelDiT 的工作表明，通过精心设计的像素建模架构，可以在像素空间实现与潜在空间模型相媲美甚至更优的生成质量和效率，为未来的像素级生成模型开辟了新的道路，并证明了像素级细节建模对于生成模型的重要性。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong></p>
<ul>
<li><strong>模型容量和训练数据限制：</strong> 在文本到图像生成任务中，PixelDiT-T2I 模型（1.3B 参数）在生成几何和纹理都非常复杂的目标时（如人手、复杂的建筑场景）有时会遇到困难。这可能是由于模型容量不足或高质量训练数据不够充分所致。</li>
<li><strong>计算成本：</strong> 尽管 PixelDiT 在像素空间实现了效率的提升，但与潜在空间模型相比，其计算成本（GFLOPS）仍然相对较高，尤其是在处理非常高分辨率的图像时。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>提升模型容量和数据质量：</strong> 针对文本到图像生成中复杂场景的生成问题，未来的工作可以尝试进一步增大模型容量，并收集更多高质量的文本-图像对数据。</li>
<li><strong>进一步优化计算效率：</strong> 探索更高效的像素建模技术或注意力机制，以进一步降低计算成本，使其在更高分辨率下更具竞争力。</li>
<li><strong>探索更广泛的应用：</strong> 将 PixelDiT 的像素空间扩散思想应用于其他计算机视觉任务，如图像修复、超分辨率、风格迁移等，并探索其在视频生成等领域的潜力。</li>
<li><strong>更精细的像素级控制：</strong> 研究如何通过更精细的条件控制机制，实现对生成图像像素级细节的更精确控制。</li>
</ul>
<p>总而言之，PixelDiT 是一项重要的研究工作，它成功地在像素空间构建了一个高效且高性能的扩散 Transformer 模型，克服了传统潜在空间模型的局限性，并在图像生成质量、细节保留和可扩展性方面取得了显著进展。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these issues, we propose PixelDiT, a single-stage, end-to-end model that eliminates the need for the autoencoder and learns the diffusion process directly in the pixel space.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.20645v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.20645v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.20644v1'></a></p>
<h2 id="vision-language-memory-for-spatial-reasoning"><a href="https://arxiv.org/abs/2511.20644v1">Vision-Language Memory for Spatial Reasoning</a></h2>
<p><strong>Authors:</strong> Zuntao Liu, Yi Du, Taimeng Fu, Shaoshu Su, Cherie Ho, Chen Wang</p>
<p><strong>Published:</strong> 2025-11-25</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Spatial reasoning is a critical capability for intelligent robots, yet current vision-language models (VLMs) still fall short of human-level performance in video-based spatial reasoning. This gap mainly stems from two challenges: a semantic-geometric misalignment that prevents consistent 3D understanding, and the absence of persistent memory to retain 3D representation and understanding over time. To address these limitations, we present VLM<script type="math/tex">^2</script>, a Vision-Language Model with persistent Memory for spatial reasoning with a view-consistent, 3D-aware representation purely from 2D video. Specifically, to enhance long-horizon reasoning, we incorporate a dual-memory module, consisting of a working memory that operates as a sliding window to focus on immediate context, and an episodic memory that consolidates and stores critical long-term information. This design enables efficient and long-horizon spatial reasoning with a fixed computational cost. Extensive experiments on multiple benchmarks show that VLM<script type="math/tex">^2</script> achieves state-of-the-art performance among video-only models, significantly advancing the frontier of visual-spatial intelligence.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Vision-Language Memory for Spatial Reasoning”的全面摘要，重点关注其核心贡献、方法、结果和意义：</p>
<p><strong>论文题目：</strong> Vision-Language Memory for Spatial Reasoning (视觉语言记忆的空间推理)</p>
<p><strong>作者：</strong> Zuntao Liu, Yi Du, Taimeng Fu, Shaoshu Su, Cherie Ho, Chen Wang</p>
<hr />
<p><strong>摘要</strong></p>
<p><strong>1. 主要问题/研究问题：</strong></p>
<p>该论文旨在解决当前视觉语言模型（VLMs）在视频理解中的空间推理能力不足的问题。尽管VLMs在理解静态图像方面取得了显著进展，但在处理视频序列时，它们在人类水平的空间推理能力方面仍存在差距。这种差距主要归因于两个关键挑战：
*   <strong>语义-几何特征不匹配 (semantic-geometric misalignment)：</strong> 导致模型难以进行一致的3D理解。2D视觉编码器提取的语义特征虽然理解力强，但缺乏精确的度量和位置感知；而3D几何模型提供的结构化信息又容易受到视角变化的影响，导致不稳定性。
*   <strong>缺乏持久性记忆 (absence of persistent memory)：</strong> 导致模型无法在长时间序列中保留3D表示和理解。在机器人导航等需要跨越长时间和多个视角的任务中，模型容易遗忘之前观察到的信息，从而在长时序推理任务中失败。</p>
<p><strong>2. 关键创新/方法贡献：</strong></p>
<p>为了解决上述挑战，论文提出了 <strong>VLM²</strong>，一个具有持久性记忆的视觉语言模型，用于空间推理。其核心创新点在于：</p>
<ul>
<li>
<p><strong>视图一致的3D感知表示 (View-Consistent 3D-Aware Representation)：</strong></p>
<ul>
<li><strong>自适应3D位置注入 (Adaptive 3D Position Injection)：</strong> 通过预测3D坐标并将其注入到视觉特征中，使视觉特征具有3D感知能力。引入自适应门控机制，选择性地整合可靠的3D位置信息，过滤噪声，从而提高鲁棒性。</li>
<li><strong>视角感知几何对齐 (Viewpoint-Aware Geometry Alignment)：</strong> 通过将几何特征与视角信息对齐，解决几何特征的视角模糊性问题，确保不同视角下的几何特征能够被正确区分。</li>
<li><strong>语义-几何融合 (Semantic-Geometric Fusion)：</strong> 利用交叉注意力机制，将位置感知视觉特征与视角感知几何特征进行融合，生成视图一致的3D感知表示，从而解决语义-几何不匹配问题，实现跨视角的一致性3D理解。</li>
</ul>
</li>
<li>
<p><strong>双记忆模块 (Dual-Memory Module)：</strong></p>
<ul>
<li><strong>工作记忆 (Working Memory)：</strong> 采用滑动窗口机制，存储最近的帧表示，用于捕捉即时上下文信息，并允许模型通过交叉注意力选择性地检索相关信息。</li>
<li><strong>情景记忆 (Episodic Memory)：</strong> 采用固定容量的存储库，用于巩固和存储关键的长期信息。通过门控融合和基于相似度的更新机制，保留有价值的观察，防止遗忘，并保持记忆库的多样性和信息量。</li>
</ul>
</li>
</ul>
<p>这种双记忆设计能够有效缓解传统滑动窗口机制的遗忘问题，同时保持计算和存储的界限，实现高效且持久的长时序空间推理。</p>
<p><strong>3. 主要结果及其意义：</strong></p>
<p>VLM² 在多个空间推理和3D理解基准测试中取得了显著的成果：
*   <strong>状态最优 (State-of-the-Art) 性能：</strong> 在VSI-Bench、VSTI-Bench、ScanQA和SQA3D等基准测试中，VLM² 均取得了优于现有视频模型和许多其他先进模型的性能。
*   <strong>显著提升长时序推理能力：</strong> 在长视频的推理任务中，VLM² 表现出更强的能力，尤其是在需要空间结构理解的任务（如Route Plan）上，性能提升尤为明显。这证明了其双记忆模块在保留长期信息方面的有效性。
*   <strong>解决核心挑战：</strong> 实验结果表明，VLM² 成功地解决了语义-几何不匹配和缺乏持久性记忆这两个关键问题，显著推动了视觉空间智能的前沿发展。</p>
<p><strong>4. 论文中提到的局限性：</strong></p>
<p>虽然论文取得了显著的成就，但仍存在一些潜在的局限性：
*   <strong>对3D基础模型的依赖：</strong> VLM² 的3D感知表示依赖于预训练的3D基础模型（如π³）。虽然论文强调其模型仅需2D视频输入，但其3D理解能力在一定程度上依赖于底层3D模型的质量和泛化能力。
*   <strong>计算成本：</strong> 尽管论文声称其设计实现了“固定计算成本”的长时序推理，但与纯2D模型相比，引入3D表示和双记忆模块可能会增加一定的计算开销，尤其是在处理非常长的视频时。
*   <strong>数据集的局限性：</strong> 尽管使用了多个基准数据集，但这些数据集可能无法完全覆盖所有真实世界的复杂场景和推理任务。</p>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<p>基于该论文的研究，可以推测以下未来研究方向：
*   <strong>更通用的3D表示：</strong> 探索不依赖于特定3D基础模型的、更通用的3D表示学习方法，以提高模型的独立性和泛化能力。
*   <strong>更高效的记忆机制：</strong> 研究更精细的记忆压缩、检索和更新策略，以进一步优化长时序推理的效率和性能，尤其是在处理超长视频时。
*   <strong>多模态融合的深化：</strong> 将VLM² 的空间推理能力与更广泛的模态（如触觉、声音）进行融合，以实现更全面的机器人感知和交互能力。
*   <strong>真实世界部署与泛化：</strong> 在更复杂的真实世界环境中部署和评估VLM²，并研究其在不同场景下的泛化能力，例如在动态变化的环境中。
*   <strong>可解释性增强：</strong> 进一步研究模型如何进行空间推理，提高其决策过程的可解释性，以便于调试和信任。</p>
<p><strong>总结：</strong></p>
<p>“Vision-Language Memory for Spatial Reasoning” 论文提出了一种名为 VLM² 的创新性模型，通过构建视图一致的3D感知表示并引入双记忆模块，有效解决了当前视觉语言模型在视频空间推理中的两大核心挑战：语义-几何不匹配和缺乏持久性记忆。该模型在多个基准测试中取得了最先进的性能，显著提升了模型在长时序空间推理任务中的表现，为智能体在复杂3D环境中进行理解和交互奠定了坚实基础。其方法论上的创新，特别是自适应3D位置注入、视角感知几何对齐以及工作记忆与情景记忆的结合，为未来视觉空间智能的研究提供了重要的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these limitations, we present VLM<script type="math/tex">^2</script>, a Vision-Language Model with persistent Memory for spatial reasoning with a view-consistent, 3D-aware representation purely from 2D video.</li>
<li>Extensive experiments on multiple benchmarks show that VLM<script type="math/tex">^2</script> achieves state-of-the-art performance among video-only models, significantly advancing the frontier of visual-spatial intelligence.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.20644v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.20644v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.20643v1'></a></p>
<h2 id="concept-aware-batch-sampling-improves-language-image-pretraining"><a href="https://arxiv.org/abs/2511.20643v1">Concept-Aware Batch Sampling Improves Language-Image Pretraining</a></h2>
<p><strong>Authors:</strong> Adhiraj Ghosh, Vishaal Udandarao, Thao Nguyen, Matteo Farina, Mehdi Cherti, Jenia Jitsev, Sewoong Oh, Elisa Ricci, Ludwig Schmidt, Matthias Bethge</p>
<p><strong>Published:</strong> 2025-11-25</p>
<p><strong>Categories:</strong> cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>What data should a vision-language model be trained on? To answer this question, many data curation efforts center on the quality of a dataset. However, most of these existing methods are (i) offline, i.e. they produce a static dataset from a set of predetermined filtering criteria, and (ii) concept-agnostic, i.e. they use model-based filters which induce additional data biases. In this work, we go beyond such offline, concept-agnostic methods and advocate for more flexible, task-adaptive online concept-based curation. Our first contribution is DataConcept, a collection of 128M web-crawled image-text pairs annotated with fine-grained details about their concept composition. Building on DataConcept, we introduce Concept-Aware Batch Sampling (CABS), a simple yet effective batch sampling framework that flexibly constructs batches on-the-fly based on specific target distributions. We propose two variants: (i) Diversity Maximization (CABS-DM) to curate batches with a broad coverage of available concepts, and (ii) Frequency Maximization (CABS-FM) to curate batches with high object multiplicity. Through extensive evaluations across 28 benchmarks, we demonstrate that our CABS method significantly benefits CLIP/SigLIP model classes and yields highly performant models. Overall, CABS represents a strong open-source alternative to proprietary online data curation algorithms, enabling practitioners to define custom concept distributions that optimize for specific downstream tasks.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Concept-Aware Batch Sampling Improves Language-Image Pretraining”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Concept-Aware Batch Sampling Improves Language-Image Pretraining (概念感知批次采样提升语言-图像预训练)</p>
<p><strong>作者：</strong> Adhiraj Ghosh, Vishaal Udandarao, Thao Nguyen, Matteo Farina, Mehdi Cherti, Jenia Jitsev, Sewoong Oh, Elisa Ricci, Ludwig Schmidt, Matthias Bethge</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
该论文的核心研究问题是：<strong>在训练视觉-语言模型（VLMs）时，应该使用什么样的数据？</strong> 现有的大多数数据策展方法都侧重于数据集的“质量”，但存在两个主要局限性：
*   <strong>离线性 (Offline):</strong> 它们生成静态数据集，一旦过滤就难以重新利用。
*   <strong>概念无关性 (Concept-agnostic):</strong> 它们依赖模型进行过滤，这可能引入额外的数据偏差，并且缺乏透明度。
此外，离线过滤会加速可用训练样本的消耗，导致“数据墙”效应。论文认为，没有通用的“质量”标准，并且不同的下游任务可能需要不同的概念分布。</p>
<p><strong>2. 主要创新点/方法论贡献：</strong>
为了解决上述问题，论文提出了一个<strong>灵活、任务自适应的在线概念感知策展框架</strong>。其主要贡献包括：</p>
<ul>
<li><strong>DataConcept 数据集：</strong> 论文构建了一个包含 1.28 亿（128M）图像-文本对的大规模数据集，并为每个样本提供了<strong>细粒度的概念元数据</strong>，包括：概念标签、置信度分数、边界框以及概念感知的合成标题。这为概念感知数据策展提供了基础。</li>
<li><strong>Concept-Aware Batch Sampling (CABS) 框架：</strong> 这是一个新颖的批次采样框架，能够<strong>动态地、在训练过程中</strong>根据特定的目标概念分布来构建批次。它不预先过滤数据，而是通过调整采样策略来控制批次的内容。<ul>
<li><strong>CABS-DM (Diversity Maximization):</strong> 旨在<strong>最大化概念多样性</strong>，确保批次中概念的覆盖范围广泛，尤其有利于零样本分类任务，特别是长尾分布下的分类。</li>
<li><strong>CABS-FM (Frequency Maximization):</strong> 旨在<strong>最大化概念频率/多重性</strong>，优先选择包含更多对象/概念的样本，以提高场景复杂度的理解，特别有利于图像-文本检索任务。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
论文通过在 28 个基准测试上的广泛实验，证明了 CABS 方法的有效性：</p>
<ul>
<li><strong>显著性能提升：</strong> CABS 方法（特别是 CABS-DM 和 CABS-FM）在 CLIP 和 SigLIP 模型上均取得了显著的性能提升，在图像分类任务上最高可达 7% 的增益，在图像-文本检索任务上最高可达 9.1% 的增益，远超 IID 采样和一些现有的离线策展方法（如 MetaCLIP）。</li>
<li><strong>任务自适应性：</strong> CABS 框架能够根据下游任务（分类 vs. 检索）的需求，灵活地调整批次的概念分布，展示了其强大的任务适应能力。</li>
<li><strong>概念感知的重要性：</strong> 实验结果强调了利用概念信息进行数据策展和批次采样对于提升模型泛化能力的重要性。</li>
<li><strong>开源替代方案：</strong> CABS 提供了一个强大的开源替代方案，使研究人员和从业者能够定义自定义的概念分布，以优化特定下游任务的性能。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>概念标注成本：</strong> CABS 的一个主要缺点是概念标注的成本较高。然而，作者认为这种成本是可以分摊的，因为标注后的数据可以用于训练不同的模型。
*   <strong>运行时开销：</strong> 随着过滤比例 f 的增加，CABS 的运行时会增加。但作者指出，即使在较低的过滤比例下，CABS 也能提供性能优势。
*   <strong>模型和训练规模：</strong> 论文中未对更复杂的模型架构或大规模训练运行进行实验。</p>
<p><strong>5. 未来研究方向：</strong>
*   <strong>微调数据应用：</strong> 将 CABS 应用于微调（fine-tuning）阶段的数据。
*   <strong>新的评分函数：</strong> 探索适用于不同任务（如检索和分类平衡）的其他评分函数。
*   <strong>课程学习（Curriculum Learning）：</strong> 研究如何通过课程学习来动态更新评分函数，例如先关注单目标图像，然后转向复杂场景。
*   <strong>更复杂的模型和更大规模的训练：</strong> 探索 CABS 在更复杂的模型架构和更大规模的训练设置下的表现。</p>
<p><strong>总结：</strong>
这篇论文提出了一种新颖的概念感知批次采样框架（CABS），并构建了 DataConcept 数据集，为语言-图像预训练提供了更灵活、任务自适应的数据策展方法。通过 CABS-DM（多样性最大化）和 CABS-FM（频率最大化）两个变体，论文有效地解决了传统离线、概念无关策展方法的局限性，并在多项基准测试中取得了显著的性能提升。这项工作为构建更强大、更具泛化能力的视觉-语言模型提供了重要的理论和实践指导，并为未来的研究开辟了新的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Our first contribution is DataConcept, a collection of 128M web-crawled image-text pairs annotated with fine-grained details about their concept composition.</li>
<li>Building on DataConcept, we introduce Concept-Aware Batch Sampling (CABS), a simple yet effective batch sampling framework that flexibly constructs batches on-the-fly based on specific target distributions.</li>
<li>We propose two variants: (i) Diversity Maximization (CABS-DM) to curate batches with a broad coverage of available concepts, and (ii) Frequency Maximization (CABS-FM) to curate batches with high object multiplicity.</li>
<li>Through extensive evaluations across 28 benchmarks, we demonstrate that our CABS method significantly benefits CLIP/SigLIP model classes and yields highly performant models.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.20643v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.20643v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.20641v1'></a></p>
<h2 id="unleashing-the-power-of-vision-language-models-for-long-tailed-multi-label-visual-recognition"><a href="https://arxiv.org/abs/2511.20641v1">Unleashing the Power of Vision-Language Models for Long-Tailed Multi-Label Visual Recognition</a></h2>
<p><strong>Authors:</strong> Wei Tang, Zuo-Zheng Wang, Kun Zhang, Tong Wei, Min-Ling Zhang</p>
<p><strong>Published:</strong> 2025-11-25</p>
<p><strong>Categories:</strong> cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Long-tailed multi-label visual recognition poses a significant challenge, as images typically contain multiple labels with highly imbalanced class distributions, leading to biased models that favor head classes while underperforming on tail classes. Recent efforts have leveraged pre-trained vision-language models, such as CLIP, alongside long-tailed learning techniques to exploit rich visual-textual priors for improved performance. However, existing methods often derive semantic inter-class relationships directly from imbalanced datasets, resulting in unreliable correlations for tail classes due to data scarcity. Moreover, CLIP's zero-shot paradigm is optimized for single-label image-text matching, making it suboptimal for multi-label tasks. To address these issues, we propose the correlation adaptation prompt network (CAPNET), a novel end-to-end framework that explicitly models label correlations from CLIP's textual encoder. The framework incorporates a graph convolutional network for label-aware propagation and learnable soft prompts for refined embeddings. It utilizes a distribution-balanced Focal loss with class-aware re-weighting for optimized training under imbalance. Moreover, it improves generalization through test-time ensembling and realigns visual-textual modalities using parameter-efficient fine-tuning to avert overfitting on tail classes without compromising head class performance. Extensive experiments and ablation studies on benchmarks including VOC-LT, COCO-LT, and NUS-WIDE demonstrate that CAPNET achieves substantial improvements over state-of-the-art methods, validating its effectiveness for real-world long-tailed multi-label visual recognition.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Unleashing the Power of Vision-Language Models for Long-Tailed Multi-Label Visual Recognition”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Unleashing the Power of Vision-Language Models for Long-Tailed Multi-Label Visual Recognition (释放视觉语言模型在长尾多标签视觉识别中的强大能力)</p>
<p><strong>作者：</strong> Wei Tang, Zuo-Zheng Wang, Kun Zhang, Tong Wei, and Min-Ling Zhang</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
该论文主要解决了<strong>长尾多标签视觉识别（Long-Tailed Multi-Label Visual Recognition, LTML）</strong>这一核心挑战。在现实世界的图像中，通常包含多个对象和概念，导致标签分布高度不平衡，即“头部”类别（head classes）样本丰富，而“尾部”类别（tail classes）样本稀少。这种长尾分布使得模型倾向于偏向头部类别，而对尾部类别表现不佳。现有方法虽然尝试利用预训练的视觉语言模型（如CLIP）和长尾学习技术，但仍存在不足：
*   <strong>标签相关性建模不准确：</strong> 现有方法通常直接从不平衡的数据集中提取类别间的语义关系，导致尾部类别的相关性估计不可靠。
*   <strong>CLIP零样本范式不适用于多标签任务：</strong> CLIP的零样本能力主要针对单标签匹配进行优化，在多标签场景下效果不佳，容易偏向最显著的类别而忽略其他共现标签。</p>
<p><strong>2. 主要创新点/方法贡献：</strong>
为了克服上述挑战，作者提出了一个名为<strong>CAPNET（Correlation Adaptation Prompt Network）</strong>的新型端到端框架，其核心创新点包括：</p>
<ul>
<li><strong>显式建模标签相关性：</strong> CAPNET利用CLIP的文本编码器提取类别提示（prompts）的语义信息，并通过<strong>图卷积网络（Graph Convolutional Network, GCN）</strong>来显式地建模标签间的相关性。这避免了直接从不平衡数据集中提取相关性，从而获得更鲁棒的标签相关性估计。</li>
<li><strong>可学习的软提示（Learnable Soft Prompts）：</strong> 引入可学习的软提示来优化CLIP的文本嵌入，以适应多标签任务的需求。</li>
<li><strong>分布平衡的Focal Loss：</strong> 采用<strong>分布平衡的Focal Loss（Distribution-Balanced Focal Loss）</strong>，并结合<strong>类别感知重加权（class-aware re-weighting）</strong>，以优化在类别不平衡情况下的训练。</li>
<li><strong>测试时集成（Test-Time Ensembling, TTE）：</strong> 提出一种测试时集成策略，通过聚合多个扰动图像的预测来提高模型的泛化能力，尤其是在尾部类别上。</li>
<li><strong>参数高效微调（Parameter-Efficient Fine-Tuning, PEFT）：</strong> 利用<strong>AdaptFormer</strong>等PEFT技术，对模型进行参数高效的微调，以避免在尾部类别上发生过拟合，同时不损害头部类别的性能。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
通过在VOC-LT、COCO-LT和NUS-WIDE等长尾多标签数据集上的大量实验，CAPNET取得了显著的性能提升，达到了<strong>新的SOTA（State-of-the-Art）水平</strong>。
*   <strong>整体性能提升：</strong> CAPNET在VOC-LT和COCO-LT数据集上均显著优于现有方法，尤其是在尾部类别上表现出色。例如，在VOC-LT上，CAPNET的总mAP达到87.46%，优于之前的最佳方法MLC-NC 3.09%。
*   <strong>类别均衡性：</strong> 该方法在头部、中等和尾部类别上都实现了均衡的提升，有效缓解了长尾分布带来的偏见。
*   <strong>鲁棒性与泛化性：</strong> TTE和PEFT的引入进一步增强了模型的泛化能力和鲁棒性，尤其是在数据稀疏的尾部类别上。
*   <strong>意义：</strong> CAPNET的成功表明，通过显式建模标签相关性、优化损失函数以及采用高效的微调策略，可以有效地释放预训练视觉语言模型在长尾多标签视觉识别任务中的潜力，为解决现实世界中的复杂视觉识别问题提供了有力工具。</p>
<p><strong>4. 提及的局限性：</strong>
论文中并未明确列出具体的局限性，但从其方法和实验设计中可以推断出一些潜在的方面：
*   <strong>计算成本：</strong> 虽然引入了PEFT来降低微调成本，但整个框架（包括GCN和TTE）仍然可能比简单的CLIP零样本方法需要更多的计算资源。
*   <strong>对CLIP的依赖：</strong> 该方法高度依赖于CLIP的预训练能力，其性能上限可能受到CLIP本身能力的影响。
*   <strong>超参数敏感性：</strong> 实验中对超参数（如GCN中的s和损失函数中的τ'）进行了分析，表明其对模型性能有一定影响，需要仔细调优。</p>
<p><strong>5. 未来研究方向：</strong>
论文在<strong>结论</strong>部分提出了未来研究的几个方向：
*   <strong>开放词汇设置（Open-Vocabulary Settings）：</strong> 将CAPNET扩展到开放词汇设置，使其能够识别训练时未见过的类别。
*   <strong>新兴多模态架构集成：</strong> 将CAPNET与新兴的多模态架构相结合，以进一步增强其在动态环境中的可扩展性和适应性。
*   <strong>更高效的标签相关性建模：</strong> 探索更高效、更具表达力的标签相关性建模方法。
*   <strong>更精细的参数高效微调策略：</strong> 研究更精细的PEFT策略，以在保持模型性能的同时进一步降低计算开销。</p>
<p>总而言之，这篇论文提出了一种创新的CAPNET框架，通过结合CLIP的强大视觉语言能力、图卷积网络进行标签相关性建模、分布平衡的损失函数以及高效的微调策略，显著提升了长尾多标签视觉识别的性能，为该领域的研究做出了重要贡献。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these issues, we propose the correlation adaptation prompt network (CAPNET), a novel end-to-end framework that explicitly models label correlations from CLIP's textual encoder.</li>
<li>Extensive experiments and ablation studies on benchmarks including VOC-LT, COCO-LT, and NUS-WIDE demonstrate that CAPNET achieves substantial improvements over state-of-the-art methods, validating its effectiveness for real-world long-tailed multi-label visual recognition.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.20641v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.20641v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.20640v1'></a></p>
<h2 id="motionv2v-editing-motion-in-a-video"><a href="https://arxiv.org/abs/2511.20640v1">MotionV2V: Editing Motion in a Video</a></h2>
<p><strong>Authors:</strong> Ryan Burgert, Charles Herrmann, Forrester Cole, Michael S Ryoo, Neal Wadhwa, Andrey Voynov, Nataniel Ruiz</p>
<p><strong>Published:</strong> 2025-11-25</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.GR, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>While generative video models have achieved remarkable fidelity and consistency, applying these capabilities to video editing remains a complex challenge. Recent research has explored motion controllability as a means to enhance text-to-video generation or image animation; however, we identify precise motion control as a promising yet under-explored paradigm for editing existing videos. In this work, we propose modifying video motion by directly editing sparse trajectories extracted from the input. We term the deviation between input and output trajectories a "motion edit" and demonstrate that this representation, when coupled with a generative backbone, enables powerful video editing capabilities. To achieve this, we introduce a pipeline for generating "motion counterfactuals", video pairs that share identical content but distinct motion, and we fine-tune a motion-conditioned video diffusion architecture on this dataset. Our approach allows for edits that start at any timestamp and propagate naturally. In a four-way head-to-head user study, our model achieves over 65 percent preference against prior work. Please see our project page: https://ryanndagreat.github.io/MotionV2V</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文分析：MotionV2V: Editing Motion in a Video</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话）</strong></p>
<p>该论文提出了一种新颖的视频编辑方法，名为 MotionV2V，其核心在于通过直接编辑视频中提取的稀疏运动轨迹来实现对视频运动的精确控制。通过引入“运动反事实”（motion counterfactuals）的概念，即内容相同但运动不同的视频对，并利用这些数据对运动条件化的视频扩散模型进行微调，该方法能够实现从任意时间点开始、自然传播的视频运动编辑。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<ul>
<li><strong>运动轨迹编辑作为核心:</strong> 论文的关键创新在于将视频编辑的焦点从像素级别的修改转移到更抽象的运动轨迹层面。通过提取视频中的稀疏运动轨迹，并直接对这些轨迹进行编辑（即“运动编辑”），研究人员提供了一种更直观、更可控的编辑方式。</li>
<li><strong>“运动反事实”数据集的生成:</strong> 为了训练模型，论文引入了生成“运动反事实”视频对的方法。这种方法能够生成大量内容一致但运动模式不同的视频样本，为学习运动编辑提供了高质量的训练数据。</li>
<li><strong>运动条件化的视频扩散模型:</strong> 论文将生成的“运动反事实”数据用于微调一个运动条件化的视频扩散模型。这意味着模型能够理解并生成与特定运动轨迹相匹配的视频内容，从而实现精确的运动编辑。</li>
<li><strong>任意时间点编辑与自然传播:</strong> 该方法的一个重要优势是能够从视频的任何时间点开始进行运动编辑，并且编辑效果能够自然地传播到后续帧，避免了局部编辑带来的不连贯性。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<ul>
<li><strong>推动视频编辑的精细化和可控性:</strong> MotionV2V 的方法有望将视频编辑推向一个更精细、更可控的新阶段。用户不再需要依赖复杂的后期制作软件或模糊的文本描述来修改视频中的运动，而是可以直接操纵运动轨迹。</li>
<li><strong>为视频生成和动画领域提供新思路:</strong> 尽管论文聚焦于视频编辑，但其提出的运动轨迹编辑和“运动反事实”生成方法也可能为文本到视频生成、图像动画等领域带来新的灵感，尤其是在提高生成内容的运动真实性和可控性方面。</li>
<li><strong>降低视频编辑的门槛:</strong> 通过提供更直观的编辑方式，该技术有可能降低专业视频编辑的门槛，使更多非专业用户能够轻松地对视频进行个性化修改。</li>
<li><strong>促进人机交互在视频编辑中的应用:</strong> 运动轨迹的直接编辑是一种高度人机交互的方式，预示着未来视频编辑工具将更加注重用户的直观操作和反馈。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>电影和视频制作:</strong> 电影特效、动画制作、后期剪辑等领域可以利用该技术快速修改和增强视频中的运动，例如调整角色的动作幅度、改变物体的运动轨迹等。</li>
<li><strong>虚拟现实 (VR) 和增强现实 (AR):</strong> 在创建沉浸式体验时，能够精确控制虚拟对象的运动对于提升真实感至关重要。MotionV2V 的技术可以用于动态调整 VR/AR 环境中的物体运动。</li>
<li><strong>游戏开发:</strong> 游戏中的角色动画和场景动态可以通过该技术进行更灵活的编辑和调整，提高开发效率和游戏体验。</li>
<li><strong>体育分析和回放:</strong> 能够精确编辑和重放特定运动轨迹，有助于体育分析师更深入地理解运动员的动作，或为观众提供更具吸引力的回放。</li>
<li><strong>教育和培训:</strong> 创建具有特定运动演示的教学视频，例如演示物理实验、运动技巧等，可以变得更加容易和精确。</li>
<li><strong>社交媒体内容创作:</strong> 用户可以轻松地为自己的视频添加有趣的运动效果，创作更具吸引力的社交媒体内容。</li>
</ul>
<p><strong>5. 可从摘要推断的局限性</strong></p>
<ul>
<li><strong>对稀疏轨迹提取的依赖:</strong> 该方法依赖于从输入视频中成功提取准确的稀疏运动轨迹。如果视频内容模糊、运动复杂或缺乏清晰的特征点，轨迹提取的准确性可能会受到影响，进而影响编辑效果。</li>
<li><strong>“运动反事实”生成的可控性和多样性:</strong> 虽然论文提到了生成“运动反事实”，但其生成过程的精细控制能力和生成数据的多样性（例如，是否能覆盖所有可能的运动变化类型）可能是一个挑战。</li>
<li><strong>计算成本:</strong> 视频扩散模型通常计算成本较高，尤其是在生成高分辨率和长时序的视频时。MotionV2V 的训练和推理过程可能需要大量的计算资源。</li>
<li><strong>内容一致性与运动编辑的权衡:</strong> 在进行运动编辑时，如何保证视频内容的视觉一致性（例如，物体变形、纹理变化等）与运动的自然性之间取得平衡，可能是一个需要进一步研究的问题。虽然摘要提到“分享identical content”，但实际操作中，运动的剧烈改变可能会对内容视觉一致性带来挑战。</li>
<li><strong>对特定类型运动的适应性:</strong> 摘要并未明确说明该方法对所有类型的运动（例如，刚体运动、形变运动、流体运动等）的适应性如何。某些复杂或非结构化的运动可能更难通过稀疏轨迹进行有效编辑。</li>
<li><strong>用户研究的局限性:</strong> 虽然用户研究结果显示了优势，但“四向头对头用户研究”的具体设计、参与者数量和多样性等信息并未在摘要中提供，因此其普适性需要进一步验证。</li>
</ul>
<p>总而言之，MotionV2V 是一项非常有前景的研究，它通过将视频编辑的焦点转移到运动轨迹的直接操纵，为视频编辑领域带来了新的视角和强大的能力。其核心创新在于“运动轨迹编辑”和“运动反事实”数据集的生成，有望在多个领域产生深远影响。然而，其对轨迹提取的依赖、计算成本以及内容一致性与运动编辑的权衡等问题，也为未来的研究留下了探索的空间。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we propose modifying video motion by directly editing sparse trajectories extracted from the input.</li>
<li>To achieve this, we introduce a pipeline for generating "motion counterfactuals", video pairs that share identical content but distinct motion, and we fine-tune a motion-conditioned video diffusion architecture on this dataset.</li>
<li>Our approach allows for edits that start at any timestamp and propagate naturally.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.20640v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.20640v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.20635v1'></a></p>
<h2 id="imontage-unified-versatile-highly-dynamic-many-to-many-image-generation"><a href="https://arxiv.org/abs/2511.20635v1">iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation</a></h2>
<p><strong>Authors:</strong> Zhoujie Fu, Xianfang Zeng, Jinghong Lan, Xinyao Liao, Cheng Chen, Junyi Chen, Jiacheng Wei, Wei Cheng, Shiyu Liu, Yunuo Chen, Gang Yu, Guosheng Lin</p>
<p><strong>Published:</strong> 2025-11-25</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Pre-trained video models learn powerful priors for generating high-quality, temporally coherent content. While these models excel at temporal coherence, their dynamics are often constrained by the continuous nature of their training data. We hypothesize that by injecting the rich and unconstrained content diversity from image data into this coherent temporal framework, we can generate image sets that feature both natural transitions and a far more expansive dynamic range. To this end, we introduce iMontage, a unified framework designed to repurpose a powerful video model into an all-in-one image generator. The framework consumes and produces variable-length image sets, unifying a wide array of image generation and editing tasks. To achieve this, we propose an elegant and minimally invasive adaptation strategy, complemented by a tailored data curation process and training paradigm. This approach allows the model to acquire broad image manipulation capabilities without corrupting its invaluable original motion priors. iMontage excels across several mainstream many-in-many-out tasks, not only maintaining strong cross-image contextual consistency but also generating scenes with extraordinary dynamics that surpass conventional scopes. Find our homepage at: https://kr1sjfu.github.io/iMontage-web/.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话）：</strong></p>
<p>iMontage 提出了一种新颖的统一框架，旨在将强大的预训练视频模型的能力扩展到图像生成和编辑领域。该框架能够处理可变数量的输入和输出图像集，从而统一多种图像生成与编辑任务，并生成具有自然过渡和更广阔动态范围的图像集。其核心贡献在于，在不损害视频模型原有运动先验的前提下，赋予其强大的图像操纵能力。</p>
<p><strong>2. 关键创新或方法论：</strong></p>
<ul>
<li><strong>统一框架（Unified Framework）：</strong> iMontage 的核心创新在于其“all-in-one”的设计理念，将多种原本分散的图像生成和编辑任务整合到一个统一的框架下。这通过处理“many-to-many”的图像集（即输入和输出都可以是多个图像）来实现。</li>
<li><strong>“侵入性最小”的适应策略（Minimally Invasive Adaptation Strategy）：</strong> 这是该方法论的关键。论文提出了一种巧妙且对原有视频模型改动极小的策略，使得预训练视频模型能够学习图像数据的多样性和操纵能力，同时保留其强大的时序连贯性先验。这意味着模型不会因为学习图像任务而“遗忘”或“破坏”其原有的视频生成能力。</li>
<li><strong>定制化数据策展与训练范式（Tailored Data Curation and Training Paradigm）：</strong> 为了实现上述目标，iMontage 采用了专门的数据处理和训练方法。这表明研究人员精心设计了训练数据，以引导模型在图像域中学习所需的特征，并设计了相应的训练过程来优化这种跨模态（视频到图像）的学习。</li>
<li><strong>利用视频模型的时序先验（Leveraging Temporal Priors from Video Models）：</strong> 论文明确指出，预训练视频模型具备强大的时序连贯性先验。iMontage 的目标是将这种优势迁移到图像生成中，从而实现“自然过渡”和“更广阔的动态范围”。</li>
</ul>
<p><strong>3. 对该领域的潜在影响：</strong></p>
<ul>
<li><strong>提升图像生成与编辑的灵活性和多样性：</strong> iMontage 有潜力极大地扩展图像生成和编辑任务的范围和能力。通过统一多种任务，研究人员可以更便捷地实现复杂的图像序列生成、风格迁移、内容编辑等，并生成更具动态感和视觉冲击力的结果。</li>
<li><strong>降低模型开发成本和复杂性：</strong> 统一框架意味着开发者无需针对每个任务训练独立的模型，从而节省了计算资源和开发时间。这使得研究和应用门槛降低。</li>
<li><strong>推动跨模态学习和模型复用：</strong> 该研究展示了如何有效地复用强大的预训练视频模型来解决图像领域的挑战，为跨模态学习和模型复用提供了新的思路和范例。</li>
<li><strong>为内容创作和虚拟现实等领域提供更强大的工具：</strong> 能够生成具有高度动态和自然过渡的图像集，将极大地促进电影制作、游戏开发、虚拟现实内容创建等领域的发展。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用：</strong></p>
<ul>
<li><strong>内容创作与媒体制作：</strong> 自动生成电影片段、广告素材、社交媒体内容等，实现更高效、更具创意的视觉内容生产。</li>
<li><strong>虚拟现实（VR）与增强现实（AR）：</strong> 生成逼真且动态的虚拟环境和交互式场景，提升用户体验。</li>
<li><strong>游戏开发：</strong> 快速生成游戏中的场景、角色动画、过场动画等，加速开发流程。</li>
<li><strong>数字艺术与设计：</strong> 为艺术家和设计师提供更强大的创作工具，探索新的视觉表达形式。</li>
<li><strong>图像编辑与修复：</strong> 实现更智能、更自然的图像编辑操作，如风格转换、对象替换、动态效果添加等。</li>
<li><strong>数据增强：</strong> 为训练其他模型生成多样化、高质量的图像数据。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性：</strong></p>
<ul>
<li><strong>对预训练视频模型的依赖性：</strong> iMontage 的成功很大程度上依赖于基础预训练视频模型的质量和能力。如果基础模型本身存在缺陷，可能会限制 iMontage 的性能。</li>
<li><strong>“侵入性最小”的界限：</strong> 虽然论文强调“侵入性最小”，但任何对模型结构的调整或训练策略的改变都可能对模型原有的某些特性产生细微影响。摘要中提到“without corrupting its invaluable original motion priors”，但实际效果仍需通过实验验证。</li>
<li><strong>计算资源需求：</strong> 尽管框架统一，但处理“many-to-many”的图像集，尤其是在生成高分辨率、高动态范围的内容时，可能仍然需要大量的计算资源进行训练和推理。</li>
<li><strong>数据策展的挑战：</strong> “Tailored data curation process”表明数据准备是关键。如何有效地策展和标注数据以覆盖广泛的图像操纵能力，可能是一个复杂且耗时的工作。</li>
<li><strong>“ extraordinary dynamics”的量化和评估：</strong> 摘要中提到了“extraordinary dynamics that surpass conventional scopes”。如何客观、量化地评估这种“非凡的动态性”以及其与现有方法的对比，是论文需要详细阐述的部分。</li>
<li><strong>潜在的“模式崩溃”或不稳定性：</strong> 在尝试统一如此广泛的任务时，模型可能会面临在某些特定任务上表现不佳，或者在不同任务之间切换时出现不稳定的情况。</li>
</ul>
<p>总而言之，iMontage 是一篇非常有前景的研究，它巧妙地利用了视频模型的强大能力来解决图像生成和编辑的挑战，并提出了一个统一、灵活且强大的框架。其核心在于如何以一种“侵入性最小”的方式实现这种跨模态的迁移学习，这对于未来多模态模型的发展具有重要的启示意义。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To this end, we introduce iMontage, a unified framework designed to repurpose a powerful video model into an all-in-one image generator.</li>
<li>To achieve this, we propose an elegant and minimally invasive adaptation strategy, complemented by a tailored data curation process and training paradigm.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.20635v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.20635v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-11-26 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
